# [Information, relevance, independence]{.green}
{{< include macros.qmd >}}
{{< include macros_info.qmd >}}
{{< include macros_marg_cond.qmd >}}

## Independence of sentences {#sec-indep-sentences}

In an ordinary situation represented by background information $\yI$, if you have to infer whether a coin will land heads, then knowing that it is raining outside has no impact on your inference: the information about rain is [**irrelevant**]{.blue} for your inference. In other words, your degree of belief about the coin remains the same if you include the information about rain in the conditional.

In probability notation, representing "[The coin lands heads]{.midgrey}" with $\se{H}$ and "[It rains outside]{.midgrey}" with $\se{R}$, this irrelevance means

$$
\P(\se{H} \| \yI) = \P(\se{H} \| \se{R} \and \yI)
$$

\

More generally two *sentences* $\yA$, $\yB$ are said to be [**mutually irrelevant** or **logically independent given information $\yI$**]{.blue} if any one of these three conditions holds:

:::{.column-margin}
"[independ**En**t](https://dictionary.cambridge.org/dictionary/english/independent)" is written with an **E**, not with an **A**.
:::

- $\P(\yA \|\yB \and \yI) = \P(\yA \| \yI)$

- $\P(\yB \| \yA \and \yI) = \P(\yB \| \yI)$

- $\P(\yA \and \yB \| \yI) = \P(\yA \| \yI) \cdot \P(\yB \| \yI)$

These three conditions are equivalent to one another. In the first condition, $\P(\yA \|\yB \and \yI)$ is undefined if $\P(\yB\|\yI)=0$, but in this case independence still holds; analogously in the second condition.

:::{.callout-warning}
## {{< fa exclamation-circle >}}
- Irrelevance or independence is not an absolute notion, but **relative to some background knowledge**. Two sentences may be independent given some background information, and **not** independent given another.

- Independence as defined above is a **logical**, not **physical**, notion. It isn't stating anything about physical dependence between phenomena related to the sentences $\yA$ and $\yB$. It's simply stating that information about one does not affect an agent's beliefs about the other.
:::

## Independence of quantities {#sec-indep-quantities}

The notion of irrelevance of two sentences can be generalized to quantities. Take two quantities $X$ and $Y$. They are said to be [**mutually irrelevant** or **logically independent given information $\yI$**]{.blue} if any one of these three condition holds [*for all possible values $x$ of $X$ and $y$ of $Y$*]{.blue}:

- $\P(X\mo x \|Y\mo y \and \yI) = \P(X\mo x \| \yI)$\ \ \ [all $x,y$]{.midgrey}

- $\P(Y\mo y \| X\mo x \and \yI) = \P(Y\mo y \| \yI)$\ \ \ [all $x,y$]{.midgrey}

- $\P(X\mo x \and Y\mo y \| \yI) = \P(X\mo x \| \yI) \cdot \P(Y\mo y \| \yI)$\ \ \ [all $x,y$]{.midgrey}

\

Note the difference between independence of *two sentences* and independence of *two quantities*. The latter independence involves not just two, but many sentences: as many as the values of $X$ and $Y$.

In fact it may happen that for some particular values $x^*$ of $X$ and $y^*$ $Y$ the probabilities become independent, say:


$$
\P(X\mo x^* \| Y\mo y^* \and \yI) = \P(X\mo x^* \|\yI)
$$

but this equality does *not* occur for other values. In this case the quantities $X$ and $Y$ are *not* independent given information $\yI$. The general idea is that two quantities are independent if knowledge about one of them cannot change an agent's beliefs about the other, *no matter what their values might be*.

:::{.callout-warning}
## {{< fa exclamation-circle >}}
- Also in this case, irrelevance or independence is not an absolute notion, but **relative to some background knowledge**. Two quantities may be independent given some background information, and **not** independent given another.

- Also in this case, independence is a **logical**, not **physical**, notion. It isn't stating anything about physical dependence between phenomena related to the quantities $X$ and $Y$. It's simply stating that information about one does not affect an agent's beliefs about the other.
:::


:::{.callout-caution}
## {{< fa user-edit >}} Exercise
Consider our familiar next-patient inference problem with quantities urgency $U$ and transportation $T$. Assume a different background information $\yJ$ that leads to the following joint probability distribution:

+:------------------------------:+:---------------:+:---------------:+:---------------:+:---------------:+
|$\P(U\mo u \and T\mo t\|\yJ)$   |                 |**transportation at arrival** $T$                    |
+--------------------------------+-----------------+-----------------+-----------------+-----------------+
|                                |                 |ambulance        |helicopter       |other            |
+--------------------------------+-----------------+-----------------+-----------------+-----------------+
|**urgency** $U$                 |urgent           |0.15             |0.08             |0.02             |
+                                +-----------------+-----------------+-----------------+-----------------+
|                                |non-urgent       |0.45             |0.04             |0.26             |
+--------------------------------+-----------------+-----------------+-----------------+-----------------+
: {.sm}

- Calculate the marginal probability distribution $\P(U\|\yJ)$ and the conditional probability distribution $\P(U \| T\mo\ambu \and\yJ)$, and compare them. Is the value $T\mo\ambu$ relevant for inferences about $U$?

- Calculate the conditional probability distribution $\P(U \| T\mo\heli \and\yJ)$, and compare it with the marginal $\P(U\|\yJ)$. Is the value $T\mo\heli$ relevant for inferences about $U$?

- Are the quantities $U$ and $T$ independent, given the background knowledge $\yJ$?
:::



## Information and uncertainty {#sec-info-uncertainty}

The definition of irrelevance given above appears to be very "black or white": either two sentences or quantities are independent, or they aren't. But in reality there is no such dichotomy. We can envisage some scenario $\yI$ where for instance the probabilities $\P(Y\mo y \| X\mo x \and \yI)$ and $\P(Y\mo y \| \yI)$ are extremely close in value, although not exactly equal:

$$
\P(Y\mo y \| X\mo x \and \yI)
= \P(Y\mo y \| \yI) + \delta(x,y)
$$

with $\delta(x,y)$ very small. This would mean that knowledge about $X$ modifies an agent's belief just a little; and depending on the situation such modification could be unimportant. In this situation the two quantities would be "independent" for all practical purposes. Therefore there are *degrees of relevance* rather than a dichotomy "relevant vs irrelevant".

This suggests that we try to quantify such degrees. This quantification would also give a measure of how "important" a quantity can be for inferences about another quantity.

This is the domain of [**Information Theory**]{.blue}, which would require a course by itself to be properly explored. In this chapter we shall just get an overview of the main ideas and notions of this theory.

::::{.column-margin}
::: {.callout-tip}
## {{< fa rocket >}} For the extra curious
- [*Information Theory, Inference, and Learning Algorithms*](https://hvl.instructure.com/courses/25074/modules/items/660092)

- [*Elements of Information Theory*](https://hvl.instructure.com/courses/25074/modules/items/703785)
:::
::::

\

The notion of degree of relevance is important in data science and machine learning, because it rigorously quantifies two related, intuitive ideas often occurring in these fields:

- ["Correlation": in its general meaning, it's the idea that if an agent's knowledge about some quantity changes, then knowledge about another quantity may change as well.]{.green}

- ["Feature importance": it's the idea that knowledge about some aspects of a given problem may lead to improved inferences about other aspects.]{.yellow}

