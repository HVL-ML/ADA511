{{< include macros.qmd >}}
{{< include macros_exchangeability.qmd >}}

# A general inference problem {#sec-general-inference}

So far we have been jumping back and forth between "inference" and "data", keeping the two topics somewhat separated. Now we shall finally combine them, to solve inference problems typically encountered in machine learning. Before continuing, let us summarize what we have learned so far, also to motivate why those two topics have been kept separated.

## Recap {#sec-recap}

On the "inference side" we gave a concrete and operational explanation of what "drawing an inference" means: it is the calculation of the probabilities -- the degrees of belief -- of some sentences, from the probabilities of others. This means that in principle we can draw inferences and apply the probability calculus to *literally anything* that can be expressed by language.

We also saw the following points:

- The calculation of probabilities only uses four fundamental -- and mathematically quite simple -- rules ([§ @sec-fundamental]). Any inference, even those make by the most complex machine-learning algorithms, is just a repeated application of those four rules (sometimes with approximating shortcuts for the sake of speed).

- No inference can be drawn unless some probabilities are first posited as a starting point. This is just another face of the formal-logic fact that no theorem (besides tautologies) can be derived by logic, unless some axioms are given first.

- The four fundamental rules are determined by basic requirements of logical consistency. Modifying the rules would lead to inconsistent inferences and sub-optimal decisions.

\

<!-- :::{.column-margin} -->
<!-- ```{mermaid} -->
<!-- flowchart BT -->
<!--   A[data notions] -\-> B([sentences]) -.-> C{{probability calculus}} -->
<!-- ``` -->
<!-- ::: -->


:::{.column-margin}
```{mermaid}
flowchart BT
  A[quantity] --o B([sentences]) --> C{{probability calculus}}
  D[value] ---o B
  E[population] --o B
  F[...] -..-o B
  G[(problem)] --x A & D & E
  G[(problem)] -.-x F
```
:::

On the "data side" we introduced a handful of notions, such as "quantity", "having a value", "domain", "quantity type", "population", "variate", "frequency", and others. We learned how to use them, and paid attention to some of their counter-intuitive properties.

These notions allow us to *speak*, in a precise way, about typical situations and problems that arise in engineering and other scientific contexts. In essence we have introduced a particular kind of *sentences* to express engineering problems -- *so that we can apply the probability calculus to them* and draw inferences about them.

This specific language has its roots in physics and mathematics, where it has been successfully used and refined for several centuries. But it might evolve in different directions in the future, to make allowance for new inferential problems. Later we shall indeed expand it in a couple of directions to cover particular operations that we do with data.
