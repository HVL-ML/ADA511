# Machine learning - an introduction
{{< include macros.qmd >}}

So far we have learned about concepts of doing inference, and about various properties of data. Now it is time to combine the two, and create a mathematical *model* that can use data from the past to make predictions about the unknown. Machine learning methods can be wildly different both in terms of operation and complexity, but they all behave as functions that take in a data point $\mathbf{x}$, a set of values $\theta$ which we call *parameters*, and returns the prediction $y$:

$$ 
    y = f(\mathbf{x}, \theta).
$$

The prediction $y$ can be any quantity discussed in [ยง @sec-basic-types] -- most commonly we want to classify the data (and we say we are doing *classification*), or we want to estimate some value (in which case we call it *regression*).

As engineers, when faced with the task of using data to model the behaviour of some system, our job is twofold. First, we need to select a suitable method to serve as the function $f$, and second, we need to find the optimal values for the parameters $\theta$. In [ADA501](https://www.hvl.no/en/studies-at-hvl/study-programmes/courses/2023/ADA501) we will see how to choose an analytical $f$ that corresponds to certain physical systems, while in our course, we will look at methods where $f$ can be practically anything. In fact, we will not even require $f$ to be a deterministic function -- consider for instance generative models like ChatGPT or DALL-E, which creates different outputs for the same input, each time it is run. 

Finding the parameters $\theta$ is what takes us from a general method, to the specific solution to a problem. As we will see, the way of finding them differs between the various machine learning methods, but the principle is always the same: We need to choose a *loss function*, and then iteratively adjust $\theta$ so that the loss, when computed on known data, becomes as small as possible. The loss function should represent the difference between what the model outputs, and the correct answer -- the better the model, the smaller the difference. The choice of this loss function depends on what kind of problem we wish to solve, and we will look at the common ones shortly. But at this point we can already define the three main types of machine learning:

* If we know the "correct answers" (the labels) for all entries in our data set, this is what the model should aim to learn, and we call it **supervised learning**. 
* If we do not have the data labels, we can still write a loss function corresponding to what we want the model to do with the data. Most commonly, we want to find groups, or *clusters*, of similar-looking data points. If we provide a way of computing scores for good clusters, the model is left to find cluster labels by it self, and we call it **unsupervised learning**.
* If the decision of a model on one data point affects the value of future data points, like for a self-driving car that may turn left and crash, or turn right and follow the road, we need a loss function that penalises certain actions and rewards others. This is called **reinforcement learning**.

