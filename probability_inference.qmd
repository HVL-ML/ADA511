# Probability inference
{{< include macros.qmd >}}
{{< include macros_prob_inference.qmd >}}

In most engineering and data-science problems we don't know the truth or falsity of outcomes and hypotheses that interest us. But this doesn't mean that nothing can be said or done in such situations. Now we shall finally see how to draw *uncertain* inferences, that is, how to calculate the *probability* of something that interests us, given particular data, information, and assumptions.

So far we have used the term "probability" somewhat informally and intuitively. It is time to make it more precise and to emphasize some of its most important aspects. Then we'll dive into the rules of probability-inference.


## When truth isn't known: probability {#sec-probability-def}

When we cross a busy city street we look left and right to check whether any cars are approaching. We typically don't look *up* to check whether something is falling from the sky. Yet, couldn't it be `false` that cars are approaching at that moment? and couldn't it be `true` that [some object is falling from the sky ](https://www.aerotime.aero/articles/32818-cessna-door-falls-off-lands-in-parking-lot)? Of course both events are possible. Then why do we look left and right, but not up?

The main reason
<!-- ^[We shall see later that one more factor enters this explanation.] --> 
is that we *believe strongly* that cars might be approaching, and *believe very weakly* that some object might be falling from the sky. In other words, we consider the first occurrence to be *very probable*, and the second extremely *improbable*.

We shall take the notion of [**probability**]{.blue} as intuitively understood (just as we did with the notion of truth). Terms equivalent for "probability" are [*degree of belief*]{.blue}, [*plausibility*]{.blue}, [*credibility*]{.blue}.

:::{.callout-warning}
## {{< fa exclamation-circle >}} Avoid *likelihood* as a synonym for *probability*
In technical discourse, "likelihood" means something different and is *not* a synonym of "probability", as we'll explain later.
:::

Probabilities are quantified between $0$ and $1$, or equivalently between $0\%$ and $100\%$. Assigning to a sentence a probability `1` is the same as saying that it is `true`; and a probability `0`, that it is `false`. A probability of $0.5$ represents a belief completely symmetric with respect to truth and falsity.

Let's emphasize and agree on some important facts about probabilities:

- [**{{< fa bookmark >}} Probabilities are assigned to *sentences***]{.blue}. We already discussed this point in [§ @sec-sentence-notation], but let's reiterate it. Consider an engineer working on a problem of electric-power distribution in a specific geographical region. At a given moment the engineer may believe with $75\%$ probability that the measured average power output in the next hour will be 100 MW. The $75\%$ probability is assigned not to the quantity "100 MW", but to the *sentence*
$$
\pr{The measured average power output in the next hour will be 100\,MW}
$$
    This difference is extremely important. Consider the alternative sentence
$$
\pr{The average power output in the next hour will be \emph{set} to 100\,MW}
$$
	the numerical quantity is the same, but the meaning is very different. The probability can therefore be very different (if the engineer is the person deciding how to set that output, the probability is $100\%$). The probability depends not only on a number, but on what it's being done with that number -- measuring, setting, third-party reporting, and so on. Often we write simply ["$O = \mathrm{100\,W}$"]{.together} provided that the full sentence behind this kind of shorthand is understood.
	

- [**{{< fa bookmark >}} Probabilities are agent- and context-dependent**]{.blue}. A coin is tossed, comes down heads, and is quickly hidden from view. Alice sees that it landed heads-up. Bob instead doesn't manage to see the outcome and has no clue. Alice considers the sentence $\pr{Coin came down heads}$ to be `true`, that is, to have $100\%$ probability. Bob considers the same sentence to have $50\%$ probability.

    Note how Alice and Bob assign two different probabilities to the same sentence; yet both assignments are completely rational. If Bob assigned $100\%$ to $\pr{heads}$, we would suspect that he had seen the outcome after all; if he assigned $0\%$ to $\pr{heads}$, we would consider that groundless and silly. We would be baffled if Alice assigned $50\%$ to $\pr{heads}$, because she saw the outcome was actually heads; we would hypothesize that she feels unsure about what she saw.

    An omniscient agent would know the truth or falsity of every sentence, and assign only probabilities `0` or `1`. Some authors speak of "*actual* (but unknown) probabilities". But if there were "actual" probabilities, they would be all `0` or `1`, and it would be pointless to speak about probabilities at all -- every inference would be a truth-inference.


- [**{{< fa bookmark >}} Probabilities are not frequencies**]{.blue}. Consider the fraction of defective mechanical components to total components produced per year in some factory. This quantity can be physically measured and, once measured, would be agreed upon by every agent. It is a *frequency*, not a degree of belief or probability.

    It is important to understand the difference between *probability* and *frequency*: mixing them up may lead to sub-optimal decisions. Later we shall say more about the difference and the precise relations between probability and frequency.
	
	Frequencies can be unknown to some agents. Probabilities cannot be "unknown": they can only be difficult to calculate. Be careful when you read authors speaking of an "unknown probability": they actually mean either "unknown frequency", or a probability that has to be calculated (it's "unknown" in the same sense that the value of $1-0.7 \cdot 0.2/(1-0.3)$ is "unknown" to you right now).


- [**{{< fa bookmark >}} Probabilities are not physical properties**]{.blue}. Whether a tossed coin lands heads up or tails up is fully determined by the initial conditions (position, orientation, momentum, rotational momentum) of the toss and the boundary conditions (air velocity and pressure) during the flight. The same is true for all macroscopic engineering phenomena (even quantum phenomena have never been proved to be non-deterministic, and there are [deterministic and experimentally consistent](https://doi.org/10.48550/arXiv.quant-ph/9504010)  mathematical representations of quantum theory). So we cannot measure a probability using some physical apparatus; and the mechanisms underlying any engineering problem boil down to physical laws, not to probabilities.

::: {.callout-caution}
## {{< fa book >}} Reading
[*Dynamical Bias in the Coin Toss*](https://hvl.instructure.com/courses/25074/modules/items/661553).
<!-- We conclude that coin tossing is "physics" not "random." -->
:::

\

These points listed above are not just a matter of principle. They have important practical consequences. A data scientist who is not attentive to the source of the data (measured? set? reported, and so maybe less trustworthy?), or who does not carefully assess the context of a probability, or who mixes a probability with a frequency, or who does not take advantage (when possible) of the physics involved in the a problem -- such data scientist will design systems with sub-optimal performance^[This fact can be mathematically proven.] -- or even cause deaths.


## An unsure inference

Consider now the following variation of the trivial inference problem of [§ @sec-trivial-inference].

> This electric component had an early failure. If an electric component fails early, then at production it either didn't pass the heating test or didn't pass the shock test. The probability that it didn't pass both tests is 10%. There's no reason to believe that the component passed the heating test, more than it passed the shock test.

The inspector wants to assess, also in this case, whether the component did not pass the heating test.

From the data and information given, what would you say is the probability that the component didn't pass the heating test?

:::{.callout-caution}
## {{< fa user-edit >}} Exercises
- Try to argue why a conclusion cannot drawn with certainty in this case. One way to argue this is to present two different scenarios that fit the data given but have opposite conclusions.
- Try to reason intuitively and assess the probability that the component didn't pass the heating test. Should it be larger or smaller than 50%? Why?
:::


## Probability notation

For this inference problem we can't find a `true` or `false` final value. The truth-inference rules (@eq-t-not)--(@eq-t-unity) therefore cannot help us here. In fact even the ["$T(\dotso \| \dotso$"]{.together} notation is unsuitable, because it only admits the values $1$ (`true`) and $0$ (`false`).

Let us first generalize this notation in a straightforward way:

First, let's represent the probability or degree of belief of a sentence by a number in the range [$[0,1]$,]{.together} that is, between $\mathbf{1}$ (certainty or `true`) and $\mathbf{0}$ (impossibility or `false`). The value $0.5$ represents that the belief in the truth of the sentence is as strong as that in its falsity.

Second, let's symbolically write that the probability of a proposal $\yY$, given a conditional $\yX$, is some number $p$, as follows:
$$
\P(\yY \| \yX) = p
$$
Note that this notation includes the notation for truth-values as a special case:
$$
\P(\yY \| \yX) = 0\text{ or }1
\quad\Longleftrightarrow\quad
\mathrm{T}(\yY \| \yX) = 0\text{ or }1
$$

## Inference rules

Extending our truth-inference notation to probability-inference notation has been straightforward. But which rules should we use for drawing inferences when probabilities are involved?

The amazing result is that *the rules for truth-inference, formulae (@eq-t-not)--(@eq-t-unity), extend also to probability-inference*. The only difference is that they now hold for all values in the range $[0,1]$, rather than for $0$ and $1$ only.

This important result was taken more or less for granted at least since Laplace in the 1700s. But was formally proven for the first time in the 1946 by R. T. Cox. The proof has been refined since then. What kind of proof is it? It shows that if we don't follow the rules we are doomed to arrive at illogical conclusions; we'll show some examples later.

\

Finally, here are the fundamental rules of all inference. They are encoded by the following equations, which must always hold for any atomic or complex sentences [$\yX,\yY,\yZ$:]{.together}

::::: {.column-page}
:::: {.callout-note style="text-align:center;font-size:120%"}
## [**{{< fa landmark >}}   THE FUNDAMENTAL LAWS OF INFERENCE   {{< fa landmark >}}**]{.blue style="font-size:150%"}

:::{style="font-size:150%"}
"Not" $\boldsymbol{\lnot}$ rule
: $$\P(\lnot \yX \| \yZ) 
+ \P(\yX \| \yZ)
= 1$$
\

"And" $\boldsymbol{\land}$ rule
: $$
\P(\yX \land \yY \| \yZ) 
= \P(\yX \| \yY \land \yZ) \cdot
\P(\yY \| \yZ) 
= \P(\yY \| \yX \land \yZ) \cdot
\P(\yX \| \yZ) 
$$
\

"Or" $\boldsymbol{\lor}$ rule
: $$\P(\yX \lor \yY \| \yZ) 
= \P(\yX \| \yZ) +
\P(\yY \| \yZ) 
- \P(\yX \land \yY \| \yZ)
$$
\

Self-consistency rule
: $$\P(\yX \| \yX \land \yZ) 
= 1
$$
:::
::::
:::::

It is amazing that **ALL** inference is nothing else but a repeated application of these four rules -- billions of times or more in some cases. All machine-learning algorithms are just applications or approximations of these rules. Methods that you may have heard about in statistics are just specific applications of these rules. Truth inferences are also special applications of these rules. Most of this course is, at bottom, just a study of how to apply these rules in particular kinds of problems.

::: {.callout-caution}
## {{< fa book >}} Reading
- [*Probability, Frequency and Reasonable Expectation*](https://hvl.instructure.com/courses/25074/modules/items/660094)

- Ch. 2 of [*Bayesian Logical Data Analysis for the Physical Sciences*](https://hvl.instructure.com/courses/25074/modules/items/660390)

- §§ 1.0--1.2 of [*Data Analysis*](https://hvl.instructure.com/courses/25074/modules/items/661040)

- Feel free to skim through §§ 2.0--2.4 of [*Probability Theory*](https://hvl.instructure.com/courses/25074/modules/items/660090)
:::

\ 

The fundamental inference rules are used in the same way as their truth-inference special case: Each equality can be rewritten in different ways according to the usual rules of algebra. Then left and right side of the equality thus obtained can replace each other in a proof.

## Solution of the uncertain-inference example

Armed with the fundamental rules of inference, let's solve our earlier inference problem. As usual we first analyse it, find what are its proposal and conditional, and which starting inferences are given in the problem.

### Atomic sentences

$$
\begin{aligned}
\yh &\coloneqq \pr{The component passed the heating test}
\\
\ys &\coloneqq \pr{The component passed the shock test}
\\
\yf &\coloneqq \pr{The component had an early failure}
\\
\yJ &\coloneqq \prq{(all other implicit background information)}
\end{aligned}
$$
The background information in this example is different from the previous, truth-inference one, so we use the different symbol $\yJ$ for it.

### Proposal, conditional, and target inference

The proposal is $\lnot\yh$, just like in the truth-inference example.

The conditional is different now. We know that the component failed early, but we don't know whether it passed the shock test. Hence the conditional is
$\yf \land \yJ$.

The target inference is therefore
$$
\color[RGB]{238,102,119}\P(\lnot\yh \| \yf \land \yJ)
$$


### Starting inferences

We are told that if an electric component fails early, then at production it didn't pass either the heating test or the shock test. Let's write this as
$$
\color[RGB]{34,136,51}\P(\lnot\yh \lor \lnot\ys \| \yf \land \yJ) = 1
$$
We are also told that there is a $10\%$ probability that both tests fail
$$
\color[RGB]{34,136,51}\P(\lnot\yh \land \lnot\ys \| \yf \land \yJ) = 0.1
$$

Finally the problem says that there's no reason to believe that the component didn't pass the heating test, more than it didn't pass the shock test. This can be written as follows:
$$
\color[RGB]{34,136,51}\P(\lnot\yh \| \yf \land \yJ) = \P(\lnot\ys \| \yf \land \yJ)
$$
Note this interesting situation: we are not given the numerical values of these two probabilities, we are only told that they are equal. This is an example of application of the *principle of indifference*, which we'll discuss more in detail later.

### Final inference

Also in this case there is no unique way of applying the rules to reach our target inference, but all ways lead to the same result. Let's try to proceed backwards:

::: {.column-page-inset-right}
$$
\begin{aligned}
&\color[RGB]{238,102,119}\P(\lnot\yh \| \yf \land \yJ)&&
\\[1ex]
&\qquad= {\color[RGB]{34,136,51}\P(\lnot\ys \lor \lnot\yh \| \yf \land \yJ)}
+ {\color[RGB]{34,136,51}\P(\lnot\ys \land \lnot\yh \| \yf \land \yJ)}
- \P(\lnot\ys \| \yf \land \yJ)
&&\text{\small ∨-rule}
\\[1ex]
&\qquad= {\color[RGB]{34,136,51}1}
+ {\color[RGB]{34,136,51}0.1}
- \P(\lnot\ys \| \yf \land \yJ)
&&\text{\small starting inferences}
\\[1ex]
&\qquad= 0.1 + \color[RGB]{34,136,51}\P(\ys \| \yf \land \yJ)
&&\text{\small ¬-rule}
\\[1ex]
&\qquad= 0.1 + \P(\yh \| \yf \land \yJ)
&&\text{\small starting inference}
\\[1ex]
&\qquad= 0.1 + 1 -\color[RGB]{238,102,119}\P(\lnot\yh \| \yf \land \yJ)
&&\text{\small ¬-rule}
\end{aligned}
$$
:::

The target probability appears on the left and right side with opposite signs. We can solve for it:
$$
\begin{aligned}
2\,{\color[RGB]{238,102,119}\P(\lnot\yh \| \yf \land \yJ)} &= 0.1 + 1
\\[1ex]
{\color[RGB]{238,102,119}\P(\lnot\yh \| \yf \land \yJ)} &= 0.55
\end{aligned}
$$

So the probability that the component didn't pass the heating test is $55\%$.

:::{.callout-caution}
## {{< fa user-edit >}} Exercises
- Try to find an intuitive explanation of why the probability is 55%, slightly larger than 50%. If your intuition says this probability is wrong, then

    + Check the proof of the inference for mistakes, or try to find a proof with a different path.
    + Examine your intuition critically and educate it.

- Check how the target probability $\P(\lnot\yh \| \yf \land \yJ)$ changes if we change the value of the probability $\P(\lnot\ys \land \lnot\yh \| \yf \land \yJ)$ from $0.1$.

    + What result do we obtain if [$\P(\lnot\ys \land \lnot\yh \| \yf \land \yJ)=0$?]{.together} Can it be intuitively explained?
    + What if [$\P(\lnot\ys \land \lnot\yh \| \yf \land \yJ)=1$?]{.together} Does the result make sense?
:::


## How the inference rules are used

In the solution above you noticed that the equations of the fundamental rules are not only used to obtain some of the probabilities appearing in them from the remaining probabilities.

The rules represent, first of all, [*constraints of logical consistency*]{.blue}^[The technical term is [**coherence**]{.blue}.] among probabilities. For instance, if we have probabilities\ \ [$\P(\yY\|\yX\land \yZ)=0.1$,]{.together}\ \ [$\P(\yX\|\yZ)=0.7$,]{.together}\ \ and [$\P(\yX\land \yY\|\yZ)=0.2$,]{.together}\ \ then there's an inconsistency somewhere, because these values violate the and-rule:\ \ [$0.2 \ne 0.1 \cdot 0.7$.]{.together}\ \ In this case we must find the inconsistency and solve it. However, since probabilities are quantified by real numbers, it's possible and acceptable to have slight discrepancies within numerical round-off errors.

The rules also imply more general constraints. For example we must *always* have
$$
\begin{gathered}
\P(\yX\land \yY \|\yZ) \le \min\bigl\{\P(\yX\|\yZ),\  \P(\yY\|\yZ)\bigr\}
\\
\P(\yX\lor \yY \|\yZ) \ge \max\bigl\{\P(\yX\|\yZ),\  \P(\yY\|\yZ)\bigr\}
\end{gathered}
$$

::: {.callout-caution icon=false}
## {{< fa user-edit >}} Exercise
Try to prove the two constraints above.
:::



### Derived rules

The fundamental rules above are in principle all we need to use to draw inferences from other inferences. But from them it is possible to derive some "shortcut" rules.

First, it is possible to show that all rules you may know from Boolean algebra *are a consequence of the fundamental rules*. So we can always make the following convenient replacements anywhere in a probability expression:

::: {.callout-note}
## Derived rules: Boolean algebra
$$
\begin{gathered}
\lnot\lnot \yX = \yX
\qquad
\yX \land \yX = \yX \lor \yX = \yX
\\[1ex]
\yX\land \yY = \yY \land \yX
\qquad
\yX \lor \yY = \yY \lor \yX
\\[1ex]
\yX \land (\yY \lor \yZ) = (\yX \land \yY) \lor (\yX \land \yZ)
\\[1ex]
\yX \lor (\yY \land \yZ) = (\yX \lor \yY) \land (\yX \lor \yZ)
\\[1ex]
\lnot (\yX \land \yY) = \lnot \yX \lor \lnot \yY
\qquad
\lnot (\yX \lor \yY) = \lnot \yX \land \lnot \yY
\end{gathered}
$$
:::

Two other derived rules are used extremely often, so we treat them separately.

## Law of total probability or "extension of the conversation"

Suppose we have a set of $n$ sentences $\{\yX_1, \yX_2, \dotsc, \yX_n\}$ having these two properties:

- They are [**mutually exclusive**]{.blue}, meaning that the "and" of any two of them is false, given a conditional $\yZ$:
    $$
	\P(\yX_1\land\yX_2\|\yZ) = 0\ , \quad
	\P(\yX_1\land\yX_3\|\yZ) = 0\ , \quad
\dotsc \ , \quad
	\P(\yX_{n-1}\land\yX_n\|\yZ) = 0
	$$
	
- They are [**exhaustive**]{.blue}, meaning that the "or" of all of them is true, given a conditional [$\yZ$:]{.together}
    $$
	\P(\yX_1\lor \yX_2 \lor \dotsb \lor \yX_n \|\yZ) = 1
	$$
	
Then the probability of a sentence [$\yY$,]{.together} conditional on [$\yZ$,]{.together}, is equal to a combination of probabilities conditional on the single $\yX_1,\yX_2,\dotsc$:
$$
\begin{aligned}
\P(\yY \| \yZ) &=
\P(\yY \| \yX_1 \land \yZ)\cdot \P(\yX_1 \| \yZ) +{}
\\&\qquad\P(\yY \| \yX_2 \land \yZ)\cdot \P(\yX_2 \| \yZ) +{}
\\&\qquad\dotsb +{}
\\&\qquad\P(\yY \| \yX_n \land \yZ)\cdot \P(\yX_n \| \yZ)
\end{aligned}
$$

This rule is useful when it is difficult to assess the probability of a sentence conditional on the background information, but it is easier to assess the probabilities of that sentence conditional on several auxiliary sentences -- often representing hypotheses that exclude one another, and of which we know at least one is true. The name [**extension of the conversation**]{.blue} for this derived rule comes from the fact that we are able to call the additional sentences into play.

This situation occurs very often in concrete applications, especially in problems where the probabilities of several competing hypotheses have to be assessed.

## Bayes's theorem

The probably most famous -- or infamous -- rule derived from the laws of inference is [**Bayes's theorem**]{.blue}. It allows us to relate the probability where two sentences $\yY,\yX$ appear in the proposal and the conditional, with one where they are exchanged:
$$
\P(\yY \| \yX \land \yZ) =
\frac{\P(\yX \| \yY \land \yZ)\cdot \P(\yY \| \yZ)}{\P(\yX \| \yZ)}
$$
Obviously this rule can only be used if [$\P(\yX \| \yZ) > 0$,]{.together} that is, if the sentence $\yX$ is not false conditional on [$\yZ$.]{.together}

:::{.column-margin} 
![Bayes's theorem guest-starring in [*The Big Bang Theory*](https://www.imdb.com/title/tt0898266/)](bayes_big-bang.jpg)
:::

:::{.callout-caution}
## {{< fa user-edit >}} Exercise
Prove Bayes's theorem from the fundamental rules.
:::

Bayes's theorem is extremely useful when it is difficult to assess the probability of a sentence -- typically a hypothesis -- given some conditional -- typically data -- but it is easier to assess the probability of the data conditional on the hypothesis. Note, however, that the sentences $\yY$ and $\yX$ in the theorem can be about anything whatsoever: $\yY$ does not always need to be a "hypothesis", and $\yX$ "data".

Bayes's theorem is often proclaimed to be the rule according to which we "update our beliefs". The meaning of this proclamation is the following.

Let's say that at some point $\yZ$ represents all your knowledge. Your degree of belief about some sentence $\yY$ is then (at least in theory) the value of [$\P(\yY \| \yZ)$.]{.together} At some later point, let's say that you get to know -- maybe thanks to a measurement that you made -- that the sentence $\yX$ is true. Your whole knowledge is at that point is represented no longer by [$\yZ$,]{.together} but by [$\yX \land \yZ$.]{.together} Your degree of belief about $\yY$ is then given by the value of [$\P(\yY \| \yX\land\yZ)$.]{.together} Bayes's theorem allows you to find your degree of belief about $\yY$ conditional on your new state of knowledge, from the one conditional on your old state of knowledge.

Note, however, that this chronological element comes only from this particular way of using Bayes's theorem. The theorem can be used to connect any two states of knowledge $\yZ$ and [$\yX\land\yZ$,]{.together} no matter their temporal order, and even if they happen simultaneously.

:::{.callout-caution}
## {{< fa user-edit >}} Exercise
Using Bayes's theorem and the fundamental laws of inference, prove that if [$\P(\yX \| \yZ)=1$,]{.together} that is, if you already know that $\yX$ is true in your current state of knowledge [$\yZ$,]{.together} then
$$
\P(\yY \| \yX \land \yZ) = \P(\yY \| \yZ)
$$
that is, your beliefs about $\yY$ don't change.

Is this result reasonable?
:::


## consequences of not following the rules, 

@@ §12.2.3 of AI



* _Exercise: [Monty-Hall problem & variations](The_Monty_Hall_problem-exercise.pdf)_

* _Exercise: clinical test & diagnosis_


## Common points of certain and uncertain inference

> _No premises? No conclusions!_



:::{.callout-important appearance="default"}
## {{< fa exclamation-circle >}} Differences in terminology
- Some texts speak of the probability of a "random^[What does "random" mean? Good luck finding an understandable and non-circular definition in texts that use that word. In these notes, if the word "random" is ever used, it means "unpredictable" or "unsystematic".] variable", or more precisely of the probability that a random variable takes on a particular value. As you notice, we have just expressed that idea by means of a *sentence*. The viewpoint and terminology of random variables is a special case of that of sentences. As already discussed, in concrete applications it is important to know how a variable "takes on" a value: for example it could be directly measured, indirectly reported, or purposely set. Thinking in terms of sentences, rather than of random variables, allows us to account for these important differences.

- Some texts speak of the probability of an "event". For all purposes an "event" is just what's expressed in a sentence.

It's a question for sociology of science why some people keep on using less flexible points of view or terminologies. Probably they just memorize them as students and then a fossilization process sets in.
:::
