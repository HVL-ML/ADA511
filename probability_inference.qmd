# Probability inference
{{< include _macros.qmd >}}

## When truth isn't known: probability

In most real-life and engineering situations we don't know the truth or falsity of sentences and hypotheses that interest us. But this doesn't mean that nothing can be said or done in such situations.

When we cross a busy city street we look left and right to check whether any cars are approaching. We typically don't look up to check whether something is falling from the sky. Yet, couldn't it be `false` that cars are approaching? and couldn't it be `true` that [some object is falling from the sky ](https://www.aerotime.aero/articles/32818-cessna-door-falls-off-lands-in-parking-lot)? Of course both events are possible. Then why do we look left and right, but not up?

The main reason^[We shall see later that one more factor enters the explanation.] is that we *believe strongly* that cars might be approaching, *believe very weakly* that some object might be falling from the sky. In other words, we consider the first occurrence to be very *probable*; the second, extremely improbable.

We shall take the notion of **probability** as intuitively understood (just as we did with the notion of truth). Terms equivalent for "probability" are *degree of belief*, *plausibility*, *credibility*.

:::{.callout-important appearance="simple"}
##
{{< fa exclamation-circle >}} In technical discourse, *likelihood* means something different and is *not* a synonym of "probability", as we'll explain later.
:::

Probabilities are quantified between `0` and `1`, or equivalently between `0%` and `100%`. Assigning to a sentence a probability `1` is the same as saying that it is `true`; and a probability `0`, that it is `false`. A probability of `0.5` represents a belief completely symmetric with respect to truth and falsity.

It is important to emphasize and agree on some facts about probabilities:

- **Probabilities are assigned to *sentences***. Consider an engineer working on a problem of electric-power distribution in a specific geographical region. At a given moment the engineer may believe with `75%` probability that the measured average power output in the next hour will be 100 MW. The `75%` probability is assigned not to the quantity "100 MW", but to the *sentence*
$$
\pr{The measured average power output in the next hour will be 100\,MW}
$$
    This difference is extremely important. Consider the alternative sentence
$$
\pr{The average power output in the next hour will be \emph{set} to 100\,MW}
$$
	the quantity is the same, but the meaning is very different. The probability can therefore be very different (if the engineer is the person deciding the output, the probability is `100%`). The probability depends not only on a number, but on what it's being done with that number -- measuring, setting, third-party reporting, and so on. Often we still write simply $\pr{\(\mathsf{O = 100\,W}\)}$ or even just $\pr{100\,W}$, provided that the full sentence behind the shorthand is understood.
	

- **Probabilities are agent- and context-dependent**. A coin is tossed, comes down heads, and is quickly hidden from view. Alice sees that it landed heads-up. Bob instead doesn't manage to see the outcome and has no clue. Alice considers the sentence $\pr{Coin came down heads}$ to be `true`, that is, to have `100%` probability. Bob considers the same sentence to have `50%` probability.

    Note how Alice and Bob assign two different probabilities to the same sentence; yet both assignments are completely rational. If Bob assigned `100%` to $\pr{heads}$, we would suspect that he had seen the outcome after all; if he assigned `0%` to $\pr{heads}$, we would consider that groundless and silly. We would be baffled if Alice assigned `50%` to $\pr{heads}$, because she saw the outcome was actually heads; we would hypothesize that she feels unsure about what she saw.

    An omniscient agent would know the truth or falsity of every sentence, and assign only probabilities `0` or `1`. Some authors speak of "*actual* (but unknown) probabilities"; if there were "actual" probabilities, they would be all `0` or `1`, and it would be pointless to speak about probabilities at all -- every inference would be a truth inference.


- **Probabilities are not frequencies**. The fraction of defective mechanical components to total components produced per year in some factory is a quantity that can be physically measured and would be agreed upon by every agent. It is a *frequency*, not a degree of belief or probability. It is important to understand the difference between them, to avoid making sub-optimal decisions; we shall say more about this difference later. Frequencies can be unknown to some agents, probabilities cannot be unknown (but can be difficult to calculate). Be careful when you read authors speaking of an "unknown probability"; either they actually mean "unknown frequency", or a probability that has to be calculated (it's "unknown" in the same sense that the value of $1-0.7 \cdot 0.2/(1-0.3)$ is unknown to you right now).


- **Probabilities are not physical properties**. Whether a tossed coin lands heads up or tails up is fully determined by the initial conditions (position, orientation, momentum, rotational momentum) of the toss and the boundary conditions (air velocity and pressure) during the flight. The same is true for all macroscopic engineering phenomena (even quantum phenomena have never been proved to be non-deterministic, and there are [deterministic and experimentally consistent](https://doi.org/10.48550/arXiv.quant-ph/9504010)  mathematical representations of quantum theory). So we cannot measure a probability using some physical apparatus; and the mechanisms underlying any engineering problem boil down to physical laws, not to probabilities.

::: {.callout-caution}
## {{< fa book-open >}} Reading
[*Dynamical Bias in the Coin Toss*](https://hvl.instructure.com/courses/25074/modules/items/661553)
<!-- We conclude that coin tossing is "physics" not "random." -->
:::

These facts are not just a matter of principle. They have important practical consequences. A data engineer who is not attentive to the source of the data (measured? set? reported, and so maybe less trustworthy?), or who does not carefully assess the context of a probability, or who mixes it up with something else, or who does not take advantage (when possible) of the physics involved in the engineering problem, will design a system with sub-optimal performance^[This fact can be mathematically proven.] -- or even cause deaths.

## No new building blocks

In discussing [truth-inference](truth_inference.qmd) we introduced notations such as  $\mathrm{T}(a \| b \land D)$, which stands for the truth-value `0` or `1` of sentence $a$ in the context of data $D$ and supposing (even if only hypothetically) sentence $b$ to be true. We can simply extend this notation to probability-values, using a $\P$ instead of $\mathrm{T}$:
$$\P(a \| b \land D) \in [0,1]$$
represents the probability or degree of belief in sentence $a$ in the context of data $D$ and supposing also sentence $b$ to be true. Keep in mind that both $a$ and $b$ could be complex sentences (for instance $a = (\lnot c \lor d) \land e$). Note that truth-values are included as the special cases`1` or `0`:
$$
\P(a \| b \land D) = 0\text{ or }1
\quad\Longleftrightarrow\quad
\mathrm{T}(a \| b \land D) = 0\text{ or }1
$$

## Probability-inference rules

Extending our truth-inference notation to probability-inference notation has been straightforward. But how do we draw inferences when probabilities are involved? 

Consider the inference about my umbrella in a more uncertain situation:

:::{.column-page-inset-right}
$$
\frac{
\P(\pr{My umbrella is either blue or red}\|D)=1\quad
\P(\pr{My umbrella is not red} \| D)=0.5
}{
\P(\pr{My umbrella is blue} \| D) = \mathord{?}
}
$$
:::

or more compactly, using the symbols we introduced earlier,
$$
\frac{
\P\bigl[(b \lor r) \land \lnot (b \land r)\|D\bigr]=1\quad
\P(\lnot r\| D)=0.5
}{
\P( b \| D) = \mathord{?}
}
$$
This says, above the line, that: according to our data $D$ my umbrella is either blue or red (and can't be both), with full certainty; and according to our data we have no preferential beliefs on whether my umbrella is not red. What should then be the probability of my umbrella being blue, according to our data?

Intuitively that probability should be `50%`: $\P( b \| D)=0.5$. But which rules did we follow in arriving at this probability? More generally, which rules should we follow in assigning new probabilities from given ones?

The amazing result is that *the rules for truth-inference, formulae (@eq-t-not, @eq-t-or, @eq-t-and, @eq-t-unity), extend also to probability-inference*. The only difference is that they now hold for all values in the range $[0,1]$, rather than only values $0$ and $1$.

This important result was taken more or less for granted at least since Laplace in the 1700s. But was formally proven for the first time in the 1940s by R. T. Cox; the proof has been refined since then. What kind of proof is it? It shows that if we don't follow the rules we arrive at illogical conclusions; we'll show some examples later.

Here are the fundamental rules of probability inference. In these rules, all probabilities can have values in the range $\P() \in [0,1]$, and the symbols $a,b,D$ represent sentences of any complexity:



::::: {.column-page style="color:#4477AA"}
:::: {.callout-note appearance="default" icon=false style="text-align:center; color:#4477AA"}
## {{< fa landmark >}}   THE FUNDAMENTAL LAWS OF INFERENCE   {{< fa landmark >}}

"Not" $\boldsymbol{\lnot}$ rule
: $$\P(\lnot a \| D) 
+ \P(a \| D)
= 1$$
\

"And" $\boldsymbol{\land}$ rule
: $$
\P(a \land b \| D) 
= \P(a \| b \land D) \cdot
\P(b \| D) 
= \P(b \| a \land D) \cdot
\P(a \| D) 
$$
\

"Or" $\boldsymbol{\lor}$ rule
: $$\P(a \lor b \| D) 
= \P(a \| D) +
\P(b \| D) 
- \P(a \land b \| D)
$$
\

Self-consistency rule
: $$\P(a \| a \land D) 
= 1
$$
::::
:::::

It is amazing that **ALL** inference is nothing else but a repeated application of these four rules -- billions of times or more, in some inferences. All machine-learning algorithms are just applications or approximations of these rules. Methods that you may have heard about in statistics are just specific applications of these rules. Truth inferences are also special applications of these rules. Most of this course is, at bottom, just a study of how to apply these rules in particular kinds of problems.

::: {.callout-caution}
## {{< fa book-open >}} Reading
- [*Probability, Frequency and Reasonable Expectation*](https://hvl.instructure.com/courses/25074/modules/items/660094)

- Ch. 2 of [*Bayesian Logical Data Analysis for the Physical Sciences*](https://hvl.instructure.com/courses/25074/modules/items/660390)

- §§ 1.0--1.2 of [*Data Analysis*](https://hvl.instructure.com/courses/25074/modules/items/661040)

- Feel free to skim through §§ 2.0--2.4 of [*Probability Theory*](https://hvl.instructure.com/courses/25074/modules/items/660090)
:::


## How the inference rules are used

The fundamental rules represent, first of all, constraints of logical consistency among probabilities. If we have probabilities $\P(a\|D)=0.7$, $\P(b\|a\land D)=0.1$, $\P(a\land b\|D)=0.2$, then there's an inconsistency somewhere, because these values violate the and-rule:\ \ $0.2 \ne 0.1 \cdot 0.7$.\ \ In this case we must find the inconsistency and solve it. Since probabilities are quantified by real numbers, however, it's possible and acceptable to have slight discrepancies owing to numerical round-off errors.

The rules also imply more general constraints. For example we must *always* have
\begin{gather*}
\P(a\land b \|D) \le \min\bigl\{\P(a\|D),\  \P(b\|D)\bigr\}
\\
\P(a\lor b \|D) \ge \max\bigl\{\P(a\|D),\  \P(b\|D)\bigr\}
\end{gather*}

::: {.callout-caution icon=false}
## {{< fa pen >}} Exercise
Try to prove the two constraints above
:::


The main use of the rules in concrete applications is for calculating new probabilities from given ones. The calculated probabilities will be automatically consistent. For each equation shown in the rules we can calculate one probability given the remaining ones in the equation, with some special cases when values of $0$ or $1$ appear.

For example, if we have $\P(a \land b\| D)=0.2$ and  $\P(a\|D)=0.7$, from the and-rule we can find $\P(b\| a \land D)$:
\begin{multline*}
\underbracket{\color[RGB]{34,136,51}\P(a \land b \| D)}_{0.2}
= {\color[RGB]{238,102,119}\P(b \| a \land D)} \cdot
\underbracket{\color[RGB]{34,136,51}\P(a \| D)}_{0.7}
\\[1em]
\Longrightarrow\quad
{\color[RGB]{238,102,119}\P(b\| a \land D)} = 
\frac{\color[RGB]{34,136,51}
\P(a\land b \| D)
}{\color[RGB]{34,136,51}
\P(a\|D)
} = \frac{0.2}{0.7} 
\approx 0.2857
\end{multline*}
\

Let us now solve the umbrella inference from the previous section. Starting from
$$
\P\bigl[(b \lor r) \land \lnot (b \land r)\|D\bigr]=1 \ ,
\quad
\P(\lnot r\| D)=0.5
$$
we arrive at
$$
\P( b \| D) = 0.5
$$

by following from top to bottom the steps depicted here:

::: {.border}
![](umbrella_inference2.svg)
:::


@@ example medical diagnosis

### Derived rules

The rules above are in principle all we need to use. But from them it is possible to derive some additional shortcut rules that are automatically consistent with the fundamental ones.

First, it is possible to show that all rules you may know from Boolean algebra are a consequence of the fundamental rules. For example, we can always make the following convenient replacements anywhere in a probability expression:
$$
\begin{gathered}
A \land A = A \lor A = A
\qquad
\lnot\lnot A = A
\\[1ex]
A\land B = B \land A
\qquad
A \lor B = B \lor A
\\[1ex]
\lnot (A \land B) = \lnot A \lor \lnot B
\qquad
\lnot (A \lor B) = \lnot A \land \lnot B
\\[1ex]
A \land (B \lor C) = (A \land B) \lor (A \land C)
\\[1ex]
A \lor (B \land C) = (A \lor B) \land (A \lor C)
\end{gathered}
$$

Two other derived rules are used extremely often, so we treat them separately.

## Law of total probability or "extension of discourse"




## Bayes's theorem



## consequences of not following the rules, 

@@ §12.2.3 of AI



* _Exercise: [Monty-Hall problem & variations](The_Monty_Hall_problem-exercise.pdf)_

* _Exercise: clinical test & diagnosis_


## Common points of certain and uncertain inference

> _No premises? No conclusions!_



:::{.callout-important appearance="simple"}
## {{< fa exclamation-circle >}} Differences in terminology
- Some texts speak of the probability of a "random^[What does "random" mean? Good luck finding an understandable and non-circular definition in texts that use that word. In these notes, if the word "random" is ever used, it means "unpredictable" or "unsystematic".] variable", or more precisely of the probability that a random variable takes on a particular value. As you notice, we have just expressed that idea by means of a *sentence*. The viewpoint and terminology of random variables is a special case of that of sentences. As already discussed, in concrete applications it is important to know how a variable "takes on" a value: for example it could be directly measured, indirectly reported, or purposely set. Thinking in terms of sentences, rather than of random variables, allows us to account for these important differences.

- Some texts speak of the probability of an "event". For all purposes an "event" is just what's expressed in a sentence.

It's a question for sociology of science why some people keep on using less flexible points of view or terminologies. Probably they just memorize them as students and then a fossilization process sets in.
:::
