# [Third connection with machine learning]{.red} {#sec-3-connection-ML}
{{< include macros.qmd >}}
{{< include macros_prob_inference.qmd >}}

In [chapter @sec-2-connection-ML] we made a second *tentative* connection between the notions about probability explored until then, and notions from machine learning. We considered the possibility that a machine-learning algorithm is like an agent that has some [built-in background information (corresponding to the algorithm's architecture)]{.yellow}, has received [pieces of information (corresponding to the data about perfectly known instances of the task, and possibly partial data about a new instance)]{.green}, and is assessing a [not-previously known piece of information (other partial aspects of a new task instance)]{.red}:

$$
\P(\underbracket[0ex]{\red\se{D}_{N+1}}_{\mathclap{\red\text{outcome?}}} \| 
\green\underbracket[0ex]{\se{D}_N \land \dotsb \land \se{D}_2 \land \se{D}_1}_{\mathclap{\green\text{training data?}}} 
\black\land \underbracket[0ex]{\yellow\yI}_{\mathrlap{\yellow\uparrow\ \text{architecture?}}})
$$

The correspondence about [training data]{.green} and [architecture]{.yellow} seems somewhat convincing, the one about [outcome]{.red} will need more exploration, because it seems to involve some decision process -- and we haven't fully explored the machinery of decision-making yet.

Having introduced the notion of quantity in the latest chapters [-@sec-quantities-types-basic] and [-@sec-quantities-types-multi], we recognize that training data about a task instance concern some quantity and its value, so they can be expressed by a sentence like $D_i\mo d_i$, where

- $i$ is the instance: $1,2,\dotsc,N$
- $D_i$ describes the kind of data at instance $i$, for example "128 × 128 image with 24-bit colour depth"
- $d_i$ is the value of the data at instance $i$, for example the one here at the margin

:::{.column-margin}
![](saitama_smile.png){width=128}
:::

And similarly for the outcome of a new task instance where the algorithm is applied for real, which we consider as instance $N+1$. So we can rewrite the correspondence above as follows:

$$
\P(\underbracket[0ex]{\red D_{N+1} \mo d_{N+1}}_{\mathclap{\red\text{outcome?}}} \| 
\green\underbracket[0ex]{ D_N \mo d_N \and \dotsb \and  D_2 \mo d_2 \and  D_1 \mo d_1}_{\mathclap{\green\text{training data?}}} 
\black\and \underbracket[0ex]{\yellow\yI}_{\mathrlap{\yellow\uparrow\ \text{architecture?}}})
$$

This kind of inference is what we explored in the "next-three-patients" scenario of [§ @sec-conditional-joint-sim] and some of the following sections.

Let's extend this tentative connection even further. It may be necessary to somewhat detach ourselves from common machine-learning terminology, either because it focuses on aspects that to us are secondary, or because it can be misleading if taken too literally or according to the everyday meaning of some words.

\
Machine-learning textbooks usually make a distinction between "supervised learning" and "unsupervised learning". This distinction or categorization is not very useful to a data engineer or data scientist, for several reasons:

- {{< fa bullseye >}}\ \ It focuses on a procedure rather than on a purpose. The important questions to us are: [What do we wish to infer or choose?]{.blue} and [From which kind of information?]{.green} These questions define the problem we want to solve. The procedure may then be chosen depending on other contingent factors, resources, and so on.

<!-- It's somewhat like saying that the difference between car and aeroplane is that the latter has wings. Sure -- but *why?* The focus on wings misses the essential difference between these two means of transportation: they operate through different material media and exploit different kinds of physics; that's why the second has wings. -->

- {{< fa shuffle >}}\ \ It groups together some classes of problems that are quite different from an inferential viewpoint, and conversely separates classes of problems that are quite similar.

Below we shall introduce a different categorization that tries to focus on different cases of desired information and of available information.

\

A statement that is also common in machine-learning textbooks is that "[in supervised learning the algorithm learns a functional relationship between some kind of input and some kind of output]{.midgrey}". Such statement is misleading, because it seems to imply that a functional relationship actually exists between some quantities -- it's only waiting to be discovered and "learned". In many important problems and applications, for example in medicine, this is actually not true: **there isn't any functional relationship between input and output at all**. Even if any possible "noise" were removed, there still would *not* be any functional relationship between the clean, actual input and output ([here is an example](steven-seagal-emotion-chart.jpg){.external}). (From this point of view the very terms "input" and "output" become somewhat misleading.)

In many important problems there's only a *statistical* or *probabilistic* relationship between quantities that we can observe and quantities that we'd like to infer.

The lack of a real input-output function has very important consequences. If a machine-learning algorithm yields only *one* output among the possible ones that are consistent with the input, then it means that **it must choose one of the possibilities**. How is this choice made? This is obviously a decision-making problem, and as we know from chapters [-@sec-framework] and [-@sec-basic-decisions] the optimal choice is determined by Decision Theory. Its determination requires:

1. the probabilities of the possible outputs

2. the utilities of the possible choices







Unfortunately the explanation given for this distinction is sometimes misleading:

- [{{< fa meh-rolling-eyes >}}]{.purple}\ \ Some books say that in supervised learning the algorithm "learns a functional relationship between some kind of input and some kind of output". This is usually *not* true: in the vast majority of applications there isn't any *functional* relationship between input and output at all; at most only a *statistical* or *probabilistic* one. This is clear from the fact that two training datapoints can have identical inputs but different outputs ([here is an example](steven-seagal-emotion-chart.jpg){.external}); and you remember from Calculus I that we can't speak of a function in this case. It's unclear how something that doesn't exist can be learned.

    Books that give this kind of explanation are unfortunately oversimplifying things, to the point of being incorrect^[Paraphrasing [M. W. Zemansky](https://hvl.instructure.com/courses/25074/modules/items/679142):\
	*"Teaching machine learning\
	Is as easy as a song:\
	You think you make it simpler\
	When you make it slightly wrong!"*]. The algorithm is actually doing something more complex -- which we shall analyse in detail later.


- [{{< fa meh >}}]{.purple}\ \ Yet other books say that the distinction rests in the kind of data used for training: "input-output" pairs for supervised learning, and only "inputs" for unsupervised learning. It's good that this description doesn't mention "functions", but it is still unsatisfactory, because it confuses the means with the purpose. It's a little like saying that the difference between car and aeroplane is that the latter has wings. Sure -- but *why?* This description misses the essential difference between these two means of transportation: they operate through different material media and exploit different kinds of physics; that's why the second has wings.

    Books that give this kind of explanation focus on a more "operational" kind of knowledge, which is sufficient for their specific goals. The very terms "supervised learning" and "unsupervised learning" indeed emphasize this operational side. But for our goal we need to look beyond operational differences and to understand their reasons.

- [{{< fa smile >}}]{.lightblue}\ \ More enlightening books explain that the distinction rests in what the algorithm needs for each new application: in supervised learning, it uses features -- that is, additional information -- available at each new application instance; whereas in unsupervised learning it doesn't: no new instance-dependent information is given.

    From this point of view, we also see that the distinction between "supervised" and "unsupervised" becomes less sharp: we can imagine to increase the information that's used at each new instance from zero ("unsupervised") to larger and larger amounts ("supervised").


Going back to our tentative correspondence with inference and decision-making agents, we see a strong similarity between unsupervised & supervised learning and two kinds of inference:

- In the unsupervised case, even if the quantities ${\green D_1}, {\green D_2}, {\green \dotsc}, {\green D_N}$  in the known instances and in the new instance ${\red D_{N+1}}$ might consist of joint or complex quantities ([chapter @sec-quantities-types-multi]), we are not interested in their possible decomposition into component quantities. So we still have the tentative connection above:

    $$
    \P(\underbracket[0ex]{\red D_{N+1} \mo d_{N+1}}_{\mathclap{\red\text{outcome?}}} \| 
    \green\underbracket[0ex]{ D_N \mo d_N \and \dotsb  \and  D_1 \mo d_1}_{\mathclap{\green\text{training data?}}} 
    \black\and \underbracket[0ex]{\yellow\yI}_{\mathrlap{\yellow\uparrow\ \text{architecture?}}})
    $$

    This is the kind of inference explored in the "next-three-patients" scenario of [§ @sec-conditional-joint-sim]. As another example or scenario, the agent may have been given information about a collection of images, and then tries to guess what the next image could be.


- In the supervised case, the quantities in the known instances and in the new application are joint quantities:

    $$
	\begin{gathered}
	{\green D_N} = ({\green Y_N}, {\green X_N}),
	\ {\green\dotsc},\
	{\green D_1} = ({\green Y_1}, {\green X_1})
	\\[1ex]
	{\red D_{N+1}} = ({\red Y_{N+1}}, {\green X_{N+1}})
	\end{gathered}
	$$
	
	and we are interested in the $X$ and $Y$ component quantities separately. For instance, the $X$-quantity could be a 128 × 128-pixel image, as in the example above, and the $Y$-quantity could be a name whose domain is all characters in the [One Punch Man](https://onepunchman.fandom.com) series: $\set{\cat{Saitama}, \cat{Genos}, \cat{Fubuki}, \cat{MetalBat}, \dotsc}$. The reason we make this separation is that, upon applying the algorithm in a new task instance, *one of these component quantities*, say $\red X_{N+1}$, *can actually be observed by the agent*; so it is known. It's the other component quantity, $\red Y_{N+1}$, that the agent is uncertain about. The $X$ quantities are often called "features"; and the $Y$ quantities, "labels" (or "classes"). The agent therefore needs to draw the following inference:

    $$
    \P\bigl(
	{\red Y_{N+1} \mo y_{N+1}}
	\pmb{\|[\big]} 
	{\green X_{N+1} \mo x_{N+1}}\, \and\,
    \green Y_N \mo y_N \and X_N \mo x_N \and
	\dotsb \and 
	Y_1 \mo y_1 \and X_1 \mo x_1 
    \black\and {\yellow\yI} \bigr)
    $$
	
	Or, to give an example with a pictorial representation:
	
	![](saitama_example.png){width=75%}
    
    This is the more complicated version of the "next-three-patients" scenario with "urgency & transportation", discussed in [§ @sec-conditional-joint-general].

An interesting aspect of this new tentative correspondence is that there isn't any essential difference between unsupervised and supervised cases. Consider the last probability above; using the formula for a conditional probability ([§ @sec-conditional-probs]) we have

:::{.column-page-right}
$$
\begin{aligned}
    &\P\bigl(
	{\red Y_{N+1} \mo y_{N+1}}
	\pmb{\|[\big]} 
	{\green X_{N+1} \mo x_{N+1}}\, \and\,
    \green Y_N \mo y_N \and X_N \mo x_N \and
	\dotsb \and 
	Y_1 \mo y_1 \and X_1 \mo x_1 
    \black\and {\yellow\yI} \bigr)
	\\[2ex]
	&\qquad{}=
	\frac{
	    \P\bigl(
	\grey\overbracket[1px]{\vphantom{\Bigl(}
	\red Y_{N+1} \mo y_{N+1} \and
	\green X_{N+1} \mo x_{N+1}
	}^\mathclap{{\red D_{N+1}\mo d_{N+1}}}
	\black
		\and
		\grey\overbracket[1px]{\vphantom{\Bigl(}
    \green Y_N \mo y_N \and X_N \mo x_N
		}^\mathclap{{\green D_{N}\mo d_{N}}}\green
	\and
	\dotsb \and 
		\grey\overbracket[1px]{\vphantom{\Bigl(}
	\green Y_1 \mo y_1 \and X_1 \mo x_1 
		}^\mathclap{{\green D_{1}\mo d_{1}}}
    \black\pmb{\|[\big]} {\yellow\yI} \bigr)
}{
	 \sum_{\red y} \P\bigl(
	{\red Y_{N+1} \mo y} \and
	{\green X_{N+1} \mo x_{N+1}}
		\and
    \green Y_N \mo y_N \and X_N \mo x_N \and
	\dotsb \and 
	Y_1 \mo y_1 \and X_1 \mo x_1 
    \black\pmb{\|[\big]}  {\yellow\yI} \bigr)
}
\end{aligned}
$$
:::

<!-- :::{.column-page-right} -->
<!-- $$ -->
<!-- \begin{aligned} -->
<!--     &\P\bigl( -->
<!-- 	{\red Y_{N+1} \mo y_{N+1}} -->
<!-- 	\pmb{\|[\big]}  -->
<!-- 	{\red X_{N+1} \mo x_{N+1}}\, \and\, -->
<!--     \green Y_N \mo y_N \and X_N \mo x_N \and -->
<!-- 	\dotsb \and  -->
<!-- 	Y_1 \mo y_1 \and X_1 \mo x_1  -->
<!--     \black\and {\yellow\yI} \bigr) -->
<!-- 	\\[2ex] -->
<!-- 	&\qquad{}= -->
<!-- 	\frac{ -->
<!-- 	    \P\bigl( -->
<!-- 	{\red Y_{N+1} \mo y_{N+1}} \and -->
<!-- 	{\red X_{N+1} \mo x_{N+1}} -->
<!-- 	\black -->
<!-- 		\pmb{\|[\big]}  -->
<!--     \green Y_N \mo y_N \and X_N \mo x_N -->
<!-- 	\and -->
<!-- 	\dotsb \and  -->
<!-- 	\green Y_1 \mo y_1 \and X_1 \mo x_1  -->
<!--     \black\and {\yellow\yI} \bigr) -->
<!-- }{ -->
<!-- 	 \sum_{\red y_{N+1}} \P\bigl( -->
<!-- 	{\red Y_{N+1} \mo y_{N+1}} \and -->
<!-- 	{\red X_{N+1} \mo x_{N+1}} -->
<!-- 		\pmb{\|[\big]}  -->
<!--     \green Y_N \mo y_N \and X_N \mo x_N \and -->
<!-- 	\dotsb \and  -->
<!-- 	Y_1 \mo y_1 \and X_1 \mo x_1  -->
<!--     \black\and {\yellow\yI} \bigr) -->
<!-- } -->
<!-- \\[2ex] -->
<!-- &\P\bigl(\blue -->
<!-- 	{Y_{\infty} \mo y_{\infty}} -->
<!-- 	\and -->
<!-- 	{X_{\infty} \mo x_{\infty}}\, \and\, -->
<!-- 	\dotsb \and  -->
<!--     Y_2 \mo y_2 \and X_2 \mo x_2 \and -->
<!-- 	Y_1 \mo y_1 \and X_1 \mo x_1  -->
<!--     \black\and {\yellow\yI} \bigr) -->
<!-- \\[2ex] -->
<!-- &f\bigl(\blue -->
<!-- 	{Y_{\infty} \mo y_{\infty}} -->
<!-- 	\and -->
<!-- 	{X_{\infty} \mo x_{\infty}}\, \and\, -->
<!-- 	\dotsb \and  -->
<!--     Y_2 \mo y_2 \and X_2 \mo x_2 \and -->
<!-- 	Y_1 \mo y_1 \and X_1 \mo x_1  -->
<!--     \black \bigr) -->
<!-- \\[2ex] -->
<!-- &f\bigl(\blue -->
<!-- 	{Y_{1000} \mo y_{1000}} -->
<!-- 	\and -->
<!-- 	{X_{1000} \mo x_{1000}}\, \and\, -->
<!-- 	\dotsb \and  -->
<!--     Y_2 \mo y_2 \and X_2 \mo x_2 \and -->
<!-- 	Y_1 \mo y_1 \and X_1 \mo x_1  -->
<!--     \black \bigr) -->
<!-- \end{aligned} -->
<!-- $$ -->
<!-- ::: -->

This is a fraction between the probability for the "unsupervised case" (which we wrote above compactly in terms of $D$ rather than $(Y,X)$), and the same probability summed up for all possible values of $\red Y_{N+1}$, which in the image-example above would be:

![](cond_prob_saitama.png){width=50%}

with a sum ($\red y$) over all possible character names of the series.



So if the agent has the probability for the "unsupervised case", then with a simple computation it also has the probability for the "supervised case", and many other cases that cannot be categorized as "unsupervised" or "supervised" (there is also a connection with "generative" vs "discriminative" algorithms here).

\

We seem to have found a formula that should lie at the core of any machine-learning algorithm: the **formula for conditional probability**, which covers all possible inference cases: with or without features, with or without previous data, and many other cases. If we want that an inference algorithm which "learns" from example data, and possibly use features, shall be optimal, then it must operate on the conditional-probability formula in one way or another -- at least approximately.

What does such an optimal algorithm require, in order to operate in full? From the discussion of [chapter @sec-learning] we see that it requires the following:

- [{{< fa cubes >}} A joint probability distribution about all quantities representing features and instances; past, present, and future.]{.yellow}

    This joint probability distribution also expresses what kind of background knowledge the algorithm assumes, and therefore the kind of problems it can be applied to. It thus forms the "architecture" and "internal parameters" of the algorithm. For example, in [§ @sec-conditional-joint-sim] and its exercises we examined a joint distribution expressing the belief that "the more often a specific case is seen, the more probable it is at the next instance"; and also another distribution expressing the opposite belief: "the more often a specific case is seen, the *less* probable it is at the next instance".

- [{{< fa arrow-right-to-bracket >}} Data from previous instances, that is, "training" or "learning" data.]{.green}

- [{{< fa arrow-right-to-bracket >}} Optionally, some quantities or "feature" data about the present instance.]{.lightblue}

Of these three requirements, the first seems the most difficult to implement. How to *encode* a belief distribution about a possibly infinite number of quantities, some of which may refer to the future? And how to choose such a belief distribution in a reasonable way?

This is what we discuss in the next chapters.



<!-- ---- -->

<!-- :::{.columns} -->

<!-- ::::{.column width=25%} -->
<!-- :::: -->

<!-- ::::{.column width=50%} -->
<!-- ![](steven-seagal-emotion-chart.jpg){width=100%} -->
<!-- [Example of dataset in which the same input (facial expression) is associated with different outputs (mood labels)]{.small} -->
<!-- :::: -->

<!-- ::::{.column width=25%} -->
<!-- :::: -->
<!-- ::: -->
