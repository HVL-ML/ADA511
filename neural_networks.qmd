# Neural networks
{{< include macros.qmd >}}

Neural networks are performing extremely well on complex tasks such as language modelling and realistic image generation, but the principle behind how they work, is quite simple. 

The basic building block is a **node**, which receives some values as input, and computes a single output:


![](fig_neural_network_node.png){width=400 fig-align="center" #fig-neural-network-node}

The output is computed from the inputs $x_0, x_1, \dots, x_N$, each of which is multiplied by a weight $w_1, w_2, \dots w_N$, and summed together along with an additional parameter $b$, which is typically called a *bias*. You can probably identify this step as good old linear regression:

$$
a = b + \sum_{i=0}^N w_i x_i \,.
$$

A key property to neural networks, however, is to introduce *nonlinear* relationships. This is done by evaluating the output from each node by an **activation function** $h$. The final output $z$ from the node is then

$$
z = h(a) = h \left( b + \sum_{i=0}^N w_i x_i \right)
$$

The activation function is typically rather simple -- the most popular is the rectified linear unit (ReLU), which propagates positive values but sets all negative values to zero:

$$
\text{ReLU}(x) = \max(0, x) \,.
$$

