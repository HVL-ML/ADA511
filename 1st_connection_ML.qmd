# [Second connection with machine learning]{.red} {#sec-1st-connection-ML}
{{< include macros.qmd >}}
{{< include macros_prob_inference.qmd >}}

In these first chapters we have been developing notions and methods about agents drawing inferences and making decisions, sentences expressing facts and information, and probabilities expressing uncertainty and certainty. Let's draw some first qualitative connections between these notions and notions typically used in machine learning.

A machine-learning algorithm is usually presented in textbooks as something that first "learns" from some training data, and thereafter performs some kind of task -- typically it yields a response or outcome of some kind. More precisely, the training data are instances or examples of the task that the algorithm is expected to perform. These instances have a special status because their details are fully known, whereas new instances, where the algorithm will be applied, have some uncertain elements: typically they have an ideal or optimal outcome, but this outcome is unknown beforehand. The response given by the algorithm in new instances depend on the algorithm's internal architecture and parameters; for brevity we shall just use "architecture" to mean both.

Let's try to rephrase this description from the point of view developed in the past chapters. A machine-learning algorithm is given [known pieces of information (the training data)]{.green}, and then forms some kind of connection with [another piece of information of a similar kind (the outcome in a new application) that was not known beforehand]{.red}. The connection depends on the [algorithm's architecture]{.yellow}.

## "Learning" and "output" from the point of view of inference & decision {#sec-1stconn-inference}

The remarks above reveal similarities with what an agent does when drawing an inference: it uses known pieces of information, expressed by sentences ${\green\se{D}_1}, {\green\se{D}_2}, {\green\dots}, {\green\se{D}_N}$, together with some background or built-in information $\yellow\yI$, in order to calculate the probability of a piece of information of a similar kind, expressed by a sentence $\red\se{D}_{N+1}$:

$$
\P(\red\se{D}_{N+1}\black \| 
\green\se{D}_{N} \land \dotsb \land \se{D}_2 \land \se{D}_1 \black\land {\yellow\yI})
$$


We can thus consider a first *tentative* correspondence:

$$
\P(\underbracket[0ex]{\red\se{D}_{N+1}}_{\mathclap{\red\text{outcome?}}} \| 
\green\underbracket[0ex]{\se{D}_N \land \dotsb \land \se{D}_2 \land \se{D}_1}_{\mathclap{\green\text{training data?}}} 
\black\land \underbracket[0ex]{\yellow\yI}_{\mathrlap{\yellow\uparrow\ \text{architecture?}}})
$$

This correspondence seems convincing with regard to [architecture]{.yellow} and [training data]{.green}: in both cases we're speaking about the use of pre-existing or built-in information, combined with additional one.

But it is less convincing with regarding to the [outcome]{.red}, because an agent gives the probabilities for several possible "outputs", it doesn't just yield one. This indicates that there must be also be some **decision** involved among the possible outcomes.

We'll return to this tentative connection later.


## Why different outputs? {#sec-1stconn-outputs}

In the past chapters we have seen, over and over, what was claimed in the introduction to the present lecture notes: that an inference & decision problem has only one optimal solution. Once we specify the utilities and the initial probabilities of the problem, the fundamental rules of inference and the principle of maximal expected utility leads to one unique answer (unless, of course, there are several optimal ones with equal expected utilities).

Different machine-learning algorithms, trained with the same training data, often give different answers or outputs to the same problem. Where do these differences come from? From the point of view of decision theory there are three possibilities, which don't exclude one another:

- [The initial probabilities given to the algorithms are different.]{.yellow} Since the training data are the same, this means that the **background information built into** one machine-learning algorithm is different from those built into another.

    It is therefore important to [*understand what are the built-in background information and initial probabilities*]{.blue} of different machine-learning algorithms. The built-in assumptions of an algorithm must match those of the real problem as closely as possible, in order to avoid sub-optimal or even disastrously wrong answers and outputs.

- [The **utilities built into** one machine-learning algorithm are different]{.yellow} from those built into another.

    It is therefore also important to [*understand what are the built-in utilities*]{.blue} of different machine-learning algorithms. The built-in utilities must also match those of the real problem as closely as possible.

- [The calculations made by the algorithms are approximate]{.yellow}, and different algorithms make different kinds of approximations. This means that the algorithms don't arrive at the unique answer determined by decision theory, but to some other answers which may be approximately close to the correct one and to one another -- or not.

    It is therefore important to [*understand what are the calculation approximations*]{.blue} made by different machine-learning algorithms. Some approximations may be too crude for some real problems, and may again lead to sub-optimal or even disastrously wrong answers and outputs.
