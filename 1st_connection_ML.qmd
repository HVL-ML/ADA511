# A first connection with machine learning {#sec-1st-connection-ML}
{{< include macros.qmd >}}
{{< include macros_prob_inference.qmd >}}

In these first chapters we have been developing notions and methods about agents having to draw inferences and making decisions, sentences expressing facts and information, and probabilities expressing uncertainty and certainty. Let's draw some first qualitative connections between these notions and notions typically used in machine learning.

A machine-learning algorithm is usually presented in textbooks as something that first "learns" from some training data, and thereafter performs some kind of task -- typically it yields a response or outcome of some kind.

Putting this description in more general terms, such an algorithm uses some [given, known pieces of information (the training data)]{.green}, and then forms some kind of connection with [another piece of information of a similar kind (the outcome) that was not known beforehand]{.red}. The connection depends on the [algorithm's architecture and internal design (often called "model")]{.yellow}.

This has strong similarities to what an agent does when drawing an inference: it uses some known pieces of information, expressed by sentences ${\green\se{D}_1}, {\green\se{D}_2}, {\green\dots}, {\green\se{D}_N}$, together with some background or built-in information $\yellow\yI$; and then calculates the probability of a piece of information of a similar kind, expressed by a sentence $\red\se{D}_{N+1}$:

$$
\P(\red\se{D}_{N+1}\black \| 
\green\se{D}_{N} \land \dotsb \land \se{D}_2 \land \se{D}_1 \black\land {\yellow\yI})
$$


We thus see a *tentative* correspondence here:

$$
\P(\underbracket[0ex]{\red\se{D}_{N+1}}_{\mathclap{\red\text{outcome?}}} \| 
\green\underbracket[0ex]{\se{D}_N \land \dotsb \land \se{D}_2 \land \se{D}_1}_{\mathclap{\green\text{training data?}}} 
\black\land \underbracket[0ex]{\yellow\yI}_{\mathrlap{\yellow\uparrow\ \text{architecture?}}})
$$

This correspondence is surely spot-on regarding [architecture]{.yellow} and [training data]{.green}: in both cases we're speaking about the use of pre-existing or built-in information, combined with additional one.

But the correspondence can't be completely correct regarding the [outcome]{.red}, because an agent gives the probabilities for several possible "outputs", it doesn't just yield one. This indicates that there must be also be some **decision** involved among the possible outcomes.

We'll return to this connection later.
