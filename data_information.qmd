# Data and information
{{< include macros.qmd >}}

Most decisions and inferences in engineering and data science involve quantities or entities with some kind of mathematical properties: they can be expressed by a number or -- think of images or network graphs -- by collections of numbers. The sentences that appear in decision-making and inferences are therefore often of the kind "the quantity $X$ was measured to have value $x$". We commonly refer to these values as "data".

Data come in many different kinds, with different properties. Some statements only make sense with particular kinds of data. We therefore pay a quick visit to the data zoo, emphasizing aspects that are important for inference and decision-making.

## Probability of data values

We shall speak of "quantities", denoting them by letters such as $X$. A quantity can be a physical quantity, such as a length, power output, velocity, force; or something more complex like an image, video clip, 3D scan, or a network graph with nodes and links.

A quantity has a value that must belong to a given set. For instance, a length can have values in the positive real numbers; a force can have values in a 3D vector space; a colour image can have values in the set of all possible $N\times N$ grids of three numbers each, with the numbers in the range [$[0,1]$.]{.together}

We take it for granted that a quantity does have one value, and one value only (even if the value itself can consist of several numbers, for example).

When we are uncertain about the value of a quantity, we assign a degree of belief to all possible cases. For a length, the cases could be "[The length is measured to have value 0.1 m]{.midgrey}", "[The length is measured to have value 0.2 m]{.midgrey}", and so on. We can abbreviate them as
$$
L = \mathrm{0.1\,m} \ , \qquad
L = \mathrm{0.2\,m} \ , \qquad
\dotsc
$$
(in practice the number of possible cases is always finite; but we'll get back to this point later). We recognize these as *mutually exclusive* and *exhaustive* sentences.

Our belief about the variable is then expressed by a collection of probabilities:
$$
\P(L \mo \mathrm{0.1\,m} \| \yI) \ , \quad
\P(L \mo \mathrm{0.2\,m} \| \yI) \ , \quad
\dotsc
$$
that sum up to one:
$$
\P(L \mo \mathrm{0.1\,m} \| \yI) +
\P(L \mo \mathrm{0.2\,m} \| \yI) +
\dotsb
= 1
$$
This collection is called a [**probability distribution**]{.blue}, because we are distributing a unit amount of probability among several sentences.

:::{.callout-caution}
## {{< fa user-edit >}} Exercise
Consider three sentences $\yX_1, \yX_2, \yX_3$ that are mutually exclusive and exhaustive on conditional [$\yI$,]{.together} that is:
$$
\begin{gathered}
\P(\yX_1 \land \yX_2 \| \yI) =
\P(\yX_1 \land \yX_3 \| \yI) =
\P(\yX_2 \land \yX_3 \| \yI) = 0
\\
\P(\yX_1 \lor \yX_2 \lor \yX_3 \| \yI) = 1
\end{gathered}
$$
Prove, using the fundamental rules of inferences and any derived rules from [§ @sec-probability], that we must then have
$$
\P(\yX_1 \| \yI) + \P(\yX_2 \| \yI) + \P(\yX_3 \| \yI) = 1
$$
:::

## Kinds of data

### Binary

### Nominal

### Ordinal

### Continuous

* unbounded

* bounded

* censored

### Complex data

2D, 3D, images, graphs, etc.

### "Soft" data

* orders of magnitude

* physical bounds

## Data transformations

* log

* probit

* logit


