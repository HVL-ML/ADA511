# Probability distributions
{{< include macros.qmd >}}

[*(Make sure you're familiar with [§ @sec-data-types] before you begin.)*]{.small style="color: #EE6677;"}

## Distribution of probabilities among values

When an agent is uncertain about what the value of a quantity is, this uncertainty is expressed and quantified by assigning a degree of belief to all the possible cases, conditional on the agent's knowledge. For a temperature measurement, for instance, the cases could be "[The temperature is measured to have value 270 K]{.midgrey}", "[The temperature is measured to have value 271 K]{.midgrey}", and so on. We can abbreviate these sentences, denoting the temperature with $T$, as
$$
T = 270\,\mathrm{m} \ , \qquad
T = 271\,\mathrm{m} \ , \qquad
T = 272\,\mathrm{m} \ , \qquad
\dotsc
$$
We recognize these as *mutually exclusive* and *exhaustive* sentences.

Our belief about the quantity is then expressed by a collection of probabilities, conditional on the agent's state of knowledge $\yI$:
$$
\P(T \mo 270\,\mathrm{K} \| \yI) \ , \quad
\P(T \mo 271\,\mathrm{K} \| \yI) \ , \quad
\P(T \mo 272\,\mathrm{K} \| \yI) \ , \quad
\dotsc
$$
that sum up to one:
$$
\P(T \mo 270\,\mathrm{K} \| \yI) +
\P(T \mo 271\,\mathrm{K} \| \yI) +
\P(T \mo 272\,\mathrm{K} \| \yI) +
\dotsb
= 1
$$
This collection of probabilities is called a [**probability distribution**]{.blue}.

:::{.callout-warning}
## {{< fa exclamation-circle >}} What's "distributed"?
It's the *probability* that's distributed among the possible values, not the quantity, as illustrated in the side picture. The quantity cannot be "distributed": it has one, definite value, which is however unknown to us.
:::

::::{.column-margin}
![](illustration_prob_distr2.png)
::::


:::{.callout-caution}
## {{< fa user-edit >}} Exercise
Consider three sentences $\yX_1, \yX_2, \yX_3$ that are mutually exclusive and exhaustive on conditional [$\yI$,]{.together} that is:
$$
\begin{gathered}
\P(\yX_1 \land \yX_2 \| \yI) =
\P(\yX_1 \land \yX_3 \| \yI) =
\P(\yX_2 \land \yX_3 \| \yI) = 0
\\
\P(\yX_1 \lor \yX_2 \lor \yX_3 \| \yI) = 1
\end{gathered}
$$
Prove, using the fundamental rules of inferences and any derived rules from [§ @sec-probability], that we must then have
$$
\P(\yX_1 \| \yI) + \P(\yX_2 \| \yI) + \P(\yX_3 \| \yI) = 1
$$
:::

Let's see how probability distributions can be represented and visualized. We start with probability distributions over discrete values.

## Representation of discrete probability distributions

A probability distribution over a discrete set of values can obviously be displayed in a table of values and their probabilities. For instance

|*value*| 270 K | 271 K | 272 K | ...|
|-|:-:|:-:|:-:|:-:|
|*probability*|0.1|0.2|0.5|...|




* Density function

* Histogram

* Scatter plot

Behaviour of representations under transformations of data.



## Summaries of distributions of probability

### Location

Median, mean

### Dispersion or range

Quantiles & quartiles, interquartile range, median absolute deviation, standard deviation, half-range

### Resolution

Differential entropy

### Behaviour of summaries under transformations of data and errors in data



## Outliers and out-of-population data

(Warnings against tail-cutting and similar nonsense-practices)



## Marginal and conditional distributions of probability



## Collecting and sampling data

### "Representative" samples

Size of minimal representative sample = (2^entropy)/precision

* _Exercise: data with 14 binary variates, 10000 samples_

### Unavoidable sampling biases

In high dimensions, all datasets are outliers.

Data splits and cross-validation cannot correct sampling biases



## Quirks and warnings about high-dimensional data





