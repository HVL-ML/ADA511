# Probability distributions


## Probability of data values

We shall speak of "quantities", denoting them by letters such as $X$. A quantity can be a physical quantity, such as a length, power output, velocity, force; or something more complex like an image, video clip, 3D scan, or a network graph with nodes and links.

A quantity has a value that must belong to a given set. For instance, a length can have values in the positive real numbers; a force can have values in a 3D vector space; a colour image can have values in the set of all possible $N\times N$ grids of three numbers each, with the numbers in the range [$[0,1]$.]{.together}

We take it for granted that a quantity does have one value, and one value only (even if the value itself can consist of several numbers, for example).

When we are uncertain about the value of a quantity, we assign a degree of belief to all possible cases. For a length, the cases could be "[The length is measured to have value 0.1 m]{.midgrey}", "[The length is measured to have value 0.2 m]{.midgrey}", and so on. We can abbreviate them as
$$
L = \mathrm{0.1\,m} \ , \qquad
L = \mathrm{0.2\,m} \ , \qquad
\dotsc
$$
(in practice the number of possible cases is always finite; but we'll get back to this point later). We recognize these as *mutually exclusive* and *exhaustive* sentences.

Our belief about the variable is then expressed by a collection of probabilities:
$$
\P(L \mo \mathrm{0.1\,m} \| \yI) \ , \quad
\P(L \mo \mathrm{0.2\,m} \| \yI) \ , \quad
\dotsc
$$
that sum up to one:
$$
\P(L \mo \mathrm{0.1\,m} \| \yI) +
\P(L \mo \mathrm{0.2\,m} \| \yI) +
\dotsb
= 1
$$
This collection is called a [**probability distribution**]{.blue}, because we are distributing a unit amount of probability among several sentences.

:::{.callout-caution}
## {{< fa user-edit >}} Exercise
Consider three sentences $\yX_1, \yX_2, \yX_3$ that are mutually exclusive and exhaustive on conditional [$\yI$,]{.together} that is:
$$
\begin{gathered}
\P(\yX_1 \land \yX_2 \| \yI) =
\P(\yX_1 \land \yX_3 \| \yI) =
\P(\yX_2 \land \yX_3 \| \yI) = 0
\\
\P(\yX_1 \lor \yX_2 \lor \yX_3 \| \yI) = 1
\end{gathered}
$$
Prove, using the fundamental rules of inferences and any derived rules from [§ @sec-probability], that we must then have
$$
\P(\yX_1 \| \yI) + \P(\yX_2 \| \yI) + \P(\yX_3 \| \yI) = 1
$$
:::



## The difference between Statistics and Probability Theory

_Statistics_ is the study of collective properties of collections of data. It does not imply that there is any uncertainty.

_Probability theory_ is the quantification and propagation of uncertainty. It does not imply that we have collections of data.



## What's "distributed"?

Difference between distribution of probability and distribution of (a collection of) data.



## Distributions of probability

### Representations

* Density function

* Histogram

* Scatter plot

Behaviour of representations under transformations of data.



## Summaries of distributions of probability

### Location

Median, mean

### Dispersion or range

Quantiles & quartiles, interquartile range, median absolute deviation, standard deviation, half-range

### Resolution

Differential entropy

### Behaviour of summaries under transformations of data and errors in data



## Outliers and out-of-population data

(Warnings against tail-cutting and similar nonsense-practices)



## Marginal and conditional distributions of probability



## Collecting and sampling data

### "Representative" samples

Size of minimal representative sample = (2^entropy)/precision

* _Exercise: data with 14 binary variates, 10000 samples_

### Unavoidable sampling biases

In high dimensions, all datasets are outliers.

Data splits and cross-validation cannot correct sampling biases



## Quirks and warnings about high-dimensional data





