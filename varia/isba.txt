Machine Learning and Artificial Intelligence are developing extremely fast, they are increasingly used in ethically sensitive domains such as medicine, and see many possibilities of improvement with near-future technologies. When factors such as these converge, one rediscovers the importance of the scientific foundations of a field.


Bachelor and master students in data science or computer science with an 


Machine Learning and Artificial Intelligence are developing so fast that bachelor and master students must


We tried to design the course to cater for students with a background in computer science


Most present-day machine-learning algorithms are essentially doing "nonparametric density regression" (which includes parametric modelling as a special case).

The course approaches probability theory from the point of view of Johnson, Jeffreys, Cox, Jaynes, Hailperin, with sentences or propositions as the carrier of knowledge and information. This fits well

We took sentences or propositions as the building blocks, Ã  la Johnson, Jeffreys, Cox, Jaynes, Hailperin, Jeffrey. They are particularly suited to an artificial-intelligence approach because they can flexibly represent information, hypotheses, and decisions.

The introduction of Bayesian probability \&\ decision theory appears very natural from an artificial-intelligence perspective. The students wants to build an AI agent that has to make inferences and decisions under uncertainty.

Probability emerges as a necessary notion to quantify the agent's uncertainty or degree of belief. From this perspective the students readily grasped the difference between probability and frequency, and the necessity of initial degrees of belief (prior) to "jump-start" the agent.


The basic rules of coherence or Cox's axioms appears
