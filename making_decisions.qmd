# [From inferences to decisions]{.blue} {#sec-from-inf-to-dec}
{{< include macros.qmd >}}
{{< include macros_decisions.qmd >}}

## Accept or discard? -- again

Our very first example of engineering problem ([chapter @sec-intro]) in these lecture notes was the decision whether to accept or discard an electronic component just produced in an assembly line. In [chapter @sec-basic-decisions] we met the theory to deal with this kind of decision-making problem: Decision Theory.

We saw that Decision Theory requires the agent, who's making the decision, to calculate the probabilities of the unknown outcomes. All our discussion and chapters thereafter focused on the calculation of those probabilities, which often is the most difficult part of the decision making task.

At long last we have the knowledge to calculate such probabilities, both for general problems, and more in detail problems enjoying special properties, for instance when the agent's beliefs are exchangeable ([chapter @sec-exchangeable-beliefs]). Most important, we saw how previous data and background information can be used to determine these probabilities, and how they influence and modify these probabilities. We have even built a real, prototype agent that can flexibly calculate probabilities in problems involving nominal variates.

Let's face the decision-making problem again, and complete the construction of our prototype agent by implementing the final decision-making step.^[Please go back to [chapter @sec-basic-decisions] and review the notions and terminology introduced there.]


## Outcomes, decisions, utilities {#sec-decisions-utilities}

Recall the structure of a basic decision:

![](basic_decision_tree.png){width=100%}

In order to make a decision, the agent needs:

- {{< fa cube >}} The set of possible [*decisions*]{.yellow}, which we represent as sentences like $\yD$.

- {{< fa cube >}} The set of possible [*outcomes*]{.red}, whose truth is unknown to the agent. In the kind of decision problems that we are examining, the outcomes correspond to the possible values $\red Y\mo y$ of what we have called the [predictand]{.red} variate from [§ @sec-cat-problems] onwards.


- {{< fa cube >}} The [*probabilities*]{.green} of the outcomes\ \ $\P({\red Y\mo y} \| \yD \and {\green X\mo x} \and \data \and \yI)$ which are determined by the agent's background information $\yI$, by any other available information, such as $\data$ about previously observed units and the values of some predictor variates $\green X\mo x$ for the current unit; and possibly also by the agent's decision $\yD$ (see below).

- {{< fa cube >}} The [*utilities*]{.blue} of the decisions and the outcomes, which we denote
    
    $$
\uu({\red Y\mo y} \and \yD \| \yI)
$$
    
	This notation reminds us that the utilities assigned by the agent also depend on its background information $\yI$

Let's not forget some important points about the notions above:

:::{.callout-important}
## {{< fa exclamation-triangle >}}

- We are not assuming any temporal or "causal" relationship between [decisions]{.yellow} and [outcomes]{.red}: **our framework works independently of these relationships**. In some decision-making problems the outcomes happen *after* a decision is made, and may be "influenced" by it or not. In other decision-making problems the outcomes have already happened *before* a decision is made.
    
	In a transportation-choice problem, for instance, the outcome "[wet from rain]{.red}" may happen after we make the decision "[go on foot]{.yellow}" rather than "[go by bus]{.yellow}". In an image-classification problem, on the other hand, the outcome "[true label is $\cat{cat}$]{.red}" was already determined before we made the decision "[classify as $\cat{dog}$]{.yellow}".

\

- In connection with the warning above, in some problems the probabilities of the outcomes may depend on the decision; that is, knowledge about the decision is *relevant* to the knowledge about the outcome (see again [§ @sec-info-chapter]):
    
	$$\P({\red Y\mo y} \| \yD \and {\green X\mo x} \and \data \and \yI)$$
	
	For instance, in the transportation-choice problem the probability of the outcome "[wet from rain]{.red}" is higher conditional on the decision "[go on foot]{.yellow}" than on the decision "[go by bus]{.yellow}".
	
	In many decision-making problems typical of machine learning, such as classification, information about the decision is *irrelevant* to the information about the outcome (which usually has already happened), and we have
	
	$$
	\P({\red Y\mo y} \| \yD \and {\green X\mo x} \and \data \and \yI)
	= \P({\red Y\mo y} \| {\green X\mo x} \and \data \and \yI)
	$$

:::

In what follows we shall consider problems, such as classification, where knowledge of the agent's decision is irrelevant to the outcome's probability. We shall nevertheless keep the more general notation\ \ $\P({\red Y\mo y} \| \yD \and {\green X\mo x} \and \data \and \yI)$.


## Maximization of expected utility {#sec-max-exp-utilities}

The expected utility of each possible decision $\yD$ is calculated as a weighted average:

$$\uu(\yD \|\yI) 
= \sum_{\red y} \uu(\yD \and {\red Y\mo y} \| \yI) \cdot
\P({\red Y\mo y} \| \yD \and {\green X\mo x} \and \data \and \yI)
$$

The agent's final decision satisfies the

:::::{.column-page-right}
:::{.callout-note style="font-size:120%"}
## [**Principle of maximal expected utility**]{style="font-size:110%"}
::::{style="font-size:120%"}

The optimal decision, which should be made by the agent, is the one with maximal expected utility:

$$
\yD_{\text{optimal}} =
\argmax_{\yD} \sum_{\red y} \uu(\yD \and {\red Y\mo y} \| \yI) \cdot
\P({\red Y\mo y} \| \yD \and {\green X\mo x} \and \data \and \yI)
$$

(where, in some tasks, the probabilities may not depend on $\yD$)

::::
:::
:::::

In the formula above, "$\argmax\limits_z G(z)$" is the value $z$ which maximizes $G(z)$. [Note the difference: $\max\limits_z G(z)$ is the value of the maximum itself, whereas $\argmax\limits_z G(z)$ is the value of the *argument* that gives the maximum. For instance\ \ $\max\limits_z (1-z)^2 = 0$,\ \ but\ \ $\argmax\limits_z (1-z)^2 = 1$.]{.small .grey}

It may happen that there are several decisions which have equal, maximal expected utilities. In this case any one of them can be chosen. A useful strategy, in this case, is to choose among them with equal probabilities (such strategy helps minimizing the loss from possible small errors in the specification of the utilities, or from the presence of an antagonist agent which tries to predict what our agent is doing).


### Numerical implementation in simple cases

The principle of maximal expected utility is straightforward to calculate in many important problems. First, the collection of expected utilities can be obtained with a simple matrix multiplication. We represent the utilities for all decisions and all outcomes with a matrix $\um$, having one row per decision and one column per outcome:

$$
\um \defd
\begin{bmatrix}
\uu(\yD' \and {\red Y\mo y}' \| \yI)
&
\uu(\yD' \and {\red Y\mo y}'' \| \yI)
& \dotso
\\
\uu(\yD'' \and {\red Y\mo y}' \| \yI)
&
\uu(\yD'' \and {\red Y\mo y}'' \| \yI)
& \dotso
\\
\dotso&\dotso&\dotso
\end{bmatrix}
$$

If the probabilities of the outcomes do **not** depend on the decisions, we represent them as a column matrix $\Pm$, having one entry per outcome:

$$
\Pm \defd
\begin{bmatrix}
\P({\red Y\mo y}' \| \yD \and {\green X\mo x} \and \data \and \yI)
\\
\P({\red Y\mo y}'' \| \yD \and {\green X\mo x} \and \data \and \yI)
\\
\dotso
\end{bmatrix}
$$

Then the collection of expected utilities is a column matrix, having one entry per decision, given by the matrix product $\um \Pm$. All that's left is to check which of the entries in this final matrix is maximal.


## Concrete example: targeted advertisement {#sec-max-exp-util-example}

As a concrete example, let's keep on using the adult-income task from [chapter @sec-example-opm1], in a typical present-day scenario.

Some corporation, which offers a particular phone app, wants to [bombard its users with advertisements](https://pluralistic.net/2023/07/24/rent-to-pwn/), because advertisement generates much more revenue than making the users pay for the app. For each user the corporation can choose one among three ad-types, let's call them $\yA, \yB, \yC$. The revenue obtained from these ad-types varies depending on whether the target user's income is $\yl$ or $\yh$. A separate study run by the corporation has shown that the average revenues (per user per minute) depending on the three ad-types and the income levels are as follows:

:::{.columns}

::::{.column width="25%"}

::::

::::{.column width="50%"}

+:-------------------:+:-------------------:+--------------------:+--------------------:+
|                                           | $\income$                                 |
+                                           +---------------------+---------------------+
|                                           | $\yl$               |  $\yh$              |
+---------------------+---------------------+---------------------+---------------------+
| [ad-type]{.yellow}  |   $\yA$             | $\blue -1\,\$$      |  $\blue 3\,\$$      |
+                     +---------------------+---------------------+---------------------+
|                     |   $\yB$             | $\blue 2\,\$$       |  $\blue 2\,\$$      |
+                     +---------------------+---------------------+---------------------+
|                     |   $\yC$             | $\blue 3\,\$$       | $\blue -1\,\$$      |
+---------------------+---------------------+---------------------+---------------------+
: Revenue depending on ad-type and income level {#tbl-income .sm}

::::

::::{.column width="25%"}

::::
:::

Ad-type $\yB$ is a kind of neutral advertisement that leads to revenue independently of the target user's income. Ad-type $\yA$ targets high-income users, leading to higher revenue from them; but it leads to a loss if shown to the wrong target (more money spent on making and deploying the ad than what is gained from users' purchases). Vice versa, ad-type $\yB$ targets low-income users, with a reverse effect.

The corporation doesn't have access to its users' income levels, but it covertly collects, through some other app, all or some of the eight predictor variates $\green\mathit{workclass}$, $\green\mathit{education}$, $\green\dotsc$, $\green\mathit{sex}$, $\green\mathit{native\_country}$ from each of its users. The corporation has also access to the adult-income dataset (or let's say a more recent version of it).

In this scenario, the corporation would like to use an AI agent that can choose and show the optimal ad-type to each user.

\

Our prototype agent from chapters [-@sec-code-design], [-@sec-code-workflow], [-@sec-example-opm1] can be used for such a task. It has already been trained with the dataset, and can use any subset (possibly even empty) of the eight predictors to calculate the probability for the two income levels.

All that's left is to equip our prototype agent with a function that outputs the optimal decision, given the calculated probabilities and the set of utilities. In our code this is done by the function `decide()` described in [chapter @sec-code-workflow]:

:::{.small}

> {{< fa code >}} [`decide(probs, utils=NULL, all=FALSE)`](https://github.com/pglpm/ADA511/blob/master/code/OPM-nominal/decide.R)
> 
> Arguments:
> : - *`probs`*: a probability distribution for one or more variates.
>     - *`utils`*: a named matrix or array of utilities. The **rows** of the matrix correspond to the available decisions, the **columns** or remaining array dimensions correspond to the possible values of the predictand variates.
>     - *`all`*: a boolean, `TRUE` if the output should have all decisions having maximal expected utilities (if more than one); `FALSE` (default) if the output should only have one of them, selected with equal probabilities.
> 
> Output:
> : - a vector of strings with the decisions having maximal expected utility, if *`all`* is `TRUE`, or just one of them, selected with equal probability, if *`all`* is `FALSE` (the default).	
> 
> Notes:
> : - If *`utils`* is missing or `NULL`, a matrix of the form $\begin{bsmallmatrix}1&0&\dotso\\0&1&\dotso\\\dotso&\dotso&\dotso\end{bsmallmatrix}$ is assumed (which corresponds to using *accuracy* as evaluation metric).

:::

### Example

A new user logs in; all eight predictors are available for this user:

:::{.small}

$$\green
\begin{aligned}
&\mathit{workclass} \mo \cat{Private}
&& \mathit{education} \mo \cat{Bachelors} 
\\ & \mathit{marital\_status} \mo \cat{Never-married} 
&& \mathit{occupation} \mo \cat{Prof-specialty} 
\\ & \mathit{relationship} \mo \cat{Not-in-family} 
&& \mathit{race} \mo \cat{White} 
\\ & \mathit{sex} \mo \cat{Female} 
&& \mathit{native\_country} \mo \cat{United-States}
\end{aligned}
$$

:::

The agent calculates (using the `infer()` function) the probabilities for the two income levels, which turn out to be

:::{.small}

$$
\begin{aligned}
&\P({\red\income\mo\yl} \| \predictor, \data, \yI) = 83.3\%
\\&\P({\red\income\mo\yh} \| \predictor, \data, \yI) = 16.7\%
\end{aligned}
$$

:::

and can be represented by the column matrix

$$\Pm \defd
\begin{bmatrix}
0.833\\0.167
\end{bmatrix}
$$

The utilities previously given can be represented by the matrix

$$\um \defd
\begin{bmatrix}
-1&3\\2&2\\3&-1
\end{bmatrix}
$$

Multiplying the two matrices above we obtain the expected utilities of the three ad-types for the present user:

:::{.column-body-outset-right}

$$
\um \Pm = 
\blue\begin{bmatrix}
-1&3\\2&2\\3&-1
\end{bmatrix}
\,
\green\begin{bmatrix}
0.833\\0.167
\end{bmatrix}\black
=
\begin{bmatrix}
{\blue -1}\cdot{\green 0.833}
+ {\blue 3}\cdot{\green 0.167}
\\
{\blue 2}\cdot{\green 0.833}
+ {\blue 2}\cdot{\green 0.167}
\\
{\blue 3}\cdot{\green 0.833}
+ ({\blue -1})\cdot{\green 0.167}
\end{bmatrix}
=
\begin{bmatrix}
-0.332\\
2.000\\
\boldsymbol{2.332}
\end{bmatrix}
$$

:::

The highest expected utility is that of ad-type $\yC$, which is therefore shown to the user.

### Powerful flexibility of the agent

In the previous chapters we already emphasized and witnessed the flexibility of the optimal predictor machine with regard to the availability of the predictors: it can draw an inference even if some or all predictors are missing.

Now we can see another powerful kind of flexibility: the optimal predictor machine can in principle *use different sets of decisions and different utilities for each new application*. The decision criterion is not "hard-coded"; it can be customized on the fly.

The possible number of ad-types and the utilities could even be a function of the predictor values. For instance, there could be a set of three ad-types targeting users with $\green\mathit{education}\mo\cat{Bachelors}$, a different set of four ad-types targeting users with $\green\mathit{education}\mo\cat{Preschool}$, and so on.

\

## Generalization of basic decision theory {#sec-DT-generalization}

The simple decision-making problems and framework that we have discussed in these notes are only the basic blocks of Decision Theory. This theory covers more complicated decision problems. We only mention some examples:

- *Sequential decisions*


[{{< fa person-digging >}} *To be finished* {{< fa person-digging >}}]{.yellow}


<!-- ### Factors that enter utility quantification -->




<!-- ## Making decisions under uncertainty: maximization of expected utility -->







