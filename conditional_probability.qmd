# [Conditional probability distributions]{.green}
{{< include macros.qmd >}}
{{< include macros_marg_cond.qmd >}}

## Isn't all probability "conditional"? {#sec-all-prob-conditional}

When we introduced the notion of degree of belief -- a.k.a. probability -- in [chapter @sec-probability], we stressed the fact that *every probability is conditional on some state of knowledge or information*. So the term "conditional probability" sounds like a [pleonasm](https://dictionary.cambridge.org/dictionary/english/pleonasm), just like saying "round circle".

This term must be understood in a way analogous to "marginal probability": it applies in situations where we have two or more sentences of interest, and we speak of "conditional probability" to emphasize that one or more of those sentences appear in the conditional (right side of "$\|$") of the probability, as we now show.


## Conditional probability: acquiring knowledge {#sec-conditional-probs}

Consider once more the next-patient arrival scenario of [§ @sec-repr-joint-prob], with joint quantity $(U,T)$ and an agent's joint probability distribution as in [table @tbl-urgent-arrival]. Suppose that the agent must forecast whether the next patient will require $\urge$ or $\nonu$ care, so it needs to calculate the probability distribution for $U$ (that is, the probabilities for $U\mo\urge$ and $U\mo\nonu$). 

In the first exercise of [§ @sec-marginal-probs] you found that the marginal probability that the next patient will need urgent care is

$$\P(U\mo\urge \| \yi[H]) = 18\%$$

this is the agent's degree of belief if it has the knowledge encoded in the sentence $\yi[H]$, nothing more and nothing less.

But now let's imagine that the agent *receives a new piece of information*: it is told that the next patient is being transported by helicopter. In other words, the agent now knows that the sentence $T\mo\heli$ is true. The agent's complete knowledge is then encoded in the `and`ed sentence

$$T\mo\heli \land \yi[H]$$

which should therefore appear in the conditional. The agent's belief that the next patient requires urgent care is therefore

$$\P(U\mo\urge \| T\mo\heli \and \yi[H])$$

Calculation of this probability can be done by just one application of the `and`-rule:

:::{.column-page-inset-right}
$$
\begin{aligned}
&\P(U\mo\urge \and T\mo\heli \| \yi[H]) =
\P(U\mo\urge \| T\mo\heli \and \yi[H]) \cdot
\P(T\mo\heli \| \yi[H])
\\[3ex]
&\quad\implies\quad
\P(U\mo\urge \| T\mo\heli \and \yi[H])
=
\frac{
\P(U\mo\urge \and T\mo\heli \| \yi[H])
}{
\P(T\mo\heli \| \yi[H])
}
\end{aligned}
$$
:::

We do have the joint probability for $U\mo\urge \land T\mo\heli$ that appears in the numerator of the fraction above. The probability in the denominator is just a marginal probability for $T$, and we know how to calculate that too from [§ @sec-marginal-probs]. Finally we find

$$
\P(U\mo\urge \| T\mo\heli \and \yi[H])
=\frac{
\P(U\mo\urge \and T\mo\heli \| \yi[H])
}{
\sum_u\P(U\mo u \and T\mo\heli \| \yi[H])
}
$$

where it's understood that the sum index $u$ runs over the values $\set{\urge, \nonu}$.

This is called a [**conditional probability**]{.blue}; in this case, the conditional probability of $U\mo\urge$ **given** $T\mo\heli$.

:::{.callout-caution}
## {{< fa user-edit >}} Exercise
- Using the values from [table @tbl-urgent-arrival] and the formula for marginal probabilities, calculate:

    + The conditional probability that the next patient needs urgent care, given that the patient is being transported by helicopter.
    
    + The conditional probability that the next patient is being transported by helicopter, given that the patient needs urgent care.

- Now discuss and find an intuitive explanation for these comparisons:

    + The two probabilities you obtained above. Are they equal? why or why not?
	
	+ The *marginal* probability that the next patient will be transported by helicopter, with the *conditional* probability that the patient will be transported by helicopter *given* that it's urgent. Are they equal? if not, which is higher, and why?
:::

\



<!-- Considering now a more generic case of a joint probability with component quantities $\green X$ and $\red Y$, the probability for a specific value of $\red Y$, conditional on knowledge that $\green X\mo x$ and  some information $\yI$, is given by -->

<!-- $$ -->
<!-- \P({\red Y\mo y}\| {\green X\mo x} \and \yI) = \frac{ -->
<!-- \P({\red Y\mo y} \and {\green X\mo x} \| \yI) -->
<!-- }{ -->
<!-- \sum_{\red \upsilon} \P({\red Y\mo \upsilon} \and {\green X\mo x} \| \yI) -->
<!-- } -->
<!-- $$ -->

<!-- <\!-- :::{.callout-warning} -\-> -->
<!-- <\!-- ## -\-> -->
<!-- <\!-- {{< fa exclamation-circle >}} Always keep in mind that these are just convenient shorthands for -\-> -->
<!-- <\!-- $$ -\-> -->
<!-- <\!-- \P(\pr{The quantity \(X\) has value \(x\)} \land  -\-> -->
<!-- <\!-- \pr{The quantity \(Y\) has value \(y\)} -\-> -->
<!-- <\!-- \| \yI) -\-> -->
<!-- <\!-- $$ -\-> -->
<!-- <\!-- ::: -\-> -->

<!-- Now consider these two situations: -->

<!-- 1. the agent acquires knowledge that $X$ has true value $x$, so its state of knowledge has changed; -->

<!-- 2. for inference purposes, the agents needs to *hypothetically* assume that the quantity $X$ has value $x$ (even if that might not be the case in reality); -->

<!-- and, in either situation, the agent needs the probability that $Y$ has value $y$. This probability is written, in either situation as -->

<!-- $$ -->
<!-- \P({\red Y\mo y} \| {\green X\mo x} \and \yI) -->
<!-- $$ -->

