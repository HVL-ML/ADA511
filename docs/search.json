[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ADA511: Data science and data-driven engineering",
    "section": "",
    "text": "Science is built up with facts, as a house is with stones. But a collection of facts is no more a science than a heap of stones is a house.     (H. Poincaré)\n\nPreface\n**WARNING: THIS IS A WORKING DRAFT. TEXT WILL CHANGE A LOT. MANY PASSAGES ARE JUST TEMPORARY, INCOHERENT, AND DISJOINTED.\nTo be written.\n\nDifference between car mechanic and automotive engineer\n“Engineering based on data” is just how engineering and science in general have been in the past 400 years or so. Nothing new there.\nThe amount of available data has changed. This may lead to a reduction – or in some cases an increase – in uncertainty, and therefore to different solutions.\nLuckily the fundamental theory to deal with large amount of data is exactly the same to deal with small amounts. So the foundations haven’t changed.\n\nThis course makes you acquainted with the foundations.\n\nMany results of science that we readily believe are in fact quite extraordinary claims. Take a moment to reflect on how unbelievable the following propositions would have appeared to a keen and intelligent observer of nature from 500 years ago. The earth is very old, well over 4 billion years of age; it exists in a near-vacuum and revolves around the sun, which is about 150 million kilometers away; in the sun a great deal of energy is produced by nuclear fusion, the same kind of process as the explosion of a hydrogen bomb; all material objects are made up of invisible molecules and atoms, which are in turn made up of elementary particles, all far too small ever to be seen or felt directly; in each cell of a living creature there is a hypercomplex molecule called DNA, which largely determines the shape and functioning of the organism; and so on. Most members of today’s educated public subscribing to the “Western” civilization would assent to most of these propositions without hesitation, teach them confidently to their children, and become indignant when some ignorant people question these truths. However, if they were asked to say why they believe these items of scientific common sense, most would be unable to produce any convincing arguments. It may even be that the more basic and firm the belief is, the more stumped we tend to feel in trying to justify it. Such a correlation would indicate that unquestioning belief has served as a substitute for genuine understanding.\n\n\nIt is neither desirable nor any longer effective to try bullying people into accepting the authority of science. Instead, all members of the educated public can be invited to participate in science, in order to experience the true nature and value of scientific inquiry. This does not mean listening to professional scientists tell condescending stories about how they have discovered wonderful things, which you should believe for reasons that are too difficult for you to understand in real depth and detail. Doing science ought to mean asking your own questions, making your own investigations, and drawing your own conclusions for your own reasons. Of course it will not be feasible to advance the “cutting edge” or “frontier”” of modern science without first acquiring years of specialist training. However, the cutting edge is not all there is to science, nor is it necessarily the most valuable part of science. Questions that have been answered are still worth asking again, so you can understand for yourself how to arrive at the standard answers, and possibly discover new answers or recover forgotten answers that are valuable."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Accept or discard?",
    "section": "",
    "text": "Let’s start with a question that could arise in a particular engineering problem:\n\nA particular kind of electronic component is produced on an assembly line. At the end of the line, there is an automated inspection device that works as follows with every newly produced component coming out of the line.\nThe inspection device first makes some tests on the new component. The tests give an uncertain forecast of whether that component will fail within its first year of use, or after.\nThen the device decides whether the component is accepted and packaged for sale, or discarded and thrown away.\nWhen a new electronic component is sold, the manufacturer has a net gain of \\(1\\$\\). If the component fails within a year of use, however, the manufacturer incur net loss of \\(11\\$\\) (12$ loss, minus the 1$ gained at first), owing to warranty refunds and damage costs to be paid to the buyer. When a new electronic component is discarded, the manufacturer has \\(0\\$\\) net gain.\nFor a specific new electronic component, just come out of the assembly line, the tests of the automated inspection device indicate that there is a \\(10\\%\\) probability that the component will fail within its first year of use.\n\n\n\n\nShould the inspection device accept or discard the new component?\n\nFirst, try to give and motivate an answer.\nThis is not the real question of this exercise, however. In fact it doesn’t matter if you don’t get the correct answer; not even if you don’t manage to get an answer at all.\n\n\n\n\n\n\n Very first exercise!\n\n\n\nThe purpose here is for you to do some introspection about your own reasoning. Then examine and discuss these points:\n\nWhich numerical elements in the problem seem to affect the answer?\nCan these numerical elements be clearly separated? How would you separate them?\nHow would the answer change, if these numerical elements were changed? Feel free to change them, also in extreme ways, and see how the answer would change.\nCould we solve the problem if we didn’t have the probabilities? Why?\nCould we solve the problem if we didn’t know the various gains and losses? Why?\nCan this problem be somehow abstracted, and then transformed into another one with completely different details? For instance, consider translating along these lines:\n\ninspection device → computer pilot of self-driving car\ntests → camera image\nfail within a year → pedestrian in front of car\naccept/discard → keep on going/ break"
  },
  {
    "objectID": "framework.html#what-does-the-intro-problem-tell-us",
    "href": "framework.html#what-does-the-intro-problem-tell-us",
    "title": "2  Framework",
    "section": "2.1 What does the intro problem tell us?",
    "text": "2.1 What does the intro problem tell us?\nLet’s approach the “accept or discard?” problem of the previous chapter 1 in an intuitive way.\n\n\nWe’re jumping the gun here, because we haven’t learned the method to solve this problem yet!\nFirst let’s say that we accept the component. What happens?\nWe must try to make sense of that \\(10\\%\\) probability that the component fails within a year. Different people do this with different imagination tricks. We can imagine, for instance, that this situation is repeated 100 times. In 10 of these repetitions the accepted electronic component is sold and fails within a year after selling. In the remaining 90 repetitions, the component is sold and works fine for at least a year.\nIn each of the 10 imaginary repetitions in which the component fails early, the manufacturer loses \\(11\\$\\). That’s a total loss of \\(10 \\cdot 11\\$ = 110\\$\\). In each of the 90 imaginary repetitions in which the component doesn’t fail early, the manufacturer gains \\(1\\$\\). That’s a total gain of \\(90\\$\\). So over all 100 imaginary repetitions the manufacturer gains\n\\[\n10\\cdot (-11\\$) + 90\\cdot 1\\$ = {\\color[RGB]{238,102,119} -20\\$} \\ ,\n\\]\nthat is, the manufacturer has not gained, but lost \\(20\\$\\) ! That’s an average of \\(0.2\\$\\) lost per repetition.\nNow let’s say that we discard the component instead. What happens? In this case we don’t need to invoke imaginary repetitions, but even if we do, it’s clear that the manufacturer doesn’t gain or lose anything – that is, the “gain” is \\(0\\$\\) – in each and all of the repetitions.\nThe conclusion is that if in a situation like this we accept the component, then we’ll lose \\(0.2\\$\\) on average; whereas if we discard it, then on average we won’t lose anything or gain anything.\nObviously the best, or “least worst”, decision to make is to discard the component.\n\n\n\n\n\n\n Exercises\n\n\n\n\nNow that we have an idea of the general reasoning, check what happens with different values of the probability of failure and of the failure cost: is it still best to discard? For instance, try with\n\nfailure probability 10% and failure cost 5$;\nfailure probability 5% and failure cost 11$;\nfailure probability 10%, failure cost 11$, non-failure gain 2$.\n\nFeel free to get wild and do plots.\nIdentify the failure probability at which accepting the component doesn’t lead to any loss or any gain, so it doesn’t matter whether we discard or accept. (You can solve this as you prefer: analytically with an equation, visually with a plot, by trial & error on several cases, or whatnot.)\nConsider the special case with failure probability 0% and failure cost 10$. This means no new component will ever fail. To decide in such a case we do not need imaginary repetitions; but confirm that we arrive at the same logical conclusion whether we reason through imaginary repetitions or not.\nConsider this completely different problem:\n\nA patient is examined by a brand-new medical diagnostics AI system.\nThe AI first performs some clinical tests on the patient. The tests give an uncertain forecast of whether the patient has a particular disease or not.\nThen the AI decides whether the patient should be dismissed without treatment, or treated with a particular medicine.\nIf the patient is dismissed, then the life expectancy doesn’t increase or decrease if the disease is not present, but it decreases by 10 years if the disease is actually present. If the patient is treated, then the life expectancy decreases by 1 year if the disease is not present (owing to treatment side-effects), but also if the disease is present (because it cures the disease, so the life expectancy doesn’t decrease by 10 years; but it still decreases by 1 year owing to the side effects).\nFor this patient, the clinical tests indicate that there is a \\(10\\%\\) probability that the patient has the disease.\n\nShould the diagnostic AI dismiss or treat the patient? Find differences and similarities, even numerical, with the assembly-line problem.\n\n\n\n\n\nFrom the solution of the problem and from the exploring exercises, we gather some instructive points:\n\nIs it enough if we simply know that the component is less likely to fail than not? in other words, if we simply know that the probability of failure is less than \\(50\\%\\)?\nObviously not. We found that if the failure probability is \\(10\\%\\) then it’s best to discard; but if it’s \\(5\\%\\) then it’s best to accept. In both cases the component was less likely to fail than not, but the decisions were different. Moreover, we found that the probability affected the loss if one made the non-optimal decision. Therefore:\nKnowledge of exact probabilities is absolutely necessary for making the best decision\nIs it enough if we simply know that failure leads to a cost? that is, that its gain is less than the gain for non-failure?\nObviously not. The situation is similar to that with the probability. In the exercise we found that if the failure cost is \\(11\\$\\) then it’s best to discard; but if it’s \\(5\\$\\) then it’s best to accept. It’s also best to accept if the failure cost is \\(11\\$\\) but the non-failure gain is \\(2\\$\\). Therefore:\nKnowledge of the exact gains and losses is absolutely necessary for making the best decision\nIs this kind of decision situation only relevant to assembly lines and sales?\nBy all means not. We found a clinical situation that’s exactly analogous: there’s uncertainty, there are gains and losses (of time rather than money), and the best decision depends on both."
  },
  {
    "objectID": "framework.html#our-focus-decision-making-inference-and-data-science",
    "href": "framework.html#our-focus-decision-making-inference-and-data-science",
    "title": "2  Framework",
    "section": "2.2 Our focus: decision-making, inference, and data science",
    "text": "2.2 Our focus: decision-making, inference, and data science\nEvery data-driven engineering project is unique, with its unique difficulties and problems. But there are also problems common to all engineering projects.\nIn the scenarios we explored above, we found an extremely important problem-pattern. There is a decision or choice to make (and “not deciding” is not an option – or it’s just another kind choice). Making a particular decision will lead to some consequences, some leading to a desired goal, others leading to something undesirable. The decision is difficult because its consequences are not known with certainty, given the information and data available in the problem. We may lack information and data about past or present details, about future events and responses, and so on. This is what we call a problem of decision-making under uncertainty or under risk1, or simply a “decision problem” for short.1 We’ll avoid the word “risk” because it has several different technical meanings in the literature, some even contradictory.\nThis problem-pattern appears literally everywhere. But our explored scenarios also suggest that this problem-pattern has a sort of systematic solution method.\nIn this course we’re going to focus on decision problems and their systematic solution method. We’ll learn a framework and some abstract notions that allow us to frame and analyse this kind of problem, and we’ll learn a universal set of principles to solve it. This set of principles goes under the name of Decision Theory. \nBut what do decision-making under uncertainty and Decision Theory have to do with data and data science? The three are profoundly and tightly related on many different planes:\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nDecision theory in expert systems and artificial intelligence\n\n\n\nWe saw that probability values are essential in a decision problem. How do we find them? As you can imagine, data play an important part in their calculation. In our intro example, the failure probability must come from observations or experiments on similar electronic components.\nWe saw that also the values of gains and losses are essential. Data play an important part in their calculation as well.\nData science is based on the laws of Decision Theory. Here’s an analogy: a rocket engineer relies on fundamental physical laws (balance of momentum, energy, and so on) for making a rocket work. Failure to account for those laws leads at best to sub-optimal solutions, at worst to disasters. As we shall see, the same is true for a data scientist and the rules of decision theory.\nMachine-learning algorithms, in particular, are realizations or approximations of the rules of Decision Theory. This is clear, for instance, considering that the main task of a machine-learning classifier is to decide among possible output labels or classes.\nThe rules of Decision Theory are also the foundations upon which artificial-intelligence agents, which must make optimal inferences and decisions, are built.\n\nThese five planes will constitute the major parts of the present course.\n\n\n@@ TODO add examples: algorithm giving outputs is a decision agent. @@ Include one with https://hjerterisiko.helsedirektoratet.no\nThere are other important aspects in engineering problems, besides the one of making decisions under uncertainty. For instance the discovery or the invention of new technologies and solutions. These aspects can barely be planned or decided; but their fruits, once available, should be handled and used optimally – thus leading to a decision problem.\nArtificial intelligence is proving to be a valuable aid in these more creative aspects too. This kind of use of AI is outside the scope of the present notes. Some aspects of this creativity-assisting use, however, do fall within the domain of the present notes. A pattern-searching algorithm, for example, can be optimized by means of the method we are going to study."
  },
  {
    "objectID": "framework.html#our-goal-optimality-not-success",
    "href": "framework.html#our-goal-optimality-not-success",
    "title": "2  Framework",
    "section": "2.3 Our goal: optimality, not “success”",
    "text": "2.3 Our goal: optimality, not “success”\nWhat should we demand from a systematic method for solving decision problems?\nBy definition, in a decision problem under uncertainty there is generally no method to determine the decision that surely leads to the desired consequence – if such a method existed, then the problem would not have any uncertainty! Therefore, if there is a method to deal with decision problems, its goal cannot be the determination of the successful decision. This also means that a priori we cannot blame an engineer for making an unsuccessful decision in a situation of uncertainty.\nImagine two persons, Henry and Tina, who must bet on “heads” or “tails” under the following conditions (but who otherwise don’t get any special thrill from betting):\n\nIf the bet is “heads” and the coin lands “heads”, the person wins a small amount of money; but if it lands “tails”, they lose a large amount of money.\nIf the bet is “tails” and the coin lands “tails”, the person wins a small amount of money; if it lands “heads”, they lose the same small amount of money.\n\nHenry chooses the first bet, on “heads”. Tina chooses the second bet, on “tails”. The coin comes down “heads”. So Henry wins the small amount of money, while Tina loses the same small amount. What would we say about their decisions?\nHenry’s decision was lucky, and yet irrational: he risked losing much more money than in the second bet, without any possibility of at least winning more. Tina’s decision was unlucky, and yet rational: the possibility and amount of winning was the same in the two bets, and she chose the bet with the least amount of loss. We expect that any person making Henry’s decision in similar, future bets will eventually lose more money than any person making Tina’s decision.\nThis example shows two points. First, “success” is generally not a good criterion to judge a decision under uncertainty; success can be the pure outcome of luck, not of smarts. Second, even if there is no method to determine which decision is successful, there is a method to determine which decision is rational or optimal, given the particular gains, losses, and uncertainties involved in the decision problem. We had a glimpse of this method in our introductory scenarios.\nLet us emphasize, however, that we are not giving up on “success”, or trading it for “optimality”. Indeed we’ll find that Decision Theory automatically leads to the successful decision in problems where uncertainty is not present or is irrelevant. It’s a win-win. It’s important to keep this point in mind:\n\n\n\n\n\n\n\n\n\n\n\nAiming to find the solutions that are successful can make us fail to find those that are optimal when the successful ones cannot be determined.\nAiming to find the solutions that are optimal makes us automatically find those that are successful when those can be determined.\n\n\n\nWe shall later witness this fact with our own eyes, and will take it up again in the discussion of some misleading techniques to evaluate machine-learning algorithms."
  },
  {
    "objectID": "framework.html#decision-theory",
    "href": "framework.html#decision-theory",
    "title": "2  Framework",
    "section": "2.4 Decision Theory",
    "text": "2.4 Decision Theory\nSo far we have mentioned that Decision Theory has the following features:\n\n it tells us what’s optimal and, when possible, what’s successful\n it takes into consideration decisions, consequences, costs and gains\n it is able to deal with uncertainties\n\nWhat other kinds of features should we demand from it, in order to be applied to as many kinds of decision problems as possible, and to be relevant for data science?\nIf we find an optimal decision in regards to some outcome, it may still happen that the decision can be realized in several ways that are equivalent in regard to the outcome, but inequivalent in regard to time or resources. In the assembly-line scenario, for example, the decision discard could be carried out by burning, recycling, and so on. We thus face a decision within a decision. In general, a decision problem may involve several decision sub-problems, in turn involving decision sub-sub-problems, and so on.\nIn data science, a common engineering goal is to design and build an automated or AI-based device capable of making an optimal decision in a specific kind of uncertain situations. Think for instance of an aeronautic engineer designing an autopilot system, or a software company designing an image classifier.\nDecision Theory turns out to meet these demands too, thanks to the following features:\n\n it is susceptible to recursive, sequential, and modular application\n it can be used not only for human decision-makers, but also for automated or AI devices\n\n\n\nDecision Theory has a long history, going back to Leibniz in the 1600s and partly even to Aristotle in the −300s, and appearing in its present form around 1920–1960. What’s remarkable about it is that it is not only a framework, but the framework we must use. A logico-mathematical theorem shows that any framework that does not break basic optimality and rationality criteria has to be equivalent to Decision Theory. In other words, any “alternative” framework may use different technical terminology and rewrite mathematical operations in a different way, but it boils down to the same notions and operations of Decision Theory. So if you wanted to invent and use another framework, then either (a) it would lead to some irrational or illogical consequences, or (b) it would lead to results identical to Decision Theory’s. Many frameworks that you are probably familiar with, such as optimization theory or Boolean logic, are just specific applications or particular cases of Decision Theory.\nThus we list one more important characteristic of Decision Theory:\n\n it is normative\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\n\nJudgment under uncertainty\nHeuristics and Biases\nThinking, Fast and Slow\n\n\n\nNormative contrasts with descriptive. The purpose of Decision Theory is not to describe, for example, how human decision-makers typically make decisions. Because human decision-makers typically make irrational, sub-optimal, or biased decisions. That’s exactly what we want to avoid and improve!"
  },
  {
    "objectID": "basic_decisions.html#graphical-representation-and-elements",
    "href": "basic_decisions.html#graphical-representation-and-elements",
    "title": "3  Basic decision problems",
    "section": "3.1 Graphical representation and elements",
    "text": "3.1 Graphical representation and elements\nA basic decision problem can be represented by a diagram like this:\n\n\nIt has one decision node, usually represented by a square , from which the available decisions depart as lines. Each decision leads to an uncertainty node, usually represented by a circle , from which the possible outcomes depart as lines. Each outcome leads to a particular utility value. The uncertainty of each outcome is quantified by a probability.\nA basic decision problem is analysed in terms of these elements:\n\n Agent, and background or prior information. The agent is the person or device that has to make the decision. An agent always possess (or has been programmed with) specific background information that is used and taken for granted in the decision-making process. This background information determines the probabilities and utilities of the outcomes, together with other available data and information. Since different agents typically have different background information, we shall somehow conflate agents and prior information.\n\n\n\nWe’ll use the neutral pronouns it/its when referring to an agent, since an agent could be a person or a machine.\n\n Decisions, also called courses of actions, available to the agent. They are assumed to be mutually exclusive and exhaustive; this can always be achieved by recombining them if necessary, as we’ll discuss later.\n Outcomes of the possible decisions. Every decision can have a different set of outcomes, or some outcomes can appear for several or all decisions (in this case they are reported multiple times in the decision diagram). Note that even if an outcome can happen for two or more different decisions, its probabilities can still be different depending on the decision.\n Probabilities for each of the outcomes. Their values typically depend on the background information, the decision, and the additional data.\n Utilities: the gains or losses associated with each of the possible outcomes. Their values also depend on the background information, the decision, and the additional data.\n Data and other additional information, sometimes called evidence. They differ from the background information in that they can change with every decision instance made by the same agent, while the background information stays the same. In the assembly-line scenario, for example, the test results could be different for every new electric component.\n\n\n\n\n\n\n\n\n\n\n\n\n\n Remember: What matters is to be able to identify these elements in a concrete problem, understanding their role. Their technical names don’t matter.\n\n\nNote that it is not always the case that the outcomes are unknown and the data are known. As we’ll discuss later, in some situations we reason in hypothetical or counterfactual ways, using hypothetical data and considering outcomes which have already occurred.\n\n\n\n\n\n\n Reading\n\n\n\n§ 1.1.4 in Artificial Intelligence\n\n\n\n\n\n\n\n\n Exercise\n\n\n\n\nIdentify the elements above in the assembly-line decision problem of the introduction 1.\nSketch the diagram of the assembly-line decision problem.\n\n\n\nSome of the decision-problem elements listed above may need to be in turn analysed by a decision sub-problem. For instance, the utilities could depend on uncertain factors: thus we have a decision sub-problem to determine the optimal values to be used for the utilities of the main problem. This is an example of the modular character of decision theory.\nWe shall soon see how to mathematically represent these elements.\nThe elements above must be identified unambiguously in every decision problem. The analysis into these elements greatly helps in making the problem and its solution well-defined.\nAn advantage of decision theory is that its application forces us to make sense of an engineering problem. A useful procedure is to formulate the general problem in terms of the elements above, identifying them clearly. If the definition of any of the terms involves uncertainty of further decisions, then we analyse it in turn as a decision sub-problem, and so on.\n\nSuppose someone (probably a politician) says: “We must solve the energy crisis by reducing energy consumption or producing more energy”. From a decision-making point of view, this person has effectively said nothing whatsoever. By definition the “energy crisis” is the problem that energy production doesn’t meet demand. So this person has only said “we would like the problem to be solved”, without specifying any solution. A decision-theory approach to this problem requires us to specify which concrete courses of action should be taken for reducing consumption or increasing productions, and what their probable outcomes, costs, and gains would be.\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nSee MacKay’s options-vs-costs rational analysis in Sustainable Energy – without the hot air"
  },
  {
    "objectID": "basic_decisions.html#inference-utility-maximization",
    "href": "basic_decisions.html#inference-utility-maximization",
    "title": "3  Basic decision problems",
    "section": "3.2 Inference, utility, maximization",
    "text": "3.2 Inference, utility, maximization\nThe solution of a basic decision-making problem can be roughly divided into three main stages: inference, utility assessment, and expected-utility maximization.\n Inference is the stage where the probabilities of the possible outcomes are calculated. Its rules are given by the Probability Calculus. Inference is independent from decision: in some situations we may simply wish to assess whether some hypotheses, conjectures, or outcomes are more or less plausible than others, without making any decision. This kind of assessment can be very important in problems of communication and storage, and it is specially considered by Information Theory.\nThe calculation of probabilities can be the part that demands most thinking, time, and computational resources in a decision problem. It is also the part that typically makes most use of data – and where data can be most easily misused.\nRoughly half of this course will be devoted in understanding the laws of inference, their applications, uses, and misuses.\n\n Utility assesment is the stage where the gains or losses of the possible outcomes are calculated. Often this stage requires further inferences and further decision-making sub-problems. The theory underlying utility assessment is still much underdeveloped, compared to probability theory.\n\n Expected-utility maximization is the final stage where the probabilities and gains or costs of the possible outcomes are combined, in order to determine the optimal decision."
  },
  {
    "objectID": "inference.html#sec-inference-scenarios",
    "href": "inference.html#sec-inference-scenarios",
    "title": "4  What is an inference?",
    "section": "4.1 The wide scope and characteristics of inferences",
    "text": "4.1 The wide scope and characteristics of inferences\nLet’s see a couple more informal examples of inference problems. For some of them an underlying decision-making problem is also alluded to:\n\nLooking at the weather we try to assess if it’ll rain today, to decide whether to take an umbrella.\nConsidering a patient’s symptoms, test results, and medical history, a clinician tries to assess which disease affects a patient, so as to decide on the optimal treatment.\nLooking at the present game position  the X-player, which moves next, wonders whether placing the next X on the mid-right position leads to a win.\nFrom the current set of camera frames, the computer of a self-driving car needs to assess whether a particular patch of colours in the frames is a person, so as to slow down the car and stop.\nGiven that \\(G=6.67 \\cdot 10^{-11}\\,\\mathrm{m^3\\,s^{-2}\\,kg^{-1}}\\), \\(M = 5.97 \\cdot 10^{24}\\,\\mathrm{kg}\\) (mass of the Earth), and \\(r = 6.37 \\cdot 10^{6}\\,\\mathrm{m}\\) (radius of the Earth), a rocket engineer needs to know how much is \\(\\sqrt{2\\,G\\,M/r\\,}\\).\nWe’d like to know whether the rolled die is going to show .\nAn aircraft’s autopilot system needs to assess how much the aircraft’s roll will change if the right wing’s angle of attack is increased by \\(0.1\\,\\mathrm{rad}\\).\nBy looking at the dimensions, shape, texture of a newly dug-out fossil bone, an archaeologist wonders whether it belonged to a Tyrannosaurus rex.\nA voltage test on a newly produced electronic component yields a reading of \\(100\\,\\mathrm{mV}\\). The electronic component turns out to be defective. An engineer wants to assess whether the voltage-test reading could have been \\(100\\,\\mathrm{mV}\\), if the component had not been defective.\nSame as above, but the engineer wants to assess whether the voltage-test reading could have been \\(80\\,\\mathrm{mV}\\), if the component had not been defective.\n\n\n\n\nFrom measurements of the Sun’s energy output and of concentrations of various substances in the Earth’s atmosphere over the past 500 000 years, and of the emission rates of various substances in the years 1900–2022, climatologists and geophysicists try to assess the rate of mean-temperature increase in the years 2023–2100.\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nCh. 10 in A Survival Guide to the Misinformation Age.\n\n\n\n\n\n\n\n\n\n\n Exercises\n\n\n\n\nFor each example above, pinpoint what has to be inferred, and also the agent interested in the inference.\nPoint out which of the examples above explicitly give data or information that should be used for the inference.\nFor the examples that do not give explicit data or information, speculate what information could be implicitly assumed. For those that do give explicit data, speculate which other additional information could be implicitly assumed.\nCan any of the inferences above be done perfectly, that is, without any uncertainty, based the data given explicitly or implicitly?\nFind the examples that explicitly involve a decision. In which of them does the decision affect the results of the inference? In which it does not?\nAre any of the inferences “one-time only” – that is, their object or the data on which they are based have never happened before and will never happen again?\nAre any of the inferences based on data and information that come chronologically after the object of the inference?\nAre any of the inferences about something that is actually already known to the agent that’s making the inference?\nAre any of the inferences about something that actually did not happen?\nDo any of the inferences use “data” or “information” that are actually known (within the scenario itself) to be fictive, that is, not real?\n\n\n\nFrom the examples and from your answers to the exercise we observe some very important characteristics of inferences:\n\nSome inferences can be made exactly, that is, without uncertainty: it is possible to say whether the object of the inference is true or false. Other inferences, instead, involve an uncertainty.\nAll inferences are based on some data and information, which may be explicitly expressed or only implicitly understood.\nAn inference can be about something past, but based on present or future data and information: inferences can show all sorts of temporal relations.\nAn inference can be essentially unrepeatable, because it’s about something unrepeatable or based on unrepeatable data and information.\nThe data and information on which an inference is based can actually be unknown; that is, they can be only momentarily contemplated as real. Such an inference is said to be based on hypothetical reasoning.\nThe object of an inference can actually be something already known to be false or not real: the inference tries to assess it in the case that some data or information had been different. Such an inference is said to be based on counterfactual reasoning."
  },
  {
    "objectID": "inference.html#where-are-inferences-drawn-from",
    "href": "inference.html#where-are-inferences-drawn-from",
    "title": "4  What is an inference?",
    "section": "4.2 Where are inferences drawn from?",
    "text": "4.2 Where are inferences drawn from?\nThis question is far from trivial. In fact it has connections with the earth-shaking development and theorems in the foundations of mathematics of the 1900s.\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nMathematics: The Loss of Certainty.\n\n\nThe proper answer to this question will take up the next sections. But a central point can be emphasized now:\n\n\n\n\n\n\n\n\n\n\n\nInferences can only be drawn from other inferences.\n\n\n\nIn order to draw an inference – calculate a probability – we usually go up a chain: we must first draw other inferences, and for drawing those we must draw yet other inferences, and so on.\nAt some point we must stop at inferences that we take for granted without further proof. These typically concern direct experiences and observations. For instance, you see a tree in front of you, so you can take “there’s a tree here” as a true fact. Yet, notice that the situation is not so clear-cut: how do you know that you aren’t hallucinating, for example, and there’s actually no tree there? That is taken for granted. If you analyse the possibility of hallucination, you realize that you are taking other things for granted, and so on. Probably most philosophical research in the history of humanity has been about grappling with this runaway process – which is also a continuous source of sci-fi films. In logic and mathematical logic, this corresponds to the fact that to prove some theorem, we must always start from some axioms. There are “inferences” – tautologies – that can be drawn without requiring others; but they are all trivial, such as “this component failed early, or it didn’t”. They are of little use in a real problem, although they have a deep theoretical importance.\n\n\n\n\n\nSci-fi films like The Matrix ultimately draw on the fact that we must take some inferences for granted without further proof.\n\n\n\n\nIn concrete applications we start from many inferences upon which everyone, luckily, agrees. But sometimes we must also use starting inferences that are more dubious or not agreed upon by anyone. In this case the final inference has a somewhat contingent character, and we accept it (as well as the solution of any underlying decision problem) as the best available for the moment. This is partly the origin of the term “model”."
  },
  {
    "objectID": "inference.html#basic-elements-of-an-inference",
    "href": "inference.html#basic-elements-of-an-inference",
    "title": "4  What is an inference?",
    "section": "4.3 Basic elements of an inference",
    "text": "4.3 Basic elements of an inference\nLet us start to introduce some mathematical notation and more precise terminology for inferences.\nEvery inference has an “object” – what is to be assessed – as well as data, information, or hypotheses on which it is based. We call proposal1 the object of the inference, and conditional2 what the inference is based upon. We separate them with a vertical bar3  “\\(\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}}\\)”,  which can be pronounced given or conditional on: \\[\n\\textit{[proposal]}\\ \\pmb{\\nonscript\\:\\Big\\vert\\nonscript\\:\\mathopen{}}\\\n\\textit{[conditional]}\n\\]1 Johnson’s (1924) terminology. Keynes (1921) uses “conclusion”. Modern textbooks do not seem to use any specialized term.2 Modern terminology. Other terms used: “evidence”, “premise”, “supposal”.3 Originally a solidus, introduced by Keynes (1921).\nWe have seen that to calculate the probability for an inference, we must start from the probabilities of other inferences. A basic inference process therefore can be schematized like this:\n\n\n\n\n\n\n\n\nThe next important task ahead of us is to introduce a flexible and enough general mathematical representation for the objects and the bases of an inference. Then we shall finally study the rules for drawing correct inferences."
  },
  {
    "objectID": "sentences.html#sec-central-comps",
    "href": "sentences.html#sec-central-comps",
    "title": "5  Sentences",
    "section": "5.1 The central components of knowledge representation",
    "text": "5.1 The central components of knowledge representation\nWhen speaking of “data”, what comes to mind to many people is basically numbers or collections of numbers. Maybe numbers, then, could be used to represent all the variety of items exemplified above. This option, however, turns out to be too restrictive.\nI give you this number: “\\(8\\)”, saying that it is “data”. But what is it about? You, as an agent, can hardly call this number a piece of information, because you have no clue what to do with it. Instead, if I tell you: “The number of official planets in the solar system is 8”, then we can say that I’ve given you data. So “data” is not just numbers: a number is not “data” unless there’s an additional verbal, non-numeric context accompanying it, even if only implicitly. Sure, we could represent this meta-data information as numbers too; but this move would only shift the problem one level up: we would need an auxiliary verbal context explaining what the meta-data numbers are about.\nData can, moreover, be completely non-numeric. A clinician saying “The patient has fully recovered from the disease” (we imagine to know who’s the patient and what was the disease) is giving us a piece of information that we could further use, for instance, to make prognoses about other, similar patients. The clinician’s statement surely is “data”, but essentially non-numeric data. Sure, in some situations we can represent it as “1”, while “0” would represent “not recovered”; but the opposite convention could also be used, or the numbers “0.3” and “174”. These numbers have intrinsically nothing to do with the clinician’s “recovery” data.\nBut the examples above actually reveal the answer to our needs. In the examples we expressed the data by means of sentences. Clearly any measurement result, decision outcome, hypothesis, not-real event, assumption, data, and any piece of information can be expressed by a sentence.\nWe shall therefore use sentences, also called propositions or statements,1 to represent and communicate all the kinds of “items” that can be the proposal or conditional of an inference. In some cases we can of course summarize a sentence by a number, as a shorthand, when the full meaning of the sentence is understood.\n\n1 These three terms are not always equivalent in formal logic, but here we’ll use them as synonyms.\nSentences are the central components of knowledge representation in AI agents. For example they appear at the heart of automated control programs and fault-management systems in NASA spacecrafts.\n\n\n (From the SMART paper)\n\n\n\n\n\n\n Reading\n\n\n\n\n§ 7.1 in Artificial Intelligence.\nTake a quick look at these:\n\nSMART: A propositional logic-based trade analysis and risk assessment tool for a complex mission\naround p. 22 in No More Band-Aids: Integrating FM into the Onboard Execution Architecture\n§ 2.1 in Deliberation for autonomous robots: A survey\npart IV in Model-based programming of intelligent embedded systems and robotic space explorers"
  },
  {
    "objectID": "sentences.html#identifying-and-working-with-sentences",
    "href": "sentences.html#identifying-and-working-with-sentences",
    "title": "5  Sentences",
    "section": "5.2 Identifying and working with sentences",
    "text": "5.2 Identifying and working with sentences\nBut what is a sentence, more exactly? The everyday meaning of this word will work for us, even though there are more precise definitions – and still a lot of research in logic an artificial intelligence on how to define and use sentences. We shall adopt this useful definition:\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nPropositions\n\n\n\n\n\n\n\n\n\n\n\n\nA “sentence” is a verbal message for which we can determine whether it is true or false, at least in principle and in such a way that all interested receivers of the message would agree.\n\n\nFor instance, in most engineering contexts the phrase “This valve will operate for at least two months” is a sentence; whereas the phrase “Apples are much tastier than pears” is not, because it’s a matter of personal taste – there’s no objective criterion to determine its truth or falsity (however, the phrase “Rita finds apples tastier than pears” could be a sentence; its truth is found by asking Rita). In a data-science context, the phrase “The neural-network algorithm has better performance than the random-forest one” is not a sentence unless we have objectively specified what “better” means, for example by using a particular comparison metric.\nSome expressions in fact, even involving technical terms, may appear to be sentences at first, but a deeper analysis may reveal that they are not. A famous example is the sentence “The two events (at different spatial locations) are simultaneous”. Einstein showed that there’s no physical way to determine whether such an expression is true or false. Its truth turns out to be a matter of convention (also in Newtonian mechanics). The Theory of Relativity was born from this observation.\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nOn the electrodynamics of moving bodies.\n\n\nOne sentence can be expressed by many different phrases and in different languages. For instance, “The temperature is 248.15 K”, “Temperaturen ligger på minus 25 grader”, and “25 °C is the value of the temperature” all represent the same sentence.\nA sentence can contain numbers, pictures, and graphs.\nWorking with sentences, and keeping in mind that inference is about sentences, is important in several respects:\nFirst, it leads to clarity in engineering problems and makes them more goal-oriented. A data engineer must acquire information and convey information. “Acquiring information” does not simply consist in making measurements or counting something: the engineer must understand what is being measured and why. If data is gathered from third parties, the engineer must ask what exactly the data mean and how they were acquired. In designing and engineering a solution, it is important to understand what information or outcomes the end user exactly wants. The “what”, “why”, “how” are expressed by sentences. A data engineer will often ask “wait, what do you mean by that?”. This question is not just an unofficial parenthesis in the official data-transfer workflow between the engineer and someone else. It is an integral part of that workflow: it means that some information has not been completely transferred yet.\nSecond, it is extremely important in AI and machine-learning design. A (human) engineer may proceed informally when drawing inferences, without worrying about “sentences” unless a need for disambiguation arises. A data engineer who’s designing or programming an algorithm that will do inferences automatically, must instead be unambiguous and cover beforehand all possible cases that the algorithm will face.\n\n\nWe agree that the proposal and the conditional of an inference have to be sentences. This means that the proposal of the inference must be something that can only be true or false. Many inferences, especially when they concern numerical measurements, are actually collections of inferences. For example, an inference about the result of rolling a die actually consists of six separate inferences with the proposals \\[\n\\begin{aligned}\n&\\textsf{\\small`The result of the roll is 1'}\n\\\\\n&\\textsf{\\small`The result of the roll is 2'}\n\\\\\n&\\dotso\n\\\\\n&\\textsf{\\small`The result of the roll is 6'}\n\\end{aligned}\n\\]\nLater on we shall see how to work with more complex inferences without thinking about this detail. In real applications it can be useful, on some occasions, to pause and reduce an inference to its basic set of true/false inferences; this analysis may reveal contradictions in our inference. A simple way to do this is to reduce the complex inference into a set of yes/no questions.\nThis kind of analysis is also important in information-theoretic situations: the information content provided by an inference, when measured in Shannons, is related to the minimal amount of yes/no questions that the inference answers.\n\n\n\n\n\n\n Exercise\n\n\n\nRewrite each inference scenario of § 4.1 in a formal way, as one or more inferences\n\\[\n\\textit{[proposal]}\\ \\pmb{\\nonscript\\:\\Big\\vert\\nonscript\\:\\mathopen{}}\\ \\textit{[conditional]}\n\\]\nwhere proposal and conditional are well-defined sentences.\nIn ambiguous cases, use your judgement and motivate your choices."
  },
  {
    "objectID": "sentences.html#sec-sentence-notation",
    "href": "sentences.html#sec-sentence-notation",
    "title": "5  Sentences",
    "section": "5.3 Notation and abbreviations",
    "text": "5.3 Notation and abbreviations\nWriting full sentences would take up a lot of space. Even an expression such as “The speed is 10 m/s” is not a sentence, strictly speaking, because it leaves unspecified the speed of what, when it was measured and in which frame of reference, what we mean by “speed”, how the unit “m/s” is defined, and so on.\nTypically we leave the full content of a sentence to be understood from the context, and we denote the sentence by a simple expression such as the one above,\n\\[\n\\textsf{\\small The speed is 10\\,m/s}\n\\]\nor even more compactly introducing physical symbols:\n\\[\nv = 10\\,\\mathrm{m/s}\n\\]\nwhere \\(v\\) is a physical variable denoting the speed; or even writing simply\n\\[\n10\\,\\mathrm{m/s}\n\\]\nIn some problems it’s useful to introduce symbols to denote sentences. In these notes we’ll use sans-serif italic letters: \\(\\mathsfit{A},\\mathsfit{B},\\mathsfit{a},\\mathsfit{b},\\dotsc\\),, possibly with sub- or super-scripts. For instance, the sentence “The speed is 10 m/s” could be denoted by the symbol \\(\\mathsfit{S}_{10}\\). We abbreviate such a definition like this:\n\\[\n\\mathsfit{S}_{10} \\coloneqq \\textsf{\\small`The speed is 10\\,m/s'}\n\\]\nwhich means “the symbol \\(\\mathsfit{S}_{10}\\) is defined to be the sentence \\(\\textsf{\\small`The speed is 10\\,m/s'}\\)”.\n\n\n\n\n\n\n We must be wary of how much we shorten sentences\n\n\n\nConsider these three: \\[\n\\begin{aligned}\n&\\textsf{\\small`The speed is measured to be 10\\,m/s'}\n\\\\\n&\\textsf{\\small`The speed is set to 10\\,m/s'}\n\\\\\n&\\textsf{\\small`The speed is reported, by a third party, to be 10\\,m/s'}\n\\end{aligned}\n\\] The quantity “10 m/s” is the same in all three sentences, but their meanings are very different. They represent different kinds of data. These differences greatly affect any inference about or from these data. For instance, in the third case an engineer may not take the indirectly-reported speed “10 m/s” at face value, unlike the first case. In a scenario where all three sentences can occur, it would be ambiguous to simply write “\\(v = 10\\,\\mathrm{m/s}\\)”: would the equal-sign mean “measured”, “set”, or “indirectly reported”?\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nHow would you denote the three sentences above, to make their differences clear?\n\n\n\n\n\n\n\n\n\n\nGet familiar with abbreviations of sentences\n\n\n\nTo summarize, a sentence like\n\\[\n\\textsf{\\small`The temperature $T$ has value $x$'}\n\\]\ncould be abbreviated in these different ways:\n\nA symbol for the sentence (note the sans-serif font):\n\n\\[\n\\mathsfit{S}\n\\]\n\nSome key word appearing in the sentence:\n\n\\[\n\\textsf{\\small temperature}\n\\]\n\nAn equality:\n\n\\[\nT\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}x\n\\]\n\nThe quantity appearing in the sentence:\n\n\\[\nT\n\\]\n\nThe value appearing in the sentence:\n\n\\[\nx\n\\]\nGet familiar with these kinds of abbreviations because they’re very common. Some text may even jump from one abbreviation to another in the same page or paragraph!"
  },
  {
    "objectID": "sentences.html#connecting-sentences",
    "href": "sentences.html#connecting-sentences",
    "title": "5  Sentences",
    "section": "5.4 Connecting sentences",
    "text": "5.4 Connecting sentences\n\nAtomic sentences\nIn analysing the measurement results, decision outcomes, hypotheses, assumptions, data and information that enter into an inference problem, it is convenient to find a collection of basic sentences or, using a more technical term, atomic sentences out of which all other sentences of interest can be constructed. These atomic sentences often represent elementary pieces of information in the problem.\nConsider for instance the following complex sentence, which could appear in our assembly-line scenario:\n\n“The electronic component is still whole after the shock test and the subsequent heating test. The voltage reported in the final power test is either 90 mV or 110 mV.”\n\n\nIn this statement we can identify at least four atomic sentences, which we denote by these symbols: \\[\\begin{aligned}\n\\mathsfit{s} &\\coloneqq \\textsf{\\small`The component is whole after the shock test'}\n\\\\\n\\mathsfit{h} &\\coloneqq \\textsf{\\small`The component is whole after the heating test'}\n\\\\\n\\mathsfit{v}_{90} &\\coloneqq \\textsf{\\small`The power-test voltage reading is 90\\,mV'}\n\\\\\n\\mathsfit{v}_{110} &\\coloneqq \\textsf{\\small`The power-test voltage reading is 110\\,mV'}\n\\end{aligned}\n\\]\nThe inference may actually require additional atomic sentences. For instance, it might become necessary to consider atomic sentences with other values for the reported voltage, such as \\[\\begin{aligned}\n\\mathsfit{v}_{110} &\\coloneqq \\textsf{\\small`The power-test voltage reading is 100\\,mV'}\n\\\\\n\\mathsfit{v}_{80} &\\coloneqq \\textsf{\\small`The power-test voltage reading is 80\\,mV'}\n\\end{aligned}\\] and so on.\n\n\nConnectives\nHow do we construct complex sentences, like the one above, out of atomic sentences?\nWe consider three ways: one operation to change a sentence into another related to it, and two operations to combine two or more sentences together. These operations are called connectives; you may have encountered them already in Boolean algebra. Our natural language offers many more operations to combine sentences, but these three connectives turn out to be all we need in virtually all engineering and data-science problems:\n\n\n\n\n\n\n\n\n\n\n\n\nNot:  \\(\\lnot\\)\n\nexample:\n\n\n\\[\n\\lnot \\mathsfit{s} = \\textsf{\\small`The component is broken after the shock test'}\n\\]\n\nAnd:  \\(\\land\\)\n\nexample:\n\n\n\\[\n\\mathsfit{s} \\land \\mathsfit{h} = \\textsf{\\small`The component is whole after the shock and heating tests'}\n\\]\n\nOr:  \\(\\lor\\)\n\nexample:\n\n\n\\[\n\\mathsfit{v}_{90} \\lor \\mathsfit{v}_{110} = \\textsf{\\small`The power-test voltage reading is 90\\,mV, or 110\\,mV, or both'}\n\\]\n\n\n\nThese connectives can be applied multiple times, to form increasingly complex sentences.\n\n\n\n\n\n\n Important subtleties of the connectives:\n\n\n\n\nThere is no strict correspondence between the words “not”, “and”, “or” in natural language and the three connectives. For instance the and connective could correspond to the words “but” or “whereas”, or just to a comma “ , ”.\nNot means not some kind of complementary quality, but the denial. For instance,  \\(\\lnot\\textsf{\\small`The chair is black'}\\)  generally does not mean  \\(\\textsf{\\small`The chair is white'}\\) ,   (although in some situations these two sentences could amount to the same thing).\nIt’s always best to declare explicitly what the not of a sentence concretely means. In our example we take \\[\n  \\lnot\\textsf{\\small`The component is whole'} \\coloneqq \\textsf{\\small`The component is broken'}\n  \\] But in other examples the negation of “being whole” could comprise several different conditions. A good guideline is to always state the not of a sentence in positive terms.\nOr does not exclude that both the sentences it connects can be true. So in our example  \\(\\mathsfit{v}_{90} \\lor \\mathsfit{v}_{110}\\)  does not exclude, a priori, that the reported voltage could be both 90 mV and 110 mV. (There is a connective for that: “exclusive-or”, but it can be constructed out of the three we already have.)\n\n\n\nFrom the last remark we see that the sentence\n\\[\n\\textsf{\\small`The power-test voltage reading is 90\\,mV or 110\\,mV'}\n\\]\ndoes not correspond to   \\(\\mathsfit{v}_{90} \\lor \\mathsfit{v}_{110}\\) .  It is implicitly understood that a voltage reading cannot yield two different values at the same time. Convince yourself that the correct way to write that sentence is this: \\[\n(\\mathsfit{v}_{90} \\lor \\mathsfit{v}_{110})\n\\land\n\\lnot(\\mathsfit{v}_{90} \\land \\mathsfit{v}_{110})\n\\]\nFinally, the full complex sentence of the present example can be written in symbols as follows:\n\n“The electronic component is still whole after the shock test and the subsequent heating test. The voltage reported in the final power test is either 90 mV or 110 mV.”\n\n\\[\n\\textcolor[RGB]{102,204,238}{\\mathsfit{s}} \\land \\textcolor[RGB]{34,136,51}{\\mathsfit{h}} \\land\n(\\textcolor[RGB]{238,102,119}{\\mathsfit{v}_{90}} \\lor \\textcolor[RGB]{170,51,119}{\\mathsfit{v}_{110}})\n\\land\n\\lnot\n(\\textcolor[RGB]{238,102,119}{\\mathsfit{v}_{90}} \\land \\textcolor[RGB]{170,51,119}{\\mathsfit{v}_{110}})\n\\]\n\n\n\n\n\n\n\n\n Reading\n\n\n\nJust take a quick look at § 7.4.1 in Artificial Intelligence and note the similarities with what we’ve just learned. In these notes we follow a faster approach leading directly to probability logic."
  },
  {
    "objectID": "sentences.html#if-then",
    "href": "sentences.html#if-then",
    "title": "5  Sentences",
    "section": "5.5 “If… then…”",
    "text": "5.5 “If… then…”\nSentences expressing data and information in natural language also appear connected with if… then…. For instance: “If the voltage reading is 200 mV, then the component is defective”. This kind of expression actually indicates that the following inference\n\\[\n\\textsf{\\small`The component is defective'} \\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{} \\textsf{\\small`The voltage reading is 200\\,mV'}\n\\]\nis true.\nThis kind of information is very important because it often is the starting point from which to arrive at the final inferences we’re interested in. We shall discuss it more in detail in the next sections.\n\n\n\n\n\n\n Careful\n\n\n\nThere is a connective in logic, called “material conditional”, which is also often translated as “if… then…”. But it is not the same as the inference relation discussed above. “If… then…” in natural language usually denotes an inference rather than a material conditional.\nResearch is still ongoing on these topics. If you are curious and in for a headache, look over The logic of conditionals.\n\n\n\n\n\nWe are now equipped with all the notions and symbolic notation to deal with our next task: learning the rules for drawing correct inferences.\n@@ TODO: add connections to impossibility of large language models to learn maths (Gödel & Co.)."
  },
  {
    "objectID": "truth_inference.html#sec-trivial-inference",
    "href": "truth_inference.html#sec-trivial-inference",
    "title": "6  Truth inference",
    "section": "6.1 A trivial inference",
    "text": "6.1 A trivial inference\nConsider again the assembly-line scenario of § 1, and suppose that an inspector has the following information about an electric component:\n\nThis electric component had an early failure (within a year of use). If an electric component fails early, then at production it didn’t pass either the heating test or the shock test. This component passed the shock test.\n\nThe inspector wants to assess whether the component did not pass the heating test.\nFrom the data and information given, the conclusion is that the component for sure did not pass the heating test. This conclusion is certain and somewhat trivial. But how did we obtain it? Which rules did we follow to arrive at it from the given data?\nFormal logic, with its deduction systems, is the huge field that formalizes and makes rigorous the rules that a rational person or an artificial intelligence should use in drawing sure inferences like the one above. We’ll now get a glimpse of it, as a trampoline for jumping towards more general and uncertain inferences."
  },
  {
    "objectID": "truth_inference.html#analysis-and-representation-of-the-problem",
    "href": "truth_inference.html#analysis-and-representation-of-the-problem",
    "title": "6  Truth inference",
    "section": "6.2 Analysis and representation of the problem",
    "text": "6.2 Analysis and representation of the problem\nFirst let’s analyse our simple problem and represent it with more compact symbols.\n\nAtomic sentences\nWe can introduce the following atomic sentences and symbols: \\[\n\\begin{aligned}\n\\mathsfit{h}&\\coloneqq \\textsf{\\small`The component passed the heating test'}\n\\\\\n\\mathsfit{s}&\\coloneqq \\textsf{\\small`The component passed the shock test'}\n\\\\\n\\mathsfit{f}&\\coloneqq \\textsf{\\small`The component had an early failure'}\n\\\\\n\\mathsfit{I}&\\coloneqq \\textsf{\\small (all other implicit background information)}\n\\end{aligned}\n\\]\n\n\nProposal\nThe proposal is \\(\\lnot\\mathsfit{h}\\), but in the present case we could also have chosen \\(\\mathsfit{h}\\).\n\n\nConditional\nThe bases for the inference are two known facts in the present case: \\(\\mathsfit{s}\\) and \\(\\mathsfit{f}\\). There may also be other obvious facts implicitly assumed in the inference, which we denote by \\(\\mathsfit{I}\\).\n\n\nStarting inferences\nLet us emphasize again that any inference is drawn from other inferences, which are either taken for granted, or drawn in turn from others. In the present case we are told that if an electric component fails early, then at production it didn’t pass either the heating test or the shock test. We write this as\n\\[\n\\lnot\\mathsfit{h}\\lor \\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I}\n\\]\nand we shall take this to be true (that is, to have probability \\(100\\%\\)).\nBut our scenario actually has at least one more, hidden, inference. We said that the component failed early, and that it did pass the shock test. This means, in particular, that it must be possible for the component to pass the shock test, even if it fails early. This means that\n\\[\n\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I}\n\\]\ncannot be false.\n\n\nTarget inference\nThe inference that the inspector wants to draw can be compactly written:\n\n\\[\n\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{s}\\land \\mathsfit{f}\\land \\mathsfit{I}\n\\]"
  },
  {
    "objectID": "truth_inference.html#truth-inference-rules",
    "href": "truth_inference.html#truth-inference-rules",
    "title": "6  Truth inference",
    "section": "6.3 Truth-inference rules",
    "text": "6.3 Truth-inference rules\n\nDeduction systems; a specific choice\nFormal logic gives us a set of rules for correctly drawing sure inferences, when such inferences are possible. These rules can be formulated in different ways, leading to a wide variety of deduction systems (each one with a wide variety of possible notations). The picture on the margin, for instance, shows how a proof of how our inference would look like, using the so-called sequent calculus, which consists of a dozen or so inference rules.\n\n\n\n\n\nThe bottom formula is the target inference. Each line denotes the application of an inference rule, from one or more inferences above the line, to one below the line. The two formulae with no line above are our starting inference, and a tautology.\n\n\n\n\nWe choose to compactly encode all truth-inference rules in the following way.\nFirst, represent true by the number \\(\\mathbf{1}\\), and false by \\(\\mathbf{0}\\).\nSecond, symbolically write that a proposal \\(\\mathsfit{Y}\\) is true, given a conditional \\(\\mathsfit{X}\\), as follows:\n\\[\n\\mathrm{T}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}) = 1\n\\]\nor “\\(=0\\)” if it’s false.\nThe rules of truth-inference are then encoded by the following equations, which must always hold for any atomic or complex sentences \\(\\mathsfit{X},\\mathsfit{Y},\\mathsfit{Z}\\):\n\n\n\n\n\n\n\n\n\n\n\n\n\nRule for “not”:\n\n\\[\\mathrm{T}(\\lnot \\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n+ \\mathrm{T}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= 1 \\tag{6.1}\\]\n\nRule for “and”:\n\n\\[\n\\mathrm{T}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= \\mathrm{T}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}\\land \\mathsfit{Z}) \\cdot\n\\mathrm{T}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= \\mathrm{T}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}\\land \\mathsfit{Z}) \\cdot\n\\mathrm{T}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\tag{6.2}\\]\n\nRule for “or”:\n\n\\[\\mathrm{T}(\\mathsfit{X}\\lor \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= \\mathrm{T}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) +\n\\mathrm{T}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n- \\mathrm{T}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\tag{6.3}\\]\n\nRule of self-consistency:\n\n\\[\\mathrm{T}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}\\land \\mathsfit{Z})\n= 1\n\\tag{6.4}\\]\n\n\n\nHow to use the rules: Each equality can be rewritten in different ways according to the usual rules of algebra. Then the resulting left side can be replaced by the right side, and vice versa. The numerical values of starting inferences can be replaced in the corresponding expressions.\n\n\n\nLet’s see two examples:\n\nfrom one rule for “and” we can obtain the equality \\[\n{\\color[RGB]{102,204,238}\\mathrm{T}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}\\land \\mathsfit{Z})}\n=\\color[RGB]{204,187,68}\\frac{\\mathrm{T}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})}{\\mathrm{T}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})}\n\\] provided that \\(\\mathrm{T}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) \\ne 0\\). Then wherever we see the left side, we can replace it with the fraction on the right side, and vice versa.\nfrom the rule for “or” we can obtain the equality \\[\n{\\color[RGB]{102,204,238}\n\\mathrm{T}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) - \\mathrm{T}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})}\n=\\color[RGB]{204,187,68}\n\\mathrm{T}(\\mathsfit{X}\\lor \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) - \\mathrm{T}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\] Again wherever we see the left side, we can replace it with the sum on the right side, and vice versa.\n\n\n\nTarget inference in our scenario\nLet’s see how these rules allow us to arrive at our target inference,\n\\[\n\\color[RGB]{238,102,119}\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{s}\\land \\mathsfit{f}\\land \\mathsfit{I})\n\\]\nstarting from the given ones \\[\n\\color[RGB]{34,136,51}\\mathrm{T}(\\lnot\\mathsfit{h}\\lor \\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I}) = 1\n\\ ,\n\\qquad\n\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I}) \\ne 0\n\\]\nOne possibility is to work backwards from the target inference:\n\n\\[\n\\begin{aligned}\n&\\color[RGB]{238,102,119}\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{s}\\land \\mathsfit{f}\\land \\mathsfit{I})&&\n\\\\[1ex]\n&\\qquad=\\frac{\\mathrm{T}(\\lnot\\mathsfit{h}\\land \\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n{\\color[RGB]{34,136,51}\\underbracket[1pt]{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}_{\\ne 0}}\n&&\\text{\\small ∧-rule and starting inference}\n\\\\[1ex]\n&\\qquad=\\frac{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\lnot\\mathsfit{h}\\land \\mathsfit{f}\\land \\mathsfit{I})\\cdot\n\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})\n}\n{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n&&\\text{\\small ∧-rule}\n\\\\\n&\\qquad=\\frac{\\bigl[1-\\mathrm{T}(\\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\lnot\\mathsfit{h}\\land \\mathsfit{f}\\land \\mathsfit{I})\\bigr]\\cdot\n\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})\n}\n{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n&&\\text{\\small ¬-rule}\n\\\\\n&\\qquad=\\frac{\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})-\n\\mathrm{T}(\\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\lnot\\mathsfit{h}\\land \\mathsfit{f}\\land \\mathsfit{I})\\cdot\n\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})\n}\n{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n&&\\text{\\small algebra}\n\\\\\n&\\qquad=\\frac{\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})-\n\\mathrm{T}(\\lnot\\mathsfit{s}\\land \\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})\n}\n{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n&&\\text{\\small  ∧-rule}\n\\\\\n&\\qquad=\\frac{{\\color[RGB]{34,136,51}\\mathrm{T}(\\lnot\\mathsfit{h}\\land \\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}-\n\\mathrm{T}(\\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})\n}\n{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n&&\\text{\\small  ∨-rule}\n\\\\\n&\\qquad=\\frac{{\\color[RGB]{34,136,51}1} -\n\\mathrm{T}(\\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})\n}\n{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n&&\\text{\\small starting inference}\n\\\\\n&\\qquad=\\frac{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n&&\\text{\\small ¬-rule}\n\\\\[1ex]\n&\\qquad=\\color[RGB]{238,102,119}1\n&&\\text{\\small algebra}\n\\end{aligned}\n\\]\n\nTherefore \\(\\color[RGB]{238,102,119}\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{s}\\land \\mathsfit{f}\\land \\mathsfit{I}) = 1\\). We find that, indeed, the electronic component must for sure have failed the heating test!\n\n\n\n\n\n\n Exercise\n\n\n\nRetrace the proof above step by step. At each step, how was its particular rule (indicated on the right) used?\n\n\n\n\nThe way in which the rules can be applied to arrive at the target inference is not unique. In fact, in some concrete applications it can require a lot of work to find how to connect target inference with starting ones via the rules. The result, however, will always be the same:\n\n\n\n\n\n\n\n\n\n\n\nThe rules of truth-inference are self-consistent: even if applied in different sequences of steps, they always lead to the same final result.\n\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nProve the target inference \\(\\color[RGB]{238,102,119}\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{s}\\land \\mathsfit{f}\\land \\mathsfit{I}) = 1\\) using the rules of truth-inference, but beginning from the starting inference \\(\\color[RGB]{34,136,51}\\mathrm{T}(\\lnot\\mathsfit{h}\\land \\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})=1\\).\n\n\n\n\n[Optional] Equivalence with truth-tables\nIf you have studied Boolean algebra, you may be familiar with truth-tables; for instance the one for “and” displayed on the side. The truth-inference rules (6.1)–(6.4) contain the truth-tables that you already know as special cases.\n\n\n\n\n\n\\(\\mathsfit{X}\\)\n\\(\\mathsfit{Y}\\)\n\\(\\mathsfit{X}\\land \\mathsfit{Y}\\)\n\n\n\n\n1\n1\n1\n\n\n1\n0\n0\n\n\n0\n1\n0\n\n\n0\n0\n0\n\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nUse the truth-inference rules for “or” and “and” to build the truth-table for “or”. Check if it matches the one you already knew.\n\n\nThe truth-inference rules (6.1)–(6.4) are more complicated than truth-tables, but have two important advantages First, they allow us to work with conditionals, and to move sentences between proposals and conditionals. Second, they provide a smoother transition to the rules for probability-inference."
  },
  {
    "objectID": "truth_inference.html#logical-ai-agents-and-their-limitations",
    "href": "truth_inference.html#logical-ai-agents-and-their-limitations",
    "title": "6  Truth inference",
    "section": "6.4 Logical AI agents and their limitations",
    "text": "6.4 Logical AI agents and their limitations\nThe truth-inference discussed in this section are also the rules that a logical AI agent should follow. For example, the automated control and fault-management programs in NASA spacecrafts, mentioned in § 5.1, are programmed according to these rules.\n\n\n\n\n\n\n Reading\n\n\n\nLook over Ch. 7 in Artificial Intelligence.\n\n\nMany – if not most – inference problems that human and AI agents must face are, however, of the uncertain kind: it is not possible to surely infer the truth of some outcome, and the truth of some initial data or initial inferences may not be known either. We shall now see how to generalize the truth-inference rules to uncertain situations.\n\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nOur cursory visit of formal logic only showed a microscopic part of this vast field. The study of truth-inference rules continues still today, with many exciting developments and applications. Feel free take a look at\n\nLogic in Computer Science\nMathematical Logic for Computer Science\nNatural Deduction Systems in Logic"
  },
  {
    "objectID": "probability_inference.html#sec-probability-def",
    "href": "probability_inference.html#sec-probability-def",
    "title": "7  Probability inference",
    "section": "7.1 When truth isn’t known: probability",
    "text": "7.1 When truth isn’t known: probability\nWhen we cross a busy city street we look left and right to check whether any cars are approaching. We typically don’t look up to check whether something is falling from the sky. Yet, couldn’t it be false that cars are approaching at that moment? and couldn’t it be true that some object is falling from the sky? Of course both events are possible. Then why do we look left and right, but not up?\nThe main reason  is that we believe strongly that cars might be approaching, and believe very weakly that some object might be falling from the sky. In other words, we consider the first occurrence to be very probable, and the second extremely improbable.\nWe shall take the notion of probability as intuitively understood (just as we did with the notion of truth). Terms equivalent for “probability” are degree of belief, plausibility, credibility.\n\n\n\n\n\n\n Avoid likelihood as a synonym for probability\n\n\n\nIn technical discourse, “likelihood” means something different and is not a synonym of “probability”, as we’ll explain later.\n\n\nProbabilities are quantified between \\(0\\) and \\(1\\), or equivalently between \\(0\\%\\) and \\(100\\%\\). Assigning to a sentence a probability 1 is the same as saying that it is true; and a probability 0, that it is false. A probability of \\(0.5\\) represents a belief completely symmetric with respect to truth and falsity.\nLet’s emphasize and agree on some important facts about probabilities:\n\n Probabilities are assigned to sentences. We already discussed this point in § 5.3, but let’s reiterate it. Consider an engineer working on a problem of electric-power distribution in a specific geographical region. At a given moment the engineer may believe with \\(75\\%\\) probability that the measured average power output in the next hour will be 100 MW. The \\(75\\%\\) probability is assigned not to the quantity “100 MW”, but to the sentence\n\n\\[\n\\textsf{\\small`The measured average power output in the next hour will be 100\\,MW'}\n\\]\nThis difference is extremely important. Consider the alternative sentence\n\\[\n\\textsf{\\small`The average power output in the next hour will be \\emph{set} to 100\\,MW'}\n\\]\nthe numerical quantity is the same, but the meaning is very different. The probability can therefore be very different (if the engineer is the person deciding how to set that output, the probability is $100\\%$). The probability depends not only on a number, but on what it's being done with that number -- measuring, setting, third-party reporting, and so on. Often we write simply \"$O = \\mathrm{100\\,W}$\" provided that the full sentence behind this kind of shorthand is understood.\n\n Probabilities are agent- and context-dependent. A coin is tossed, comes down heads, and is quickly hidden from view. Alice sees that it landed heads-up. Bob instead doesn’t manage to see the outcome and has no clue. Alice considers the sentence \\(\\textsf{\\small`Coin came down heads'}\\) to be true, that is, to have \\(100\\%\\) probability. Bob considers the same sentence to have \\(50\\%\\) probability.\nNote how Alice and Bob assign two different probabilities to the same sentence; yet both assignments are completely rational. If Bob assigned \\(100\\%\\) to \\(\\textsf{\\small`heads'}\\), we would suspect that he had seen the outcome after all; if he assigned \\(0\\%\\) to \\(\\textsf{\\small`heads'}\\), we would consider that groundless and silly. We would be baffled if Alice assigned \\(50\\%\\) to \\(\\textsf{\\small`heads'}\\), because she saw the outcome was actually heads; we would hypothesize that she feels unsure about what she saw.\nAn omniscient agent would know the truth or falsity of every sentence, and assign only probabilities 0 or 1. Some authors speak of “actual (but unknown) probabilities”. But if there were “actual” probabilities, they would be all 0 or 1, and it would be pointless to speak about probabilities at all – every inference would be a truth-inference.\n Probabilities are not frequencies. Consider the fraction of defective mechanical components to total components produced per year in some factory. This quantity can be physically measured and, once measured, would be agreed upon by every agent. It is a frequency, not a degree of belief or probability.\nIt is important to understand the difference between probability and frequency: mixing them up may lead to sub-optimal decisions. Later we shall say more about the difference and the precise relations between probability and frequency.\nFrequencies can be unknown to some agents. Probabilities cannot be “unknown”: they can only be difficult to calculate. Be careful when you read authors speaking of an “unknown probability”: they actually mean either “unknown frequency”, or a probability that has to be calculated (it’s “unknown” in the same sense that the value of \\(1-0.7 \\cdot 0.2/(1-0.3)\\) is “unknown” to you right now).\n Probabilities are not physical properties. Whether a tossed coin lands heads up or tails up is fully determined by the initial conditions (position, orientation, momentum, rotational momentum) of the toss and the boundary conditions (air velocity and pressure) during the flight. The same is true for all macroscopic engineering phenomena (even quantum phenomena have never been proved to be non-deterministic, and there are deterministic and experimentally consistent mathematical representations of quantum theory). So we cannot measure a probability using some physical apparatus; and the mechanisms underlying any engineering problem boil down to physical laws, not to probabilities.\n\n\n\n\n\n\n\n Reading\n\n\n\nDynamical Bias in the Coin Toss. \n\n\n\n\nThese points listed above are not just a matter of principle. They have important practical consequences. A data scientist who is not attentive to the source of the data (measured? set? reported, and so maybe less trustworthy?), or who does not carefully assess the context of a probability, or who mixes a probability with a frequency, or who does not take advantage (when possible) of the physics involved in the a problem – such data scientist will design systems with sub-optimal performance1 – or even cause deaths.1 This fact can be mathematically proven."
  },
  {
    "objectID": "probability_inference.html#sec-uncertain-inference",
    "href": "probability_inference.html#sec-uncertain-inference",
    "title": "7  Probability inference",
    "section": "7.2 An unsure inference",
    "text": "7.2 An unsure inference\nConsider now the following variation of the trivial inference problem of § 6.1.\n\nThis electric component had an early failure. If an electric component fails early, then at production it either didn’t pass the heating test or didn’t pass the shock test. The probability that it didn’t pass both tests is 10%. There’s no reason to believe that the component passed the heating test, more than it passed the shock test.\n\nThe inspector wants to assess, also in this case, whether the component did not pass the heating test.\nFrom the data and information given, what would you say is the probability that the component didn’t pass the heating test?\n\n\n\n\n\n\n Exercises\n\n\n\n\nTry to argue why a conclusion cannot drawn with certainty in this case. One way to argue this is to present two different scenarios that fit the data given but have opposite conclusions.\nTry to reason intuitively and assess the probability that the component didn’t pass the heating test. Should it be larger or smaller than 50%? Why?"
  },
  {
    "objectID": "probability_inference.html#probability-notation",
    "href": "probability_inference.html#probability-notation",
    "title": "7  Probability inference",
    "section": "7.3 Probability notation",
    "text": "7.3 Probability notation\nFor this inference problem we can’t find a true or false final value. The truth-inference rules (6.1)–(6.4) therefore cannot help us here. In fact even the “\\(\\mathrm{T}(\\dotso \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\dotso)\\)” notation is unsuitable, because it only admits the values \\(1\\) (true) and \\(0\\) (false).\nLet us first generalize this notation in a straightforward way:\nFirst, let’s represent the probability or degree of belief of a sentence by a number in the range \\([0,1]\\), that is, between \\(\\mathbf{1}\\) (certainty or true) and \\(\\mathbf{0}\\) (impossibility or false). The value \\(0.5\\) represents that the belief in the truth of the sentence is as strong as that in its falsity.\nSecond, let’s symbolically write that the probability of a proposal \\(\\mathsfit{Y}\\), given a conditional \\(\\mathsfit{X}\\), is some number \\(p\\), as follows:\n\\[\n\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}) = p\n\\]\nNote that this notation includes the notation for truth-values as a special case: \\[\n\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}) = 0\\text{ or }1\n\\quad\\Longleftrightarrow\\quad\n\\mathrm{T}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}) = 0\\text{ or }1\n\\]"
  },
  {
    "objectID": "probability_inference.html#sec-fundamental",
    "href": "probability_inference.html#sec-fundamental",
    "title": "7  Probability inference",
    "section": "7.4 Inference rules",
    "text": "7.4 Inference rules\nExtending our truth-inference notation to probability-inference notation has been straightforward. But which rules should we use for drawing inferences when probabilities are involved?\nThe amazing result is that the rules for truth-inference, formulae (6.1)–(6.4), extend also to probability-inference. The only difference is that they now hold for all values in the range \\([0,1]\\), rather than for \\(0\\) and \\(1\\) only.\nThis important result was taken more or less for granted at least since Laplace in the 1700s. But was formally proven for the first time in the 1946 by R. T. Cox. The proof has been refined since then. What kind of proof is it? It shows that if we don’t follow the rules we are doomed to arrive at illogical conclusions; we’ll show some examples later.\n\n\nFinally, here are the fundamental rules of all inference. They are encoded by the following equations, which must always hold for any atomic or complex sentences \\(\\mathsfit{X},\\mathsfit{Y},\\mathsfit{Z}\\):\n\n\n\n\n\n\n\n   THE FUNDAMENTAL LAWS OF INFERENCE   \n\n\n\n\n\n“Not” \\(\\boldsymbol{\\lnot}\\) rule\n\n\\[\\mathrm{P}(\\lnot \\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n+ \\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= 1\\]\n\n\n“And” \\(\\boldsymbol{\\land}\\) rule\n\n\\[\n\\mathrm{P}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= \\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}\\land \\mathsfit{Z}) \\cdot\n\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= \\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}\\land \\mathsfit{Z}) \\cdot\n\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\]\n\n\n“Or” \\(\\boldsymbol{\\lor}\\) rule\n\n\\[\\mathrm{P}(\\mathsfit{X}\\lor \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= \\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) +\n\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n- \\mathrm{P}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\]\n\n\nSelf-consistency rule\n\n\\[\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}\\land \\mathsfit{Z})\n= 1\n\\]\n\n\n\n\n\n\nIt is amazing that ALL inference is nothing else but a repeated application of these four rules – billions of times or more in some cases. All machine-learning algorithms are just applications or approximations of these rules. Methods that you may have heard about in statistics are just specific applications of these rules. Truth inferences are also special applications of these rules. Most of this course is, at bottom, just a study of how to apply these rules in particular kinds of problems.\n\n\n\n\n\n\n Reading\n\n\n\n\nProbability, Frequency and Reasonable Expectation\nCh. 2 of Bayesian Logical Data Analysis for the Physical Sciences\nCh. 1 of Probability\n§§ 1.0–1.2 of Data Analysis\nFeel free to skim through §§ 2.0–2.4 of Probability Theory\n\n\n\n \nThe fundamental inference rules are used in the same way as their truth-inference special case: Each equality can be rewritten in different ways according to the usual rules of algebra. Then left and right side of the equality thus obtained can replace each other in a proof."
  },
  {
    "objectID": "probability_inference.html#solution-of-the-uncertain-inference-example",
    "href": "probability_inference.html#solution-of-the-uncertain-inference-example",
    "title": "7  Probability inference",
    "section": "7.5 Solution of the uncertain-inference example",
    "text": "7.5 Solution of the uncertain-inference example\nArmed with the fundamental rules of inference, let’s solve our earlier inference problem. As usual we first analyse it, find what are its proposal and conditional, and which starting inferences are given in the problem.\n\nAtomic sentences\n\\[\n\\begin{aligned}\n\\mathsfit{h}&\\coloneqq \\textsf{\\small`The component passed the heating test'}\n\\\\\n\\mathsfit{s}&\\coloneqq \\textsf{\\small`The component passed the shock test'}\n\\\\\n\\mathsfit{f}&\\coloneqq \\textsf{\\small`The component had an early failure'}\n\\\\\n\\mathsfit{J}&\\coloneqq \\textsf{\\small (all other implicit background information)}\n\\end{aligned}\n\\] The background information in this example is different from the previous, truth-inference one, so we use the different symbol \\(\\mathsfit{J}\\) for it.\n\n\nProposal, conditional, and target inference\nThe proposal is \\(\\lnot\\mathsfit{h}\\), just like in the truth-inference example.\nThe conditional is different now. We know that the component failed early, but we don’t know whether it passed the shock test. Hence the conditional is \\(\\mathsfit{f}\\land \\mathsfit{J}\\).\nThe target inference is therefore\n\\[\n\\color[RGB]{238,102,119}\\mathrm{P}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\n\\]\n\n\nStarting inferences\nWe are told that if an electric component fails early, then at production it didn’t pass either the heating test or the shock test. Let’s write this as\n\\[\n\\color[RGB]{34,136,51}\\mathrm{P}(\\lnot\\mathsfit{h}\\lor \\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J}) = 1\n\\]\nWe are also told that there is a \\(10\\%\\) probability that both tests fail\n\\[\n\\color[RGB]{34,136,51}\\mathrm{P}(\\lnot\\mathsfit{h}\\land \\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J}) = 0.1\n\\]\nFinally the problem says that there’s no reason to believe that the component didn’t pass the heating test, more than it didn’t pass the shock test. This can be written as follows:\n\\[\n\\color[RGB]{34,136,51}\\mathrm{P}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J}) = \\mathrm{P}(\\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\n\\]\nNote this interesting situation: we are not given the numerical values of these two probabilities, we are only told that they are equal. This is an example of application of the principle of indifference, which we’ll discuss more in detail later.\n\n\nFinal inference\nAlso in this case there is no unique way of applying the rules to reach our target inference, but all ways lead to the same result. Let’s try to proceed backwards:\n\n\\[\n\\begin{aligned}\n&\\color[RGB]{238,102,119}\\mathrm{P}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})&&\n\\\\[1ex]\n&\\qquad= {\\color[RGB]{34,136,51}\\mathrm{P}(\\lnot\\mathsfit{s}\\lor \\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})}\n+ {\\color[RGB]{34,136,51}\\mathrm{P}(\\lnot\\mathsfit{s}\\land \\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})}\n- \\mathrm{P}(\\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\n&&\\text{\\small ∨-rule}\n\\\\[1ex]\n&\\qquad= {\\color[RGB]{34,136,51}1}\n+ {\\color[RGB]{34,136,51}0.1}\n- \\mathrm{P}(\\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\n&&\\text{\\small starting inferences}\n\\\\[1ex]\n&\\qquad= 0.1 + \\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\n&&\\text{\\small ¬-rule}\n\\\\[1ex]\n&\\qquad= 0.1 + \\mathrm{P}(\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\n&&\\text{\\small starting inference}\n\\\\[1ex]\n&\\qquad= 0.1 + 1 -\\color[RGB]{238,102,119}\\mathrm{P}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\n&&\\text{\\small ¬-rule}\n\\end{aligned}\n\\]\n\nThe target probability appears on the left and right side with opposite signs. We can solve for it: \\[\n\\begin{aligned}\n2\\,{\\color[RGB]{238,102,119}\\mathrm{P}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})} &= 0.1 + 1\n\\\\[1ex]\n{\\color[RGB]{238,102,119}\\mathrm{P}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})} &= 0.55\n\\end{aligned}\n\\]\nSo the probability that the component didn’t pass the heating test is \\(55\\%\\).\n\n\n\n\n\n\n Exercises\n\n\n\n\nTry to find an intuitive explanation of why the probability is 55%, slightly larger than 50%. If your intuition says this probability is wrong, then\n\nCheck the proof of the inference for mistakes, or try to find a proof with a different path.\nExamine your intuition critically and educate it.\n\nCheck how the target probability \\(\\mathrm{P}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\\) changes if we change the value of the probability \\(\\mathrm{P}(\\lnot\\mathsfit{s}\\land \\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\\) from \\(0.1\\).\n\nWhat result do we obtain if \\(\\mathrm{P}(\\lnot\\mathsfit{s}\\land \\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})=0\\)? Can it be intuitively explained?\nWhat if \\(\\mathrm{P}(\\lnot\\mathsfit{s}\\land \\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})=1\\)? Does the result make sense?"
  },
  {
    "objectID": "probability_inference.html#how-the-inference-rules-are-used",
    "href": "probability_inference.html#how-the-inference-rules-are-used",
    "title": "7  Probability inference",
    "section": "7.6 How the inference rules are used",
    "text": "7.6 How the inference rules are used\nIn the solution above you noticed that the equations of the fundamental rules are not only used to obtain some of the probabilities appearing in them from the remaining probabilities.\nThe rules represent, first of all, constraints of logical consistency2 among probabilities. For instance, if we have probabilities  \\(\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{X}\\land \\mathsfit{Z})=0.1\\),  \\(\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z})=0.7\\),  and \\(\\mathrm{P}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z})=0.2\\),  then there’s an inconsistency somewhere, because these values violate the and-rule:  \\(0.2 \\ne 0.1 \\cdot 0.7\\).  In this case we must find the inconsistency and solve it. However, since probabilities are quantified by real numbers, it’s possible and acceptable to have slight discrepancies within numerical round-off errors.2 The technical term is coherence.\nThe rules also imply more general constraints. For example we must always have \\[\n\\begin{gathered}\n\\mathrm{P}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}) \\le \\min\\set{\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}),\\  \\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z})}\n\\\\\n\\mathrm{P}(\\mathsfit{X}\\lor \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}) \\ge \\max \\set*{\\int\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}),\\  \\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z})}\n\\end{gathered}\n\\]\n\n\n\n\n\n\n Exercise\n\n\n\nTry to prove the two constraints above."
  },
  {
    "objectID": "probability_inference.html#derived-rules",
    "href": "probability_inference.html#derived-rules",
    "title": "7  Probability inference",
    "section": "7.7 Derived rules",
    "text": "7.7 Derived rules\nThe fundamental rules above are in principle all we need to use to draw inferences from other inferences. But from them it is possible to derive some “shortcut” rules.\n\nBoolean algebra\nFirst, it is possible to show that all rules you may know from Boolean algebra are a consequence of the fundamental rules. So we can always make the following convenient replacements anywhere in a probability expression:\n\n\n\n\n\n\nBoolean algebra\n\n\n\n\\[\n\\begin{gathered}\n\\lnot\\lnot \\mathsfit{X}= \\mathsfit{X}\n\\qquad\n\\mathsfit{X}\\land \\mathsfit{X}= \\mathsfit{X}\\lor \\mathsfit{X}= \\mathsfit{X}\n\\\\[1ex]\n\\mathsfit{X}\\land \\mathsfit{Y}= \\mathsfit{Y}\\land \\mathsfit{X}\n\\qquad\n\\mathsfit{X}\\lor \\mathsfit{Y}= \\mathsfit{Y}\\lor \\mathsfit{X}\n\\\\[1ex]\n\\mathsfit{X}\\land (\\mathsfit{Y}\\lor \\mathsfit{Z}) = (\\mathsfit{X}\\land \\mathsfit{Y}) \\lor (\\mathsfit{X}\\land \\mathsfit{Z})\n\\\\[1ex]\n\\mathsfit{X}\\lor (\\mathsfit{Y}\\land \\mathsfit{Z}) = (\\mathsfit{X}\\lor \\mathsfit{Y}) \\land (\\mathsfit{X}\\lor \\mathsfit{Z})\n\\\\[1ex]\n\\lnot (\\mathsfit{X}\\land \\mathsfit{Y}) = \\lnot \\mathsfit{X}\\lor \\lnot \\mathsfit{Y}\n\\qquad\n\\lnot (\\mathsfit{X}\\lor \\mathsfit{Y}) = \\lnot \\mathsfit{X}\\land \\lnot \\mathsfit{Y}\n\\end{gathered}\n\\]\n\n\n\n\n\n\nLaw of total probability or “extension of the conversation”\nSuppose we have a set of \\(n\\) sentences \\(\\{\\mathsfit{Y}_1, \\mathsfit{Y}_2, \\dotsc, \\mathsfit{Y}_n\\}\\) having these two properties:\n\nThey are mutually exclusive, meaning that the “and” of any two of them is false, given a conditional \\(\\mathsfit{Z}\\): \\[\n  \\mathrm{P}(\\mathsfit{Y}_1\\land\\mathsfit{Y}_2\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}) = 0\\ , \\quad\n  \\mathrm{P}(\\mathsfit{Y}_1\\land\\mathsfit{Y}_3\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}) = 0\\ , \\quad\n\\dotsc \\ , \\quad\n  \\mathrm{P}(\\mathsfit{Y}_{n-1}\\land\\mathsfit{Y}_n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}) = 0\n  \\]\nThey are exhaustive, meaning that the “or” of all of them is true, given a conditional \\(\\mathsfit{Z}\\): \\[\n  \\mathrm{P}(\\mathsfit{Y}_1\\lor \\mathsfit{Y}_2 \\lor \\dotsb \\lor \\mathsfit{Y}_n \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}) = 1\n  \\]\n\nThen the probability of a sentence \\(\\mathsfit{X}\\), conditional on \\(\\mathsfit{Z}\\), is equal to a combination of probabilities conditional on \\(\\mathsfit{Y}_1,\\mathsfit{Y}_2,\\dotsc\\):\n\n\n\n\n\n\nDerived rule: extension of the conversation\n\n\n\n\n\\[\n\\begin{aligned}\n\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) =\n\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}_1 \\land \\mathsfit{Z})\\cdot \\mathrm{P}(\\mathsfit{Y}_1 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) +\n\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}_2 \\land \\mathsfit{Z})\\cdot \\mathrm{P}(\\mathsfit{Y}_2 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) +{}&\n\\\\[2ex]\n\\dotsb + \\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}_n \\land \\mathsfit{Z})\\cdot \\mathrm{P}(\\mathsfit{Y}_n \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})&\n\\end{aligned}\n\\]\n\n\n\nThis rule is useful when it is difficult to assess the probability of a sentence conditional on the background information, but it is easier to assess the probabilities of that sentence conditional on several auxiliary sentences – often representing hypotheses that exclude one another, and of which we know at least one is true. The name extension of the conversation for this derived rule comes from the fact that we are able to call the additional sentences into play.\nThis situation occurs very often in concrete applications, especially in problems where the probabilities of several competing hypotheses have to be assessed.\nThe next derived rule is used extremely often, so we discuss it separately."
  },
  {
    "objectID": "probability_inference.html#sec-bayes-theorem",
    "href": "probability_inference.html#sec-bayes-theorem",
    "title": "7  Probability inference",
    "section": "7.8 Bayes’s theorem",
    "text": "7.8 Bayes’s theorem\nThe probably most famous – or infamous – rule derived from the laws of inference is Bayes’s theorem. It allows us to relate the probability where two sentences \\(\\mathsfit{Y},\\mathsfit{X}\\) appear in the proposal and the conditional, with one where they are exchanged:\n\n\n\n\n\n\nDerived rule: Bayes’s theorem\n\n\n\n\n\\[\n\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}\\land \\mathsfit{Z}) =\n\\frac{\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}\\land \\mathsfit{Z})\\cdot \\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})}{\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})}\n\\]\n\n\n\nObviously this rule can only be used if \\(\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) &gt; 0\\), that is, if the sentence \\(\\mathsfit{X}\\) is not false conditional on \\(\\mathsfit{Z}\\).\n\n\n\n\n\nBayes’s theorem guest-starring in The Big Bang Theory\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nProve Bayes’s theorem from the fundamental rules of inference.\n\n\nBayes’s theorem is extremely useful when we want to assess the probability of a sentence, typically a hypothesis, given some conditional, typically data; and we can easily assess the probability of the data conditional on the hypothesis. Note, however, that the sentences \\(\\mathsfit{Y}\\) and \\(\\mathsfit{X}\\) in the theorem can be about anything whatsoever: \\(\\mathsfit{Y}\\) does not always need to be a “hypothesis”, and \\(\\mathsfit{X}\\) “data”.\n\nCombining with the extension of the conversation\nBayes’s theorem is often with several sentences \\(\\{\\mathsfit{Y}_1, \\mathsfit{Y}_2, \\dotsc, \\mathsfit{Y}_n\\}\\) that are mutually exclusive and exhaustive. Typically these represent competing hypotheses. In this case the probability of the sentence \\(\\mathsfit{X}\\) in the denominator can be expressed using the rule of extension of the conversation:\n\n\n\n\n\n\n\nDerived rule: Bayes’s theorem with extension of the conversation\n\n\n\n\n\\[\n\\mathrm{P}(\\mathsfit{Y}_1 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}\\land \\mathsfit{Z}) =\n\\frac{\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}_1 \\land \\mathsfit{Z})\\cdot \\mathrm{P}(\\mathsfit{Y}_1 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})}{\n\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}_1 \\land \\mathsfit{Z})\\cdot \\mathrm{P}(\\mathsfit{Y}_1 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) +\n\\dotsb + \\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}_n \\land \\mathsfit{Z})\\cdot \\mathrm{P}(\\mathsfit{Y}_n \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n}\n\\]\n\nand similarly for \\(\\mathsfit{Y}_2\\) and so on.\n\n\n\nWe will use this form of Bayes’s theorem very frequently.\n\n\nMany facets\nBayes’s theorem is a very general result of the fundamental rules of inference, valid for any sentences \\(\\mathsfit{X},\\mathsfit{Y},\\mathsfit{Z}\\). This generality leads to many uses and interpretations.\nThe theorem is often proclaimed to be the rule according to which we “update our beliefs”. The meaning of this proclamation is the following. Let’s say that at some point \\(\\mathsfit{Z}\\) represents all your knowledge. Your degree of belief about some sentence \\(\\mathsfit{Y}\\) is then (at least in theory) the value of \\(\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\\). At some later point, let’s say that you get to know – maybe thanks to an observation you made – that the sentence \\(\\mathsfit{X}\\) is true. Your whole knowledge at that point is represented no longer by \\(\\mathsfit{Z}\\), but by \\(\\mathsfit{X}\\land \\mathsfit{Z}\\). Your degree of belief about \\(\\mathsfit{Y}\\) is then given by the value of \\(\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}\\land\\mathsfit{Z})\\). Bayes’s theorem allows you to find your degree of belief about \\(\\mathsfit{Y}\\) conditional on your new state of knowledge, from the one conditional on your old state of knowledge.\nThis chronological element, however, comes only from this particular way of using Bayes’s theorem. The theorem can more generally be used to connect any two states of knowledge \\(\\mathsfit{Z}\\) and \\(\\mathsfit{X}\\land\\mathsfit{Z}\\), no matter their temporal order, even if they happen simultaneously, and even if they belong to two different agents.\n\n\n\n\n\n\n Exercise\n\n\n\nUsing Bayes’s theorem and the fundamental laws of inference, prove that if \\(\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})=1\\), that is, if you already know that \\(\\mathsfit{X}\\) is true in your current state of knowledge \\(\\mathsfit{Z}\\), then\n\\[\n\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}\\land \\mathsfit{Z}) = \\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\]\nthat is, your degree of belief about \\(\\mathsfit{Y}\\) doesn’t change.\nIs this result reasonable?\n\n\n\n\n\n\n\n\n Reading\n\n\n\n\n§§ 4.1–4.3 in Medical Decision Making give one more point of view on Bayes’s theorem.\nCh. 3 of Probability\nA graphical explanation of how Bayes’s theorem works mathematically (using a specific interpretation of the theorem):"
  },
  {
    "objectID": "probability_inference.html#consequences-of-not-following-the-rules",
    "href": "probability_inference.html#consequences-of-not-following-the-rules",
    "title": "7  Probability inference",
    "section": "7.9 Consequences of not following the rules",
    "text": "7.9 Consequences of not following the rules\n@@ §12.2.3 of AI\n\nExercise: Monty-Hall problem & variations\nExercise: clinical test & diagnosis"
  },
  {
    "objectID": "probability_inference.html#remarks-on-terminology-and-notation",
    "href": "probability_inference.html#remarks-on-terminology-and-notation",
    "title": "7  Probability inference",
    "section": "7.10 Remarks on terminology and notation",
    "text": "7.10 Remarks on terminology and notation\n\nLikelihood\nIn everyday language, “likely” is often a synonym of “probable”, and “likelihood” of “probability”. But in technical questions about probability, inference, and decision-making, “likelihood” has a very different meaning. Keep in mind this important difference of definition:\n\\(\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{X})\\) is:\n\nthe probability of \\(\\mathsfit{Y}\\) given \\(\\mathsfit{X}\\) (or conditional on \\(\\mathsfit{X}\\)),\nthe likelihood of \\(\\mathsfit{X}\\) in view of \\(\\mathsfit{Y}\\).\n\nLet’s express this also in a different way:\n\n\\(\\mathrm{P}({\\color[RGB]{68,119,170}\\mathsfit{Y}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{X})\\) is the probability of \\(\\mathsfit{Y}\\) given \\(\\mathsfit{X}\\),\n\\(\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}{\\color[RGB]{68,119,170}\\mathsfit{Y}})\\) is the likelihood of \\(\\mathsfit{Y}\\) in view of \\(\\mathsfit{X}\\).\n\n\n\n\n\n\n\n\n\n\n\nA priori there is no relation between the probability and the likelihood of a sentence \\(\\mathsfit{Y}\\): this sentence could have very high probability and very low likelihood, and vice versa.\n\n\nIn these notes we’ll avoid the possibly confusing term “likelihood”. All we need to express can be phrased in terms of “probability”.\n\n\nOmitting background information\nIn the analyses of the inference examples of § 6.1 and § 7.2 we defined sentences (\\(\\mathsfit{I}\\) and \\(\\mathsfit{J}\\)) expressing all background information, and always included these sentences in the conditionals of the inferences – because those inferences obviously depended on that background information.\nIn many concrete inference problems the background information usually stays there in the conditional from beginning to end, while the other sentences jump around between conditional and proposal as we apply the rules of inference. For this reason the background information is often omitted from the notation, being implicitly understood. For instance, if the background information is denoted \\(\\mathsfit{I}\\), one writes\n\n“\\(\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{X})\\)”  instead of  \\(\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{X}\\land \\mathsfit{I})\\)\n“\\(\\mathrm{P}(\\mathsfit{Y})\\)”  instead of  \\(\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\\)\n\nThis is what’s happening when you see in books probabilities “\\(P(x)\\)” without conditional.\nSuch practice may be convenient, but be wary of it, especially in particular situations:\n\nIn some inference problems we suddenly realize that we must distinguish between cases that depend on hypotheses, say \\(\\mathsfit{H}_1\\) and \\(\\mathsfit{H}_2\\), that were buried in the background information \\(\\mathsfit{I}\\). If the background information \\(\\mathsfit{I}\\) is explicitly reported in the notation, this is no problem: we can rewrite it as \\[ \\mathsfit{I}= (\\mathsfit{H}_1 \\lor \\mathsfit{H}_2) \\land \\mathsfit{I}'\\] and proceed, for example using the rule of extension of the conversation. If the background information was not explicitly written, this may lead to confusion and mistakes. For instance there may suddenly appear two instances of \\(\\mathrm{P}(\\mathsfit{X})\\) with different values, just because one of them is invisibly conditional on \\(\\mathsfit{I}\\), the other on \\(\\mathsfit{I}'\\).\nIn some inference problems we are considering several different instances of background information – for example because more than one agent is involved. It’s then extremely important to write the background information explicitly, lest we mix up the different agents’s degrees of belief.\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nA once-famous paper published the quantum-theory literature, arrived at completely wrong results simply by omitting background information, mixing up probabilities having different conditionals.\n\n\nThis kind of confusion from poor notation happens more often than one thinks, and even appears in scientific literature.\n\n\n“Random variables”\nSome texts speak of the probability of a “random variable”, or more precisely of the probability “that a random variable takes on a particular value”. As you notice, we have just expressed that idea by means of a sentence. The viewpoint and terminology of random variables is therefore a special case of that based on sentences, which we use here.\nThe dialect of “random variables” does not offer any advantages in concepts, notation, terminology, or calculations, but it has some shortcomings:\n\nAs discussed in § 7.1, in concrete applications it is important to know how a quantity “takes on” a value: for example it could be directly measured, indirectly reported, or purposely set to that specific value. Thinking and working in terms of sentences, rather than of random variables, allows us to account for these important differences.\nVery often the object (proposal) of a probability is not a “variable”: it is actually a constant value that is simply unknown.\nWhat does “random” (or “chance”) mean? Good luck finding an understandable and non-circular definition in texts that use that word; strangely enough, they never define it. In these notes, if the word “random” is ever used, it stands for “unpredictable” or “unsystematic”.\n\n\n\n\n\n\nJames Clerk Maxwell is one of the main founders of statistical mechanics and kinetic theory (and electromagnetism). Yet he never used the word “random” in his technical writings. Maxwell is known for being very clear and meticulous with explanations and terminology.\n\n\nIt’s a question for sociology of science why some people keep on using less flexible points of view or terminologies. Probably they just memorize them as students and then a fossilization process sets in.\n\n\nFinally, some texts speak of the probability of an “event”. For all purposes an “event” is just what’s expressed in a sentence."
  },
  {
    "objectID": "probability_distributions.html#distribution-of-probabilities-among-values",
    "href": "probability_distributions.html#distribution-of-probabilities-among-values",
    "title": "8  Probability distributions",
    "section": "8.1 Distribution of probabilities among values",
    "text": "8.1 Distribution of probabilities among values\nWhen an agent is uncertain about the value of a quantity, its uncertainty is expressed and quantified by assigning a degree of belief, conditional on the agent’s knowledge, to all the possible cases regarding the true value.\nFor a temperature measurement, for instance, the cases could be “The temperature is measured to have value 271 K”, “The temperature is measured to have value 272 K”, and so on up to 275 K. These cases are expressed by mutually exclusive and exhaustive sentences. Denoting th temperature with \\(T\\), these sentences can be abbreviated as \\[\n{\\color[RGB]{34,136,51}T = 271\\,\\mathrm{K}} \\ , \\quad\n{\\color[RGB]{34,136,51}T = 272\\,\\mathrm{K} \\ ,} \\quad\n{\\color[RGB]{34,136,51}T = 273\\,\\mathrm{K} \\ ,} \\quad\n{\\color[RGB]{34,136,51}T = 274\\,\\mathrm{K} \\ ,} \\quad\n{\\color[RGB]{34,136,51}T = 275\\,\\mathrm{K}} \\ .\n\\]\nThe agent’s belief about the quantity is then expressed by the probabilities about these sentences, conditional on the agent’s state of knowledge \\({\\color[RGB]{204,187,68}\\mathsfit{I}}\\):  $$\n\\[\\begin{aligned}\n\\mathrm{P}({\\color[RGB]{34,136,51}T \\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}271\\,\\mathrm{K}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{204,187,68}\\mathsfit{I}}) &= {\\color[RGB]{170,51,119}0.04} \\\\[1ex]\n\\mathrm{P}({\\color[RGB]{34,136,51}T \\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}272\\,\\mathrm{K}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{204,187,68}\\mathsfit{I}}) &= {\\color[RGB]{170,51,119}0.10} \\\\[1ex]\n\\mathrm{P}({\\color[RGB]{34,136,51}T \\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}273\\,\\mathrm{K}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{204,187,68}\\mathsfit{I}}) &= {\\color[RGB]{170,51,119}0.18} \\\\[1ex]\n\\mathrm{P}({\\color[RGB]{34,136,51}T \\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}274\\,\\mathrm{K}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{204,187,68}\\mathsfit{I}}) &= {\\color[RGB]{170,51,119}0.28} \\\\[1ex]\n\\mathrm{P}({\\color[RGB]{34,136,51}T \\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}275\\,\\mathrm{K}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{204,187,68}\\mathsfit{I}}) &= {\\color[RGB]{170,51,119}0.40}\n\\end{aligned}\\]\n\\[\nthat sum up to one:\n\\]\n\\[\\begin{aligned}\n&\\mathrm{P}({\\color[RGB]{34,136,51}T \\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}271\\,\\mathrm{K}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{204,187,68}\\mathsfit{I}}) +\n\\dotsb +\n\\mathrm{P}({\\color[RGB]{34,136,51}T \\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}275\\,\\mathrm{K}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{204,187,68}\\mathsfit{I}}) \\\\\n&\\quad{}=\n{\\color[RGB]{170,51,119}0.04}+{\\color[RGB]{170,51,119}0.10}+{\\color[RGB]{170,51,119}0.18}+{\\color[RGB]{170,51,119}0.28}+{\\color[RGB]{170,51,119}0.40} \\\\\n&\\quad{}=\n{\\color[RGB]{170,51,119}1}\n\\end{aligned}\\]\n$$ This collection of probabilities is called a probability distribution.\n\n\n\n\n\n\n What’s “distributed”?\n\n\n\nThe probability is distributed among the possible values, not the quantity, as illustrated in the side picture. The quantity cannot be “distributed”: it has one, definite value, which is however unknown to us.\n\n\n\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nConsider three sentences \\(\\mathsfit{X}_1, \\mathsfit{X}_2, \\mathsfit{X}_3\\) that are mutually exclusive and exhaustive on conditional \\(\\mathsfit{I}\\), that is: $$\n\\[\\begin{gathered}\n\\mathrm{P}(\\mathsfit{X}_1 \\land \\mathsfit{X}_2 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) =\n\\mathrm{P}(\\mathsfit{X}_1 \\land \\mathsfit{X}_3 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) =\n\\mathrm{P}(\\mathsfit{X}_2 \\land \\mathsfit{X}_3 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = 0\n\\\\\n\\mathrm{P}(\\mathsfit{X}_1 \\lor \\mathsfit{X}_2 \\lor \\mathsfit{X}_3 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = 1\n\\end{gathered}\\]\n\\[\nProve, using the fundamental rules of inferences and any derived rules from [§ @sec-probability], that we must then have\n\\]\n(_1 | ) + (_2 | ) + (_3 | ) = 1 $$\n\n\nLet’s see how probability distributions can be represented and visualized for the basic types of quantities discussed in § 12.\nWe start with probability distributions over discrete domains."
  },
  {
    "objectID": "probability_distributions.html#discrete-probability-distributions",
    "href": "probability_distributions.html#discrete-probability-distributions",
    "title": "8  Probability distributions",
    "section": "8.2 Discrete probability distributions",
    "text": "8.2 Discrete probability distributions\n\nTables and functions\nA probability distribution over a discrete domain can obviously be displayed as a table of values and their probabilities. For instance\n\n\n\n\n\n\n\n\n\n\n\nvalue\n271 K\n272 K\n273 K\n274 K\n275 K\n\n\n\n\nprobability\n0.04\n0.10\n0.18\n0.28\n0.40\n\n\n\nIn the case of ordinal or interval quantities it is sometimes possible to express the probability as a function of the value. For instance, the probability distribution above could be summarized by this function of the value \\({\\color[RGB]{34,136,51}t}\\): \\[\n\\mathrm{P}({\\color[RGB]{34,136,51}T\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}t} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{204,187,68}\\mathsfit{I}}) =\n{\\color[RGB]{170,51,119}\\frac{({\\color[RGB]{34,136,51}t}/\\textrm{\\small K} - 269)^2}{90}}\n\\quad\\text{\\small (rounded to two decimals)}\n\\]\n\n\nA graphical representation is often helpful to detect features, peculiarities, and even inconsistencies in one or more probability distributions.\n\n\nHistograms and area-based representations\nA probability distribution for a nominal, ordinal, and discrete interval quantity can be neatly represented by a histogram.\n\n\n\n\n\nHistogram for the probability distribution over possible component failures\n\n\nThe possible values are placed on a line. For an ordinal or interval quantity, the sequence of values on the line should correspond to their natural order. For a nominal quantity the order is irrelevant.\nA rectangle is then drawn above each value. Typically the rectangles are contiguous. The bases of the rectangles are all equal, and the areas of the rectangles are proportional to the probabilities. Since the bases are equal, this implies that the heights of the rectangles are also proportional to the probabilities.\nSuch kind of drawing can of course be horizontal, vertical, upside-down, and so on, depending on convenience.\nSince the probabilities must sum to one, the total area of the rectangles represents the unit of area. So in principle there is no need of writing probability values on some vertical axis, or grid, or similar visual device, because the probability value can be visually read as the ratio of a rectangle area to the total area. An axis or grid can nevertheless be helpful. Alternatively the probabilities can be reported above or below each rectangle.\nNominal quantities do not have any specific order, so their values do not need to be ordered on a line. Other area-based representations, such as pie charts, can also be used for these quantities.\n\n\nLine-based representations\nHistograms give faithful representations of discrete probability distributions. Their graphical bulkiness, however, can be a disadvantage in some situations; for instance when we want to have a clearer idea of how the probability distribution varies across values (for ordinal or interval quantities); or when we want to compare several probability distributions over the same values.\n\n\n Representation of the same pair of probability distributions with a histogram plot and a line plot \nIn these cases we can use standard line plots, or variations thereof. Compare the examples on the margin figure: the line plot displays more cleanly the differences between the “before-inspection” and “after-inspection” probability distributions."
  },
  {
    "objectID": "probability_distributions.html#probability-distributions-over-infinite-discrete-values",
    "href": "probability_distributions.html#probability-distributions-over-infinite-discrete-values",
    "title": "8  Probability distributions",
    "section": "8.3 Probability distributions over infinite discrete values",
    "text": "8.3 Probability distributions over infinite discrete values\n@@ TODO"
  },
  {
    "objectID": "probability_distributions.html#probability-densities",
    "href": "probability_distributions.html#probability-densities",
    "title": "8  Probability distributions",
    "section": "8.4 Probability densities",
    "text": "8.4 Probability densities\nDistributions of probability over continuous domains present several counter-intuitive aspects, which essentially arise because we are dealing with uncountable infinities – while often still using linguistic expressions that make at most sense for countable infinities. Here we follow a practical and realistic approach for working with such distributions.\nConsider a quantity \\(X\\) with a continuous domain. When we say that such a quantity has some value \\(x\\) we really mean that it has a value somewhere in the range   \\(x -\\epsilon/2\\)  to  \\(x+\\epsilon/2\\),   where the width \\(\\epsilon\\) is usually extremely small. For example, for double-precision values stored in a computer, the width must be at least \\(\\epsilon \\approx 2\\cdot 10^{-16}\\) :\n## R code\n&gt; 1.234567890123456 == 1.234567890123455\n[1] FALSE\n\n&gt; 1.2345678901234567 == 1.2345678901234566\n[1] TRUE\nand a value 1.3 really represents a range between 1.29999999999999982236431605997495353221893310546875 and 1.300000000000000266453525910037569701671600341796875, this range coming from the internal binary representation of 1.3. Often the width \\(\\epsilon\\) is much larger than the computer’s precision, and comes from the precision with which the value is experimentally measured.\nProbabilities are therefore assigned to such small ranges, not to single values. Since these ranges are very small, they are also very numerous. The total probability assigned to all of them must still amount to \\(1\\); therefore each small range receives an extremely small amount of probability. A standard Gaussian distribution for a real quantity, for instance, assigns a probability of approximately \\(8\\cdot 10^{-17}\\), or \\(0.00000000000000008\\), to a range of width \\(2\\cdot 10^{-16}\\) around the value \\(0\\). All other ranges are assigned even smaller probabilities.\nIn would be impractical to work with such small probabilities. We use probability densities instead. As implied by the term “density”, a probability density is the amount of probability \\(P\\) assigned to a standard range of width \\(\\epsilon\\), divided by that width. For example, if the probability assigned to a range of width  \\(\\epsilon=2\\cdot10^{-16}\\)  around \\(0\\) is  \\(P=7.97885\\cdot10^{-17}\\),  then the probability density around \\(0\\) is \\[\n\\frac{P}{\\epsilon} =\n\\frac{7.97885\\cdot10^{-17}}{2\\cdot10^{-16}} = 0.398942\n\\] which is a simpler number to work with.\nProbability densities are convenient because they usually do not depend on the range width \\(\\epsilon\\), if it’s small enough. Owing to physics reasons, we don’t expect a situation where \\(X\\) is between \\(0.9999999999999999\\) and \\(1.0000000000000001\\) to be very different from one where \\(X\\) is between \\(1.0000000000000001\\) and \\(1.0000000000000003\\). The probabilities assigned to these two small ranges of width \\(\\epsilon=2\\cdot 10^{-16}\\) each will therefore be approximately equal, let’s say \\(P\\) each. Now if we use a small range of width \\(\\epsilon\\) around \\(X=1\\), the probability is \\(P\\), and the probability density is \\(P/\\epsilon\\). If we consider a range of double width \\(2\\,\\epsilon\\) around \\(X=1\\), then the probability is \\(P+P\\) instead, but the probability density is still \\[\\frac{P+P}{2\\,\\epsilon} =\n\\frac{1.59577\\cdot10^{-16}}{4\\cdot10^{-16}}\n= 0.398942\n\\] As you see, even if we consider a range with double the width as before, the probability density is still the same.\n\n\nIn these notes we’ll denote probability densities with a lowercase \\(\\mathrm{p}\\), with the following notation: \\[\n\\underbracket[0pt]{\\mathrm{p}}_{\\mathrlap{\\color[RGB]{119,119,119}\\!\\uparrow\\ \\textit{lowercase}}}(X\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) \\coloneqq\n\\frac{\n\\overbracket[0pt]{\\mathrm{P}}^{\\mathrlap{\\color[RGB]{119,119,119}\\!\\downarrow\\ \\textit{uppercase}}}(\\textsf{\\small`\\(X\\) has value between \\(x-\\epsilon/2\\) and \\(x+\\epsilon/2\\)'} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}{\\epsilon}\n\\] This definition works even if we don’t specify the exact value of \\(\\epsilon\\), as long as it’s small enough.\n\n\n\n\n\n\n Probability densities are not probabilities\n\n\n\nThe expression “\\(\\mathrm{p}(X\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}2.5 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})=0.3\\)” does not mean “There is a \\(0.3\\) probability that \\(X\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}2.5\\)”. The probability that \\(X\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}2.5\\) exactly is, if anything, zero.\nThat expression means “There is a  \\(0.3\\cdot \\epsilon\\)   probability that \\(X\\) is between \\(2.5-\\epsilon/2\\) and \\(2.5+\\epsilon/2\\), for any \\(\\epsilon\\) small enough”.\nIn fact, probability densities can be larger than 1, because they are obtained by dividing by a number, the range width, that is in principle arbitrary. This fact shows that they cannot be probabilities.\nIt is important not to mix up probability and probability densities: we shall see later that densities have very different properties, for example with respect to maxima and averages.\n\n\nA helpful practice (though followed by few texts) is to always write a probability density as \\[\\mathrm{p}(X\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\,\\mathrm{d}x\\] where “\\(\\mathrm{d}x\\)” stands for the width of a small range around \\(x\\). This notation is also helpful with integrals. Unfortunately it becomes a little cumbersome when we are dealing with more than one quantity."
  },
  {
    "objectID": "probability_distributions.html#representation-of-probability-densities",
    "href": "probability_distributions.html#representation-of-probability-densities",
    "title": "8  Probability distributions",
    "section": "8.5 Representation of probability densities",
    "text": "8.5 Representation of probability densities\n\nLine-based representations\nThe histogram and the line representations become indistinguishable for a probability density.\nIf we represent the probability \\(P\\) assigned to a small range of width \\(\\epsilon\\) as the area of a rectangle, and the width of the rectangle is equal to \\(\\epsilon\\), then the height \\(P/\\epsilon\\) of the rectangle is numerically equal to the probability density. The difference from histograms for discrete quantities lies in the values reported on the vertical axis: for discrete quantities the values are probabilities (the areas of the rectangles), but for continuous quantities they are probability densities (the heights of the rectangles). This is also evident from the fact that the values reported on the vertical axis can be larger than 1, as in the plots on the side.\nThe rectangles, however, are so thin (usually thinner than a pixel on a screen) that they appear just as vertical lines, and together they look just like a curve delimiting a coloured area. If we don’t colour the area underneath we just have a line-based representation of the probability density.\n\n\n   As the width \\(\\epsilon\\) of the small ranges is decreased, a histogram based on these widths become indistinguishable from a line plot\n\n\nScatter plots\nLine plots of a probability density are very informative, but they can also be slightly deceiving. Try the following experiment.\nConsider a continuous quantity \\(X\\) with the following probability density:\n\nWe want to represent the amount of probability in any small range, say between \\(X\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}0\\) and \\(X\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}0.1,\\) by drawing in that range a number of short thin lines, the number being proportional to the probability. So a range with 10 lines has twice the probability of a range with 5 lines. The probability density around a value is therefore roughly represented by the density of the lines around that value.\nSuppose that we have 50 lines available to distribute this way. Where should we place them?\n\n\n\n\n\n\n\n Exercise\n\n\n\n\nWhich of these plots shows the correct placement of the 50 lines? (NB: the position of the correct answer is determined by a pseudorandom-number generator.)\n\n\n\n\n\n\n(A)\n\n\n\n\n\n\n\n(B)\n\n\n\n\n\n\n\n\n\n(C)\n\n\n\n\n\n\n\n(D)\n\n\n\n\n\n\n\n\n\nIn a scatter plot, the probability density is (approximately) represented by density of lines, or points, or similar objects, as in the examples above (only one above, though, correctly matches the density represented by the curve).\nAs the experiment and exercise above may have demonstrated, line plots sometimes give us slightly misleading ideas of how the probability is distributed across the domain; for example, peaks at some values make us overestimate the probability density around those values. Scatter plots often give a less misleading representation of the probability density.\nScatter plots are also useful for representing probability densities in more than one dimension – sometimes even in infinite dimensions! They can moreover be easier to produce computationally than line plots.\n@@ TODO Behaviour of representations under transformations of data.\n\n\n\n\n\n\n\n\n Reading\n\n\n\n\n§§ 5.3–5.3.1 of Risk Assessment and Decision Analysis with Bayesian Networks"
  },
  {
    "objectID": "probability_distributions.html#sec-combined-probs",
    "href": "probability_distributions.html#sec-combined-probs",
    "title": "8  Probability distributions",
    "section": "8.6 Combined probabilities",
    "text": "8.6 Combined probabilities\nA probability distribution is defined over a set of mutually exclusive and exhaustive sentences. In some inference problems, however, we do not need the probability of those sentences, but of some other sentence that can be obtained from them by an or operation. The probability of this sentence can then be obtained by a sum, according to the or-rule of inference. We can call this a combined probability. Let’s explain this procedure with an example.\nBack to our initial assembly-line scenario from § 1, the inference problem was to predict whether a specific component would fail within a year or not. Consider the time when the component will fail (if sold), and represent it by the quantity \\(F\\) with the following 24 different values, where “\\(\\mathrm{mo}\\)” stands for “months”: \\[\\begin{aligned}\n&\\textsf{\\small`The component will fail during it 1st month of use'}\\\\\n&\\textsf{\\small`The component will fail during it 2nd month of use'}\\\\\n&\\dotsc \\\\\n&\\textsf{\\small`The component will fail during it 23rd month of use'}\\\\\n&\\textsf{\\small`The component will fail during it 24th month of use or after'}\n\\end{aligned}\\] which we can shorten to  \\(F\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}1,\\ F\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}2,\\ \\dotsc,\\ F\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}24\\). Note the slightly different meaning of the last value.\n\n\n\n\n\n\n Exercise\n\n\n\nWhat is the basic type of the quantity \\(F\\)? Which other characteristics does it have? for instance discrete? unbounded? rounded? uncensored?\n\n\nSuppose that the inspection device – our agent – has internally calculated a probability distribution for \\(F\\), conditional on its internal programming and the results of the tests on the component, collectively denoted \\(\\mathsfit{I}\\). The probabilities, compactly written, are \\[\n\\mathrm{P}(F\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}1 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}), \\quad\n\\mathrm{P}(F\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}2 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}), \\quad\n\\dotsc, \\quad\n\\mathrm{P}(F\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}24 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\n\\] Their values are stored in this csv file and plotted in the histogram on the side.\n\nWhat’s important for the agent’s decision about rejecting or accepting the component, is not the exact time when it will fail, but only whether it will fail within the first year or not. That is, the agent needs the probability of the sentence \n\n\\(\\textsf{\\small`The component will fail within a year of use'}\\). But this sentence is just the or of the first 12 sentences expressing the values of \\(F\\): $$\n\\[\\begin{aligned}\n&\\textsf{\\small`The component will fail within a year of use'}\n\\\\&\\qquad{}\\equiv\n\\textsf{\\small`The component will fail during it 1st month of use'}\n\\lor{}\n\\\\&\\qquad\\qquad\n\\textsf{\\small`The component will fail during it 2nd month of use'}\n\\lor \\dotsb\n\\\\&\\qquad\\qquad\n\\dotsb \\lor\n\\textsf{\\small`The component will fail during it 12th month of use'}\n\\\\[1ex]&\\qquad{}\\equiv\n(F\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}1) \\lor (F\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}2) \\lor \\dotsb \\lor (F\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}12)\n\\end{aligned}\\]\n\\[\nThe probability needed by the agent is therefore\n\\]\n(F F F| )\n\\[\nwhich can be calculated using the `or`-rule, considering that the sentences involved are mutually exclusive:\n\\]\n\\[\\begin{aligned}\n&\\mathrm{P}(\\textsf{\\small`The component will fail within a year of use'} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\\\[1ex]&\\qquad{}=\n\\mathrm{P}(F\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}1 \\lor F\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}2 \\lor \\dotsb \\lor F\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}12\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\\\[1ex]&\\qquad{}=\n\\mathrm{P}(F\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}1 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}) + \\mathrm{P}(F\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}2 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}) + \\dotsb + \\mathrm{P}(F\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}12 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\n\\\\[1ex]&\\qquad{}= \\sum_{f=1}^{12} \\mathrm{P}(F\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}f \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\end{aligned}\\]\n$$\n\n\n\n\n\n\nSum notation\n\n\n\nWe shall often use the \\(\\sum\\)-notation for sums, as in the example above. A notation like “\\(\\displaystyle\\sum_{i=5}^{20}\\)” means: write multiple copies of what’s written on its right side, and in each copy replace the symbol “\\(i\\)” with values from \\(5\\) to \\(20\\), in turn; then sum up these copies. The symbol “\\(i\\)” is called the index of the sum. Sometimes the initial and final values, \\(5\\) and \\(20\\) in the example, are omitted if they are understood from the context, and the sum is written simply “\\(\\displaystyle\\sum_{i}\\)”.\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nUsing your favourite programming language:\n\nLoad the csv file containing the probabilities.\nInspect this file, find the headers of its columns and so on.\nCalculate the probability that the component will fail within a year of use.\nCalculate the probability that the component will fail “within two months or use or after a year of use”."
  },
  {
    "objectID": "joint_probability.html#joint-probabilities",
    "href": "joint_probability.html#joint-probabilities",
    "title": "9  Joint probability distributions",
    "section": "9.1 Joint probabilities",
    "text": "9.1 Joint probabilities\nA joint quantity, as discussed in § 13.1, is just a collection or set of quantities of basic types. Saying that a joint quantity has a particular value means that each basic component quantity has a particular value in its specific domain. This is expressed by an and of sentences.\nConsider for instance the joint quantity \\(X\\) consisting of the age \\(\\color[RGB]{102,204,238}A\\) and sex \\(\\color[RGB]{34,136,51}S\\) of a specific person. The fact that \\(X\\) has a particular value is expressed by a composite sentence such as\n\\[\n\\textsf{\\small`The person's age is 25 years and the sex is female'}\n\\]\nwhich we can compactly write with an and:\n\\[\n{\\color[RGB]{102,204,238}A\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}25\\,\\mathrm{y}} \\land {\\color[RGB]{34,136,51}S\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}\\mathrm{f}}\n\\]\nAll the possible composite sentences of this kind are mutually exclusive and exhaustive.\nAn agent’s uncertainty about \\(X\\)’s true value is therefore represented by a probability distribution over all and-ed sentences of this kind, representing all possible joint values: $$ ({A ,} | )  , ({A ,} | )  , \n\\[\nwhere $\\mathsfit{I}$ is the agent's state of knowledge, and the probabilities sum up to one. We call each of these probabilities a [**joint probability**]{.blue}, and their collection a [**joint probability distribution**]{.blue}. Usually these probabilities are written in much abbreviated form, and the $\\land$ is represented by a comma; for instance you can commonly find the following notation:\n\\]\n(A, S | )  , (A, S | )\n\\[\nor even just\n\\]\n(25, | )  , (31, | ) $$"
  },
  {
    "objectID": "joint_probability.html#joint-probability-densities",
    "href": "joint_probability.html#joint-probability-densities",
    "title": "9  Joint probability distributions",
    "section": "9.2 Joint probability densities",
    "text": "9.2 Joint probability densities\n@@ TODO"
  },
  {
    "objectID": "joint_probability.html#representation-of-joint-probability-distributions",
    "href": "joint_probability.html#representation-of-joint-probability-distributions",
    "title": "9  Joint probability distributions",
    "section": "9.3 Representation of joint probability distributions",
    "text": "9.3 Representation of joint probability distributions\nThere is a wide variety of ways of representing joint probability distributions, and new ways are invented (and rediscovered) all the time. In some cases, especially when the quantity has more than three component quantities, it can become impossible to graphically represent the probability distribution in a faithful way. Therefore one often tries to represent only some aspects or features of interest from the full distribution. Whenever you see a plot of a joint probability distribution, you should carefully read what the plot shows and how it was made. Here we only illustrate some examples and ideas for representations.\n\nTables\nWhen a quantity is bivariate and its two component quantities are both discrete and finite, the joint probabilities can be reported as a table.\nExample: Consider the next patient that will arrive at a particular hospital. There’s the possibility of arrival by ambulance, helicopter, or other transportation means; and the possibility that the patient will need urgent non-urgent care. These can be seen as two quantities \\(A\\) (nominal) and \\(U\\) (binary). When these two quantities are taken together; their joint probability distribution is as follows, conditional on the hospital’s data \\(\\mathsfit{I}_{\\text{H}}\\):\n\n\nTable 9.1: Joint probability distribution for transportation at arrival and urgency\n\n\n\n\n\n\n\n\n\n\\(\\mathrm{P}(A\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}a,U\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}u\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}_{\\text{H}})\\)\n\narrival \\(A\\)\n\n\n\n\nambulance\nhelicopter\nother\n\n\nurgency \\(U\\)\nurgent\n0.11\n0.04\n0.03\n\n\nnon-urgent\n0.17\n0.01\n0.64\n\n\n\n\nWe see for instance that the most probable possibility is that the next patient will arrive by transportation means other than ambulance and helicopter, and won’t require urgent care.\nIt is also possible to replace the numerical probability values with graphical representations; for example as shades of a colour, or squares with different areas.\n@@ TODO ref to marginals section\n\n\nMulti-line plots\nJoint probability distributions over one discrete and one continuous quantity (or ordinal discrete with numerous values) can be represented as a collection of line plots: each line plot represent the probability density for the continuous quantity and a specific value of the discrete quantity.\nConsider for instance the probability that the next patient who arrives at a particular hospital has a given age (continuous quantity) and may require urgent care or not (binary quantity). The joint probability density can be visualized as in the plot in the margin. From the plot we can see that the probability of urgent patients is generally lower than non-urgent ones. A possibly disadvantage of this kind of plots is evident: the details, such as peaks, of the density for some values of the discrete quantity may be barely visible.\n\n\n\n\n\nSurface plots\n\n\nScatter plots\n@@ TODO work also for 3D\n\n\n\n\n\n\n Reading\n\n\n\n\n§§ 5.3.2–5.3.3 of Risk Assessment and Decision Analysis with Bayesian Networks\n§ 12.2.2 of Artificial Intelligence\n§§ 5.1–5.5 of Probability"
  },
  {
    "objectID": "marginal_conditional_probability.html#sec-marginal-probs",
    "href": "marginal_conditional_probability.html#sec-marginal-probs",
    "title": "10  Marginal and conditional probabilities",
    "section": "10.1 Marginal probability distributions",
    "text": "10.1 Marginal probability distributions\nIn some situations an agent has the joint probability distribution for a joint quantity, but it needs the probability for one of the component quantities only, irrespective of what the values for the other components might be.\nConsider for instance the joint probability for the next-patient arrival scenario of table 9.1. We may be interested in the probability that the next patient will need urgent care, independently of how the patient gets to the hospital. This probability can be found, as usual, by analysing the problem in terms of sentences and using the basic rules of inference from § 7.4.\nThe sentence of interest is\n\\[\n\\textsf{\\small`The next patient will require urgent care'}\n\\]\nwhich is equivalent to\n\\[\n\\textsf{\\small`The next patient will require urgent care, and will arrive by ambulance, helicopter, or other means'}\n\\]\nThis last sentence can be written in terms of and and or connectives:\n\\[\n\\begin{aligned}\n&\\textsf{\\small`The next patient will require urgent care'} \\land{}\n\\\\[1ex]\n&\\qquad\\bigl(\\textsf{\\small`The next patient will arrive by ambulance'} \\lor {}\n\\\\&\\qquad\\qquad\n\\textsf{\\small`The next patient will arrive by helicopter'} \\lor {}\n\\\\&\\qquad\\qquad\n\\textsf{\\small`The next patient will arrive by other means'}\\bigr)\n\\end{aligned}\n\\]\nwhich we can compactly write as\n\\[\n\\begin{aligned}\n&\\textsf{\\small urgent}\\land\n(\\textsf{\\small ambulance} \\lor \\textsf{\\small helicopter} \\lor \\textsf{\\small other})\n\\\\[1ex]\n&\\qquad{}=\n(\\textsf{\\small urgent}\\land \\textsf{\\small ambulance}) \\lor\n(\\textsf{\\small urgent}\\land \\textsf{\\small helicopter}) \\lor\n(\\textsf{\\small urgent}\\land \\textsf{\\small other})\n\\end{aligned}\n\\]\nwhere the second, equivalent form comes from the derived rules of Boolean algebra of § 7.7.1.\nThe last sentence is an or of mutually exclusive sentences. Its probability is therefore given by the or rule:\n\\[\n\\begin{aligned}\n&\\mathrm{P}\\bigl[\n(\\textsf{\\small urgent}\\land \\textsf{\\small ambulance}) \\lor\n(\\textsf{\\small urgent}\\land \\textsf{\\small helicopter}) \\lor\n(\\textsf{\\small urgent}\\land \\textsf{\\small other})\n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}} \\bigr]\n\\\\[1ex]\n&\\qquad{}=\n\\mathrm{P}(\\textsf{\\small urgent}\\land \\textsf{\\small ambulance}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}} ) +\n\\mathrm{P}(\\textsf{\\small urgent}\\land \\textsf{\\small ambulance}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}} ) +\n\\mathrm{P}(\\textsf{\\small urgent}\\land \\textsf{\\small other}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}} )\n\\end{aligned}\n\\]\nThe probability for a value of the urgency quantity, independently of the value of the arrival-means quantity, can be found by summing all joint probabilities with all possible arrival-means values. Using the \\(\\sum\\)-notation we can write     \n\\[\n\\mathrm{P}(\\textsf{\\small urgent} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}}) =\n\\sum_{\\mathclap{a=\\textsf{\\small ambulance}}}^{\\mathclap{\\textsf{\\small other}}}\n\\mathrm{P}(\\textsf{\\small urgent}, A\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}a \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}})\n\\]\nThis is called a marginal probability.\n\n\n\n\n\n\n Exercise\n\n\n\nUsing the values from table 9.1, calculate:\n\nthe marginal probability that the next patient will need urgent care\nthe marginal probability that the next patient will arrive by helicopter\n\n\n\nConsidering now a more abstract case for a bivariate quantity with component quantities \\(X\\) and \\(Y\\), the probability for a specific value of \\(X\\), conditional on some information \\(\\mathsfit{I}\\) and irrespective of what the value of \\(Y\\) might be, would be given by\n\\[\n\\mathrm{P}(X\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}x\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = \\sum_{y} \\mathrm{P}(Y\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}y, X\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\]\nYou may notice the similarity with the expression for a combined probability from § 8.6. Indeed a marginal probability is just a special case of a combined probability: we are combining all probabilities that exhaust the possibilities for the quantity \\(Y\\).\n\n\n\n\n\n\n Exercise: test your understanding\n\n\n\nUsing again the values from table 9.1, calculate the probability that the next patient will need urgent care and will arrive either by ambulance or by helicopter."
  },
  {
    "objectID": "marginal_conditional_probability.html#sec-conditional-probs",
    "href": "marginal_conditional_probability.html#sec-conditional-probs",
    "title": "10  Marginal and conditional probabilities",
    "section": "10.2 Conditional probability distributions",
    "text": "10.2 Conditional probability distributions\nA joint probability distribution quantifies an agent’s uncertainty about all the quantities that compose a joint quantity. This means that in general the agent has no sure certainty about any of them (there are of course special cases where probabilities can be \\(0\\) or \\(1\\), but they are not the general case). Suppose that the agent has a joint probability for the quantities \\(X\\) and \\(Y\\), conditional on a state of knowledge \\(\\mathsfit{I}\\): \\[ \\mathrm{P}(X\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}x,Y\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}) \\]         \nNow consider these two situations:\n\nthe agent acquires knowledge that \\(X\\) has true value \\(x\\), so its state of knowledge has changed;\nfor inference purposes, the agents needs to hypothetically assume that the quantity \\(X\\) has value \\(x\\) (even if that might not be the case in reality);\n\nand, in either situation, the agent needs the probability that \\(Y\\) has value \\(y\\). This probability is written, in either situation as\n\\[\n\\mathrm{P}(Y\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}x, \\mathsfit{I})\n\\]\nIt is usually called a conditional probability. It is somewhat a misnomer, because all probabilities are conditional on something. The implicit understanding here is that new information has been added to the conditional with respect to some base state of knowledge.\nWhat is the numerical value of the conditional probability above? We simply use the and-rule: $$ (Yy | Xx, ) = \n\\[\nThe denominator is the *marginal probability* for $X$, which can also be calculated from the joint distribution as discussed in [§ @sec-marginal-probs]:\n\\]\n(Xx | ) = _{y} (Yy, Xx |) $$\nWe can thus rewrite the conditional probability as \\[\n\\mathrm{P}(Y\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}x, \\mathsfit{I}) =\n\\frac{\\mathrm{P}(Y\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}y, X\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})}{\\sum_{y} \\mathrm{P}(Y\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}y, X\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})}\n\\]\n\n\n\n\n\n\n Exercise\n\n\n\nConsider the next-patient problem with the joint probability of table 9.1:\n\nCalculate the probability that the next patient needs urgent care, knowing that the patient will arrive by helicopter.\nCompare the conditional probability you just found with the marginal probability that the next patient needs urgent care, independently of transportation means (calculated in a previous exercise). Which is higher? Why?\nCalculate the probability that the next patient arrives by helicopter, knowing that the patient will need urgent care.\nCompare the conditional probability from 3. with that from 1. Why are they so different? Why is the probability “\\(\\textsf{\\small helicopter}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\textsf{\\small urgent}\\)” so low, when “\\(\\textsf{\\small urgent}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\textsf{\\small helicopter}\\)” is so high instead? find an intuitive explanation."
  },
  {
    "objectID": "marginal_conditional_probability.html#the-importance-of-marginal-distributions-for-drawing-inferences",
    "href": "marginal_conditional_probability.html#the-importance-of-marginal-distributions-for-drawing-inferences",
    "title": "10  Marginal and conditional probabilities",
    "section": "10.3 The importance of marginal distributions for drawing inferences",
    "text": "10.3 The importance of marginal distributions for drawing inferences\n@@ TODO"
  },
  {
    "objectID": "quantities_types.html#quantities",
    "href": "quantities_types.html#quantities",
    "title": "12  Quantities and data types",
    "section": "12.1 Quantities",
    "text": "12.1 Quantities\n\nQuantities, values, domains\nMost decisions and inferences in engineering and data science involve things or properties of real things. We represent them by mathematical objects – most often, collections of numbers – with particular mathematical properties and operations. The mathematical properties reflect the kind of activities that we can do with these things. For instance, colours are represented by particular tuples of numbers, and these tuples can be multiplied by some numeric weights and added, to obtain another tuple. This mathematical operation represents the fact that colours can be obtained by mixing other colours in different proportions. Physics and engineering are founded on this approach.\nIt’s difficult to find a general term to denote any instance of such “things” and their mathematical representation, but it’s convenient to find one for presenting the general theory without getting bogged down in individual cases. We’ll borrow the term quantity from physics and engineering.\nWe distinguish between a quantity and its value. For instance, a quantity could be “The temperature at the point with coordinates  60.3775029, 5.3869233, 643,  at time 1895-10-04T10:03:14Z”; and its value could be \\(24\\,\\mathrm{°C}\\).\nThis distinction is necessary in inference and decision problems, because we may not know the value of a particular quantity. We then consider every possible value that quantity could have, and we can assign a probability to each. The set of possible values is called the domain of the quantity. In our temperature example, the domain is the set of all possible temperatures from \\(0\\,\\mathrm{K}\\) and above.\nAnother example:\n\nquantity: the image taken by a particular camera at a particular time, represented by a specific collection of numbers (say \\(128\\times128\\times3\\) integer numbers between \\(0\\) and \\(255\\))\nvalues: one possible value is this:  (corresponding to a grid of \\(128\\times128\\times3\\) specific numbers), another possible value is this: , and there are many other possible values\ndomain: the collection of \\(256^{3\\times128\\times128} \\approx 10^{118 370}\\) possible images (corresponding to the collection of possible grids of numeric values)\n\nOther examples of quantities and domains:\n\nThe distance between two objects in the Solar System at a specific time. The domain could be, say, all values from \\(0\\,\\mathrm{m}\\) to \\(6\\cdot10^{12}\\,\\mathrm{m}\\) (Pluto’s average orbital distance).\nThe number of total views of some online video (at a specific time), with a domain, say, from 0 to 20 billions.\nThe force on an object (at a specific time and place). The domain could be, say, 3D vectors with components in \\([-100\\,\\mathrm{N},\\,+100\\,\\mathrm{N}]\\).\nThe degree of satisfaction in a customer survey, with five possible values Not at all satisfied, Slightly satisfied, Moderately satisfied, Very satisfied, Extremely satisfied.\nThe graph representing a particular social network. Individuals are represented by nodes, and different kinds of relationships by directed or undirected links between nodes, possibly with numbers indicating their strength. The domain consists of all possible graphs with, say, \\(0\\) to \\(10000\\) nodes and all possible combinations of links and weights between the nodes.\nThe relationship between the input voltage and output current of an electric component. The domain could be all possible continuous curves from \\([0\\,\\mathrm{V}, 10\\,\\mathrm{V}]\\) to \\([0\\,\\mathrm{A}, 1\\,\\mathrm{A}]\\).\nA 1-minute audio track recorded by a device with a sampling frequency of 48 kHz. The domain could be all possible 2 880 000 values in \\([0,1]\\).\nThe subject of an image, with domain of three possible values cat, dog, something else.\nThe roll, pitch, yaw of a rocket (at a specific time and place), with domain \\((-180°,+180°]\\times(-90°,+90°]\\times(-180°,+180°]\\).\n\n\n\nThe vague term “data” typically means the values of a collection of quantities.\n\n\n\n\n\n\n Quantity vs variate\n\n\n\nIn these notes, we agree that a quantity has one, and only one, value.\nWe can consider something that changes with time, or with space, or from individual to individual, or from unit to unit. This “something” is then a collection of quantities: one for each time, or space, or individual. We call this collection a variate, especially when it refers to individuals or unit; or a variable.\nThese are just terminological conventions adopted in these notes. Different texts often adopt different terms. What matters is not the terms, but that you have a clear understanding of the difference between the two notions that here we call “quantity” and “variate”.\n\n\n\n\nNotation\nWe shall denote quantities by italic letters, such as \\(X\\) for example. The sentences that appear in decision-making and inferences are therefore often of the kind “the quantity \\(X\\) was observed to have value \\(x\\)”, where “\\(x\\)” stands for a specific value, for instance . This kind of sentences are often abbreviated like “\\(X\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}x\\)”.\n\n\n\n\n\n\n\n\n\n\n but keep in mind our discussion from § 5.3: we must make clear what that “\\(=\\)” means; it could mean “observed”, “set”, “reported”, and so on."
  },
  {
    "objectID": "quantities_types.html#basic-types-of-quantities",
    "href": "quantities_types.html#basic-types-of-quantities",
    "title": "12  Quantities and data types",
    "section": "12.2 Basic types of quantities",
    "text": "12.2 Basic types of quantities\nAs the examples above show, quantities and data come in all sorts and with different degrees of complexity. There is no clear-cut divide between different sorts of quantities. The same quantity can moreover be viewed and represented in many different ways, depending on the specific context, problem, purpose, and background information.\nIt is possible, however, to roughly differentiate between a handful of basic types of quantities, from which more complex types are built. Here is one kind of differentiation that is useful for inference problems about quantities:\n\nNominal\nA nominal or categorical quantity has a domain with a discrete and usually finite number of values. The values are not related by any mathematical property, and do not have any specific order.\nThis means that it does not make sense to say, for instance, that some value is “twice” or “1.5 times” another, or “larger” or “later” than another one. Nor does it make sense to “add” two quantities. In particular, there is no notion of average for a nominal quantity.\nExamples: the possible breeds of a dog, or the characters of a film.\nIt is of course possible to represent the values of a nominal quantity with numbers; say 1 for Dachshund, 2 for Labrador, 3 for Dalmatian, and so on. But that doesn’t mean that\nDalmatian\\({}-{}\\)Labrador\\({}={}\\)Labrador\\({}-{}\\)Dachshund\nor similar nonsense.\n\n\nOrdinal\nAn ordinal quantity has a domain with a discrete and usually finite number of values. The values are not related by any mathematical property, but they do have a specific order.\nThis means that it does not make sense to say that some value is “twice” or “1.5 times” another, and we cannot “add” two values. But it does make sense to say, for any two values, which one has higher rank, for example “stronger”, or “later”, “larger”, and similar. Also in this case there is no notion of average for an ordinal quantity.\nExample: a pain-intensity scale. A patient can say whether some pain is more severe than another, but it isn’t clear what pain “twice as severe” as another would mean (although there’s a lot of research on more precise quantification of pain). Another example: the “strength of friendship” in a social network. We can say that we have a “stronger friendship” with a person than with another; but it doesn’t make sense to say that we are “four times stronger friends” with a person than with another.\nIt is possible to represent the values of an ordinal quantity with numbers which reflect the order of the values. But it’s important to keep in mind that differences or averages of such numbers does not make sense. For this reason the use of numbers can be deceptive at times. A less deceptive possibility is to represent ordered values by alphabet letters, for example.\n\n\nBinary\nA binary or dichotomous quantity has only two possible values. It can be seen as a special case of a nominal or ordinal quantity, but the fact of having only two values lends it some special properties in inference problems. This is why we list it separately.\nObviously it doesn’t make much sense to speak of the difference or average of the two values; and their ranking is trivial even if it makes sense.\nThere’s an abundance of examples of binary quantities: yes/no answers, presence/absence of something, and so on.\n\n\nInterval\nAn interval quantity has a domain that can be discrete or continuous, finite or infinite. The values do admit some mathematical operations, at least convex combination and subtraction. They also admit an ordering.\nThis means that we can say, at the very least, whether the interval or “distance” between a pair of values is the same, or larger, or smaller than the interval between another pair. For this reason we can also say whether a value is larger than another. We can also take weighted sums of values, called convex combinations (but simple addition of values may still be meaningless, though).\nOwing to these mathematical properties, it does make sense to speak of the average for an interval quantity.\nThe number of electronic components produced in a year by an assembly line is an example of a discrete interval quantity. The power output of a nuclear plant at a given time is an example of a continuous one.\nIt is also possible to speak of ratio quantities, which are a special case of interval quantities, but we won’t have use of this distinction in the present notes.\n\n\nHow to decide the basic type of a quantity?\nTo attribute a basic type to a quantity we must ultimately check how that quantity is defined, obtained, and used. In some cases the values of the quantity may give some clue; for example, if we see values “\\(2.74\\)”, “\\(8.23\\)”, “\\(3.01\\)”, then the quantity is probably of the interval kind. But if we see values “\\(1\\)”, “\\(2\\)”, “\\(3\\)”, it’s unclear whether the quantity is interval, ordinal, nominal, or maybe of yet some other kind.\nThe type of a quantity also depends on its use in the specific problem. A quantity of a more complex type can be treated as a simpler type if needed. For instance, the response time of some device is in principle an interval quantity; but in a specific situation we could simply label its values as slow, medium, fast, thus making it an ordinal quantity.\n@@ TODO: add examples for image spaces\n\n\n\n\n\n\n Exercises\n\n\n\n\nFor each example at the beginning of the present section, assess whether that quantity can be considered as being of a basic type, and which type.\nFor each basic type discussed above, find two more concrete examples of that type of quantity\n\n\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nOn the theory of scales of measurement"
  },
  {
    "objectID": "quantities_types.html#other-attributes-of-basic-types",
    "href": "quantities_types.html#other-attributes-of-basic-types",
    "title": "12  Quantities and data types",
    "section": "12.3 Other attributes of basic types",
    "text": "12.3 Other attributes of basic types\nIt is useful to consider other basic aspects of quantities that are somewhat transversal to “type”. These aspects are also important when drawing inferences.\n\nDiscrete vs continuous\nNominal and ordinal quantities have discrete domains. The domain of an interval quantity can be discrete or continuous. In practice all domains are discrete, since we cannot observe, measure, report, or store values with infinite precision. In a modern computer, for example, a real number can “only” take on \\(2^{64} \\approx 20 000 000 000 000 000 000\\) possible values. In many situations the available precision is so high that we can consider the quantity as continuous for all practical purposes and use the mathematics of continuous sets – derivation, integration, and so on – to our advantage.\n@@ TODO comment on repetition\n\n\nBounded vs unbounded\nOrdinal and interval quantities may have domains with no minimum value, or no maximum value, or neither. Typical terms for these situations are lower- or upper-bounded, or left- or right-bounded, and unbounded; or similar terms.\nWhether to treat a quantity domain as bounded or unbounded depends on the quantity, the specific problem, and the computational resources. For example, the number of times a link on a webpage has been clicked can in principle be (right-)unbounded. Another example is the distance between two objects: we can consider it unbounded, but in concrete problems might be bounded, say, by the size of a laboratory, or by Earth’s circumference, or the Solar System’s extension, and so on.\n\n\n\n\n\n\n Exercises\n\n\n\n\nIf you had to set a maximum number of times a web link can be clicked, what number would you choose? Try to find a reasonable number, considering factors such as how fast a person can repeatedly click on a link, how long can a website (or the Earth?) last, and how many people can live in such an extent of time.\nWhat about the age of a person? What bound would you set, if you had to treat it as a bounded quantity?\n\n\n\n\n\nFinite vs infinite\nThe domain of a discrete quantity can consist of a finite or – at least in theory – an infinite number of possible values (the domain of a continuous quantity always has an infinite number of values). A domain can be infinite and yet bounded: consider the numbers in the range \\([0,1]\\).\nWhether to treat a domain as finite or infinite depends on the quantity, the specific problem, and the computational resources. For example, the intensity of a base colour in a pixel of a particular image might really take on 256 discrete steps between \\(0\\) and \\(1\\): \\(0, 0.0039215686, 0.0078431373, \\dotsc, 1\\). But in some situations we can treat this domain as practically infinite, with any possible value between \\(0\\) and \\(1\\).\n\n\nRounded\nA continuous interval quantity may be rounded, owing to the way it’s measured. In this case the quantity could be considered discrete rather than continuous. Rounding can impact the way we do inferences about such a quantity.\n\n\n The Iris dataset from its original paper\nThe famous Iris dataset, for instance, consists of several lengths – continuous interval quantities – of parts of flowers. All values are rounded to the millimetre, even if in reality the lengths could have intermediate values, of course. The age of a person is another frequent example of an in-principle continuous quantity which is rounded, say to the year or the month.\nIn some situations it’s important to be aware of rounding, because it can lead to quantities with different unrounded values to have identical rounded ones.\n\n\nCensored\nThe measurement procedure of a quantity may have an artificial lower or upper bound. A clinical thermometer, for instance, could have a maximum reading of \\(45\\,\\mathrm{°C}\\). If we measure with it the temperature of a \\(50\\,\\mathrm{°C}\\)-hot body, we’ll read “\\(45\\,\\mathrm{°C}\\)”, not the real temperature.\nA quantity with this characteristic is called censored, more specifically left-censored or right-censored when there’s only one artificial bound. The bound is called the censoring value.\nA censoring value denotes an actual value that could also be greater or less. This is important when we draw inferences about this kind of quantities."
  },
  {
    "objectID": "data_types_multi.html#sec-data-multiv",
    "href": "data_types_multi.html#sec-data-multiv",
    "title": "13  Joint quantities and complex data types",
    "section": "13.1 Joint quantities",
    "text": "13.1 Joint quantities\nSome sets of basic quantities are just that: simple collections of quantities, in the sense that they do not have new properties or allow for new kinds of operations. We shall call these joint quantities when we need to distinguish them from quantities of a basic kind; but usually they are also simply called quantities.\nThe values of a joint quantity are just tuples of values of its basic component quantities. Their domain is the Cartesian product of the domains of the basic quantities.\nConsider for instance the age, sex1, and nationality of a particular individual. They can be represented as an interval-continuous quantity \\(A\\), a binary one \\(S\\), and a nominal one \\(N\\). We can join them together to form the joint quantity  “(age, sex, nationality)”  which can be denoted by  \\((A,S,N)\\).  One value of this joint quantity is, for example, \\((25\\,\\mathrm{y}, \\text{\\small F}, \\text{\\small Norwegian})\\). The domain could be \\[\n[0,+\\infty)\\times\n\\{\\text{\\small F}, \\text{\\small M}\\} \\times\n\\{\\text{\\small Afghan}, \\text{\\small Albanian}, \\dotsc, \\text{\\small Zimbabwean}\\}\n\\]1 We define sex by the presence of at least one Y chromosome or not. It is different from gender, which involves how a person identifies.\n\nDiscreteness, boundedness, continuity\nA joint quantity may not be simply characterized as “discrete”, or “bounded”, or “infinite”, and so on. Usually we must specify these characteristics for each of its basic component quantities instead. Sometimes a joint quantity is called, for instance, “continuous” if all its basic components are continuous; but other conventions are also used.\n\n\n\n\n\n\n Exercises\n\n\n\nConsider again the examples of § 12.1.1. Do you find any examples of joint quantities?"
  },
  {
    "objectID": "data_types_multi.html#sec-data-complex",
    "href": "data_types_multi.html#sec-data-complex",
    "title": "13  Joint quantities and complex data types",
    "section": "13.2 Complex quantities",
    "text": "13.2 Complex quantities\nSome complex quantities can be represented as sets of quantities of basic types. These sets, however, are “more than the sum of their parts”: they possess new physical and mathematical properties and operations that do not apply or do not make sense for the single components.\nFamiliar examples are vectorial quantities from physics and engineering, such as location, velocity, force, torque. Another example are images, when represented as grids of basic quantities.\nConsider for example a 4 × 4 monochrome image, represented as a grid of 16 binary quantities \\(0\\) or \\(1\\). Three possible values could be these:\n    \nrepresented by the numeric matrices   \\(\\begin{psmallmatrix}1&0&0&0\\\\0&0&0&0\\\\0&0&0&0\\\\0&0&0&0\\end{psmallmatrix}\\), \\(\\begin{psmallmatrix}0&1&0&0\\\\0&0&0&0\\\\0&0&0&0\\\\0&0&0&0\\end{psmallmatrix}\\), \\(\\begin{psmallmatrix}0&0&0&0\\\\0&0&0&0\\\\0&0&0&0\\\\0&0&0&1\\end{psmallmatrix}\\).\nFrom the point of view of the individual binary quantities, these three “values” are equally different from one another: where one of them has grid value \\(1\\), the others have \\(0\\). But properly considered as images, we can say that the first and the second are somewhat more “similar” or “closer” to each other than the first and the third. This similarity can be represented and quantified by a metric over the domain of all such images. This metric involves all basic binary quantities at once; it is not a property of each of them individually.\nMore generally, complex quantities have additional, peculiar properties, represented by mathematical structures, which distinguish them from simpler joint quantities; although there is not a clear separation between the two.\nThese properties and structures are very important for inference problems, and usually make them computationally very hard. The importance of machine-learning methods lies to a great extent in the fact that they allow us to do approximate inference on these kinds of complex data. The peculiar structures of these data, however, are often also the cause of striking failures of some machine-learning methods, for example the reason why they may classify incorrectly, or correctly but for the wrong reasons."
  },
  {
    "objectID": "statistics.html#the-difference-between-statistics-and-probability-theory",
    "href": "statistics.html#the-difference-between-statistics-and-probability-theory",
    "title": "15  Statistics",
    "section": "15.1 The difference between Statistics and Probability Theory",
    "text": "15.1 The difference between Statistics and Probability Theory\nStatistics is the study of collective properties of collections of data. It does not imply that there is any uncertainty.\nProbability theory is the quantification and propagation of uncertainty. It does not imply that we have collections of data."
  },
  {
    "objectID": "populations_exchangeability.html#sec-exchangeability",
    "href": "populations_exchangeability.html#sec-exchangeability",
    "title": "16  Populations & exchangeability",
    "section": "16.1 Exchangeable inferences",
    "text": "16.1 Exchangeable inferences\n\nIntuitive understanding\nConsider these sketches of two inference problems:\n\n\nStock exchange\n\nIn 100 days, the daily change in closing price of a stock has been positive 74 times, and negative 26 times, according a particular sequence. For instance, the data could be:\n\n\n\nIn which of the subsequent 3 days will the closing-price change be positive, and in which negative?\n\n\n\n\n\n\n\n\nMars prospecting\n\nOf the last 100 similar-sized rocks examined in a large crater on Mars, 74 contained hematite, and 26 didn’t. For instance, the data could be:\n\n\n\nwhere “Y” and “N” denote the presence and absence of hematite.\nWhich, among the next 3 rocks that will be examined, will contain hematite, and which will be hematite-free?\n\n\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nDiscuss:\n\nWhich of the two inferences above seems more difficult?…\n…Why? Speculate on which factors make one inference more difficult than the other.\nWhich differences and similarities do you find between the two inferences?\nWhich additional information could be important for drawing more precise inferences?\nWhich type of quantities appear in the two inferences?\n\n\n\n\n\n\n\n\nThe two inference problems differ in some aspects and are similar in others. Time, for example, appears to be an important aspect of the first inference, which is in fact often categorized as a “time-series analysis”. But the second inference involves or could involve time as well: the rocks could be inspected sequentially. So more precisely it is the time ordering that is more relevant in the first inference than the second.\nAn important difference between the two inference problems is revealed by the following thought-experiments. Consider how you would answer in the stock-exchange case and in the Mars-prospecting case:\n\nIt turns out that there was an error in the sequence of positive and negative datapoints, although not in the total amounts of positive and negative ones. In the stock-exchange inference, for instance, days 1 (\\(-\\)) and 3 (\\(+\\)) were erroneously swapped, as were days 100 (\\(+\\)) and 85 (\\(-\\)), and many other days; in the Mars-prospecting inference, rocks #1 (Y) and #3 (N) were erroneously swapped, as were rocks #100 (Y) and #97 (N), and many other rocks.\nShould the final inference about the next 3 datapoints be changed?\nThe information about the exact sequence of data is lost, and only the total amount of positive and negative datapoints is preserved (74 vs 26).\nWould the final inference about the next 3 datapoints be different, compared to the situation where the exact data sequence is known?\nA new inference is requested: not about the 3 datapoints following the known data, but the 3 datapoints preceding the known data: the day before day 1 and so on, or the rock before rock #1 and so on. (Or, generalizing: an inference about 3 datapoints interspersed among the known ones is requested.)\nWould the inference for the preceding 3 datapoints be different from the original one about the subsequent 3 datapoints?\n\nIn all these thought-experiments, our intuition is that the inference for the stock-exchange problem would be different, but the one for the Mars-prospecting problem would stay the same or not change appreciably. Swapping days in a stock course, matters; swapping rocks in a crater, doesn’t. There are good reasons, based on physics and dynamical-system theory, for this intuition.\nAn inference which does not change if data and unknown quantities are freely reordered, like the Mars-prospecting one, is called exchangeable. Whereas an inference in which the ordering of data and unknowns is relevant, like the stock-exchange one, is called non-exchangeable. There is not a dichotomy between the two cases; rather, there are continuous varying degrees of exchangeability or non-exchangeability. But for the moment we shall simplify this gradation into a binary distinction.\nWe shall now make this definition more precise, and then study and exploit the remarkable consequences that it has for an agent’s inferences and probability calculations.\n\n\nFormal definition\nConsider a collection of quantities \\(X_1, X_2, X_3, \\dotsc\\), all having identical domains. For instance, they could all be continuous with values in the real numbers; or all binary, with values \\(\\{\\text{\\small Yes}, \\text{\\small No}\\}\\); or all ordinal, with values \\(\\{\\text{\\small low}, \\text{\\small medium}, \\text{\\small high}\\}\\); or all join quantities, with the same join domains. This collection is in principle infinite: it is always possible to add one more quantity, with the same domain, to the collection.\nAn agent has a joint probability distribution over them, conditional on some state of knowledge \\(\\mathsfit{I}\\):\n\\[\n\\mathrm{P}(X_1\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}x_1,\\ X_2\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}x_2,\\ X_3\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}x_3,\\ \\dotso \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\]\nfor all combinations of values \\(x_1, x_2, x_3, \\dotsc\\).\nThis joint probability distribution is called infinitely exchangeable if all marginal probabilities (§ 10.1) for any sub-collection of quantities remain the same whenever the values are swapped among quantities. For example:\n\\[\n\\begin{aligned}\n&\n\\mathrm{P}(X_1 \\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}{\\color[RGB]{34,136,51}a},\\ X_2\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}{\\color[RGB]{102,204,238}b}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\\\[1ex] {}={}&\n\\mathrm{P}(X_2 \\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}{\\color[RGB]{34,136,51}a},\\ X_1\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}{\\color[RGB]{102,204,238}b}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\\\[1ex] {}={}&\n\\mathrm{P}(X_1 \\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}{\\color[RGB]{34,136,51}a},\\ X_3\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}{\\color[RGB]{102,204,238}b}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\\\[1ex] {}={}&\n\\mathrm{P}(X_3 \\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}{\\color[RGB]{34,136,51}a},\\ X_1\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}{\\color[RGB]{102,204,238}b}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\\\[1ex] {}={}&\n\\dotso\n\\\\[1ex] {}={}&\n\\mathrm{P}(X_{23} \\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}{\\color[RGB]{34,136,51}a},\\ X_{7}\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}{\\color[RGB]{102,204,238}b}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\\\[1ex] {}={}&\n\\dotso\n\\end{aligned}\n\\]\nand so on, and also\n\\[\n\\begin{aligned}\n&\n\\mathrm{P}(X_1 \\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}{\\color[RGB]{34,136,51}a},\\ X_2\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}{\\color[RGB]{102,204,238}b},\\ X_3\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}{\\color[RGB]{204,187,68}c}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\\\[1ex] {}={}&\n\\mathrm{P}(X_2 \\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}{\\color[RGB]{34,136,51}a},\\ X_1\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}{\\color[RGB]{102,204,238}b},\\ X_3\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}{\\color[RGB]{204,187,68}c}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\\\[1ex] {}={}&\n\\mathrm{P}(X_2 \\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}{\\color[RGB]{34,136,51}a},\\ X_3\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}{\\color[RGB]{102,204,238}b},\\ X_1\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}{\\color[RGB]{204,187,68}c}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\\\[1ex] {}={}&\n\\mathrm{P}(X_3 \\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}{\\color[RGB]{34,136,51}a},\\ X_4\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}{\\color[RGB]{102,204,238}b},\\ X_5\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}{\\color[RGB]{204,187,68}c}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\\\[1ex] {}={}&\n\\dotso\n\\\\[1ex] {}={}&\n\\mathrm{P}(X_{12} \\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}{\\color[RGB]{34,136,51}a},\\ X_8\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}{\\color[RGB]{102,204,238}b},\\ X_{47}\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}{\\color[RGB]{204,187,68}c}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\\\[1ex] {}={}&\n\\dotso\n\\end{aligned}\n\\]\nand also\n\\[\n\\begin{aligned}\n&\n\\mathrm{P}(X_1 \\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}{\\color[RGB]{34,136,51}a},\\ X_2\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}{\\color[RGB]{102,204,238}b},\\ X_3\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}{\\color[RGB]{204,187,68}c},\\ X_4\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}{\\color[RGB]{170,51,119}d}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\\\[1ex] {}={}&\n\\mathrm{P}(X_3 \\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}{\\color[RGB]{34,136,51}a},\\ X_4\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}{\\color[RGB]{102,204,238}b},\\ X_2\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}{\\color[RGB]{204,187,68}c},\\ X_1\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}{\\color[RGB]{170,51,119}d}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\\\[1ex] {}={}&\n\\dotso\n\\\\[1ex] {}={}&\n\\mathrm{P}(X_{91} \\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}{\\color[RGB]{34,136,51}a},\\ X_{72}\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}{\\color[RGB]{102,204,238}b},\\ X_6\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}{\\color[RGB]{204,187,68}c},\\ X_{13}\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}{\\color[RGB]{170,51,119}d}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\\\[1ex] {}={}&\n\\dotso\n\\end{aligned}\n\\]\nand so on – no matter how many and which quantities we take, and no matter what the values \\({\\color[RGB]{34,136,51}a},{\\color[RGB]{102,204,238}b},{\\color[RGB]{204,187,68}c},{\\color[RGB]{170,51,119}d}\\dotsc\\) might be.\nIn the following we shall simply call this type of probability distribution exchangeable, omitting “infinitely”. But keep in mind that there are also finitely exchangeable distributions, and they have different properties. Here, when we say “exchangeable”, we always strictly mean “infinitely exchangeable”.\n\n\n\n\n\n\n\n\n\n\nExchangeability does not mean that all probabilities need to be equal. For instance, if \\({\\color[RGB]{102,204,238}b}\\ne{\\color[RGB]{204,187,68}c}\\) then these probabilities can be different:\n\\[\\begin{gathered}\n\\mathrm{P}(X_1 \\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}{\\color[RGB]{34,136,51}a},\\ X_2\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}{\\color[RGB]{102,204,238}b}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\mathrel{\\boldsymbol{\\ne}}\n\\mathrm{P}(X_1 \\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}{\\color[RGB]{34,136,51}a},\\ X_2\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}{\\color[RGB]{204,187,68}c}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\\\\n\\mathrm{P}(X_1 \\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}{\\color[RGB]{34,136,51}a},\\ X_2\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}{\\color[RGB]{102,204,238}b}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\mathrel{\\boldsymbol{\\ne}}\n\\mathrm{P}(X_{23} \\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}{\\color[RGB]{34,136,51}a},\\ X_7\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}{\\color[RGB]{204,187,68}c}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\end{gathered}\\]\nalthough we must still have\n\\[\n\\mathrm{P}(X_1 \\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}{\\color[RGB]{34,136,51}a},\\ X_2\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}{\\color[RGB]{204,187,68}c}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\mathrel{\\boldsymbol{=}}\n\\mathrm{P}(X_{23} \\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}{\\color[RGB]{34,136,51}a},\\ X_7\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}{\\color[RGB]{204,187,68}c}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\]\n\n\n\n\nThe definition of exchangeability given above can also be put in a different way. For an exchangeable probability distribution, the joint probability of a collection of quantities can only depend on the different values appearing in it and their multiplicities. For instance, the numerical value of the marginal, joint probability\n\\[\n\\mathrm{P}(X_{91} \\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}{\\color[RGB]{102,204,238}b},\\ X_{72}\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}{\\color[RGB]{34,136,51}a},\\ X_6\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}{\\color[RGB]{102,204,238}b},\\ X_{13}\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}{\\color[RGB]{204,187,68}c}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\]\nwhere \\({\\color[RGB]{34,136,51}a},{\\color[RGB]{102,204,238}b},{\\color[RGB]{204,187,68}c}\\) are different from one another, can only depend on the fact that there are the three different values \\({\\color[RGB]{34,136,51}a},{\\color[RGB]{102,204,238}b},{\\color[RGB]{204,187,68}c}\\), and that \\({\\color[RGB]{34,136,51}a}\\) appears once, \\({\\color[RGB]{102,204,238}b}\\) twice, and \\({\\color[RGB]{204,187,68}c}\\) once. So any other marginal, joint probability containing the value \\({\\color[RGB]{34,136,51}a}\\) once, \\({\\color[RGB]{102,204,238}b}\\) twice, and \\({\\color[RGB]{204,187,68}c}\\) once, must be equal to the one above.\n\n\n\n\n\n\n Exercise\n\n\n\nFor each of the marginal probabilities below, determine whether the joint probability they come from is surely non-exchangeable, and explain why. Note that we can’t determine whether a probability distribution is exchangeable unless we’re given all its probabilities, for all combinations of quantities and values.\n\n\n\\[\\begin{aligned}\n&\\mathrm{P}(X_1\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}3,\\  X_4\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}5\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}) = 0.15\n\\\\\n&\\mathrm{P}(X_1\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}5,\\  X_4\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}3\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}) = 0.15\n\\\\\n&\\mathrm{P}(X_1\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}7,\\  X_4\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}3\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}) = 0.25\n\\\\\n&\\mathrm{P}(X_1\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}3,\\  X_4\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}7\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}) = 0.25\n\\end{aligned}\n\\]\n\n\\[\\begin{aligned}\n&\\mathrm{P}(Y_1\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}3,\\  Y_4\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}5\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}) = 0.07\n\\\\\n&\\mathrm{P}(Y_1\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}5,\\  Y_4\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}3\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}) = 0.07\n\\\\\n&\\mathrm{P}(Y_9\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}3,\\  Y_{21}\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}5\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}) = 0.4\n\\\\\n&\\mathrm{P}(Y_9\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}5,\\  Y_{21}\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}3\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}) = 0.4\n\\end{aligned}\n\\]\n\n\\[\\begin{aligned}\n&\\mathrm{P}(Z_1\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}\\text{\\small high},\\  Z_7\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}\\text{\\small low},\\  Z_{20}\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}\\text{\\small high}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}) = 0.11\n\\\\\n&\\mathrm{P}(Z_1\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}\\text{\\small high},\\  Z_7\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}\\text{\\small high},\\  Z_{20}\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}\\text{\\small low}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}) = 0.35\n\\\\\n&\\mathrm{P}(Z_2\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}\\text{\\small low},\\  Z_3\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}\\text{\\small low},\\  Z_9\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}\\text{\\small high}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}) = 0\n\\\\\n&\\mathrm{P}(Z_2\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}\\text{\\small high},\\  Z_3\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}\\text{\\small low},\\  Z_9\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}\\text{\\small low}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}) = 0\n\\end{aligned}\n\\]\n\n\\[\\begin{aligned}\n&\\mathrm{P}(U_1\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}6.4,\\  U_2\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}8.7,\\  U_3\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}-0.3\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}) = 0.1\n\\\\\n&\\mathrm{P}(U_1\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}1.7,\\  U_2\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}0.5,\\  U_3\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}-0.3\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}) = 0.1\n\\\\\n&\\mathrm{P}(U_1\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}-0.3,\\  U_2\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}6.4,\\  U_3\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}0.5\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}) = 0.01\n\\\\\n&\\mathrm{P}(U_1\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}-0.3,\\  U_2\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}1.7,\\  U_3\\mathrel{\\mkern-2.5mu\\textrm{\\small=}\\mkern-2.5mu}0.5\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}) = 0.4\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\nExample\nAs an example of a fully-specified exchangeable distribution, consider the collection of quantities \\(X_1, X_2, \\dotsc\\) with common domain \\(\\set{1,2,3}\\)"
  },
  {
    "objectID": "populations_exchangeability.html#interpretations-of-exchangeability",
    "href": "populations_exchangeability.html#interpretations-of-exchangeability",
    "title": "16  Populations & exchangeability",
    "section": "16.2 Interpretations of exchangeability",
    "text": "16.2 Interpretations of exchangeability\nOur focus, however, is not on the whys of this intuition, but on the consequences that it has for inference and probability calculations."
  },
  {
    "objectID": "making_decisions.html#decisions-possible-situations-and-consequences",
    "href": "making_decisions.html#decisions-possible-situations-and-consequences",
    "title": "17  Making decisions",
    "section": "17.1 Decisions, possible situations, and consequences",
    "text": "17.1 Decisions, possible situations, and consequences"
  },
  {
    "objectID": "making_decisions.html#gains-and-losses-utilities",
    "href": "making_decisions.html#gains-and-losses-utilities",
    "title": "17  Making decisions",
    "section": "17.2 Gains and losses: utilities",
    "text": "17.2 Gains and losses: utilities\n\nFactors that enter utility quantification\nUtilities can rarely be assigned a priori."
  },
  {
    "objectID": "making_decisions.html#making-decisions-under-uncertainty-maximization-of-expected-utility",
    "href": "making_decisions.html#making-decisions-under-uncertainty-maximization-of-expected-utility",
    "title": "17  Making decisions",
    "section": "17.3 Making decisions under uncertainty: maximization of expected utility",
    "text": "17.3 Making decisions under uncertainty: maximization of expected utility"
  }
]