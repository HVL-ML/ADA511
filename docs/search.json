[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ADA511: Data science and data-driven engineering",
    "section": "",
    "text": "Science is built up with facts, as a house is with stones. But a collection of facts is no more a science than a heap of stones is a house.     (H. Poincaré)\n\nPreface\n**WARNING: THIS IS A WORKING DRAFT. TEXT WILL CHANGE A LOT. MANY PASSAGES ARE JUST TEMPORARY, INCOHERENT, AND DISJOINTED.\nTo be written.\n\nDifference between car mechanic and automotive engineer\n“Engineering based on data” is just how engineering and science in general have been in the past 400 years or so. Nothing new there.\nThe amount of available data has changed. This may lead to a reduction – or in some cases an increase – in uncertainty, and therefore to different solutions.\nLuckily the fundamental theory to deal with large amount of data is exactly the same to deal with small amounts. So the foundations haven’t changed.\n\nThis course makes you acquainted with the foundations."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Accept or discard?",
    "section": "",
    "text": "Let’s start with a question that could arise in a particular engineering problem:\n\nA particular kind of electronic component is produced on an assembly line. At the end of the line, there is an automated inspection device that works as follows with every newly produced component coming out of the line.\nThe inspection device first makes some tests on the new component. The tests give an uncertain forecast of whether that component will fail within its first year of use, or after.\nThen the device decides whether the component is accepted and packaged for sale, or discarded and thrown away.\nWhen a new electronic component is sold, the manufacturer has a net gain of \\(1\\$\\). If the component fails within a year of use, however, the manufacturer incur net loss of \\(11\\$\\) (12$ loss, minus the 1$ gained at first), owing to warranty refunds and damage costs to be paid to the buyer. When a new electronic component is discarded, the manufacturer has \\(0\\$\\) net gain.\nFor a specific new electronic component, just come out of the assembly line, the tests of the automated inspection device indicate that there is a \\(10\\%\\) probability that the component will fail within its first year of use.\n\n\n\n\nShould the inspection device accept or discard the new component?\n\nFirst, try to give and motivate an answer.\nThis is not the real question of this exercise, however. In fact it doesn’t matter if you don’t get the correct answer; not even if you don’t manage to get an answer at all.\n\n\n\n\n\n\n Very first exercise!\n\n\n\nThe purpose here is for you to do some introspection about your own reasoning. Then examine and discuss these points:\n\nWhich numerical elements in the problem seem to affect the answer?\nCan these numerical elements be clearly separated? How would you separate them?\nHow would the answer change, if these numerical elements were changed? Feel free to change them, also in extreme ways, and see how the answer would change.\nCould we solve the problem if we didn’t have the probabilities? Why?\nCould we solve the problem if we didn’t know the various gains and losses? Why?\nCan this problem be somehow abstracted, and then transformed into another one with completely different details? For instance, consider translating along these lines:\n\ninspection device → computer pilot of self-driving car\ntests → camera image\nfail within a year → pedestrian in front of car\naccept/discard → keep on going/ break"
  },
  {
    "objectID": "framework.html#what-does-the-intro-problem-tell-us",
    "href": "framework.html#what-does-the-intro-problem-tell-us",
    "title": "2  Framework",
    "section": "2.1 What does the intro problem tell us?",
    "text": "2.1 What does the intro problem tell us?\nLet’s approach the “accept or discard?” problem of the previous chapter 1 in an intuitive way.\n\n\nWe’re jumping the gun here, because we haven’t learned the method to solve this problem yet!\nFirst let’s say that we accept the component. What happens?\nWe must try to make sense of that \\(10\\%\\) probability that the component fails within a year. Different people do this with different imagination tricks. We can imagine, for instance, that this situation is repeated 100 times. In 10 of these repetitions the accepted electronic component is sold and fails within a year after selling. In the remaining 90 repetitions, the component is sold and works fine for at least a year.\nIn each of the 10 imaginary repetitions in which the component fails early, the manufacturer loses \\(11\\$\\). That’s a total loss of \\(10 \\cdot 11\\$ = 110\\$\\). In each of the 90 imaginary repetitions in which the component doesn’t fail early, the manufacturer gains \\(1\\$\\). That’s a total gain of \\(90\\$\\). So over all 100 imaginary repetitions the manufacturer gains \\[\n10\\cdot (-11\\$) + 90\\cdot 1\\$ = {\\color[RGB]{238,102,119} -20\\$} \\ ,\n\\] that is, the manufacturer has not gained, but lost \\(20\\$\\) ! That’s an average of \\(0.2\\$\\) lost per repetition.\nNow let’s say that we discard the component instead. What happens? In this case we don’t need to invoke imaginary repetitions, but even if we do, it’s clear that the manufacturer doesn’t gain or lose anything – that is, the “gain” is \\(0\\$\\) – in each and all of the repetitions.\nThe conclusion is that if in a situation like this we accept the component, then we’ll lose \\(0.2\\$\\) on average; whereas if we discard it, then on average we won’t lose anything or gain anything.\nObviously the best, or “least worst”, decision to make is to discard the component.\n\n\n\n\n\n\n Exercises\n\n\n\n\nNow that we have an idea of the general reasoning, check what happens with different values of the probability of failure and of the failure cost: is it still best to discard? For instance, try with\n\nfailure probability 10% and failure cost 5$;\nfailure probability 5% and failure cost 11$;\nfailure probability 10%, failure cost 11$, non-failure gain 2$.\n\nFeel free to get wild and do plots.\nIdentify the failure probability at which accepting the component doesn’t lead to any loss or any gain, so it doesn’t matter whether we discard or accept. (You can solve this as you prefer: analytically with an equation, visually with a plot, by trial & error on several cases, or whatnot.)\nConsider the special case with failure probability 0% and failure cost 10$. This means no new component will ever fail. To decide in such a case we do not need imaginary repetitions; but confirm that we arrive at the same logical conclusion whether we reason through imaginary repetitions or not.\nConsider this completely different problem:\n\nA patient is examined by a brand-new medical diagnostics AI system.\nThe AI first performs some clinical tests on the patient. The tests give an uncertain forecast of whether the patient has a particular disease or not.\nThen the AI decides whether the patient should be dismissed without treatment, or treated with a particular medicine.\nIf the patient is dismissed, then the life expectancy doesn’t increase or decrease if the disease is not present, but it decreases by 10 years if the disease is actually present. If the patient is treated, then the life expectancy decreases by 1 year if the disease is not present (owing to treatment side-effects), but also if the disease is present (because it cures the disease, so the life expectancy doesn’t decrease by 10 years; but it still decreases by 1 year owing to the side effects).\nFor this patient, the clinical tests indicate that there is a \\(10\\%\\) probability that the patient has the disease.\n\nShould the diagnostic AI dismiss or treat the patient? Find differences and similarities, even numerical, with the assembly-line problem.\n\n\n\n\n\nFrom the solution of the problem and from the exploring exercises, we gather some instructive points:\n\nIs it enough if we simply know that the component is less likely to fail than not? in other words, if we simply know that the probability of failure is less than \\(50\\%\\)?\nObviously not. We found that if the failure probability is \\(10\\%\\) then it’s best to discard; but if it’s \\(5\\%\\) then it’s best to accept. In both cases the component was less likely to fail than not, but the decisions were different. Moreover, we found that the probability affected the loss if one made the non-optimal decision. Therefore:\nKnowledge of exact probabilities is absolutely necessary for making the best decision\nIs it enough if we simply know that failure leads to a cost? that is, that its gain is less than the gain for non-failure?\nObviously not. The situation is similar to that with the probability. In the exercise we found that if the failure cost is \\(11\\$\\) then it’s best to discard; but if it’s \\(5\\$\\) then it’s best to accept. It’s also best to accept if the failure cost is \\(11\\$\\) but the non-failure gain is \\(2\\$\\). Therefore:\nKnowledge of the exact gains and losses is absolutely necessary for making the best decision\nIs this kind of decision situation only relevant to assembly lines and sales?\nBy all means not. We found a clinical situation that’s exactly analogous: there’s uncertainty, there are gains and losses (of time rather than money), and the best decision depends on both."
  },
  {
    "objectID": "framework.html#our-focus-decision-making-inference-and-data-science",
    "href": "framework.html#our-focus-decision-making-inference-and-data-science",
    "title": "2  Framework",
    "section": "2.2 Our focus: decision-making, inference, and data science",
    "text": "2.2 Our focus: decision-making, inference, and data science\nEvery data-driven engineering project is unique, with its unique difficulties and problems. But there are also problems common to all engineering projects.\nIn the scenarios we explored above, we found an extremely important problem-pattern. There is a decision or choice to make (and “not deciding” is not an option – or it’s just another kind choice). Making a particular decision will lead to some consequences, some leading to a desired goal, others leading to something undesirable. The decision is difficult because its consequences are not known with certainty, given the information and data available in the problem. We may lack information and data about past or present details, about future events and responses, and so on. This is what we call a problem of decision-making under uncertainty or under risk1, or simply a “decision problem” for short.1 We’ll avoid the word “risk” because it has several different technical meanings in the literature, some even contradictory.\nThis problem-pattern appears literally everywhere. But our explored scenarios also suggest that this problem-pattern has a sort of systematic solution method.\nIn this course we’re going to focus on decision problems and their systematic solution method. We’ll learn a framework and some abstract notions that allow us to frame and analyse this kind of problem, and we’ll learn a universal set of principles to solve it. This set of principles goes under the name of Decision Theory. \nBut what do decision-making under uncertainty and Decision Theory have to do with data and data science? The three are profoundly and tightly related on many different planes:\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nDecision theory in expert systems and artificial intelligence\n\n\n\nWe saw that probability values are essential in a decision problem. How do we find them? As you can imagine, data play an important part in their calculation. In our intro example, the failure probability must come from observations or experiments on similar electronic components.\nWe saw that also the values of gains and losses are essential. Data play an important part in their calculation as well.\nData science is based on the laws of Decision Theory. Here’s an analogy: a rocket engineer relies on fundamental physical laws (balance of momentum, energy, and so on) for making a rocket work. Failure to account for those laws leads at best to sub-optimal solutions, at worst to disasters. As we shall see, the same is true for a data scientist and the rules of decision theory.\nMachine-learning algorithms, in particular, are realizations or approximations of the rules of Decision Theory. This is clear, for instance, considering that the main task of a machine-learning classifier is to decide among possible output labels or classes.\nThe rules of Decision Theory are also the foundations upon which artificial-intelligence agents, which must make optimal inferences and decisions, are built.\n\nThese five planes will constitute the major parts of the present course.\n\n\n@@ TODO add examples: algorithm giving outputs is a decision agent. @@ Include one with https://hjerterisiko.helsedirektoratet.no\nThere are other important aspects in engineering problems, besides the one of making decisions under uncertainty. For instance the discovery or the invention of new technologies and solutions. These aspects can barely be planned or decided; but their fruits, once available, should be handled and used optimally – thus leading to a decision problem.\nArtificial intelligence is proving to be a valuable aid in these more creative aspects too. This kind of use of AI is outside the scope of the present notes. Some aspects of this creativity-assisting use, however, do fall within the domain of the present notes. A pattern-searching algorithm, for example, can be optimized by means of the method we are going to study."
  },
  {
    "objectID": "framework.html#our-goal-optimality-not-success",
    "href": "framework.html#our-goal-optimality-not-success",
    "title": "2  Framework",
    "section": "2.3 Our goal: optimality, not “success”",
    "text": "2.3 Our goal: optimality, not “success”\nWhat should we demand from a systematic method for solving decision problems?\nBy definition, in a decision problem under uncertainty there is generally no method to determine the decision that surely leads to the desired consequence – if such a method existed, then the problem would not have any uncertainty! Therefore, if there is a method to deal with decision problems, its goal cannot be the determination of the successful decision. This also means that a priori we cannot blame an engineer for making an unsuccessful decision in a situation of uncertainty.\nImagine two persons, Henry and Tina, who must bet on “heads” or “tails” under the following conditions (but who otherwise don’t get any special thrill from betting):\n\nIf the bet is “heads” and the coin lands “heads”, the person wins a small amount of money; but if it lands “tails”, they lose a large amount of money.\nIf the bet is “tails” and the coin lands “tails”, the person wins a small amount of money; if it lands “heads”, they lose the same small amount of money.\n\nHenry chooses the first bet, on “heads”. Tina chooses the second bet, on “tails”. The coin comes down “heads”. So Henry wins the small amount of money, while Tina loses the same small amount. What would we say about their decisions?\nHenry’s decision was lucky, and yet irrational: he risked losing much more money than in the second bet, without any possibility of at least winning more. Tina’s decision was unlucky, and yet rational: the possibility and amount of winning was the same in the two bets, and she chose the bet with the least amount of loss. We expect that any person making Henry’s decision in similar, future bets will eventually lose more money than any person making Tina’s decision.\nThis example shows two points. First, “success” is generally not a good criterion to judge a decision under uncertainty; success can be the pure outcome of luck, not of smarts. Second, even if there is no method to determine which decision is successful, there is a method to determine which decision is rational or optimal, given the particular gains, losses, and uncertainties involved in the decision problem. We had a glimpse of this method in our introductory scenarios.\nLet us emphasize, however, that we are not giving up on “success”, or trading it for “optimality”. Indeed we’ll find that Decision Theory automatically leads to the successful decision in problems where uncertainty is not present or is irrelevant. It’s a win-win. It’s important to keep this point in mind:\n\n\n\n\n\n\n\n\n\n\n\nAiming to find the solutions that are successful can make us fail to find those that are optimal when the successful ones cannot be determined.\nAiming to find the solutions that are optimal makes us automatically find those that are successful when those can be determined.\n\n\n\nWe shall later witness this fact with our own eyes, and will take it up again in the discussion of some misleading techniques to evaluate machine-learning algorithms."
  },
  {
    "objectID": "framework.html#decision-theory",
    "href": "framework.html#decision-theory",
    "title": "2  Framework",
    "section": "2.4 Decision Theory",
    "text": "2.4 Decision Theory\nSo far we have mentioned that Decision Theory has the following features:\n\n it tells us what’s optimal and, when possible, what’s successful\n it takes into consideration decisions, consequences, costs and gains\n it is able to deal with uncertainties\n\nWhat other kinds of features should we demand from it, in order to be applied to as many kinds of decision problems as possible, and to be relevant for data science?\nIf we find an optimal decision in regards to some outcome, it may still happen that the decision can be realized in several ways that are equivalent in regard to the outcome, but inequivalent in regard to time or resources. In the assembly-line scenario, for example, the decision discard could be carried out by burning, recycling, and so on. We thus face a decision within a decision. In general, a decision problem may involve several decision sub-problems, in turn involving decision sub-sub-problems, and so on.\nIn data science, a common engineering goal is to design and build an automated or AI-based device capable of making an optimal decision in a specific kind of uncertain situations. Think for instance of an aeronautic engineer designing an autopilot system, or a software company designing an image classifier.\nDecision Theory turns out to meet these demands too, thanks to the following features:\n\n it is susceptible to recursive, sequential, and modular application\n it can be used not only for human decision-makers, but also for automated or AI devices\n\n\n\nDecision Theory has a long history, going back to Leibniz in the 1600s and partly even to Aristotle in the −300s, and appearing in its present form around 1920–1960. What’s remarkable about it is that it is not only a framework, but the framework we must use. A logico-mathematical theorem shows that any framework that does not break basic optimality and rationality criteria has to be equivalent to Decision Theory. In other words, any “alternative” framework may use different technical terminology and rewrite mathematical operations in a different way, but it boils down to the same notions and operations of Decision Theory. So if you wanted to invent and use another framework, then either (a) it would lead to some irrational or illogical consequences, or (b) it would lead to results identical to Decision Theory’s. Many frameworks that you are probably familiar with, such as optimization theory or Boolean logic, are just specific applications or particular cases of Decision Theory.\nThus we list one more important characteristic of Decision Theory:\n\n it is normative\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\n\nJudgment under uncertainty\nHeuristics and Biases\nThinking, Fast and Slow\n\n\n\nNormative contrasts with descriptive. The purpose of Decision Theory is not to describe, for example, how human decision-makers typically make decisions. Because human decision-makers typically make irrational, sub-optimal, or biased decisions. That’s exactly what we want to avoid and improve!"
  },
  {
    "objectID": "basic_decisions.html#graphical-representation-and-elements",
    "href": "basic_decisions.html#graphical-representation-and-elements",
    "title": "3  Basic decision problems",
    "section": "3.1 Graphical representation and elements",
    "text": "3.1 Graphical representation and elements\nA basic decision problem can be represented by a diagram like this:\n\n\nIt has one decision node, usually represented by a square , from which the available decisions depart as lines. Each decision leads to an uncertainty node, usually represented by a circle , from which the possible outcomes depart as lines. Each outcome leads to a particular utility value. The uncertainty of each outcome is quantified by a probability.\nA basic decision problem is analysed in terms of these elements:\n\n Agent, and background or prior information. The agent is the person or device that has to make the decision. An agent always possess (or has been programmed with) specific background information that is used and taken for granted in the decision-making process. This background information determines the probabilities and utilities of the outcomes, together with other available data and information. Since different agents typically have different background information, we shall somehow conflate agents and prior information.\n\n\n\nWe’ll use the neutral pronouns it/its when referring to an agent, since an agent could be a person or a machine.\n\n Decisions, also called courses of actions, available to the agent. They are assumed to be mutually exclusive and exhaustive; this can always be achieved by recombining them if necessary, as we’ll discuss later.\n Outcomes of the possible decisions. Every decision can have a different set of outcomes, or some outcomes can appear for several or all decisions (in this case they are reported multiple times in the decision diagram). Note that even if an outcome can happen for two or more different decisions, its probabilities can still be different depending on the decision.\n Probabilities for each of the outcomes. Their values typically depend on the background information, the decision, and the additional data.\n Utilities: the gains or losses associated with each of the possible outcomes. Their values also depend on the background information, the decision, and the additional data.\n Data and other additional information, sometimes called evidence. They differ from the background information in that they can change with every decision instance made by the same agent, while the background information stays the same. In the assembly-line scenario, for example, the test results could be different for every new electric component.\n\n\n\n\n\n\n\n\n\n\n\n\n\n Remember: What matters is to be able to identify these elements in a concrete problem, understanding their role. Their technical names don’t matter.\n\n\nNote that it is not always the case that the outcomes are unknown and the data are known. As we’ll discuss later, in some situations we reason in hypothetical or counterfactual ways, using hypothetical data and considering outcomes which have already occurred.\n\n\n\n\n\n\n Reading\n\n\n\n§ 1.1.4 in Artificial Intelligence\n\n\n\n\n\n\n\n\n Exercise\n\n\n\n\nIdentify the elements above in the assembly-line decision problem of the introduction 1.\nSketch the diagram of the assembly-line decision problem.\n\n\n\nSome of the decision-problem elements listed above may need to be in turn analysed by a decision sub-problem. For instance, the utilities could depend on uncertain factors: thus we have a decision sub-problem to determine the optimal values to be used for the utilities of the main problem. This is an example of the modular character of decision theory.\nWe shall soon see how to mathematically represent these elements.\nThe elements above must be identified unambiguously in every decision problem. The analysis into these elements greatly helps in making the problem and its solution well-defined.\nAn advantage of decision theory is that its application forces us to make sense of an engineering problem. A useful procedure is to formulate the general problem in terms of the elements above, identifying them clearly. If the definition of any of the terms involves uncertainty of further decisions, then we analyse it in turn as a decision sub-problem, and so on.\n\nSuppose someone (probably a politician) says: “We must solve the energy crisis by reducing energy consumption or producing more energy”. From a decision-making point of view, this person has effectively said nothing whatsoever. By definition the “energy crisis” is the problem that energy production doesn’t meet demand. So this person has only said “we would like the problem to be solved”, without specifying any solution. A decision-theory approach to this problem requires us to specify which concrete courses of action should be taken for reducing consumption or increasing productions, and what their probable outcomes, costs, and gains would be.\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nSee MacKay’s options-vs-costs rational analysis in Sustainable Energy – without the hot air"
  },
  {
    "objectID": "basic_decisions.html#inference-utility-maximization",
    "href": "basic_decisions.html#inference-utility-maximization",
    "title": "3  Basic decision problems",
    "section": "3.2 Inference, utility, maximization",
    "text": "3.2 Inference, utility, maximization\nThe solution of a basic decision-making problem can be roughly divided into three main stages: inference, utility assessment, and expected-utility maximization.\n Inference is the stage where the probabilities of the possible outcomes are calculated. Its rules are given by the Probability Calculus. Inference is independent from decision: in some situations we may simply wish to assess whether some hypotheses, conjectures, or outcomes are more or less plausible than others, without making any decision. This kind of assessment can be very important in problems of communication and storage, and it is specially considered by Information Theory.\nThe calculation of probabilities can be the part that demands most thinking, time, and computational resources in a decision problem. It is also the part that typically makes most use of data – and where data can be most easily misused.\nRoughly half of this course will be devoted in understanding the laws of inference, their applications, uses, and misuses.\n\n Utility assesment is the stage where the gains or losses of the possible outcomes are calculated. Often this stage requires further inferences and further decision-making sub-problems. The theory underlying utility assessment is still much underdeveloped, compared to probability theory.\n\n Expected-utility maximization is the final stage where the probabilities and gains or costs of the possible outcomes are combined, in order to determine the optimal decision."
  },
  {
    "objectID": "inference.html#sec-inference-scenarios",
    "href": "inference.html#sec-inference-scenarios",
    "title": "4  What is an inference?",
    "section": "4.1 The wide scope and characteristics of inferences",
    "text": "4.1 The wide scope and characteristics of inferences\nLet’s see a couple more informal examples of inference problems. For some of them an underlying decision-making problem is also alluded to:\n\nLooking at the weather we try to assess if it’ll rain today, to decide whether to take an umbrella.\nConsidering a patient’s symptoms, test results, and medical history, a clinician tries to assess which disease affects a patient, so as to decide on the optimal treatment.\nLooking at the present game position  the X-player, which moves next, wonders whether placing the next X on the mid-right position leads to a win.\nFrom the current set of camera frames, the computer of a self-driving car needs to assess whether a particular patch of colours in the frames is a person, so as to slow down the car and stop.\nGiven that \\(G=6.67 \\cdot 10^{-11}\\,\\mathrm{m^3\\,s^{-2}\\,kg^{-1}}\\), \\(M = 5.97 \\cdot 10^{24}\\,\\mathrm{kg}\\) (mass of the Earth), and \\(r = 6.37 \\cdot 10^{6}\\,\\mathrm{m}\\) (radius of the Earth), a rocket engineer needs to know how much is \\(\\sqrt{2\\,G\\,M/r\\,}\\).\nWe’d like to know whether the rolled die is going to show .\nAn aircraft’s autopilot system needs to assess how much the aircraft’s roll will change if the right wing’s angle of attack is increased by \\(0.1\\,\\mathrm{rad}\\).\nBy looking at the dimensions, shape, texture of a newly dug-out fossil bone, an archaeologist wonders whether it belonged to a Tyrannosaurus rex.\nA voltage test on a newly produced electronic component yields a reading of \\(100\\,\\mathrm{mV}\\). The electronic component turns out to be defective. An engineer wants to assess whether the voltage-test reading could have been \\(100\\,\\mathrm{mV}\\), if the component had not been defective.\nSame as above, but the engineer wants to assess whether the voltage-test reading could have been \\(80\\,\\mathrm{mV}\\), if the component had not been defective.\n\n\n\n\nFrom measurements of the Sun’s energy output and of concentrations of various substances in the Earth’s atmosphere over the past 500 000 years, and of the emission rates of various substances in the years 1900–2022, climatologists and geophysicists try to assess the rate of mean-temperature increase in the years 2023–2100.\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nCh. 10 in A Survival Guide to the Misinformation Age.\n\n\n\n\n\n\n\n\n\n\n Exercises\n\n\n\n\nFor each example above, pinpoint what has to be inferred, and also the agent interested in the inference.\nPoint out which of the examples above explicitly give data or information that should be used for the inference.\nFor the examples that do not give explicit data or information, speculate what information could be implicitly assumed. For those that do give explicit data, speculate which other additional information could be implicitly assumed.\nCan any of the inferences above be done perfectly, that is, without any uncertainty, based the data given explicitly or implicitly?\nFind the examples that explicitly involve a decision. In which of them does the decision affect the results of the inference? In which it does not?\nAre any of the inferences “one-time only” – that is, their object or the data on which they are based have never happened before and will never happen again?\nAre any of the inferences based on data and information that come chronologically after the object of the inference?\nAre any of the inferences about something that is actually already known to the agent that’s making the inference?\nAre any of the inferences about something that actually did not happen?\nDo any of the inferences use “data” or “information” that are actually known (within the scenario itself) to be fictive, that is, not real?\n\n\n\nFrom the examples and from your answers to the exercise we observe some very important characteristics of inferences:\n\nSome inferences can be made exactly, that is, without uncertainty: it is possible to say whether the object of the inference is true or false. Other inferences, instead, involve an uncertainty.\nAll inferences are based on some data and information, which may be explicitly expressed or only implicitly understood.\nAn inference can be about something past, but based on present or future data and information: inferences can show all sorts of temporal relations.\nAn inference can be essentially unrepeatable, because it’s about something unrepeatable or based on unrepeatable data and information.\nThe data and information on which an inference is based can actually be unknown; that is, they can be only momentarily contemplated as real. Such an inference is said to be based on hypothetical reasoning.\nThe object of an inference can actually be something already known to be false or not real: the inference tries to assess it in the case that some data or information had been different. Such an inference is said to be based on counterfactual reasoning."
  },
  {
    "objectID": "inference.html#where-are-inferences-drawn-from",
    "href": "inference.html#where-are-inferences-drawn-from",
    "title": "4  What is an inference?",
    "section": "4.2 Where are inferences drawn from?",
    "text": "4.2 Where are inferences drawn from?\nThis question is far from trivial. In fact it has connections with the earth-shaking development and theorems in the foundations of mathematics of the 1900s.\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nMathematics: The Loss of Certainty.\n\n\nThe proper answer to this question will take up the next sections. But a central point can be emphasized now:\n\n\n\n\n\n\n\n\n\n\n\nInferences can only be drawn from other inferences.\n\n\n\nIn order to draw an inference – calculate a probability – we usually go up a chain: we must first draw other inferences, and for drawing those we must draw yet other inferences, and so on.\nAt some point we must stop at inferences that we take for granted without further proof. These typically concern direct experiences and observations. For instance, you see a tree in front of you, so you can take “there’s a tree here” as a true fact. Yet, notice that the situation is not so clear-cut: how do you know that you aren’t hallucinating, for example, and there’s actually no tree there? That is taken for granted. If you analyse the possibility of hallucination, you realize that you are taking other things for granted, and so on. Probably most philosophical research in the history of humanity has been about grappling with this runaway process – which is also a continuous source of sci-fi films. In logic and mathematical logic, this corresponds to the fact that to prove some theorem, we must always start from some axioms. There are “inferences” – tautologies – that can be drawn without requiring others; but they are all trivial, such as “this component failed early, or it didn’t”. They are of little use in a real problem, although they have a deep theoretical importance.\n\n\n\n\n\nSci-fi films like The Matrix ultimately draw on the fact that we must take some inferences for granted without further proof.\n\n\n\n\nIn concrete applications we start from many inferences upon which everyone, luckily, agrees. But sometimes we must also use starting inferences that are more dubious or not agreed upon by anyone. In this case the final inference has a somewhat contingent character, and we accept it (as well as the solution of any underlying decision problem) as the best available for the moment. This is partly the origin of the term “model”."
  },
  {
    "objectID": "inference.html#basic-elements-of-an-inference",
    "href": "inference.html#basic-elements-of-an-inference",
    "title": "4  What is an inference?",
    "section": "4.3 Basic elements of an inference",
    "text": "4.3 Basic elements of an inference\nLet us start to introduce some mathematical notation and more precise terminology for inferences.\nEvery inference has an “object” – what is to be assessed – as well as data, information, or hypotheses on which it is based. We call proposal1 the object of the inference, and conditional2 what the inference is based upon. We separate them with a vertical bar3  “\\(\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}}\\)”,  which can be pronounced given or conditional on: \\[\n\\textit{[proposal]}\\ \\pmb{\\nonscript\\:\\Big\\vert\\nonscript\\:\\mathopen{}}\\\n\\textit{[conditional]}\n\\]1 Johnson’s (1924) terminology. Keynes (1921) uses “conclusion”. Modern textbooks do not seem to use any specialized term.2 Modern terminology. Other terms used: “evidence”, “premise”, “supposal”.3 Originally a solidus, introduced by Keynes (1921).\nWe have seen that to calculate the probability for an inference, we must start from the probabilities of other inferences. A basic inference process therefore can be schematized like this:\n\n\n\n\n\n\n\n\nThe next important task ahead of us is to introduce a flexible and enough general mathematical representation for the objects and the bases of an inference. Then we shall finally study the rules for drawing correct inferences."
  },
  {
    "objectID": "sentences.html#sec-central-comps",
    "href": "sentences.html#sec-central-comps",
    "title": "5  Sentences",
    "section": "5.1 The central components of knowledge representation",
    "text": "5.1 The central components of knowledge representation\nWhen speaking of “data”, what comes to mind to many people is basically numbers or collections of numbers. Maybe numbers, then, could be used to represent all the variety of items exemplified above. This option, however, turns out to be too restrictive.\nI give you this number: “\\(8\\)”, saying that it is “data”. But what is it about? You, as an agent, can hardly call this number a piece of information, because you have no clue what to do with it. Instead, if I tell you: “The number of official planets in the solar system is 8”, then we can say that I’ve given you data. So “data” is not just numbers: a number is not “data” unless there’s an additional verbal, non-numeric context accompanying it, even if only implicitly. Sure, we could represent this meta-data information as numbers too; but this move would only shift the problem one level up: we would need an auxiliary verbal context explaining what the meta-data numbers are about.\nData can, moreover, be completely non-numeric. A clinician saying “The patient has fully recovered from the disease” (we imagine to know who’s the patient and what was the disease) is giving us a piece of information that we could further use, for instance, to make prognoses about other, similar patients. The clinician’s statement surely is “data”, but essentially non-numeric data. Sure, in some situations we can represent it as “1”, while “0” would represent “not recovered”; but the opposite convention could also be used, or the numbers “0.3” and “174”. These numbers have intrinsically nothing to do with the clinician’s “recovery” data.\nBut the examples above actually reveal the answer to our needs. In the examples we expressed the data by means of sentences. Clearly any measurement result, decision outcome, hypothesis, not-real event, assumption, data, and any piece of information can be expressed by a sentence.\nWe shall therefore use sentences, also called propositions or statements,1 to represent and communicate all the kinds of “items” that can be the proposal or conditional of an inference. In some cases we can of course summarize a sentence by a number, as a shorthand, when the full meaning of the sentence is understood.\n\n1 These three terms are not always equivalent in formal logic, but here we’ll use them as synonyms.\nSentences are the central components of knowledge representation in AI agents. For example they appear at the heart of automated control programs and fault-management systems in NASA spacecrafts.\n\n\n (From the SMART paper)\n\n\n\n\n\n\n Reading\n\n\n\n\n§ 7.1 in Artificial Intelligence.\nTake a quick look at these:\n\nSMART: A propositional logic-based trade analysis and risk assessment tool for a complex mission\naround p. 22 in No More Band-Aids: Integrating FM into the Onboard Execution Architecture\n§ 2.1 in Deliberation for autonomous robots: A survey\npart IV in Model-based programming of intelligent embedded systems and robotic space explorers"
  },
  {
    "objectID": "sentences.html#identifying-and-working-with-sentences",
    "href": "sentences.html#identifying-and-working-with-sentences",
    "title": "5  Sentences",
    "section": "5.2 Identifying and working with sentences",
    "text": "5.2 Identifying and working with sentences\nBut what is a sentence, more exactly? The everyday meaning of this word will work for us, even though there are more precise definitions – and still a lot of research in logic an artificial intelligence on how to define and use sentences. We shall adopt this useful definition:\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nPropositions\n\n\n\n\n\n\n\n\n\n\n\n\nA “sentence” is a verbal message for which we can determine whether it is true or false, at least in principle and in such a way that all interested receivers of the message would agree.\n\n\nFor instance, in most engineering contexts the phrase “This valve will operate for at least two months” is a sentence; whereas the phrase “Apples are much tastier than pears” is not, because it’s a matter of personal taste – there’s no objective criterion to determine its truth or falsity (however, the phrase “Rita finds apples tastier than pears” could be a sentence; its truth is found by asking Rita). In a data-science context, the phrase “The neural-network algorithm has better performance than the random-forest one” is not a sentence unless we have objectively specified what “better” means, for example by using a particular comparison metric.\nSome expressions in fact, even involving technical terms, may appear to be sentences at first, but a deeper analysis may reveal that they are not. A famous example is the sentence “The two events (at different spatial locations) are simultaneous”. Einstein showed that there’s no physical way to determine whether such an expression is true or false. Its truth turns out to be a matter of convention (also in Newtonian mechanics). The Theory of Relativity was born from this observation.\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nOn the electrodynamics of moving bodies.\n\n\nOne sentence can be expressed by many different phrases and in different languages. For instance, “The temperature is 248.15 K”, “Temperaturen ligger på minus 25 grader”, and “25 °C is the value of the temperature” all represent the same sentence.\nA sentence can contain numbers, pictures, and graphs.\nWorking with sentences, and keeping in mind that inference is about sentences, is important in several respects:\nFirst, it leads to clarity in engineering problems and makes them more goal-oriented. A data engineer must acquire information and convey information. “Acquiring information” does not simply consist in making measurements or counting something: the engineer must understand what is being measured and why. If data is gathered from third parties, the engineer must ask what exactly the data mean and how they were acquired. In designing and engineering a solution, it is important to understand what information or outcomes the end user exactly wants. The “what”, “why”, “how” are expressed by sentences. A data engineer will often ask “wait, what do you mean by that?”. This question is not just an unofficial parenthesis in the official data-transfer workflow between the engineer and someone else. It is an integral part of that workflow: it means that some information has not been completely transferred yet.\nSecond, it is extremely important in AI and machine-learning design. A (human) engineer may proceed informally when drawing inferences, without worrying about “sentences” unless a need for disambiguation arises. A data engineer who’s designing or programming an algorithm that will do inferences automatically, must instead be unambiguous and cover beforehand all possible cases that the algorithm will face.\n\n\nWe agree that the proposal and the conditional of an inference have to be sentences. This means that the proposal of the inference must be something that can only be true or false. Many inferences, especially when they concern numerical measurements, are actually collections of inferences. For example, an inference about the result of rolling a die actually consists of six separate inferences with the proposals \\[\n\\begin{aligned}\n&\\textsf{\\small`The result of the roll is 1'}\n\\\\\n&\\textsf{\\small`The result of the roll is 2'}\n\\\\\n&\\dotso\n\\\\\n&\\textsf{\\small`The result of the roll is 6'}\n\\end{aligned}\n\\]\nLater on we shall see how to work with more complex inferences without thinking about this detail. In real applications it can be useful, on some occasions, to pause and reduce an inference to its basic set of true/false inferences; this analysis may reveal contradictions in our inference. A simple way to do this is to reduce the complex inference into a set of yes/no questions.\nThis kind of analysis is also important in information-theoretic situations: the information content provided by an inference, when measured in Shannons, is related to the minimal amount of yes/no questions that the inference answers.\n\n\n\n\n\n\n Exercise\n\n\n\nRewrite each inference scenario of § 4.1 in a formal way, as one or more inferences \\[\n\\textit{[proposal]}\\ \\pmb{\\nonscript\\:\\Big\\vert\\nonscript\\:\\mathopen{}}\\ \\textit{[conditional]}\n\\] where proposal and conditional are well-defined sentences.\nIn ambiguous cases, use your judgement and motivate your choices."
  },
  {
    "objectID": "sentences.html#sec-sentence-notation",
    "href": "sentences.html#sec-sentence-notation",
    "title": "5  Sentences",
    "section": "5.3 Notation",
    "text": "5.3 Notation\nWriting full sentences would take up a lot of space. Even an expression such as “The speed is 10 m/s” is not a sentence, strictly speaking, because it leaves unspecified the speed of what, when it was measured and in which frame of reference, what we mean by “speed”, how the unit “m/s” is defined, and so on.\nTypically we leave the full content of a sentence to be understood from the context, and we denote the sentence by a simple expression such as the one above, \\[\n\\textsf{\\small The speed is 10\\,m/s}\n\\] or even more compactly introducing physical symbols: \\[\nv = 10\\,\\mathrm{m/s}\n\\] where \\(v\\) is a physical variable denoting the speed; or even writing simply \\[\n10\\,\\mathrm{m/s}\n\\]\nIn some problems it’s useful to introduce symbols to denote sentences. In these notes we’ll use sans-serif italic letters: \\(\\mathsfit{A},\\mathsfit{B},\\mathsfit{a},\\mathsfit{b},\\dotsc\\),, possibly with sub- or super-scripts. For instance, the sentence “The speed is 10 m/s” could be denoted by the symbol \\(\\mathsfit{S}_{10}\\). We abbreviate such a definition like this: \\[\n\\mathsfit{S}_{10} \\coloneqq \\textsf{\\small`The speed is 10\\,m/s'}\n\\] which means “the symbol \\(\\mathsfit{S}_{10}\\) is defined to be the sentence \\(\\textsf{\\small`The speed is 10\\,m/s'}\\)”.\n\n\n\n\n\n\n We must be wary of how much we shorten sentences\n\n\n\nConsider these three: \\[\n\\begin{aligned}\n&\\textsf{\\small`The speed is measured to be 10\\,m/s'}\n\\\\\n&\\textsf{\\small`The speed is set to 10\\,m/s'}\n\\\\\n&\\textsf{\\small`The speed is reported, by a third party, to be 10\\,m/s'}\n\\end{aligned}\n\\] The quantity “10 m/s” is the same in all three sentences, but their meanings are very different. They represent different kinds of data. These differences greatly affect any inference about or from these data. For instance, in the third case an engineer may not take the indirectly-reported speed “10 m/s” at face value, unlike the first case. In a scenario where all three sentences can occur, it would be ambiguous to simply write “\\(v = 10\\,\\mathrm{m/s}\\)”: would the equal-sign mean “measured”, “set”, or “indirectly reported”?\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nHow would you denote the three sentences above, to make their differences clear?"
  },
  {
    "objectID": "sentences.html#connecting-sentences",
    "href": "sentences.html#connecting-sentences",
    "title": "5  Sentences",
    "section": "5.4 Connecting sentences",
    "text": "5.4 Connecting sentences\n\nAtomic sentences\nIn analysing the measurement results, decision outcomes, hypotheses, assumptions, data and information that enter into an inference problem, it is convenient to find a collection of basic sentences or, using a more technical term, atomic sentences out of which all other sentences of interest can be constructed. These atomic sentences often represent elementary pieces of information in the problem.\nConsider for instance the following complex sentence, which could appear in our assembly-line scenario:\n\n“The electronic component is still whole after the shock test and the subsequent heating test. The voltage reported in the final power test is either 90 mV or 110 mV.”\n\n\nIn this statement we can identify at least four atomic sentences, which we denote by these symbols: \\[\\begin{aligned}\n\\mathsfit{s} &\\coloneqq \\textsf{\\small`The component is whole after the shock test'}\n\\\\\n\\mathsfit{h} &\\coloneqq \\textsf{\\small`The component is whole after the heating test'}\n\\\\\n\\mathsfit{v}_{90} &\\coloneqq \\textsf{\\small`The power-test voltage reading is 90\\,mV'}\n\\\\\n\\mathsfit{v}_{110} &\\coloneqq \\textsf{\\small`The power-test voltage reading is 110\\,mV'}\n\\end{aligned}\n\\]\nThe inference may actually require additional atomic sentences. For instance, it might become necessary to consider atomic sentences with other values for the reported voltage, such as \\[\\begin{aligned}\n\\mathsfit{v}_{110} &\\coloneqq \\textsf{\\small`The power-test voltage reading is 100\\,mV'}\n\\\\\n\\mathsfit{v}_{80} &\\coloneqq \\textsf{\\small`The power-test voltage reading is 80\\,mV'}\n\\end{aligned}\\] and so on.\n\n\nConnectives\nHow do we construct complex sentences, like the one above, out of atomic sentences?\nWe consider three ways: one operation to change a sentence into another related to it, and two operations to combine two or more sentences together. These operations are called connectives; you may have encountered them already in Boolean algebra. Our natural language offers many more operations to combine sentences, but these three connectives turn out to be all we need in virtually all engineering and data-science problems:\n\n\n\n\n\n\n\n\n\n\n\n\nNot:  \\(\\lnot\\)\n\nexample: \\[\n\\lnot \\mathsfit{s} = \\textsf{\\small`The component is broken after the shock test'}\n\\]\n\nAnd:  \\(\\land\\)\n\nexample: \\[\n\\mathsfit{s} \\land \\mathsfit{h} = \\textsf{\\small`The component is whole after the shock and heating tests'}\n\\]\n\nOr:  \\(\\lor\\)\n\nexample: \\[\n\\mathsfit{v}_{90} \\lor \\mathsfit{v}_{110} = \\textsf{\\small`The power-test voltage reading is 90\\,mV, or 110\\,mV, or both'}\n\\]\n\n\n\n\n\nThese connectives can be applied multiple times, to form increasingly complex sentences.\n\n\n\n\n\n\n Important subtleties of the connectives:\n\n\n\n\nThere is no strict correspondence between the words “not”, “and”, “or” in natural language and the three connectives. For instance the and connective could correspond to the words “but” or “whereas”, or just to a comma “ , ”.\nNot means not some kind of complementary quality, but the denial. For instance,  \\(\\lnot\\textsf{\\small`The chair is black'}\\)  generally does not mean  \\(\\textsf{\\small`The chair is white'}\\) ,   (although in some situations these two sentences could amount to the same thing).\nIt’s always best to declare explicitly what the not of a sentence concretely means. In our example we take \\[\n  \\lnot\\textsf{\\small`The component is whole'} \\coloneqq \\textsf{\\small`The component is broken'}\n  \\] But in other examples the negation of “being whole” could comprise several different conditions. A good guideline is to always state the not of a sentence in positive terms.\nOr does not exclude that both the sentences it connects can be true. So in our example  \\(\\mathsfit{v}_{90} \\lor \\mathsfit{v}_{110}\\)  does not exclude, a priori, that the reported voltage could be both 90 mV and 110 mV. (There is a connective for that: “exclusive-or”, but it can be constructed out of the three we already have.)\n\n\n\nFrom the last remark we see that the sentence \\[\n\\textsf{\\small`The power-test voltage reading is 90\\,mV or 110\\,mV'}\n\\] does not correspond to   \\(\\mathsfit{v}_{90} \\lor \\mathsfit{v}_{110}\\) .  It is implicitly understood that a voltage reading cannot yield two different values at the same time. Convince yourself that the correct way to write that sentence is this: \\[\n(\\mathsfit{v}_{90} \\lor \\mathsfit{v}_{110})\n\\land\n\\lnot(\\mathsfit{v}_{90} \\land \\mathsfit{v}_{110})\n\\]\nFinally, the full complex sentence of the present example can be written in symbols as follows:\n\n“The electronic component is still whole after the shock test and the subsequent heating test. The voltage reported in the final power test is either 90 mV or 110 mV.”\n\n\\[\n\\textcolor[RGB]{102,204,238}{\\mathsfit{s}} \\land \\textcolor[RGB]{34,136,51}{\\mathsfit{h}} \\land\n(\\textcolor[RGB]{238,102,119}{\\mathsfit{v}_{90}} \\lor \\textcolor[RGB]{170,51,119}{\\mathsfit{v}_{110}})\n\\land\n\\lnot\n(\\textcolor[RGB]{238,102,119}{\\mathsfit{v}_{90}} \\land \\textcolor[RGB]{170,51,119}{\\mathsfit{v}_{110}})\n\\]\n\n\n\n\n\n\n\n\n Reading\n\n\n\nJust take a quick look at § 7.4.1 in Artificial Intelligence and note the similarities with what we’ve just learned. In these notes we follow a faster approach leading directly to probability logic."
  },
  {
    "objectID": "sentences.html#if-then",
    "href": "sentences.html#if-then",
    "title": "5  Sentences",
    "section": "5.5 “If… then…”",
    "text": "5.5 “If… then…”\nSentences expressing data and information in natural language also appear connected with if… then…. For instance: “If the voltage reading is 200 mV, then the component is defective”. This kind of expression actually indicates that the following inference \\[\n\\textsf{\\small`The component is defective'} \\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{} \\textsf{\\small`The voltage reading is 200\\,mV'}\n\\] is true.\nThis kind of information is very important because it often is the starting point from which to arrive at the final inferences we’re interested in. We shall discuss it more in detail in the next sections.\n\n\n\n\n\n\n Careful\n\n\n\nThere is a connective in logic, called “material conditional”, which is also often translated as “if… then…”. But it is not the same as the inference relation discussed above. “If… then…” in natural language usually denotes an inference rather than a material conditional.\nResearch is still ongoing on these topics. If you are curious and in for a headache, look over The logic of conditionals.\n\n\n\n\n\nWe are now equipped with all the notions and symbolic notation to deal with our next task: learning the rules for drawing correct inferences."
  },
  {
    "objectID": "truth_inference.html#sec-trivial-inference",
    "href": "truth_inference.html#sec-trivial-inference",
    "title": "6  Truth inference",
    "section": "6.1 A trivial inference",
    "text": "6.1 A trivial inference\nConsider again the assembly-line scenario of § 1, and suppose that an inspector has the following information about an electric component:\n\nThis electric component had an early failure (within a year of use). If an electric component fails early, then at production it didn’t pass either the heating test or the shock test. This component passed the shock test.\n\nThe inspector wants to assess whether the component did not pass the heating test.\nFrom the data and information given, the conclusion is that the component for sure did not pass the heating test. This conclusion is certain and somewhat trivial. But how did we obtain it? Which rules did we follow to arrive at it from the given data?\nFormal logic, with its deduction systems, is the huge field that formalizes and makes rigorous the rules that a rational person or an artificial intelligence should use in drawing sure inferences like the one above. We’ll now get a glimpse of it, as a trampoline for jumping towards more general and uncertain inferences."
  },
  {
    "objectID": "truth_inference.html#analysis-and-representation-of-the-problem",
    "href": "truth_inference.html#analysis-and-representation-of-the-problem",
    "title": "6  Truth inference",
    "section": "6.2 Analysis and representation of the problem",
    "text": "6.2 Analysis and representation of the problem\nFirst let’s analyse our simple problem and represent it with more compact symbols.\n\nAtomic sentences\nWe can introduce the following atomic sentences and symbols: \\[\n\\begin{aligned}\n\\mathsfit{h}&\\coloneqq \\textsf{\\small`The component passed the heating test'}\n\\\\\n\\mathsfit{s}&\\coloneqq \\textsf{\\small`The component passed the shock test'}\n\\\\\n\\mathsfit{f}&\\coloneqq \\textsf{\\small`The component had an early failure'}\n\\\\\n\\mathsfit{I}&\\coloneqq \\textsf{\\small (all other implicit background information)}\n\\end{aligned}\n\\]\n\n\nProposal\nThe proposal is \\(\\lnot\\mathsfit{h}\\), but in the present case we could also have chosen \\(\\mathsfit{h}\\).\n\n\nConditional\nThe bases for the inference are two known facts in the present case: \\(\\mathsfit{s}\\) and \\(\\mathsfit{f}\\). There may also be other obvious facts implicitly assumed in the inference, which we denote by \\(\\mathsfit{I}\\).\n\n\nStarting inferences\nLet us emphasize again that any inference is drawn from other inferences, which are either taken for granted, or drawn in turn from others. In the present case we are told that if an electric component fails early, then at production it didn’t pass either the heating test or the shock test. We write this as \\[\n\\lnot\\mathsfit{h}\\lor \\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I}\n\\] and we shall take this to be true (that is, to have probability \\(100\\%\\)).\nBut our scenario actually has at least one more, hidden, inference. We said that the component failed early, and that it did pass the shock test. This means, in particular, that it must be possible for the component to pass the shock test, even if it fails early. This means that \\[\n\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I}\n\\] cannot be false.\n\n\nTarget inference\nThe inference that the inspector wants to draw can be compactly written:\n\n\\[\n\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{s}\\land \\mathsfit{f}\\land \\mathsfit{I}\n\\]"
  },
  {
    "objectID": "truth_inference.html#truth-inference-rules",
    "href": "truth_inference.html#truth-inference-rules",
    "title": "6  Truth inference",
    "section": "6.3 Truth-inference rules",
    "text": "6.3 Truth-inference rules\n\nDeduction systems; a specific choice\nFormal logic gives us a set of rules for correctly drawing sure inferences, when such inferences are possible. These rules can be formulated in different ways, leading to a wide variety of deduction systems (each one with a wide variety of possible notations). The picture on the margin, for instance, shows how a proof of how our inference would look like, using the so-called sequent calculus, which consists of a dozen or so inference rules.\n\n\n\n\n\nThe bottom formula is the target inference. Each line denotes the application of an inference rule, from one or more inferences above the line, to one below the line. The two formulae with no line above are our starting inference, and a tautology.\n\n\n\n\nWe choose to compactly encode all truth-inference rules in the following way.\nFirst, represent true by the number \\(\\mathbf{1}\\), and false by \\(\\mathbf{0}\\).\nSecond, symbolically write that a proposal \\(\\mathsfit{Y}\\) is true, given a conditional \\(\\mathsfit{X}\\), as follows: \\[\n\\mathrm{T}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}) = 1\n\\] or “\\(=0\\)” if it’s false.\nThe rules of truth-inference are then encoded by the following equations, which must always hold for any atomic or complex sentences \\(\\mathsfit{X},\\mathsfit{Y},\\mathsfit{Z}\\):\n\n\n\n\n\n\n\n\n\n\n\n\n\nRule for “not”:\n\n\\[\\mathrm{T}(\\lnot \\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n+ \\mathrm{T}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= 1 \\tag{6.1}\\]\n\nRule for “and”:\n\n\\[\n\\mathrm{T}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= \\mathrm{T}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}\\land \\mathsfit{Z}) \\cdot\n\\mathrm{T}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= \\mathrm{T}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}\\land \\mathsfit{Z}) \\cdot\n\\mathrm{T}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\tag{6.2}\\]\n\nRule for “or”:\n\n\\[\\mathrm{T}(\\mathsfit{X}\\lor \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= \\mathrm{T}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) +\n\\mathrm{T}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n- \\mathrm{T}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\tag{6.3}\\]\n\nRule of self-consistency:\n\n\\[\\mathrm{T}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}\\land \\mathsfit{Z})\n= 1\n\\tag{6.4}\\]\n\n\n\nHow to use the rules: Each equality can be rewritten in different ways according to the usual rules of algebra. Then the resulting left side can be replaced by the right side, and vice versa. The numerical values of starting inferences can be replaced in the corresponding expressions.\n\n\n\nLet’s see two examples:\n\nfrom one rule for “and” we can obtain the equality \\[\n{\\color[RGB]{102,204,238}\\mathrm{T}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}\\land \\mathsfit{Z})}\n=\\color[RGB]{204,187,68}\\frac{\\mathrm{T}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})}{\\mathrm{T}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})}\n\\] provided that \\(\\mathrm{T}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) \\ne 0\\). Then wherever we see the left side, we can replace it with the fraction on the right side, and vice versa.\nfrom the rule for “or” we can obtain the equality \\[\n{\\color[RGB]{102,204,238}\n\\mathrm{T}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) - \\mathrm{T}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})}\n=\\color[RGB]{204,187,68}\n\\mathrm{T}(\\mathsfit{X}\\lor \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) - \\mathrm{T}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\] Again wherever we see the left side, we can replace it with the sum on the right side, and vice versa.\n\n\n\nTarget inference in our scenario\nLet’s see how these rules allow us to arrive at our target inference, \\[\n\\color[RGB]{238,102,119}\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{s}\\land \\mathsfit{f}\\land \\mathsfit{I})\n\\] starting from the given ones \\[\n\\color[RGB]{34,136,51}\\mathrm{T}(\\lnot\\mathsfit{h}\\lor \\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I}) = 1\n\\ ,\n\\qquad\n\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I}) \\ne 0\n\\]\nOne possibility is to work backwards from the target inference:\n\n\\[\n\\begin{aligned}\n&\\color[RGB]{238,102,119}\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{s}\\land \\mathsfit{f}\\land \\mathsfit{I})&&\n\\\\[1ex]\n&\\qquad=\\frac{\\mathrm{T}(\\lnot\\mathsfit{h}\\land \\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n{\\color[RGB]{34,136,51}\\underbracket[1pt]{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}_{\\ne 0}}\n&&\\text{\\small ∧-rule and starting inference}\n\\\\[1ex]\n&\\qquad=\\frac{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\lnot\\mathsfit{h}\\land \\mathsfit{f}\\land \\mathsfit{I})\\cdot\n\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})\n}\n{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n&&\\text{\\small ∧-rule}\n\\\\\n&\\qquad=\\frac{\\bigl[1-\\mathrm{T}(\\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\lnot\\mathsfit{h}\\land \\mathsfit{f}\\land \\mathsfit{I})\\bigr]\\cdot\n\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})\n}\n{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n&&\\text{\\small ¬-rule}\n\\\\\n&\\qquad=\\frac{\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})-\n\\mathrm{T}(\\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\lnot\\mathsfit{h}\\land \\mathsfit{f}\\land \\mathsfit{I})\\cdot\n\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})\n}\n{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n&&\\text{\\small algebra}\n\\\\\n&\\qquad=\\frac{\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})-\n\\mathrm{T}(\\lnot\\mathsfit{s}\\land \\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})\n}\n{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n&&\\text{\\small  ∧-rule}\n\\\\\n&\\qquad=\\frac{{\\color[RGB]{34,136,51}\\mathrm{T}(\\lnot\\mathsfit{h}\\land \\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}-\n\\mathrm{T}(\\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})\n}\n{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n&&\\text{\\small  ∨-rule}\n\\\\\n&\\qquad=\\frac{{\\color[RGB]{34,136,51}1} -\n\\mathrm{T}(\\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})\n}\n{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n&&\\text{\\small starting inference}\n\\\\\n&\\qquad=\\frac{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n&&\\text{\\small ¬-rule}\n\\\\[1ex]\n&\\qquad=\\color[RGB]{238,102,119}1\n&&\\text{\\small algebra}\n\\end{aligned}\n\\]\n\nTherefore \\(\\color[RGB]{238,102,119}\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{s}\\land \\mathsfit{f}\\land \\mathsfit{I}) = 1\\). We find that, indeed, the electronic component must for sure have failed the heating test!\n\n\n\n\n\n\n Exercise\n\n\n\nRetrace the proof above step by step. At each step, how was its particular rule (indicated on the right) used?\n\n\n\n\nThe way in which the rules can be applied to arrive at the target inference is not unique. In fact, in some concrete applications it can require a lot of work to find how to connect target inference with starting ones via the rules. The result, however, will always be the same:\n\n\n\n\n\n\n\n\n\n\n\nThe rules of truth-inference are self-consistent: even if applied in different sequences of steps, they always lead to the same final result.\n\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nProve the target inference \\(\\color[RGB]{238,102,119}\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{s}\\land \\mathsfit{f}\\land \\mathsfit{I}) = 1\\) using the rules of truth-inference, but beginning from the starting inference \\(\\color[RGB]{34,136,51}\\mathrm{T}(\\lnot\\mathsfit{h}\\land \\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})=1\\).\n\n\n\n\n[Optional] Equivalence with truth-tables\nIf you have studied Boolean algebra, you may be familiar with truth-tables; for instance the one for “and” displayed on the side. The truth-inference rules (6.1)–(6.4) contain the truth-tables that you already know as special cases.\n\n\n\n\n\n\\(\\mathsfit{X}\\)\n\\(\\mathsfit{Y}\\)\n\\(\\mathsfit{X}\\land \\mathsfit{Y}\\)\n\n\n\n\n1\n1\n1\n\n\n1\n0\n0\n\n\n0\n1\n0\n\n\n0\n0\n0\n\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nUse the truth-inference rules for “or” and “and” to build the truth-table for “or”. Check if it matches the one you already knew.\n\n\nThe truth-inference rules (6.1)–(6.4) are more complicated than truth-tables, but have two important advantages First, they allow us to work with conditionals, and to move sentences between proposals and conditionals. Second, they provide a smoother transition to the rules for probability-inference."
  },
  {
    "objectID": "truth_inference.html#logical-ai-agents-and-their-limitations",
    "href": "truth_inference.html#logical-ai-agents-and-their-limitations",
    "title": "6  Truth inference",
    "section": "6.4 Logical AI agents and their limitations",
    "text": "6.4 Logical AI agents and their limitations\nThe truth-inference discussed in this section are also the rules that a logical AI agent should follow. For example, the automated control and fault-management programs in NASA spacecrafts, mentioned in § 5.1, are programmed according to these rules.\n\n\n\n\n\n\n Reading\n\n\n\nLook over Ch. 7 in Artificial Intelligence.\n\n\nMany – if not most – inference problems that human and AI agents must face are, however, of the uncertain kind: it is not possible to surely infer the truth of some outcome, and the truth of some initial data or initial inferences may not be known either. We shall now see how to generalize the truth-inference rules to uncertain situations.\n\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nOur cursory visit of formal logic only showed a microscopic part of this vast field. The study of truth-inference rules continues still today, with many exciting developments and applications. Feel free take a look at\n\nLogic in Computer Science\nMathematical Logic for Computer Science\nNatural Deduction Systems in Logic"
  },
  {
    "objectID": "probability_inference.html#sec-probability-def",
    "href": "probability_inference.html#sec-probability-def",
    "title": "7  Probability inference",
    "section": "7.1 When truth isn’t known: probability",
    "text": "7.1 When truth isn’t known: probability\nWhen we cross a busy city street we look left and right to check whether any cars are approaching. We typically don’t look up to check whether something is falling from the sky. Yet, couldn’t it be false that cars are approaching at that moment? and couldn’t it be true that some object is falling from the sky? Of course both events are possible. Then why do we look left and right, but not up?\nThe main reason  is that we believe strongly that cars might be approaching, and believe very weakly that some object might be falling from the sky. In other words, we consider the first occurrence to be very probable, and the second extremely improbable.\nWe shall take the notion of probability as intuitively understood (just as we did with the notion of truth). Terms equivalent for “probability” are degree of belief, plausibility, credibility.\n\n\n\n\n\n\n Avoid likelihood as a synonym for probability\n\n\n\nIn technical discourse, “likelihood” means something different and is not a synonym of “probability”, as we’ll explain later.\n\n\nProbabilities are quantified between \\(0\\) and \\(1\\), or equivalently between \\(0\\%\\) and \\(100\\%\\). Assigning to a sentence a probability 1 is the same as saying that it is true; and a probability 0, that it is false. A probability of \\(0.5\\) represents a belief completely symmetric with respect to truth and falsity.\nLet’s emphasize and agree on some important facts about probabilities:\n\n Probabilities are assigned to sentences. We already discussed this point in § 5.3, but let’s reiterate it. Consider an engineer working on a problem of electric-power distribution in a specific geographical region. At a given moment the engineer may believe with \\(75\\%\\) probability that the measured average power output in the next hour will be 100 MW. The \\(75\\%\\) probability is assigned not to the quantity “100 MW”, but to the sentence \\[\n\\textsf{\\small`The measured average power output in the next hour will be 100\\,MW'}\n\\] This difference is extremely important. Consider the alternative sentence \\[\n\\textsf{\\small`The average power output in the next hour will be \\emph{set} to 100\\,MW'}\n\\] the numerical quantity is the same, but the meaning is very different. The probability can therefore be very different (if the engineer is the person deciding how to set that output, the probability is \\(100\\%\\)). The probability depends not only on a number, but on what it’s being done with that number – measuring, setting, third-party reporting, and so on. Often we write simply “\\(O = \\mathrm{100\\,W}\\)” provided that the full sentence behind this kind of shorthand is understood.\n Probabilities are agent- and context-dependent. A coin is tossed, comes down heads, and is quickly hidden from view. Alice sees that it landed heads-up. Bob instead doesn’t manage to see the outcome and has no clue. Alice considers the sentence \\(\\textsf{\\small`Coin came down heads'}\\) to be true, that is, to have \\(100\\%\\) probability. Bob considers the same sentence to have \\(50\\%\\) probability.\nNote how Alice and Bob assign two different probabilities to the same sentence; yet both assignments are completely rational. If Bob assigned \\(100\\%\\) to \\(\\textsf{\\small`heads'}\\), we would suspect that he had seen the outcome after all; if he assigned \\(0\\%\\) to \\(\\textsf{\\small`heads'}\\), we would consider that groundless and silly. We would be baffled if Alice assigned \\(50\\%\\) to \\(\\textsf{\\small`heads'}\\), because she saw the outcome was actually heads; we would hypothesize that she feels unsure about what she saw.\nAn omniscient agent would know the truth or falsity of every sentence, and assign only probabilities 0 or 1. Some authors speak of “actual (but unknown) probabilities”. But if there were “actual” probabilities, they would be all 0 or 1, and it would be pointless to speak about probabilities at all – every inference would be a truth-inference.\n Probabilities are not frequencies. Consider the fraction of defective mechanical components to total components produced per year in some factory. This quantity can be physically measured and, once measured, would be agreed upon by every agent. It is a frequency, not a degree of belief or probability.\nIt is important to understand the difference between probability and frequency: mixing them up may lead to sub-optimal decisions. Later we shall say more about the difference and the precise relations between probability and frequency.\nFrequencies can be unknown to some agents. Probabilities cannot be “unknown”: they can only be difficult to calculate. Be careful when you read authors speaking of an “unknown probability”: they actually mean either “unknown frequency”, or a probability that has to be calculated (it’s “unknown” in the same sense that the value of \\(1-0.7 \\cdot 0.2/(1-0.3)\\) is “unknown” to you right now).\n Probabilities are not physical properties. Whether a tossed coin lands heads up or tails up is fully determined by the initial conditions (position, orientation, momentum, rotational momentum) of the toss and the boundary conditions (air velocity and pressure) during the flight. The same is true for all macroscopic engineering phenomena (even quantum phenomena have never been proved to be non-deterministic, and there are deterministic and experimentally consistent mathematical representations of quantum theory). So we cannot measure a probability using some physical apparatus; and the mechanisms underlying any engineering problem boil down to physical laws, not to probabilities.\n\n\n\n\n\n\n\n Reading\n\n\n\nDynamical Bias in the Coin Toss. \n\n\n\n\nThese points listed above are not just a matter of principle. They have important practical consequences. A data scientist who is not attentive to the source of the data (measured? set? reported, and so maybe less trustworthy?), or who does not carefully assess the context of a probability, or who mixes a probability with a frequency, or who does not take advantage (when possible) of the physics involved in the a problem – such data scientist will design systems with sub-optimal performance1 – or even cause deaths.1 This fact can be mathematically proven."
  },
  {
    "objectID": "probability_inference.html#sec-uncertain-inference",
    "href": "probability_inference.html#sec-uncertain-inference",
    "title": "7  Probability inference",
    "section": "7.2 An unsure inference",
    "text": "7.2 An unsure inference\nConsider now the following variation of the trivial inference problem of § 6.1.\n\nThis electric component had an early failure. If an electric component fails early, then at production it either didn’t pass the heating test or didn’t pass the shock test. The probability that it didn’t pass both tests is 10%. There’s no reason to believe that the component passed the heating test, more than it passed the shock test.\n\nThe inspector wants to assess, also in this case, whether the component did not pass the heating test.\nFrom the data and information given, what would you say is the probability that the component didn’t pass the heating test?\n\n\n\n\n\n\n Exercises\n\n\n\n\nTry to argue why a conclusion cannot drawn with certainty in this case. One way to argue this is to present two different scenarios that fit the data given but have opposite conclusions.\nTry to reason intuitively and assess the probability that the component didn’t pass the heating test. Should it be larger or smaller than 50%? Why?"
  },
  {
    "objectID": "probability_inference.html#probability-notation",
    "href": "probability_inference.html#probability-notation",
    "title": "7  Probability inference",
    "section": "7.3 Probability notation",
    "text": "7.3 Probability notation\nFor this inference problem we can’t find a true or false final value. The truth-inference rules (6.1)–(6.4) therefore cannot help us here. In fact even the “\\(\\mathrm{T}(\\dotso \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\dotso)\\)” notation is unsuitable, because it only admits the values \\(1\\) (true) and \\(0\\) (false).\nLet us first generalize this notation in a straightforward way:\nFirst, let’s represent the probability or degree of belief of a sentence by a number in the range \\([0,1]\\), that is, between \\(\\mathbf{1}\\) (certainty or true) and \\(\\mathbf{0}\\) (impossibility or false). The value \\(0.5\\) represents that the belief in the truth of the sentence is as strong as that in its falsity.\nSecond, let’s symbolically write that the probability of a proposal \\(\\mathsfit{Y}\\), given a conditional \\(\\mathsfit{X}\\), is some number \\(p\\), as follows: \\[\n\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}) = p\n\\] Note that this notation includes the notation for truth-values as a special case: \\[\n\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}) = 0\\text{ or }1\n\\quad\\Longleftrightarrow\\quad\n\\mathrm{T}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}) = 0\\text{ or }1\n\\]"
  },
  {
    "objectID": "probability_inference.html#inference-rules",
    "href": "probability_inference.html#inference-rules",
    "title": "7  Probability inference",
    "section": "7.4 Inference rules",
    "text": "7.4 Inference rules\nExtending our truth-inference notation to probability-inference notation has been straightforward. But which rules should we use for drawing inferences when probabilities are involved?\nThe amazing result is that the rules for truth-inference, formulae (6.1)–(6.4), extend also to probability-inference. The only difference is that they now hold for all values in the range \\([0,1]\\), rather than for \\(0\\) and \\(1\\) only.\nThis important result was taken more or less for granted at least since Laplace in the 1700s. But was formally proven for the first time in the 1946 by R. T. Cox. The proof has been refined since then. What kind of proof is it? It shows that if we don’t follow the rules we are doomed to arrive at illogical conclusions; we’ll show some examples later.\n\n\nFinally, here are the fundamental rules of all inference. They are encoded by the following equations, which must always hold for any atomic or complex sentences \\(\\mathsfit{X},\\mathsfit{Y},\\mathsfit{Z}\\):\n\n\n\n\n\n\n\n   THE FUNDAMENTAL LAWS OF INFERENCE   \n\n\n\n\n\n“Not” \\(\\boldsymbol{\\lnot}\\) rule\n\n\\[\\mathrm{P}(\\lnot \\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n+ \\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= 1\\]\n\n\n“And” \\(\\boldsymbol{\\land}\\) rule\n\n\\[\n\\mathrm{P}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= \\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}\\land \\mathsfit{Z}) \\cdot\n\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= \\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}\\land \\mathsfit{Z}) \\cdot\n\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\]\n\n\n“Or” \\(\\boldsymbol{\\lor}\\) rule\n\n\\[\\mathrm{P}(\\mathsfit{X}\\lor \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= \\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) +\n\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n- \\mathrm{P}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\]\n\n\nSelf-consistency rule\n\n\\[\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}\\land \\mathsfit{Z})\n= 1\n\\]\n\n\n\n\n\n\nIt is amazing that ALL inference is nothing else but a repeated application of these four rules – billions of times or more in some cases. All machine-learning algorithms are just applications or approximations of these rules. Methods that you may have heard about in statistics are just specific applications of these rules. Truth inferences are also special applications of these rules. Most of this course is, at bottom, just a study of how to apply these rules in particular kinds of problems.\n\n\n\n\n\n\n Reading\n\n\n\n\nProbability, Frequency and Reasonable Expectation\nCh. 2 of Bayesian Logical Data Analysis for the Physical Sciences\n§§ 1.0–1.2 of Data Analysis\nFeel free to skim through §§ 2.0–2.4 of Probability Theory\n\n\n\n \nThe fundamental inference rules are used in the same way as their truth-inference special case: Each equality can be rewritten in different ways according to the usual rules of algebra. Then left and right side of the equality thus obtained can replace each other in a proof."
  },
  {
    "objectID": "probability_inference.html#solution-of-the-uncertain-inference-example",
    "href": "probability_inference.html#solution-of-the-uncertain-inference-example",
    "title": "7  Probability inference",
    "section": "7.5 Solution of the uncertain-inference example",
    "text": "7.5 Solution of the uncertain-inference example\nArmed with the fundamental rules of inference, let’s solve our earlier inference problem. As usual we first analyse it, find what are its proposal and conditional, and which starting inferences are given in the problem.\n\nAtomic sentences\n\\[\n\\begin{aligned}\n\\mathsfit{h}&\\coloneqq \\textsf{\\small`The component passed the heating test'}\n\\\\\n\\mathsfit{s}&\\coloneqq \\textsf{\\small`The component passed the shock test'}\n\\\\\n\\mathsfit{f}&\\coloneqq \\textsf{\\small`The component had an early failure'}\n\\\\\n\\mathsfit{J}&\\coloneqq \\textsf{\\small (all other implicit background information)}\n\\end{aligned}\n\\] The background information in this example is different from the previous, truth-inference one, so we use the different symbol \\(\\mathsfit{J}\\) for it.\n\n\nProposal, conditional, and target inference\nThe proposal is \\(\\lnot\\mathsfit{h}\\), just like in the truth-inference example.\nThe conditional is different now. We know that the component failed early, but we don’t know whether it passed the shock test. Hence the conditional is \\(\\mathsfit{f}\\land \\mathsfit{J}\\).\nThe target inference is therefore \\[\n\\color[RGB]{238,102,119}\\mathrm{P}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\n\\]\n\n\nStarting inferences\nWe are told that if an electric component fails early, then at production it didn’t pass either the heating test or the shock test. Let’s write this as \\[\n\\color[RGB]{34,136,51}\\mathrm{P}(\\lnot\\mathsfit{h}\\lor \\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J}) = 1\n\\] We are also told that there is a \\(10\\%\\) probability that both tests fail \\[\n\\color[RGB]{34,136,51}\\mathrm{P}(\\lnot\\mathsfit{h}\\land \\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J}) = 0.1\n\\]\nFinally the problem says that there’s no reason to believe that the component didn’t pass the heating test, more than it didn’t pass the shock test. This can be written as follows: \\[\n\\color[RGB]{34,136,51}\\mathrm{P}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J}) = \\mathrm{P}(\\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\n\\] Note this interesting situation: we are not given the numerical values of these two probabilities, we are only told that they are equal. This is an example of application of the principle of indifference, which we’ll discuss more in detail later.\n\n\nFinal inference\nAlso in this case there is no unique way of applying the rules to reach our target inference, but all ways lead to the same result. Let’s try to proceed backwards:\n\n\\[\n\\begin{aligned}\n&\\color[RGB]{238,102,119}\\mathrm{P}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})&&\n\\\\[1ex]\n&\\qquad= {\\color[RGB]{34,136,51}\\mathrm{P}(\\lnot\\mathsfit{s}\\lor \\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})}\n+ {\\color[RGB]{34,136,51}\\mathrm{P}(\\lnot\\mathsfit{s}\\land \\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})}\n- \\mathrm{P}(\\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\n&&\\text{\\small ∨-rule}\n\\\\[1ex]\n&\\qquad= {\\color[RGB]{34,136,51}1}\n+ {\\color[RGB]{34,136,51}0.1}\n- \\mathrm{P}(\\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\n&&\\text{\\small starting inferences}\n\\\\[1ex]\n&\\qquad= 0.1 + \\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\n&&\\text{\\small ¬-rule}\n\\\\[1ex]\n&\\qquad= 0.1 + \\mathrm{P}(\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\n&&\\text{\\small starting inference}\n\\\\[1ex]\n&\\qquad= 0.1 + 1 -\\color[RGB]{238,102,119}\\mathrm{P}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\n&&\\text{\\small ¬-rule}\n\\end{aligned}\n\\]\n\nThe target probability appears on the left and right side with opposite signs. We can solve for it: \\[\n\\begin{aligned}\n2\\,{\\color[RGB]{238,102,119}\\mathrm{P}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})} &= 0.1 + 1\n\\\\[1ex]\n{\\color[RGB]{238,102,119}\\mathrm{P}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})} &= 0.55\n\\end{aligned}\n\\]\nSo the probability that the component didn’t pass the heating test is \\(55\\%\\).\n\n\n\n\n\n\n Exercises\n\n\n\n\nTry to find an intuitive explanation of why the probability is 55%, slightly larger than 50%. If your intuition says this probability is wrong, then\n\nCheck the proof of the inference for mistakes, or try to find a proof with a different path.\nExamine your intuition critically and educate it.\n\nCheck how the target probability \\(\\mathrm{P}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\\) changes if we change the value of the probability \\(\\mathrm{P}(\\lnot\\mathsfit{s}\\land \\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\\) from \\(0.1\\).\n\nWhat result do we obtain if \\(\\mathrm{P}(\\lnot\\mathsfit{s}\\land \\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})=0\\)? Can it be intuitively explained?\nWhat if \\(\\mathrm{P}(\\lnot\\mathsfit{s}\\land \\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})=1\\)? Does the result make sense?"
  },
  {
    "objectID": "probability_inference.html#how-the-inference-rules-are-used",
    "href": "probability_inference.html#how-the-inference-rules-are-used",
    "title": "7  Probability inference",
    "section": "7.6 How the inference rules are used",
    "text": "7.6 How the inference rules are used\nIn the solution above you noticed that the equations of the fundamental rules are not only used to obtain some of the probabilities appearing in them from the remaining probabilities.\nThe rules represent, first of all, constraints of logical consistency2 among probabilities. For instance, if we have probabilities  \\(\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{X}\\land \\mathsfit{Z})=0.1\\),  \\(\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z})=0.7\\),  and \\(\\mathrm{P}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z})=0.2\\),  then there’s an inconsistency somewhere, because these values violate the and-rule:  \\(0.2 \\ne 0.1 \\cdot 0.7\\).  In this case we must find the inconsistency and solve it. However, since probabilities are quantified by real numbers, it’s possible and acceptable to have slight discrepancies within numerical round-off errors.2 The technical term is coherence.\nThe rules also imply more general constraints. For example we must always have \\[\n\\begin{gathered}\n\\mathrm{P}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}) \\le \\min\\bigl\\{\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}),\\  \\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z})\\bigr\\}\n\\\\\n\\mathrm{P}(\\mathsfit{X}\\lor \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}) \\ge \\max\\bigl\\{\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}),\\  \\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z})\\bigr\\}\n\\end{gathered}\n\\]\n\n\n\n\n\n\n Exercise\n\n\n\nTry to prove the two constraints above.\n\n\n\nDerived rules\nThe fundamental rules above are in principle all we need to use to draw inferences from other inferences. But from them it is possible to derive some “shortcut” rules.\nFirst, it is possible to show that all rules you may know from Boolean algebra are a consequence of the fundamental rules. So we can always make the following convenient replacements anywhere in a probability expression:\n\n\n\n\n\n\nDerived rules: Boolean algebra\n\n\n\n\\[\n\\begin{gathered}\n\\lnot\\lnot \\mathsfit{X}= \\mathsfit{X}\n\\qquad\n\\mathsfit{X}\\land \\mathsfit{X}= \\mathsfit{X}\\lor \\mathsfit{X}= \\mathsfit{X}\n\\\\[1ex]\n\\mathsfit{X}\\land \\mathsfit{Y}= \\mathsfit{Y}\\land \\mathsfit{X}\n\\qquad\n\\mathsfit{X}\\lor \\mathsfit{Y}= \\mathsfit{Y}\\lor \\mathsfit{X}\n\\\\[1ex]\n\\mathsfit{X}\\land (\\mathsfit{Y}\\lor \\mathsfit{Z}) = (\\mathsfit{X}\\land \\mathsfit{Y}) \\lor (\\mathsfit{X}\\land \\mathsfit{Z})\n\\\\[1ex]\n\\mathsfit{X}\\lor (\\mathsfit{Y}\\land \\mathsfit{Z}) = (\\mathsfit{X}\\lor \\mathsfit{Y}) \\land (\\mathsfit{X}\\lor \\mathsfit{Z})\n\\\\[1ex]\n\\lnot (\\mathsfit{X}\\land \\mathsfit{Y}) = \\lnot \\mathsfit{X}\\lor \\lnot \\mathsfit{Y}\n\\qquad\n\\lnot (\\mathsfit{X}\\lor \\mathsfit{Y}) = \\lnot \\mathsfit{X}\\land \\lnot \\mathsfit{Y}\n\\end{gathered}\n\\]\n\n\nTwo other derived rules are used extremely often, so we treat them separately."
  },
  {
    "objectID": "probability_inference.html#law-of-total-probability-or-extension-of-the-conversation",
    "href": "probability_inference.html#law-of-total-probability-or-extension-of-the-conversation",
    "title": "7  Probability inference",
    "section": "7.7 Law of total probability or “extension of the conversation”",
    "text": "7.7 Law of total probability or “extension of the conversation”\nSuppose we have a set of \\(n\\) sentences \\(\\{\\mathsfit{Y}_1, \\mathsfit{Y}_2, \\dotsc, \\mathsfit{Y}_n\\}\\) having these two properties:\n\nThey are mutually exclusive, meaning that the “and” of any two of them is false, given a conditional \\(\\mathsfit{Z}\\): \\[\n  \\mathrm{P}(\\mathsfit{Y}_1\\land\\mathsfit{Y}_2\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}) = 0\\ , \\quad\n  \\mathrm{P}(\\mathsfit{Y}_1\\land\\mathsfit{Y}_3\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}) = 0\\ , \\quad\n\\dotsc \\ , \\quad\n  \\mathrm{P}(\\mathsfit{Y}_{n-1}\\land\\mathsfit{Y}_n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}) = 0\n  \\]\nThey are exhaustive, meaning that the “or” of all of them is true, given a conditional \\(\\mathsfit{Z}\\): \\[\n  \\mathrm{P}(\\mathsfit{Y}_1\\lor \\mathsfit{Y}_2 \\lor \\dotsb \\lor \\mathsfit{Y}_n \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}) = 1\n  \\]\n\nThen the probability of a sentence \\(\\mathsfit{X}\\), conditional on \\(\\mathsfit{Z}\\), is equal to a combination of probabilities conditional on \\(\\mathsfit{Y}_1,\\mathsfit{Y}_2,\\dotsc\\):\n\n\n\n\n\n\nDerived rule: extension of the conversation\n\n\n\n\n\\[\n\\begin{aligned}\n\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) =\n\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}_1 \\land \\mathsfit{Z})\\cdot \\mathrm{P}(\\mathsfit{Y}_1 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) +\n\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}_2 \\land \\mathsfit{Z})\\cdot \\mathrm{P}(\\mathsfit{Y}_2 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) +{}&\n\\\\[2ex]\n\\dotsb + \\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}_n \\land \\mathsfit{Z})\\cdot \\mathrm{P}(\\mathsfit{Y}_n \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})&\n\\end{aligned}\n\\]\n\n\n\nThis rule is useful when it is difficult to assess the probability of a sentence conditional on the background information, but it is easier to assess the probabilities of that sentence conditional on several auxiliary sentences – often representing hypotheses that exclude one another, and of which we know at least one is true. The name extension of the conversation for this derived rule comes from the fact that we are able to call the additional sentences into play.\nThis situation occurs very often in concrete applications, especially in problems where the probabilities of several competing hypotheses have to be assessed."
  },
  {
    "objectID": "probability_inference.html#bayess-theorem",
    "href": "probability_inference.html#bayess-theorem",
    "title": "7  Probability inference",
    "section": "7.8 Bayes’s theorem",
    "text": "7.8 Bayes’s theorem\nThe probably most famous – or infamous – rule derived from the laws of inference is Bayes’s theorem. It allows us to relate the probability where two sentences \\(\\mathsfit{Y},\\mathsfit{X}\\) appear in the proposal and the conditional, with one where they are exchanged:\n\n\n\n\n\n\nDerived rule: Bayes’s theorem\n\n\n\n\n\\[\n\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}\\land \\mathsfit{Z}) =\n\\frac{\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}\\land \\mathsfit{Z})\\cdot \\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})}{\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})}\n\\]\n\n\n\nObviously this rule can only be used if \\(\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) &gt; 0\\), that is, if the sentence \\(\\mathsfit{X}\\) is not false conditional on \\(\\mathsfit{Z}\\).\n\n\n\n\n\nBayes’s theorem guest-starring in The Big Bang Theory\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nProve Bayes’s theorem from the fundamental rules of inference.\n\n\nBayes’s theorem is extremely useful when we want to assess the probability of a sentence, typically a hypothesis, given some conditional, typically data; and we can easily assess the probability of the data conditional on the hypothesis. Note, however, that the sentences \\(\\mathsfit{Y}\\) and \\(\\mathsfit{X}\\) in the theorem can be about anything whatsoever: \\(\\mathsfit{Y}\\) does not always need to be a “hypothesis”, and \\(\\mathsfit{X}\\) “data”.\n\nCombining with the extension of the conversation\nBayes’s theorem is often with several sentences \\(\\{\\mathsfit{Y}_1, \\mathsfit{Y}_2, \\dotsc, \\mathsfit{Y}_n\\}\\) that are mutually exclusive and exhaustive. Typically these represent competing hypotheses. In this case the probability of the sentence \\(\\mathsfit{X}\\) in the denominator can be expressed using the rule of extension of the conversation:\n\n\n\n\n\n\n\nDerived rule: Bayes’s theorem with extension of the conversation\n\n\n\n\n\\[\n\\mathrm{P}(\\mathsfit{Y}_1 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}\\land \\mathsfit{Z}) =\n\\frac{\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}_1 \\land \\mathsfit{Z})\\cdot \\mathrm{P}(\\mathsfit{Y}_1 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})}{\n\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}_1 \\land \\mathsfit{Z})\\cdot \\mathrm{P}(\\mathsfit{Y}_1 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) +\n\\dotsb + \\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}_n \\land \\mathsfit{Z})\\cdot \\mathrm{P}(\\mathsfit{Y}_n \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n}\n\\]\n\nand similarly for \\(\\mathsfit{Y}_2\\) and so on.\n\n\n\nWe will use this form of Bayes’s theorem very frequently.\n\n\nMany facets\nBayes’s theorem is a very general result of the fundamental rules of inference, valid for any sentences \\(\\mathsfit{X},\\mathsfit{Y},\\mathsfit{Z}\\). This generality leads to many uses and interpretations.\nThe theorem is often proclaimed to be the rule according to which we “update our beliefs”. The meaning of this proclamation is the following. Let’s say that at some point \\(\\mathsfit{Z}\\) represents all your knowledge. Your degree of belief about some sentence \\(\\mathsfit{Y}\\) is then (at least in theory) the value of \\(\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\\). At some later point, let’s say that you get to know – maybe thanks to an observation you made – that the sentence \\(\\mathsfit{X}\\) is true. Your whole knowledge at that point is represented no longer by \\(\\mathsfit{Z}\\), but by \\(\\mathsfit{X}\\land \\mathsfit{Z}\\). Your degree of belief about \\(\\mathsfit{Y}\\) is then given by the value of \\(\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}\\land\\mathsfit{Z})\\). Bayes’s theorem allows you to find your degree of belief about \\(\\mathsfit{Y}\\) conditional on your new state of knowledge, from the one conditional on your old state of knowledge.\nThis chronological element, however, comes only from this particular way of using Bayes’s theorem. The theorem can more generally be used to connect any two states of knowledge \\(\\mathsfit{Z}\\) and \\(\\mathsfit{X}\\land\\mathsfit{Z}\\), no matter their temporal order, even if they happen simultaneously, and even if they belong to two different agents.\n\n\n\n\n\n\n Exercise\n\n\n\nUsing Bayes’s theorem and the fundamental laws of inference, prove that if \\(\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})=1\\), that is, if you already know that \\(\\mathsfit{X}\\) is true in your current state of knowledge \\(\\mathsfit{Z}\\), then \\[\n\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}\\land \\mathsfit{Z}) = \\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\] that is, your degree of belief about \\(\\mathsfit{Y}\\) doesn’t change.\nIs this result reasonable?\n\n\n\n\n\n\n\n\n Reading\n\n\n\n\n§§ 4.1–4.3 in Medical Decision Making give one more point of view on Bayes’s theorem.\nA graphical explanation of how Bayes’s theorem works mathematically (using a specific interpretation of the theorem):"
  },
  {
    "objectID": "probability_inference.html#consequences-of-not-following-the-rules",
    "href": "probability_inference.html#consequences-of-not-following-the-rules",
    "title": "7  Probability inference",
    "section": "7.9 consequences of not following the rules",
    "text": "7.9 consequences of not following the rules\n@@ §12.2.3 of AI\n\nExercise: Monty-Hall problem & variations\nExercise: clinical test & diagnosis"
  },
  {
    "objectID": "probability_inference.html#remarks-on-terminology-and-notation",
    "href": "probability_inference.html#remarks-on-terminology-and-notation",
    "title": "7  Probability inference",
    "section": "7.10 Remarks on terminology and notation",
    "text": "7.10 Remarks on terminology and notation\n\nLikelihood\nIn everyday language, “likely” is often a synonym of “probable”, and “likelihood” of “probability”. But in technical questions about probability, inference, and decision-making, “likelihood” has a very different meaning. Keep in mind this important difference of definition:\n\\(\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{X})\\) is:\n\nthe probability of \\(\\mathsfit{Y}\\) given \\(\\mathsfit{X}\\) (or conditional on \\(\\mathsfit{X}\\)),\nthe likelihood of \\(\\mathsfit{X}\\) in view of \\(\\mathsfit{Y}\\).\n\nLet’s express this also in a different way:\n\n\\(\\mathrm{P}({\\color[RGB]{68,119,170}\\mathsfit{Y}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{X})\\) is the probability of \\(\\mathsfit{Y}\\) given \\(\\mathsfit{X}\\),\n\\(\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}{\\color[RGB]{68,119,170}\\mathsfit{Y}})\\) is the likelihood of \\(\\mathsfit{Y}\\) in view of \\(\\mathsfit{X}\\).\n\n\n\n\n\n\n\n\n\n\n\nA priori there is no relation between the probability and the likelihood of a sentence \\(\\mathsfit{Y}\\): this sentence could have very high probability and very low likelihood, and vice versa.\n\n\nIn these notes we’ll avoid the possibly confusing term “likelihood”. All we need to express can be phrased in terms of “probability”.\n\n\nOmitting background information\nIn the analyses of the inference examples of § 6.1 and § 7.2 we defined sentences (\\(\\mathsfit{I}\\) and \\(\\mathsfit{J}\\)) expressing all background information, and always included these sentences in the conditionals of the inferences – because those inferences obviously depended on that background information.\nIn many concrete inference problems the background information usually stays there in the conditional from beginning to end, while the other sentences jump around between conditional and proposal as we apply the rules of inference. For this reason the background information is often omitted from the notation, being implicitly understood. For instance, if the background information is denoted \\(\\mathsfit{I}\\), one writes\n\n“\\(\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{X})\\)”  instead of  \\(\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{X}\\land \\mathsfit{I})\\)\n“\\(\\mathrm{P}(\\mathsfit{Y})\\)”  instead of  \\(\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\\)\n\nThis is what’s happening when you see in books probabilities “\\(P(x)\\)” without conditional.\nSuch practice may be convenient, but be wary of it, especially in particular situations:\n\nIn some inference problems we suddenly realize that we must distinguish between cases that depend on hypotheses, say \\(\\mathsfit{H}_1\\) and \\(\\mathsfit{H}_2\\), that were buried in the background information \\(\\mathsfit{I}\\). If the background information \\(\\mathsfit{I}\\) is explicitly reported in the notation, this is no problem: we can rewrite it as \\[ \\mathsfit{I}= (\\mathsfit{H}_1 \\lor \\mathsfit{H}_2) \\land \\mathsfit{I}'\\] and proceed, for example using the rule of extension of the conversation. If the background information was not explicitly written, this may lead to confusion and mistakes. For instance there may suddenly appear two instances of \\(\\mathrm{P}(\\mathsfit{X})\\) with different values, just because one of them is invisibly conditional on \\(\\mathsfit{I}\\), the other on \\(\\mathsfit{I}'\\).\nIn some inference problems we are considering several different instances of background information – for example because more than one agent is involved. It’s then extremely important to write the background information explicitly, lest we mix up the different agents’s degrees of belief.\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nA once-famous paper published the quantum-theory literature, arrived at completely wrong results simply by omitting background information, mixing up probabilities having different conditionals.\n\n\nThis kind of confusion from poor notation happens more often than one thinks, and even appears in scientific literature.\n\n\n“Random variables”\nSome texts speak of the probability of a “random variable”, or more precisely of the probability “that a random variable takes on a particular value”. As you notice, we have just expressed that idea by means of a sentence. The viewpoint and terminology of random variables is therefore a special case of that based on sentences, which we use here.\nThe dialect of “random variables” does not offer any advantages in concepts, notation, terminology, or calculations, but it has some shortcomings:\n\nAs discussed in § 7.1, in concrete applications it is important to know how a quantity “takes on” a value: for example it could be directly measured, indirectly reported, or purposely set to that specific value. Thinking and working in terms of sentences, rather than of random variables, allows us to account for these important differences.\nVery often the object (proposal) of a probability is not a “variable”: it is actually a constant value that is simply unknown.\nWhat does “random” (or “chance”) mean? Good luck finding an understandable and non-circular definition in texts that use that word; strangely enough, they never define it. In these notes, if the word “random” is ever used, it stands for “unpredictable” or “unsystematic”.\n\n\n\n\n\n\nJames Clerk Maxwell is one of the main founders of statistical mechanics and kinetic theory (and electromagnetism). Yet he never used the word “random” in his technical writings. Maxwell is known for being very clear and meticulous with explanations and terminology.\n\n\nIt’s a question for sociology of science why some people keep on using less flexible points of view or terminologies. Probably they just memorize them as students and then a fossilization process sets in.\n\n\nFinally, some texts speak of the probability of an “event”. For all purposes an “event” is just what’s expressed in a sentence."
  },
  {
    "objectID": "probability_distributions.html#distribution-of-probabilities-among-values",
    "href": "probability_distributions.html#distribution-of-probabilities-among-values",
    "title": "8  Probability distributions",
    "section": "8.1 Distribution of probabilities among values",
    "text": "8.1 Distribution of probabilities among values\nWhen an agent is uncertain about what the value of a quantity is, this uncertainty is expressed and quantified by assigning a degree of belief to all the possible cases, conditional on the agent’s knowledge. For a temperature measurement, for instance, the cases could be “The temperature is measured to have value 270 K”, “The temperature is measured to have value 271 K”, and so on. We can abbreviate these sentences, denoting the temperature with \\(T\\), as \\[\nT = 270\\,\\mathrm{m} \\ , \\qquad\nT = 271\\,\\mathrm{m} \\ , \\qquad\nT = 272\\,\\mathrm{m} \\ , \\qquad\n\\dotsc\n\\] We recognize these as mutually exclusive and exhaustive sentences.\nOur belief about the quantity is then expressed by a collection of probabilities, conditional on the agent’s state of knowledge \\(\\mathsfit{I}\\): \\[\n\\mathrm{P}(T \\mathord{\\,=\\,}270\\,\\mathrm{K} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) \\ , \\quad\n\\mathrm{P}(T \\mathord{\\,=\\,}271\\,\\mathrm{K} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) \\ , \\quad\n\\mathrm{P}(T \\mathord{\\,=\\,}272\\,\\mathrm{K} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) \\ , \\quad\n\\dotsc\n\\] that sum up to one: \\[\n\\mathrm{P}(T \\mathord{\\,=\\,}270\\,\\mathrm{K} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) +\n\\mathrm{P}(T \\mathord{\\,=\\,}271\\,\\mathrm{K} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) +\n\\mathrm{P}(T \\mathord{\\,=\\,}272\\,\\mathrm{K} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) +\n\\dotsb\n= 1\n\\] This collection of probabilities is called a probability distribution.\n\n\n\n\n\n\n What’s “distributed”?\n\n\n\nIt’s the probability that’s distributed among the possible values, not the quantity, as illustrated in the side picture. The quantity cannot be “distributed”: it has one, definite value, which is however unknown to us.\n\n\n\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nConsider three sentences \\(\\mathsfit{X}_1, \\mathsfit{X}_2, \\mathsfit{X}_3\\) that are mutually exclusive and exhaustive on conditional \\(\\mathsfit{I}\\), that is: \\[\n\\begin{gathered}\n\\mathrm{P}(\\mathsfit{X}_1 \\land \\mathsfit{X}_2 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) =\n\\mathrm{P}(\\mathsfit{X}_1 \\land \\mathsfit{X}_3 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) =\n\\mathrm{P}(\\mathsfit{X}_2 \\land \\mathsfit{X}_3 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = 0\n\\\\\n\\mathrm{P}(\\mathsfit{X}_1 \\lor \\mathsfit{X}_2 \\lor \\mathsfit{X}_3 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = 1\n\\end{gathered}\n\\] Prove, using the fundamental rules of inferences and any derived rules from § 7, that we must then have \\[\n\\mathrm{P}(\\mathsfit{X}_1 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) + \\mathrm{P}(\\mathsfit{X}_2 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) + \\mathrm{P}(\\mathsfit{X}_3 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = 1\n\\]\n\n\nLet’s see how probability distributions can be represented and visualized. We start with probability distributions over discrete values."
  },
  {
    "objectID": "probability_distributions.html#representation-of-discrete-probability-distributions",
    "href": "probability_distributions.html#representation-of-discrete-probability-distributions",
    "title": "8  Probability distributions",
    "section": "8.2 Representation of discrete probability distributions",
    "text": "8.2 Representation of discrete probability distributions\nA probability distribution over a discrete set of values can obviously be displayed in a table of values and their probabilities. For instance\n\n\n\nvalue\n270 K\n271 K\n272 K\n…\n\n\n\n\nprobability\n0.1\n0.2\n0.5\n…\n\n\n\nBut a graphical representation is often helpful to detect features, peculiarities, and even inconsistencies in one or more probability distributions.\n\nHistograms and area-based representations\nA probability distribution for a nominal, ordinal, and discrete interval quantity can be neatly represented by a histogram.\n\n\n\n\n\nHistogram for the probability distribution over possible component failures\n\n\nThe possible values are placed on a line. For an ordinal or interval quantity, the sequence of values on the line should correspond to their natural order. For a nominal quantity the order is irrelevant.\nA rectangle is then drawn above each value. Typically the rectangles are contiguous. The bases of the rectangles are all equal, and the areas of the rectangles are proportional to the probabilities. Since the bases are equal, this implies that the heights of the rectangles are also proportional to the probabilities.\nSuch kind of drawing can of course be horizontal, vertical, upside-down, and so on, depending on convenience.\nSince the probabilities must sum to one, the total area of the rectangles represents the unit of area. So in principle there is no need of writing probability values on some vertical axis, or grid, or similar visual device, because the probability value can be visually read as the ratio of a rectangle area to the total area. An axis or grid can nevertheless be helpful. Alternatively the probabilities can be reported above or below each rectangle.\nNominal quantities do not have any specific order, so their values do not need to be ordered on a line. Other area-based representations, such as pie charts, can also be used for these quantities.\n\n\nCurve-based representations\nHistograms give faithful representations of discrete probability distributions. Their graphical bulkiness, however, can be a disadvantage in some situations; for instance when we want to have a clearer idea of how the probability distribution varies across values (for ordinal or interval quantities); or when we want to compare several probability distributions over the same values.\nIn these cases we can use standard line plots, or variations thereof. Compare the examples on the margin figure: the line plot displays more cleanly the differences between the “before-inspection” and “after-inspection” probability distributions.\n\n\n Representation of the same pair of probability distributions with a histogram plot and a line plot"
  },
  {
    "objectID": "probability_distributions.html#probability-distributions-over-infinite-discrete-values",
    "href": "probability_distributions.html#probability-distributions-over-infinite-discrete-values",
    "title": "8  Probability distributions",
    "section": "8.3 Probability distributions over infinite discrete values",
    "text": "8.3 Probability distributions over infinite discrete values\n@@ TODO"
  },
  {
    "objectID": "probability_distributions.html#probability-densities",
    "href": "probability_distributions.html#probability-densities",
    "title": "8  Probability distributions",
    "section": "8.4 Probability densities",
    "text": "8.4 Probability densities\nDistributions of probability over continuous domains present several counter-intuitive aspects, which essentially come about because we are dealing with uncountable infinities – while often still using linguistic expressions that make at most sense for countable infinities. Here we follow a practical and realistic approach for working with them.\nConsider a quantity \\(X\\) with a theoretically continuous domain. When we say that such a quantity has some value \\(x\\) we really mean that it has a value somewhere in the range \\(x -\\epsilon/2\\) to \\(x+\\epsilon/2\\), where \\(\\epsilon\\) is usually extremely small.\nFor double-precision values stored in a computer, for example, \\(\\epsilon \\approx 2\\cdot 10^{-16}\\) :\n&gt; 1.234567890123456 == 1.234567890123455\n[1] FALSE\n\n&gt; 1.2345678901234567 == 1.2345678901234566\n[1] TRUE\nand a value 1.3 represents a number between 1.29999999999999982236431605997495353221893310546875 and 1.300000000000000266453525910037569701671600341796875, this range coming from the internal binary representation of 1.3. But often the \\(\\epsilon\\) is much larger, coming from the way the value is measured.\nProbabilities are therefore effectively assigned to such small ranges, not to single values. Since these ranges are very small, they are also very numerous. The total probability assigned to all of them must still amount to \\(1\\); therefore each small range effectively receives an extremely small amount of probability. A standard Gaussian distribution for a real quantity, for instance, assigns a probability of approximately \\(8\\cdot 10^{-17}\\), or \\(0.00000000000000008\\), to a range of width \\(2\\cdot 10^{-16}\\) around the value \\(0\\) – and all other ranges are assigned even smaller probabilities.\nIn would be impractical to work with such small probabilities. We therefore use probability densities instead. As implied by the term “density”, a probability density is the amount of probability \\(P\\) within a standard range of width \\(\\epsilon\\), divided by that width.\nProbability densities are convenient because they usually do not depend on the range width \\(\\epsilon\\), if it’s small enough. Owing to physics reasons, we don’t expect a situation where \\(X\\) is between \\(0.9999999999999999\\) and \\(1.0000000000000001\\) to be very different from one where \\(X\\) is between \\(1.0000000000000001\\) and \\(1.0000000000000003\\). The probabilities assigned to these two small ranges of width \\(\\epsilon=2\\cdot 10^{-16}\\) each will therefore be approximately equal, let’s say \\(P\\) each. Now if we use a small range of width \\(\\epsilon\\) around \\(X=1\\), the probability is \\(P\\), and the probability density is \\(P/\\epsilon\\). If we consider a range of double width \\(2\\,\\epsilon\\) around \\(X=1\\), then the probability is \\(P+P\\) instead, but the probability density is still \\((P+P)/(2\\,\\epsilon) = P/\\epsilon\\).\nIn these notes we’ll denote probability densities with a lowercase \\(\\mathrm{p}\\) and use the following notation: \\[\n\\underbracket[0ex]{\\mathrm{p}}_{\\mathclap{\\color[RGB]{204,187,68}\\textit{lowercase}}}(X\\mathord{\\,=\\,}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) \\coloneqq\n\\frac{\n\\overbracket[0ex]{\\mathrm{P}}^{\\mathclap{\\color[RGB]{204,187,68}\\textit{uppercase}}}(\\textsf{\\small`\\(X\\) has value between \\(x-\\epsilon/2\\) and \\(x+\\epsilon/2\\)'} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}{\\epsilon}\n\\] This definition works even if we don’t specify the exact value of \\(\\epsilon\\), as long as it’s small enough.\n\n\n\n\n\n\n Probability densities are not probabilities\n\n\n\nThe expression “\\(\\mathrm{p}(X\\mathord{\\,=\\,}2.5 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})=0.3\\)” does not mean “There is a \\(0.3\\) probability that \\(X\\mathord{\\,=\\,}2.5\\)”. In fact, the probability that \\(X\\mathord{\\,=\\,}2.5\\) exactly is, if anything, zero.\nThat expression means “There is a  \\(0.3\\cdot \\epsilon\\)   probability that \\(X\\) is between \\(2.5-\\epsilon/2\\) and \\(2.5+\\epsilon/2\\), for any \\(\\epsilon\\) small enough”.\nIt is important not to mix up probability and probability densities: we shall see later that the latter have very different properties for example with respect to maxima and averages.\n\n\nA helpful practice (though followed by few texts) is to always write a probability density as  \\(\\mathrm{p}(X\\mathord{\\,=\\,}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\,\\mathrm{d}x\\) ,  where “\\(\\mathrm{d}x\\)” stands for a small range around \\(x\\). This notation is also helpful with integrals. Unfortunately it becomes a little cumbersome when we are dealing with more than one quantity."
  },
  {
    "objectID": "probability_distributions.html#representation-of-continuous-probability-distributions",
    "href": "probability_distributions.html#representation-of-continuous-probability-distributions",
    "title": "8  Probability distributions",
    "section": "8.5 Representation of continuous probability distributions",
    "text": "8.5 Representation of continuous probability distributions\n\nDensity function\nScatter plot\n\n@@ Behaviour of representations under transformations of data."
  },
  {
    "objectID": "probability_distributions.html#summaries-of-distributions-of-probability",
    "href": "probability_distributions.html#summaries-of-distributions-of-probability",
    "title": "8  Probability distributions",
    "section": "8.6 Summaries of distributions of probability",
    "text": "8.6 Summaries of distributions of probability\n\nLocation\nMedian, mean\n\n\nDispersion or range\nQuantiles & quartiles, interquartile range, median absolute deviation, standard deviation, half-range\n\n\nResolution\nDifferential entropy\n\n\nBehaviour of summaries under transformations of data and errors in data"
  },
  {
    "objectID": "probability_distributions.html#outliers-and-out-of-population-data",
    "href": "probability_distributions.html#outliers-and-out-of-population-data",
    "title": "8  Probability distributions",
    "section": "8.7 Outliers and out-of-population data",
    "text": "8.7 Outliers and out-of-population data\n(Warnings against tail-cutting and similar nonsense-practices)"
  },
  {
    "objectID": "probability_distributions.html#marginal-and-conditional-distributions-of-probability",
    "href": "probability_distributions.html#marginal-and-conditional-distributions-of-probability",
    "title": "8  Probability distributions",
    "section": "8.8 Marginal and conditional distributions of probability",
    "text": "8.8 Marginal and conditional distributions of probability"
  },
  {
    "objectID": "probability_distributions.html#collecting-and-sampling-data",
    "href": "probability_distributions.html#collecting-and-sampling-data",
    "title": "8  Probability distributions",
    "section": "8.9 Collecting and sampling data",
    "text": "8.9 Collecting and sampling data\n\n“Representative” samples\nSize of minimal representative sample = (2^entropy)/precision\n\nExercise: data with 14 binary variates, 10000 samples\n\n\n\nUnavoidable sampling biases\nIn high dimensions, all datasets are outliers.\nData splits and cross-validation cannot correct sampling biases"
  },
  {
    "objectID": "probability_distributions.html#quirks-and-warnings-about-high-dimensional-data",
    "href": "probability_distributions.html#quirks-and-warnings-about-high-dimensional-data",
    "title": "8  Probability distributions",
    "section": "8.10 Quirks and warnings about high-dimensional data",
    "text": "8.10 Quirks and warnings about high-dimensional data"
  },
  {
    "objectID": "data_information.html#quantities",
    "href": "data_information.html#quantities",
    "title": "10  Data types",
    "section": "10.1 Quantities",
    "text": "10.1 Quantities\nMost decisions and inferences in engineering and data science involve quantities or entities with some kind of mathematical properties: they can be expressed by a number or – think of images or network graphs – by collections of numbers. The sentences that appear in decision-making and inferences are therefore often of the kind “the quantity \\(X\\) was measured to have value \\(x\\)”. We commonly refer to these values as “data”.\nData come in many different kinds, with different properties. Some statements only make sense with particular kinds of data. We therefore pay a quick visit to the data zoo, emphasizing aspects that are important for inference and decision-making.\nWe shall speak of quantities, denoting them by letters such as \\(X\\). A quantity has a value that must belong to a given set called the domain of that quantity.\nExamples of quantities:\n\nThe distance between two objects in the Solar System at a specific time. The domain could be, say, all values from \\(0\\,\\mathrm{m}\\) to \\(6\\cdot10^{12}\\,\\mathrm{m}\\) (Pluto’s average orbital distance).\nThe number of total views of some online video (at a specific time), with a domain, say, from 0 to 20 billions.\nThe force on an object (at a specific time and place). The domain could be, say, 3D vectors with components in \\([-100\\,\\mathrm{N},\\,+100\\,\\mathrm{N}]\\).\nThe image taken by a camera. The domain could be all possible combinations of values in \\([0,1]\\) over three \\(1280\\times 720\\) grids (one grid per basic colour).\nThe degree of satisfaction in a customer survey, with five possible values Not at all satisfied, Slightly satisfied, Moderately satisfied, Very satisfied, Extremely satisfied.\nThe graph representing a social network, with domain consisting of all possible graphs with \\(0\\) to \\(10000\\) nodes and all possible combinations of links between the nodes.\nThe relationship between the input voltage and output current of an electric component. The domain could be all possible continuous curves from \\([0\\,\\mathrm{V}, 10\\,\\mathrm{V}]\\) to \\([0\\,\\mathrm{A}, 1\\,\\mathrm{A}]\\).\nA 1-minute audio track recorded by a device with a sampling frequency of 48 kHz. The domain could be all possible 2 880 000 values in \\([0,1]\\).\nThe subject of an image, with domain of three possible values cat, dog, something else.\nThe roll, pitch, yaw of a rocket (at a specific time and place), with domain \\((-180°,+180°]\\times(-90°,+90°]\\times(-180°,+180°]\\).\n\nWe take it for granted that a quantity does have one value, and one value only (even if the value itself can consist of, say, several numbers)."
  },
  {
    "objectID": "data_information.html#types-of-quantities",
    "href": "data_information.html#types-of-quantities",
    "title": "10  Data types",
    "section": "10.2 Types of quantities",
    "text": "10.2 Types of quantities\nThere is no clear-cut divide between different data types, and the same data can be seen as one type or another depending on the specific context, problem, purpose, and background information. But it is useful to make some rough distinctions, especially for inference purposes. Here are some common types:\n\nNominal\nA nominal or categorical quantity has a domain with a discrete (usually finite) number of values. The values are not related by any mathematical property, and do not have any specific order (at least not a priori).\nThis means that it does not make sense to say, for instance, that some value is “twice” or “1.5 times” another, or “larger” or “later” than another one. Nor does it make sense to “add” two quantities. In particular, there is no notion of average for a nominal quantity.\nAn example is the possible breeds of a dog, or the characters of a film.\nIt is of course possible to represent the values of a nominal quantity with numbers; say 1 for Dachshund, 2 for Labrador, 3 for Dalmatian, and so on. But that doesn’t mean that “Dalmatian\\({}-{}\\)Labrador\\({}={}\\)Labrador\\({}-{}\\)Dachshund” or similar nonsense.\n\n\nOrdinal\nAn ordinal quantity has a domain with a discrete (usually finite) number of values. The values are not related by any mathematical property, but do have a specific order.\nThis means that it does not make sense to say that some value is “twice” or “1.5 times” another, and we cannot “add” two values. But it does make sense to say, for any two values, which one has higher rank; for example “stronger”, or “later”, and similar. Also in this case there is no notion of average for an ordinal quantity.\nAn example would be a pain-intensity scale: a patient can say whether a pain is more severe than another, but it isn’t clear what a pain “twice as severe” as another would mean (although there’s a lot of research on more precise quantification of pain). Another example could be the “strength of friendship” in a social network: we can say that we have a “stronger friendship” with a person than with another; but it doesn’t make sense to say that we are “four times stronger friends”.\nAlso for ordinal quantities it is possible to represent the values with numbers. In this case the numbers can reflect the order of the values. But it’s important to keep in mind that differences or averages of such numbers wouldn’t make sense. Unfortunately the use of numbers can be deceptive in this regard. A less deceptive possibility is to represent ordered values by alphabet letters, for example.\n\n\nBinary\nA binary or dichotomous quantity has only two possible values. Although it is really a special case of a nominal or ordinal quantity, the fact of having only two values lends it some special properties in inference problems. This is why we list it separately from others.\nObviously it doesn’t make much sense to speak of the difference or average of the two values; and their ranking is trivial even if it makes sense.\nThere’s an abundance of examples of binary quantities: yes/no answers, presence/absence of something, and so on.\n\n\nInterval\nAn interval quantity has a domain that can be discrete or continuous. The values do admit some mathematical operations, at least convex combination and subtraction. They also admit an ordering.\nThis means that we can say whether the interval or “distance” between a pair of values is the same, or larger, or smaller than the interval between another pair. For this reason we can also say whether a value is larger than another. We can also take the convex combination of several values, which means we can take a weighted sum of them. Note that simple addition of values may still be meaningless, though.\nOwing to these mathematical properties, it does make sense to speak of the average for an interval quantity.\nThe number of electronic components produced in a year by an assembly line is an example of a discrete interval quantity. The power output of a nuclear plant at a given time is an example of a continuous one.\nIt is also possible to speak of ratio quantities, which are a special case of interval quantities, but we won’t have use of this distinction in the present notes.\n\n\nOther complex types\nThe types of quantities listed so far are all, in a certain sense, one-dimensional.\nSome complex quantities can be seen simply as collections of quantities of the types listed above, so their analysis and their main properties reduces somehow to the ones already discussed. For example, data about a customer may consist of the collection of two quantities name, age. The first is a nominal quantity, the second an interval one. The combined quantity “name-age” can be dealt with by considering the two quantities separately.\nBut there are many quantities whose sets of possible values have particular mathematical properties and operations that cannot be obtained by simply gathering together quantities of a one-dimensional type. Examples are quantities related to functions, colours, images, audio, video, graphs. For such types of complex quantities it is often important to be able to say whether two values are “close” to each other or “far away”. This leads to the introduction of metrics and other mathematical structures on the sets of values. The metrics may also depend on the particular purpose the quantity is used for.\nSome of these types of complex quantities will be discussed on a case-by-case basis.\n\n\nHow to recognize the type of a quantity?\nTo identify or attribute a “type” a quantity we must ultimately check how that quantity is defined, obtained, and used. In some cases the values of the quantity may give some clue; for example, if we see values “\\(2.74\\)”, “\\(8.23\\)”, “\\(3.01\\)”, then the quantity is probably of the interval kind. But if we see values “\\(1\\)”, “\\(2\\)”, “\\(3\\)”, it’s unclear whether the quantity is interval, ordinal, nominal, or maybe of yet some other kind.\nThe type of a quantity also depends on its use in the specific problem. A quantity of a more complex type can be treated as a simpler type if needed. For instance, the response time of some device is in principle an interval quantity; but in a specific situation we could simply label its values as slow, medium, fast, thus making it an ordinal quantity.\n@@ TODO: add examples for image spaces\n\n\n\n\n\n\n Exercises\n\n\n\n\nFor each example at the beginning of the present section, determine the type of quantity.\nFor each type of quantity discussed above, find two more concrete examples of that type.\n\n\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nOn the theory of scales of measurement"
  },
  {
    "objectID": "data_information.html#other-attributes-of-quantities",
    "href": "data_information.html#other-attributes-of-quantities",
    "title": "10  Data types",
    "section": "10.3 Other attributes of quantities",
    "text": "10.3 Other attributes of quantities\nIt is useful to consider other aspects of quantities that are somewhat transversal to “type”. These aspects are also important when drawing inferences about a quantity.\n\nDiscrete vs continuous\nNominal and ordinal quantities have discrete domains. The domain of an interval quantity can be discrete or continuous. In practice all domains are discrete, since we cannot give values with infinite precision. In a modern computer, for example, a real number can “only” take on \\(2^{64} \\approx 20 000 000 000 000 000 000\\) possible values. In many situations the available precision is so high that we can really consider the quantity as continuous and use the mathematics of continuous sets – derivation, integration, and so on – to our advantage.\n@@ TODO comment on repetition\n\n\nFinite vs infinite\nThe domain of a discrete quantity can have a finite or (at least in theory) an infinite number of possible values. For example, the number of times a link on a webpage can be clicked is in principle infinite. In some inference and decision-making problems it may be necessary to find a concrete finite maximum number for a quantity that is in principle infinite.\n\n\n\n\n\n\n Exercises\n\n\n\nIf you had to set a maximum number of times a web link can be clicked, which number would you choose? Try to find a reasonable number, considering factors such as how fast a person can repeatedly click on a link, how long can a website (or the Earth?) last, and how many people can live in such an extent of time.\n\n\n\n\nBounded vs unbounded\nOrdinal and interval quantities may have domains with no minimum value, or no maximum value, or neither. Typical names for these situations are upper-bounded or right-bounded, left-bounded, unbounded, and so on. A domain can be infinite and yet bounded: consider the numbers in the range \\([0,1]\\),\n\n\nRounded\nA continuous interval quantity may be rounded, owing to the way it’s measured. In this case the quantity could be considered discrete rather than continuous. Rounding can impact the way we do inferences about such a quantity.\nFor instance, the age of a person, which is in principle a continuous quantity, could be rounded down to years or months. Suppose age can only have values between 0 and 120 years, and consider the following cases:\n\nAge is measured with the precision of a minute, and we have age data about 100 persons. In this case it’s very improbable that two or more data values will be identical.\nAge is measured with the precision of a year, and we have age data about 5 persons. Also in this case it’s improbable (but less improbable than in the previous case) that two or more data values will be identical.\nAge is measured with the precision of a year, and we have age data about 1 000 000 persons. Clearly many data values must be identical in this case.\n\n\n\nCensored\nThe measurement procedure of a quantity may have an artificial lower or upper bound. A clinical thermometer, for instance, could have a maximum reading of \\(45\\,\\mathrm{°C}\\): if we measure with it the temperature of a \\(50\\,\\mathrm{°C}\\)-hot body, we’ll read “\\(45\\,\\mathrm{°C}\\)”, not the real temperature. A quantity of this type is called censored, more specifically left-censored or right-censored when there’s only one artificial bound. The bound is called the censoring value.\nA censoring value denotes an actual value that could also be greater or less. This is important when we draw inferences about this kind of quantities."
  },
  {
    "objectID": "statistics.html#the-difference-between-statistics-and-probability-theory",
    "href": "statistics.html#the-difference-between-statistics-and-probability-theory",
    "title": "11  Statistics",
    "section": "11.1 The difference between Statistics and Probability Theory",
    "text": "11.1 The difference between Statistics and Probability Theory\nStatistics is the study of collective properties of collections of data. It does not imply that there is any uncertainty.\nProbability theory is the quantification and propagation of uncertainty. It does not imply that we have collections of data."
  },
  {
    "objectID": "making_decisions.html#decisions-possible-situations-and-consequences",
    "href": "making_decisions.html#decisions-possible-situations-and-consequences",
    "title": "12  Making decisions",
    "section": "12.1 Decisions, possible situations, and consequences",
    "text": "12.1 Decisions, possible situations, and consequences"
  },
  {
    "objectID": "making_decisions.html#gains-and-losses-utilities",
    "href": "making_decisions.html#gains-and-losses-utilities",
    "title": "12  Making decisions",
    "section": "12.2 Gains and losses: utilities",
    "text": "12.2 Gains and losses: utilities\n\nFactors that enter utility quantification\nUtilities can rarely be assigned a priori."
  },
  {
    "objectID": "making_decisions.html#making-decisions-under-uncertainty-maximization-of-expected-utility",
    "href": "making_decisions.html#making-decisions-under-uncertainty-maximization-of-expected-utility",
    "title": "12  Making decisions",
    "section": "12.3 Making decisions under uncertainty: maximization of expected utility",
    "text": "12.3 Making decisions under uncertainty: maximization of expected utility"
  },
  {
    "objectID": "probability_distributions.html#representation-of-probability-densities",
    "href": "probability_distributions.html#representation-of-probability-densities",
    "title": "8  Probability distributions",
    "section": "8.5 Representation of probability densities",
    "text": "8.5 Representation of probability densities\n\nDensity function\nScatter plot\n\n@@ Behaviour of representations under transformations of data."
  }
]