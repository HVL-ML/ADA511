[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ADA511 0.1 Data science and data-driven engineering",
    "section": "",
    "text": "Dear student and aspiring data engineer\nThe goal of this course is not to help you learn how to tune the parameters of the latest kind of deep network, or how to efficiently handle large amounts of data, or how to do cross-validation in the fastest way, or what is the latest improvement in random-forest algorithms.\nThe goal of this course is to help you learn the principles to build the machine-learning algorithms and AI devices of the future. And, as a side effect, you’ll also learn how to concretely improve present-day algorithms, and also how to determine if any of them has already reached its maximal theoretical performance.\nHow can such a goal be achieved?\nThere is a small set of rules and one method that are mathematically guaranteed to output the optimal solution of any inference, prediction, classification, regression, and decision problem. You can think of this as the “unbeatable universal machine”. Or, from an AI point of view, you can think of these rules and method as the “laws of robotics” that should govern any ideal AI designed to draw inferences and make decisions.\nApproximations evolve toward the maximally optimal ideal method. The approximations used at any given time in history exploit the computational technologies then available. Deep networks, for instance, would have been a useless approximation 50 years ago, before the introduction of Graphical Processing Units.\nEvery new technological advance (think of possibly forthcoming quantum computers) opens up possibilities for new approximations that get us closer to the ideal optimum. To see and realize these possibilities, or to judge whether they have already been realized, a data scientist needs at the very least:\n-   to know the foundation of the maximally optimal method\n-   to think outside the box\nWithout the first requirement, how do you know what is the target to approximate towards, and how far you are from it? You risk:\n making an approximation that leads to worse results than before;\n evaluating the approximation in the wrong way, so you don’t even realize it’s worse than before;\n trying to improve an approximation that has already attained the theoretical optimum. Think about an engine that has already the maximal efficiency dictated by thermodynamics; and an engineer, ignorant of thermodynamics, who wastes effort in trying to improve it further.\nWithout the second requirement, you risk missing to take full advantage of the new technological possibilities. Consider the evolution of transportation: if you keep thinking in terms of how to improve a horse-carriage wooden wheels, you’ll never conceive a combustion engine; if you keep thinking in terms of how to improve combustion fuel, you’ll never conceive an electric motor. Existing approximations may of course be good starting points; but you need to clearly understand how they approximate the ideal optimum – so we’re back to the first requirement.\nIf you want to make advances in machine learning and AI, you must know how the ideal universal algorithm looks like, and you must not limit yourself to thinking of “training sets”, “cross-validation”, “supervised learning”, “overfitting”, “models”, and similar notions. In this course you’ll see for yourself that such notions are anchored to the present-day box of approximations.\nAnd we want to think outside that box.\nBut don’t worry: this course does not only want to prepare you for the future. With the knowledge and insights acquired, you will be able to propose and implement concrete improvements to present-day methods as well, or calculate whether they can’t be improved further."
  },
  {
    "objectID": "index.html#your-role-in-the-course-bugs-features",
    "href": "index.html#your-role-in-the-course-bugs-features",
    "title": "ADA511 0.1 Data science and data-driven engineering",
    "section": "Your role in the course Bugs & features",
    "text": "Your role in the course Bugs & features\nThis course is still in an experimental, “alpha” version. So you will not only learn something from it (hopefully), but also test it together with us, and help improving it for future students. Thank you for this in advance!\nFor this reason it’s good to clarify some goals and guidelines of this course: \n\n  Light maths requirements\n\nWe believe that the fundamental rules and methods can be understood and also used (at least in not too complex applications) without complex mathematics. Indeed the basic laws of inference and decision-making involve only the four basic operations \\(+ - \\times /\\). So this course only requires maths at a beginning first-year undergraduate level.\n\n\n\n  Informal style\n\nThe course notes are written in an informal style; for example they are not developed along “definitions”, “lemmata”, “theorems”. This does not mean that they are inexact. We will warn you about parts that are oversimplified or that only cover special contexts.\n\n\n\n  Names don’t constitute knowledge\n\n\n\n\nIn these course notes you’ll often stumble upon terms in blue bold and definitions in blue Italics. This typographic emphasis does not mean that those terms and definitions should be memorized: rather, it means that there are important ideas around there which you must try to understand and use. In fact we don’t care which terminology you adopt. Instead of the term statistical population, feel free to use the term pink apple if you like, as long you explain the terms you use by means of a discussion and examples.2 What’s important is that you know, can recognize, and can correctly use the ideas behind technical terms.\nMemorizing terms, definitions, and where to use them, is how large language models (like chatGPT) operate. If your study is just memorization of terms, you’ll have difficulties finding jobs in the future, because there will be algorithms that can do that better and at a cheaper cost than you.\n2 Some standard technical terms are no better. The common term random variable, for instance, often denotes something that is actually not “random” and not variable. Go figure. Using the term green banana would be less misleading!\n\n  Diverse textbooks\n\nThis course does not have only one textbook: it refers to and merges together parts from several books and articles. As you read these works, you will notice that they adopt quite different terminologies, employ different symbolic notations, give different definitions for similar ideas, and sometimes even contradict each other.\nThese differences and contradictions are a feature, not a bug!\nYou might think that this makes studying more difficult; but it actually helps you to really understand an idea and acquire real knowledge, because it forces you to go beyond words, symbols, and specific points of view and examples. This point connects with the previous point, “names don’t constitute knowledge”. The present course notes will help you build comprehension bridges across those books.\n\n\n\n  Artificial intelligence\n\nIn order to grasp and use the fundamental laws of inference and decision-making, we shall use notions that are also at the foundations of Artificial Intelligence (and less common in present-day machine learning). So you’ll also get a light introduction to AI for free. Indeed, a textbook that we’ll draw frequently from is Russell & Norvig’s Artificial Intelligence: A Modern Approach (we’ll avoid part V on machine learning, however, because it’s poorly explained and written).\n\n\n\n\n  Concrete examples\n\nSome students find it easier to grasp an idea by starting from an abstract description and then examining concrete examples; some find it easier the other way around. We try to make both happy by alternating between the two approaches. Ideas and notions are always accompanied by examples that we try to keep simple yet realistic, drawing from scenarios ranging from glass forensics to hotel booking.\n\n\n\n  Code\n\nWe shall perform inferences on concrete datasets, also comparing different methodologies. Most of these can be performed with any specific programming language, so you can use your favourite one – remember that we want to try to think outside the box of present-day technologies, and that includes present-day programming languages. Most examples in class and in exercises will be given in R and Python, but are easily translated into other languages.\n\n\n\n  Extra material\n\nThe course has strong connections with many other disciplines, such as formal logic, proof theory, psychology, philosophy, physics, and environmental sciences. We have tried to provide a lot of extra reading material in “For the extra curious” side boxes, for those who want to deepen their understanding of topics covered or just connected to the present course. Maybe you’ll stumble into a new passion or even into your life call?\n\n\n\n\n\n\n\n\n For the extra curious"
  },
  {
    "objectID": "index.html#course-structure",
    "href": "index.html#course-structure",
    "title": "ADA511 0.1 Data science and data-driven engineering",
    "section": "Course structure",
    "text": "Course structure\nThe course structure reflects the way in which the ideal universal decision-making machine works. It can be roughly divided into three or four parts, illustrated as follows (this is just a caricature, don’t take this diagram too literally):\n\n\n\n\nflowchart TB\n  P{{probability}} --o Z[/max expected utility\\]\n  subgraph probability [ ]\n  S([sentences]) --&gt; P\n  end\n  subgraph data [ ]\n  A[quantity] --o S\n%%  D[value] --o S\n  E[population] --o S\n  F[...] -.-o S\n  end\n  G[(problem)] --x A & E %% & D\n  G -.-x F\n  U{{utility}} --o Z\n  subgraph utility [ ]\n  C[decisions] --o S\n  S --&gt; U\n  T([gains]) --&gt; U\n  Q([costs]) --&gt; U\n  end\n  G --x C & T & Q\n  subgraph exp [ ]\n  Z --&gt; W(optimal solution)\n  end\n  %%\n  style G fill:#e67\n  style A fill:#cb4,stroke-width:0pt\n%%  style D fill:#cb4,stroke-width:0pt\n  style E fill:#cb4,stroke-width:0pt\n  style F fill:#cb4,stroke-width:0pt\n  style S fill:#283,color:#fff\n  style P fill:#283,stroke-width:3pt,color:#fff\n  style C fill:#6ce,stroke-width:0pt\n  style T fill:#6ce,stroke-width:0pt\n  style Q fill:#6ce,stroke-width:0pt\n  style U fill:#6ce,stroke-width:3pt\n  style Z fill:#47a,color:#fff,stroke-width:0pt,color:#fff\n  style W fill:#47a,color:#fff,stroke-width:0pt,color:#fff\n  style probability fill:#fff,stroke:#283,stroke-width:1px\n  style data fill:#fff,stroke:#cb4,stroke-width:1px\n  style utility fill:#fff,stroke:#6ce,stroke-width:1px\n  style exp fill:#fff,stroke:#47a,stroke-width:1px\n\n\n\n\n\n\nData parts develop the language in which a problem can be fed into the decision-making machine. Here you will also learn about important pitfalls in handling data.\nInference parts develop the “inference engine” of the machine. Here you will learn ideas at the foundation of AI; and you will also meet probability, but from a point of view that may be quite novel to you – and much more fun.\n\nThese two parts will alternate so that their development proceeds almost in parallel.\n\nThe utility part develops the “decision engine” of the machine. Here you will meet several ideas that will probably be quite new to you – but also very simple and intuitive.\nThe final solution part simply shows how the inference and utility engines combine together to yield the optimal solution to the problem. This part is simple, short, intuitive; it will be a breeze.\n\n\n\nAs soon as the inference and data parts are complete, you will be able to apply the machine to real inference problems, and also learn how the solution is approximated in some popular machine-learning algorithms.\nThese applications will immediately extend to decision problems as the short utility and solution part are covered. Again you will also see how this solution is approximated in other machine-learning algorithms, classification and regression ones."
  },
  {
    "objectID": "preface.html#mechanics-and-engineers",
    "href": "preface.html#mechanics-and-engineers",
    "title": "Preface",
    "section": "Mechanics and engineers",
    "text": "Mechanics and engineers\nWhat is the difference between a car mechanic and an automotive engineer?\nBoth have knowledge about cars, but their knowledge domains are different and focus on different goals.\nA car mechanic can keep your car in top-notch condition; can do different kinds of easy and difficult repairs if problems arise with it; knows whether a particular brand of valve can be used as a replacement for another brand; can recommend the optimal kind of tyres to use in a given season for different brands of cars. But a car mechanic would face difficulties in calculating the theoretical maximal efficiency of an engine; or predicting the temperature increase caused by a new kind of fuel; or exploiting the phase transition of a new kind of foam to design a safer airbag system; or calculating the optimal surface curvature for a spoiler. A car mechanic typically possesses a large amount of case-specific knowledge, and doesn’t need to know in depth the principles of electromechanics and thermochemistry, or the laws of balance of momentum, energy, entropy.\n\n\n\nVice versa, an automotive engineer can assess how to use the electromechanical properties of a new material in order to design a more efficient and environmentally friendly engine; can calculate how a new material-surface handling would affect air drag and speed; and ultimately can research how to exploit new physical phenomena to build completely new means of transportation. Yet, an automotive engineer could be completely incapable of changing a pipe in your car, or tell you whether it can use a particular brand of lubricant oil. An automotive engineer typically possesses knowledge about the principles of electromagnetism, mechanics, or thermochemistry; is acquainted with relevant physical laws; and doesn’t need to have in-depth case-specific kinds of knowledge.\n\n\n      \nNote that the differences just sketched do not imply a judgement of value. Both professions, kinds of knowledge, and goals are necessary, interesting, and couldn’t exist without each other. Choice between them is a subjective matter of personal passions and aspirations.\nIn fact there isn’t a clear divide between these two kinds of knowledge, but rather a continuum between two vague extremities. A car mechanic can have knowledge and insight about new technologies, and an automotive engineer can know how to fix a carburettor. The two sketches above are meant to expose and emphasize the existence of such a continuum of knowledge and of goals."
  },
  {
    "objectID": "preface.html#data-mechanics-and-data-engineers",
    "href": "preface.html#data-mechanics-and-data-engineers",
    "title": "Preface",
    "section": "Data mechanics and data engineers",
    "text": "Data mechanics and data engineers\nA continuum with two similar extremities can also be drawn in data science.\nSome data scientists have in-depth knowledge on, for instance, how to optimally store and read large amounts data; what kind of machine-learning algorithm to use in a given task; how to fine-tune an algorithm’s parameters, and the currently best software for this purpose. Their particular knowledge is fundamental for the working of today’s technological infrastructure.\nAt the same time, these data scientists typically face difficulties, for instance, in:\n\ncalculating the theoretical maximal accuracy or performance achievable – by any possible algorithm – in a given inference problem\nexplaining how the fundamental rules of inference and decision-making are implemented in a particular machine-learning algorithm\nidentifying which sub-optimal approximations to the fundamental rules are made by popular machine-learning algorithms\nexploiting new technologies to build new algorithms that do calculations closer to the exact theoretical ones, thereby achieving a performance closer to the theoretical optimum\n\nAnd it is also possible that they are not aware of, and maybe would be surprised by, some basic facts of data science. For instance:\n\nthere is an optimal, universal inference & decision algorithm, of which all machine-learning algorithms (from support vector machines and deep networks to random forests and large language models), are an approximation\nthere are only five or six fundamental laws upon which any inference, prediction, classification, regression, decision task is (or ought to be) based upon\nsplittings of data into “training set”, “validation set”, and similar sets, are not part of the exact application of the laws of inference and decision-making; such splittings arise as coarse approximations of the exact method.\ncross-validation and related techniques are not part of the exact method either; they also arise as approximations\noverfitting, underfitting and related notions are not problems that appear in the exact method (which takes care of them automatically); they also arise from approximations\nit is possible to calculate, within probable bounds, the maximal accuracy (or other performance metric) achievable by any classification or regression algorithm for a given application\nsome evaluation metrics, such as precision or the area under the curve of the receiver operating characteristic (AUC), have intrinsic flaws and may attribute higher values to worse-performing algorithms\n\n…because this is a kind of general and principled knowledge that these data scientists don’t need in their jobs. Their knowledge is more case-specific.\nDrawing a parallel with the car example, a data scientist with this kind of case-specific knowledge is like a “data mechanic”.\nA “data engineer”, on the other hand, is the kind of data scientist who has no difficulties with the knowledge and skills implicit in the bullet points above; but at the same time might not know what software to use for tuning parameters of a particular class of deep networks, or the best format to store particular kinds of data.\nJust like in the case of the automotive industry, the difference just sketched does not imply any judgement of value. Both kinds of knowledge and goals are important and can’t exist without each other."
  },
  {
    "objectID": "preface.html#goals-of-this-course",
    "href": "preface.html#goals-of-this-course",
    "title": "Preface",
    "section": "Goals of this course",
    "text": "Goals of this course\nThere is a plethora of academic courses, in all kinds of format, that target knowledge and goals for the “data mechanic”. Those courses are usually inadequate to cover the knowledge and goals for the “data engineer”. Some courses, misleadingly, even present approximations and recipes that are only valid for particular situations as if they were universal rules or methods instead.\nCourses that target the “data engineer” seem to be more rare. One possible reason is that this kind of knowledge is actually hidden in courses on probability, statistics, and risk analysis, presented with a language which makes only opaque and confusing connections with fields in data science and their goals; or, worse, with a language which emphasizes connections that are actually superficial and misleading.\nWe believe that it is important to teach and keep alive the less “mechanic” and more “engineer” side of data science:\n\nContinuous advances in computational technology – think of quantum computers – will offer completely novel and superior ways to approximate the exact method of inference and decision. Only the data scientist who knows the exact method and theory, and understands how present-day algorithms approximate it, will be able to exploit new technologies.\nEven without looking at the future, several present-day machine-learning algorithms could already be greatly optimized by any data engineer who is acquainted with the basic principles underlying data science.\nThe foundations of data science are the bridge to the sibling discipline of Artificial Intelligence.\n\nThe present course aspires to give an introduction to the “data engineer” side, rather than “data mechanic” one, of data science, but using a point of view more familiar to data scientists than to, say, statisticians.\nMore details about its aims, structure, and features are already given in the Dear student introduction."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Accept or discard?",
    "section": "",
    "text": "\\[\n\\DeclarePairedDelimiters{\\set}{\\{}{\\}}\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\\]\n\nLet’s start with a question that could arise in a particular engineering problem:\n\nA particular kind of electronic component is produced on an assembly line. At the end of the line, there is an automated inspection device that works as follows with every newly produced component coming out of the line.\nThe inspection device first makes some tests on the new component. The tests give an uncertain forecast of whether that component will fail within its first year of use, or after.\nThen the device decides whether the component is accepted and packaged for sale, or discarded and thrown away.\nWhen a new electronic component is sold, the manufacturer has a net gain of \\(1\\$\\), and that’s the net gain if the component works for at least a year. If the component fails within a year of use, however, the manufacturer incurs a net loss of \\(11\\$\\) (12$ loss, minus the 1$ gained at first), owing to warranty refunds and damage costs to be paid to the buyer. When a new electronic component is discarded, the manufacturer has \\(0\\$\\) net gain.\nFor a new electronic component just come out of the assembly line, the tests of the automated inspection device indicate that there is a \\(10\\%\\) probability that the component will fail within its first year of use.\n\n\n\n\nShould the inspection device accept the new component? or discard it?\n\nTry to give and motivate an answer:\n\n\n\n\n\n\n Very first exercise!\n\n\n\n\nShould the inspection device accept or discard the new component?\n\nIt doesn’t matter if you don’t get the correct answer; not even if you don’t manage to get an answer at all. The purpose here is for you to do some introspection about your own reasoning.\nThen examine and discuss the following points:\n\nWhich numerical elements in the problem seem to affect the answer?\nCan these numerical elements be clearly separated? How would you separate them?\nHow would the answer change, if these numerical elements were changed? Feel free to change them, also in extreme ways, and see how the answer would change.\nCould we solve the problem if we didn’t have the probabilities? Why?\nCould we solve the problem if we didn’t know the various gains and losses? Why?\nCan this problem be somehow abstracted, and then transformed into another one with completely different details? For instance, consider translating along these lines:\n\ninspection device → computer pilot of self-driving car\ntests → camera image\nfail within a year → pedestrian in front of car\naccept/discard → keep on going/ break"
  },
  {
    "objectID": "framework.html#what-does-the-intro-problem-tell-us",
    "href": "framework.html#what-does-the-intro-problem-tell-us",
    "title": "2  Framework",
    "section": "2.1 What does the intro problem tell us?",
    "text": "2.1 What does the intro problem tell us?\nLet’s approach the “accept or discard?” problem of the previous chapter 1 in an intuitive way.\n\n\nWe’re jumping the gun here, because we haven’t learned the method to solve this problem yet!\nFirst, what happens if we accept the component?\nWe must try to make sense of the \\(10\\%\\) probability that the component fails within a year. For the moment let’s use an intuitive imagination trick to make sense of this. Imagine that this situation is repeated 100 times. In 10 of these repetitions the accepted electronic component is sold and fails within a year after selling. In the remaining 90 repetitions, the component is sold and works fine for at least a year. Later on we’ll approach this in a more rigorous way, where the idea of “imaginary repetitions” is not needed.\nIn each of the 10 imaginary repetitions in which the component fails early, the manufacturer loses \\(\\color[RGB]{238,102,119}11\\$\\). That’s a total loss of \\({\\color[RGB]{204,187,68}10} \\cdot {\\color[RGB]{238,102,119}11\\$} = {\\color[RGB]{238,102,119}110\\$}\\). In each of the 90 imaginary repetitions in which the component doesn’t fail early, the manufacturer gains \\(\\color[RGB]{34,136,51}1\\$\\). That’s a total gain of \\({\\color[RGB]{204,187,68}90} \\cdot {\\color[RGB]{34,136,51}1\\$} = {\\color[RGB]{34,136,51}90\\$}\\). So over all 100 imaginary repetitions the manufacturer gains\n\\[\n{\\color[RGB]{204,187,68}10}\\cdot ({\\color[RGB]{238,102,119}-11\\$}) + {\\color[RGB]{204,187,68}90}\\cdot {\\color[RGB]{34,136,51}1\\$} = {\\color[RGB]{238,102,119}-20\\$}\n\\]\nthat is, the manufacturer has not gained, but lost \\(20\\$\\)! That’s an average of \\(0.2\\$\\) lost per repetition.\n\n\nNow let’s examine the second choice: what happens if we discard the component instead?\nIn this case we don’t need to invoke imaginary repetitions, but even if we do, it’s clear that the manufacturer doesn’t gain or lose anything – that is, the “gain” is \\(0\\$\\) – in each and all of the repetitions.\nThe conclusion is that if in a situation like this we accept the component, then we’ll lose \\(0.2\\$\\) on average; whereas if we discard it, then on average we’ll lose (or gain) \\(0\\$\\) on average.\nObviously the best, or “least worst”, decision to make is to discard the component.\n\n\n\n\n\n\n Exercises\n\n\n\n\nNow that we have an idea of the general reasoning, check what happens with different values of the probability of failure and of the failure cost: is it still best to discard? For instance, try with\n\nfailure probability 10% and failure cost 5$;\nfailure probability 5% and failure cost 11$;\nfailure probability 10%, failure cost 11$, non-failure gain 2$.\n\nFeel free to get wild and do plots.\nIdentify the failure probability at which accepting the component doesn’t lead to any loss or any gain, so it doesn’t matter whether we discard or accept. (You can solve this as you prefer: analytically with an equation, visually with a plot, by trial & error on several cases, or whatnot.)\nConsider the special case with failure probability 0% and failure cost 10$. This means no new component will ever fail. To decide in such a case we do not need imaginary repetitions; but confirm that we arrive at the same logical conclusion whether we reason through imaginary repetitions or not.\nConsider this completely different problem:\n\nA patient is examined by a brand-new medical diagnostics AI system.\nThe AI first performs some clinical tests on the patient. The tests give an uncertain forecast of whether the patient has a particular disease or not.\nThen the AI decides whether the patient should be dismissed without treatment, or treated with a particular medicine.\nIf the patient is dismissed, then the life expectancy doesn’t increase or decrease if the disease is not present, but it decreases by 10 years if the disease is actually present. If the patient is treated, then the life expectancy decreases by 1 year if the disease is not present (owing to treatment side-effects), but also if the disease is present (because it cures the disease, so the life expectancy doesn’t decrease by 10 years; but it still decreases by 1 year owing to the side effects).\nFor this patient, the clinical tests indicate that there is a \\(10\\%\\) probability that the patient has the disease.\n\nShould the diagnostic AI dismiss or treat the patient? Find differences and similarities, even numerical, with the assembly-line problem.\n\n\n\n\n\nFrom the solution of the problem and from the exploring exercises, we gather some instructive points:\n\nIs it enough if we simply know that the component is less likely to fail than not? in other words, if we simply know that the probability of failure is less than 50% but don’t know its precise value?\nObviously not. We found that if the failure probability is \\(10\\%\\) then it’s best to discard; but if it’s \\(5\\%\\) then it’s best to accept. In both cases the probability of failure was less than 50%, but the decisions were different. Moreover, we found that the probability affected amount of loss if one made the non-optimal decision. Therefore:\nKnowledge of precise probabilities is absolutely necessary for making the best decision\nIs it enough if we simply know that failure leads to a loss, and non-failure leads to a gain, without knowing the precise amounts of loss and gain?\nObviously not. In the exercise we found that if the failure cost is \\(11\\$\\) then it’s best to discard; but if it’s \\(5\\$\\) then it’s best to accept. It’s also best to accept if the failure cost is \\(11\\$\\) but the non-failure gain is \\(2\\$\\). Therefore:\nKnowledge of the precise gains and losses is absolutely necessary for making the best decision\nIs this kind of decision situation only relevant to assembly lines and sales?\nBy all means not. We found a clinical situation that’s exactly analogous: there’s uncertainty, there are gains and losses (of lifetime rather than money), and the best decision depends on both."
  },
  {
    "objectID": "framework.html#our-focus-decision-making-inference-and-data-science",
    "href": "framework.html#our-focus-decision-making-inference-and-data-science",
    "title": "2  Framework",
    "section": "2.2 Our focus: decision-making, inference, and data science",
    "text": "2.2 Our focus: decision-making, inference, and data science\nEvery data-driven engineering project is unique, with its unique difficulties and problems. But there are also problems common to all engineering projects.\nIn the scenarios we explored above, we found an extremely important problem-pattern. There is a decision or choice to make (and “not deciding” is not an option, or it’s just another kind of choice). Making a particular decision will lead to some consequences, some leading to something desirable, others leading to something undesirable. The decision is difficult because its consequences are not known with certainty, given the information and data available in the problem. We may lack information and data about past or present details, about future events and responses, and so on. This is what we call a problem of decision-making under uncertainty or under risk1, or simply a “decision problem” for short.1 We’ll avoid the word “risk” because it has several different technical meanings in the literature, some even contradictory.\nThis problem-pattern appears literally everywhere. But our explored scenarios also suggest that this problem-pattern has a sort of systematic solution method.\nIn this course we’re going to focus on decision problems and their systematic solution method. We’ll learn a framework and some abstract notions that allow us to frame and analyse this kind of problem, and we’ll learn a universal set of principles to solve it. This set of principles goes under the name of Decision Theory. \nBut what do decision-making under uncertainty and Decision Theory have to do with data and data science? The three are profoundly and tightly related on many different planes:\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nDecision theory in expert systems and artificial intelligence\n\n\n\nData science is based on the laws of Decision Theory. These laws are similar to what the laws of physics are to a rocket engineer. Failure to account for these fundamental laws leads at best to sub-optimal solutions, at worst to disasters.\nMachine-learning algorithms, in particular, are realizations or approximations of the rules of Decision Theory. This is clear, for instance, considering that a machine-learning classifier is actually choosing among possible output labels or classes.\nThe rules of Decision Theory are also the foundations upon which artificial-intelligence agents, which must make optimal inferences and decisions, are built.\nWe saw that probability values are essential to a decision problem. How do we find them? Data play an important part in their calculation. In our intro example, the failure probability must come from observations or experiments on similar electronic components.\nWe saw that also the values of gains and losses are essential. Data play an important part in their calculation as well.\n\nThese five planes will constitute the major parts and motivations of the present course.\n\n\nThere are other important aspects in engineering problems, besides the one of making decisions under uncertainty. For instance the discovery or the invention of new technologies and solutions. Aspects such as these two can barely be planned or decided. Their drive and direction, however, rest on a strive for improvement and optimization – and the fundamental laws of decision theory tell us what’s optimal and what’s not.\nArtificial intelligence is proving to be a valuable aid in these more creative aspects too. This kind of use of AI is outside the scope of the present notes. Some aspects of this creativity-assisting use, however, do fall within the domain of the present notes. A pattern-searching algorithm, for example, can be optimized by means of the method we are going to study."
  },
  {
    "objectID": "framework.html#sec-optimality",
    "href": "framework.html#sec-optimality",
    "title": "2  Framework",
    "section": "2.3 Our goal: optimality, not “success”",
    "text": "2.3 Our goal: optimality, not “success”\nWhat should we demand from a systematic method for solving decision problems?\nBy definition, in a decision problem under uncertainty there is generally no method to determine the decision that surely leads to the desired consequence – if such a method existed, then the problem would not have any uncertainty! Therefore, if there is a method to deal with decision problems, its goal cannot be the determination of the successful decision. This also means that a priori we cannot blame an engineer for making an unsuccessful decision in a situation of uncertainty. Then what should be the goal of such a method?\nImagine two persons, Henry and Tina, who must choose between a “heads-bet” or a “tails-bet” before a coin is tossed; the bets work as follows:\n\n“heads-bet”: if the coin lands heads, the person wins a small amount of money; but if it lands tails, they lose a large amount of money\n“tails-bet”: if the coin lands tails, the person wins a small amount of money; if it lands heads, they lose the same small amount of money\n\n\n\n\n\nflowchart LR\n  C[choose a bet!] ---|heads-bet| H([toss coin])\n  C ---|tails-bet| T([toss coin])\n  H ---|heads| HH{{+ $}}\n  H ---|tails| HT{{- $$$}}\n  T ---|heads| TH{{- $}}\n  T ---|tails| TT{{+ $}}\n  %%\n  linkStyle 0 stroke:#cb4,color:#cb4\n  linkStyle 1 stroke:#6ce,color:#6ce\n  linkStyle 2 stroke:#283,color:#283\n  linkStyle 3 stroke:#e67,color:#e67\n  linkStyle 4 stroke:#e67,color:#e67\n  linkStyle 5 stroke:#283,color:#283\n  style HH fill:#283,color:#fff,stroke-width:0px\n  style HT fill:#e67,color:#fff,stroke-width:0px\n  style TT fill:#283,color:#fff,stroke-width:0px\n  style TH fill:#e67,color:#fff,stroke-width:0px\n\n\n\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nWhich bet would you choose? why?\n\n\nHenry chooses the heads-bet. Tina chooses the tails-bet. The coin comes down heads. So Henry wins the small amount of money, while Tina loses the same small amount.\nWhat would we say about their decisions?\nHenry’s decision was lucky, and yet irrational: he risked losing much more money than he could win. Tina’s decision was unlucky, and yet rational: she wasn’t risking to lose more than she could win. Said otherwise, the two bets had the same winning prospects, but the heads-bet had more risk of loss than the tails-bet.\nWe expect that any person making Henry’s decision in similar, future bets will eventually lose more money than any person making Tina’s decision.\nThis example shows two points:\n\n“success” is generally not a good criterion to judge a decision under uncertainty; success can be the pure outcome of luck, not of smarts\neven if there is no method to determine which decision is successful, there is a method to determine which decision is rational or optimal, given the particular gains, losses, and uncertainties involved in the decision problem\n\nWe had a glimpse of this method in our introductory scenarios.\nLet us emphasize, however, that we are not giving up on “success”, or trading it for “optimality”. Indeed we’ll find that Decision Theory automatically leads to the successful decision in problems where uncertainty is not present or is irrelevant. It’s a win-win. It’s important to keep this point in mind:\n\n\n\n\n\n\n\n\n\n\n\nAiming to find the solutions that are successful can make us fail to find those that are optimal when the successful ones cannot be determined.\nAiming to find the solutions that are optimal makes us automatically find those that are successful when those can be determined.\n\n\n\nWe shall later witness this fact with our own eyes, and will take it up again in the discussion of some misleading techniques to evaluate machine-learning algorithms."
  },
  {
    "objectID": "framework.html#sec-decision-theory",
    "href": "framework.html#sec-decision-theory",
    "title": "2  Framework",
    "section": "2.4 Decision Theory",
    "text": "2.4 Decision Theory\nSo far we have mentioned that Decision Theory has the following features:\n\n it tells us what’s optimal and, when possible, what’s successful\n it takes into consideration decisions, consequences, costs and gains\n it is able to deal with uncertainties\n\nWhat other kinds of features should we demand from it, in order to be applied to as many kinds of decision problems as possible, and to be relevant for data science?\n\nIf we find an optimal decision in regards to some outcome, it may still happen that the decision can be realized in several ways that are equivalent in regard to the outcome, but inequivalent in regard to time or resources. In the assembly-line scenario, for example, the decision discard could be carried out by burning, recycling, and so on. We thus face a decision within a decision. In general, a decision problem may involve several decision sub-problems, in turn involving decision sub-sub-problems, and so on.\nIn data science, a common engineering goal is to design and build an automated or AI-based device capable of making an optimal decision in a specific kind of uncertain situations. Think for instance of an aeronautic engineer designing an autopilot system, or a software company designing an image classifier.\n\nDecision Theory turns out to meet these two demands too, thanks to the following features:\n\n it is susceptible to recursive, sequential, and modular application\n it can be used not only for human decision-makers, but also for automated or AI devices\n\n\n\nDecision Theory has a long history, going back to Leibniz in the 1600s and partly even to Aristotle in the −300s, and appearing in its present form around 1920–1960. What’s remarkable about it is that it is not only a framework, but the framework we must use. A logico-mathematical theorem shows that any framework that does not break basic optimality and rationality criteria has to be equivalent to Decision Theory. In other words, any “alternative” framework may use different technical terminology and rewrite mathematical operations in a different way, but it boils down to the same notions and operations of Decision Theory. So if you wanted to invent and use another framework, then either (a) it would lead to some irrational or illogical consequences, or (b) it would lead to results identical to Decision Theory’s. Many frameworks that you are probably familiar with, such as optimization theory or Boolean logic, are just specific applications or particular cases of Decision Theory.\nThus we list one more important characteristic of Decision Theory:\n\n it is normative\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\n\nJudgment under uncertainty\nHeuristics and Biases\nThinking, Fast and Slow\n\n\n\nNormative contrasts with descriptive. The purpose of Decision Theory is not to describe, for example, how human decision-makers typically make decisions. Because human decision-makers typically make irrational, sub-optimal, or biased decisions. That’s exactly what we want to avoid and improve!\n\n\n\n\n\n\n Study reading\n\n\n\nWho says that Decision Theory should be normative? – this is a respectable scientific question. If you found yourself wondering and doubting about this, then congratulations: that’s how a scientist should think!\nLater on we’ll examine material and arguments about this statement. If you like, feel free to already skim through the following works, as a starting point:\n\nCh. 15, especially § 15.1 and § “Bibliographical and Historical Notes” of Artificial Intelligence\nNormative Theories of Rational Choice: Expected Utility\nDecision Theory\nDecision Analysis"
  },
  {
    "objectID": "basic_decisions.html#graphical-representation-and-elements",
    "href": "basic_decisions.html#graphical-representation-and-elements",
    "title": "3  Basic decision problems",
    "section": "3.1 Graphical representation and elements",
    "text": "3.1 Graphical representation and elements\nA basic decision problem can be represented by a diagram like this:\n\nIt has one decision node, usually represented by a square , from which the available decisions depart as lines. Each decision leads to an inference node,1 usually represented by a circle , from which the possible outcomes depart as lines. Each outcome leads to a particular gain or loss, depending on the decision. The uncertainty of each outcome is quantified by a probability.1 also called chance node or uncertainty node\nA basic decision problem is analysed in terms of the following elements:\n\n\n\n\n\n\n\n\n\n\n\n\n Remember: What matters is to be able to identify these elements in a concrete problem, understanding their role. Their technical names don’t matter.\n\n\n\n Agent, and background or prior information. The agent is the person or device that has to make the decision. An agent possesses (or has been programmed with) specific background information that is used and taken for granted in the decision-making process. This background information determines the probabilities, gains, and losses of the outcomes, together with other available data and information. Different agents typically have different background information.\n\n\n\nAgent means “conductor”, “mover”, and similar (from Latin ago = to move or drive and similar meanings).\nWe’ll use the neutral pronouns it/its when referring to an agent, since an agent could be a person or a machine.\n\n Data and other additional information, sometimes called evidence. They differ from the background information in that they can change with every decision instance made by the same agent, while the background information stays the same. In the assembly-line scenario, for example, the test results could be different for every new electric component.\n Decisions available to the agent. They are assumed to be mutually exclusive and exhaustive; this can always be achieved by recombining them if necessary, as we’ll discuss later.\n\n\n\nDecisions are called courses of action in some literature.\n\n Outcomes of the possible decisions. Every decision can have a different set of outcomes, or some outcomes can appear for several or all decisions (in this case they are reported multiple times in the decision diagram). Note that even if an outcome can happen for two or more different decisions, its probabilities can still be different depending on the decision.\n\n\n\nMany other terms instead of outcome are used in the literature, for instance state or event.\n\n Probabilities for each of the outcomes and for each decision. Their values typically depend also on the background information and the additional data.\n Utilities: the gains or losses associated with each possible outcome and each decision. We shall mainly use the term utility, instead of “gain”, “loss”, and similar, for several reasons:\n\ngain and losses may involve not money, but time, or energy, or health, or emotional value, or other kinds of commodities and things that are important to us; or even a combination of them. The term “utility” is useful as a neutral term that doesn’t mean “money”, but depends on the context\nwe can just use one term instead of two: for example, when the utility is positive it’s a “gain”; when it’s negative it’s a “loss”\n\nThe particular numerical values of the utilities are always context-dependent: they may depend on the background information, the decisions, the outcomes, and the additional data.\nWe shall sometimes use the generic currency sign  ¤  to denote utilities, to make clear that gains and losses do not necessarily involve money, and not to reference any country in particular.\n\n\n\nThe relation between the elements above can be depicted as follows – but note that this is just for an intuitive illustration: \n\n\n\n\n\n\n Don’t over-interpret the decision diagram\n\n\n\n\nThe diagram above doesn’t have any temporal meaning, that is, it doesn’t mean that the decisions happen before the outcomes, or vice versa.\nIn some situations the outcome can be realized after the decision is made; for instance, someone bets on heads or tails, and then a coin is tossed.\nIn other situations, the outcome can be realized before the decision is made; for instance, sometimes a coin is tossed and covered, then one is asked to bet on what the outcome was. Another example is some research decision made by a archaeologist, the unknown being some detail about a dinosaur from millions of years ago.\nIn yet other situations the outcome may have a complex nature, and it may be realized partly before the decision is made, and partly after; for instance, someone can bet on the outcome of two coin tosses; one coin is tossed before the decision is made, and the other after.\nThe diagram above is not something that an agent must use in making decisions. It is not part of the theory. It’s just a very convenient way to visualize and operate with the mathematics underlying the theory.\nIt not always the case that the outcomes are unknown and the data are known. As we’ll discuss later, in some situations we reason in hypothetical or counterfactual ways, using hypothetical data and considering outcomes which have already occurred. In such situations we can still use diagrams like the one above, because the help us doing the calculation, although the actual outcome is already known.\n\n\n\n\n\n\n\n\n\n Study reading\n\n\n\n\n§ 1.1.4 in Artificial Intelligence\nSkim through Ch. 15 of Artificial Intelligence. No need to read thoroughly: just quickly glimpse whether there are ideas and notions that look familiar (a little like when you’re in a large crowd and look quickly around to see if there are any familiar faces)\n\n\n\n\n\n\n\n\n\n Exercise\n\n\n\n\nIdentify the elements above in the assembly-line decision problem of the introduction 1.\nSketch the decision diagram for the assembly-line decision problem.\n\n\n\nSome of the decision-problem elements listed above may need to be in turn analysed by a decision sub-problem. For instance, the utilities could depend on uncertain factors: thus we have a decision sub-problem to determine the optimal values to be used for the utilities of the main problem. This is an example of the modular character of decision theory.\nWe shall soon see how to mathematically represent these elements.\nThe elements above must be identified unambiguously in every decision problem. The analysis into these elements greatly helps in making the problem and its solution well-defined.\nAn advantage of decision theory is that its application forces us to make sense of an engineering problem. A useful procedure is to formulate the general problem in terms of the elements above, identifying them clearly. If the definition of any of the terms involves uncertainty of further decisions, then we analyse it in turn as a decision sub-problem, and so on.\n\nSuppose someone (probably a politician) says: “We must solve the energy crisis by reducing energy consumption or producing more energy”. From a decision-making point of view, this person has effectively said nothing whatsoever. By definition the “energy crisis” is the problem that energy production doesn’t meet demand. So this person has only said “we would like the problem to be solved”, without specifying any solution. A decision-theory approach to this problem requires us to specify which concrete courses of action should be taken for reducing consumption or increasing productions, and what their probable outcomes, costs, and gains would be.\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nSee MacKay’s options-vs-costs rational analysis in Sustainable Energy – without the hot air"
  },
  {
    "objectID": "basic_decisions.html#sec-decision-matrices",
    "href": "basic_decisions.html#sec-decision-matrices",
    "title": "3  Basic decision problems",
    "section": "3.2 Setting up a basic decision problem",
    "text": "3.2 Setting up a basic decision problem\nA basic decision problem can be set up along the following steps (which we illustrate afterwards with a couple of examples):\n\n\n\n\n\n\nSetup of a basic decision problem\n\n\n\n\nList all available decisions\nFor each decision, list its possible outcomes\nPool together all outcomes of all decisions, counting the common ones only once\nPrepare two tables: in each, display the decisions as rows, and the pooled outcomes as columns (or you can do the opposite: decisions as columns and outcomes as rows)\nIn one table, report the probabilities for all decision-outcome pairs. If an outcome is not available for that decision, give it a \\(0\\%\\) probability\nIn the other table, report the utilities for all decision-outcome pairs. If an outcome is not available for that decision, give it a \\(0\\) utility\n\n\n\n\nExample: the assembly-line problem\nLet’s apply the steps above in the assembly-line example of ch.  1:\n1. List all available decisions\nEasy: they are “accept the electronic component” and “discard it”.\n\n\n2. For each decision, list its possible outcomes\nIn general you will notice that some outcomes may be common to all decisions, while other outcomes can happen for some decisions only, or even for just one decision.\nIn the present example, the accept decision has two possible outcomes: “the component works with no faults for at least a year” and “the component fails within a year”.\nThe discard cannot have those outcomes, because the component is discarded. It has indeed only one outcome: “component discarded”.\n\n\n3. Pool together all outcomes of all decisions, counting the common ones only once\nIn total we have three pooled outcomes:\n\nno faults (from the accept decision)\nfails (from the accept decision)\ndiscarded (from the discard decision)\n\n\n\n4. Prepare two tables: in each, display the decisions as rows, and the pooled outcomes as columns (or you can do the opposite: decisions as columns and outcomes as rows)\nIn the present example each table looks like this:\n\n\n\n\n\n\n\n\n\n\n\nno faults for a year\nfails within a year\ndiscarded\n\n\n\n\naccept\n\n\n\n\n\ndiscard\n\n\n\n\n\n\n\n\n5. In one table, report the probabilities for all decision-outcome pairs. If an outcome is not available for that decision, give it a \\(0\\%\\) probability\n\nProbability table\n\n\n\n\n\n\n\n\n\nno faults for a year\nfails within a year\ndiscarded\n\n\n\n\naccept\n\\(90\\%\\)\n\\(10\\%\\)\n\\(0\\%\\)\n\n\ndiscard\n\\(0\\%\\)\n\\(0\\%\\)\n\\(100\\%\\)\n\n\n\nNote how the outcomes that do not exist for a particular decision have been given a \\(0\\%\\) probability (in grey). This is just a way of saying “this outcome can’t happen, if this decision is made”.\n\n\n6. In the other table, report the utilities for all decision-outcome pairs. If an outcome is not available for that decision, give it a \\(0\\) utility\n\nUtility table\n\n\n\n\n\n\n\n\n\nno faults for a year\nfails within a year\ndiscarded\n\n\n\n\naccept\n\\(+1\\$\\)\n\\(-11\\$\\)\n\\(0\\$\\)\n\n\ndiscard\n\\(0\\$\\)\n\\(0\\$\\)\n\\(0\\$\\)\n\n\n\nNote how the outcomes that do not exist for a particular decision have been given a \\(0\\$\\) utility (in grey). We shall see later that it actually doesn’t matter which utilities we give to these impossible outcomes.\n\n\n\n\n\n\n\n\n Exercises\n\n\n\nApply the steps above to the following basic decision problems (you only need to set them up with their probability & utility tables, but feel free to solve them as well, if you like):\n\nThe “heads-bet” vs “tails-bet” example of §  2.3. Assume that the “small amount” of money is \\(10\\$\\), the “large amount” is \\(1000\\$\\), and the two outcomes’ probabilities are \\(50\\%\\) each.\nPeter must reach a particular destination, and is undecided between two alternatives: go by car, or ride a bus, or go on foot. If he goes by car, he could arrive without problems, with a probability of \\(80\\%\\) and a utility of \\(10\\,¤\\), or he could get stuck in a traffic jam and arrive late, with a probability of \\(20\\%\\) and a utility of \\(-10\\,¤\\). If he rides a bus, he could arrive without problems, with a probability of \\(95\\%\\) and a utility of \\(15\\,¤\\), or arrive in time but travelling in a fully-packed bus, with a probability of \\(5\\%\\) and a utility of \\(-10\\,¤\\). If he goes on foot, he could arrive without problems, with a probability of \\(20\\%\\) and a utility of \\(20\\,¤\\), or he could get soaked from rain, with a probability of \\(80\\%\\) and a utility of \\(-5\\,¤\\).\n(We are using the symbol  “\\(¤\\)”  because Peter’s utilities are a combination of money savings, time of arrival, and comfort.)"
  },
  {
    "objectID": "basic_decisions.html#sec-make-decision",
    "href": "basic_decisions.html#sec-make-decision",
    "title": "3  Basic decision problems",
    "section": "3.3 How to make a basic decision?",
    "text": "3.3 How to make a basic decision?\nUp to now we have seen what are the elements of a basic decision problem, and how to arrange them in a diagram and with tables. But how do we determine what’s the optimal decision?\nDecision Theory says that the optimal decision is determined by the “principle of maximal expected utility”.\nWe shall study this principle more in detail toward the end of the course, although you already know its basic idea, because you intuitively used this very principle in solving all decision problems we met so far, starting from the assembly-line one.\nHowever, let’s quickly describe already now the basic procedure for this principle:\n\n\n\n\n\n\nPrinciple of maximal expected utility\n\n\n\n\nFor each decision, multiply the probability and the utility of each of its outcomes, and then sum up these products. This way you obtain the expected utility of the decision.\nChoose the decision that has the largest expected utility; if several decisions are maximal, choose any of them unsystematically.\n\n\n\nThis procedure can also be described in terms of the probability and utility tables introduced in the previous section:\n\nMultiply element-by-element the probability table and the utility table, obtaining a new table with the same number of rows and columns\nSum up the elements of each row of the new table (this sum is the expected utility); remember that every row corresponds to a decision\nChoose the decision corresponding to the largest of the sums above; if there are several maximal ones, choose among them unsystematically\n\n\nExample: the assembly-line problem\nMultiplying the Probability table and the Utility table above, element-by-element, we obtain the following table, where we also indicate the sum of each row:\n\n\nProbability × Utility table\n\n\n\n\n\n\n\n\n\n\nno faults for a year\nfails within a year\ndiscarded\nsum\n\n\n\n\naccept\n\\(+0.9\\$\\)\n\\(-1.1\\$\\)\n\\(0\\$\\)\n\\(\\boldsymbol{-0.2\\$}\\)\n\n\ndiscard\n\\(0\\$\\)\n\\(0\\$\\)\n\\(0\\$\\)\n\\(\\boldsymbol{0\\$}\\)\n\n\n\n\nand, as we already knew, discarding the electronic component is the decision with the maximal expected utility.\n\n\n\n\n\n\n Exercise\n\n\n\nFeel free to sketch some code (in your preferred programming language) that chooses the optimal decision according to the principle above. The code should take two inputs: the table or matrix of probabilities, and the table or matrix of utilities; and should give one output: the row-number of the optimal decision."
  },
  {
    "objectID": "basic_decisions.html#plan-for-the-next-chapters",
    "href": "basic_decisions.html#plan-for-the-next-chapters",
    "title": "3  Basic decision problems",
    "section": "3.4 Plan for the next chapters",
    "text": "3.4 Plan for the next chapters\nThe expected-utility maximization above is intuitive and simple, and is the last stage in a basic decision problem.\nBut there are two stages which occur before, and which are the most difficult:\n\n Inference\n\nis the stage where the probabilities of the possible outcomes are calculated. Its rules are given by the Probability Calculus. Inference is independent from decision: in some situations we may simply wish to assess whether some hypotheses, conjectures, or outcomes are more or less plausible than others, without making any decision. This kind of assessment can be very important in problems of communication and storage, and it is specially considered by Information Theory.\n\n\nThe calculation of probabilities can be the part that demands most thinking, time, and computational resources in a decision problem. It is also the part that typically makes most use of data – and where data can be most easily misused.\nRoughly half of this course will be devoted in understanding the laws of inference, their applications, uses, and misuses.\n\n\n Utility assesment\n\nis the stage where the gains or losses of the possible outcomes are calculated. Often this stage requires further inferences and further decision-making sub-problems. The theory underlying utility assessment is still much underdeveloped, compared to probability theory.\n\n\n\n\n\nWe shall now explore each of these two stages. We take up inference first because it is the most demanding and probably the one that can be optimized the most by new technologies."
  },
  {
    "objectID": "0th_connection_ML.html#a-max-success-classifier-vs-an-optimal-classifier",
    "href": "0th_connection_ML.html#a-max-success-classifier-vs-an-optimal-classifier",
    "title": "4  First connection with machine learning",
    "section": "4.1 A “max-success” classifier vs an optimal classifier",
    "text": "4.1 A “max-success” classifier vs an optimal classifier\n\n\n\n\n\n\n\n\n\n\nYou find the code for this chapter and exercises also in this JupyterLab notebook for R and (courtesy of Viktor Karl Gravdal!) this JupyterLab notebook for python.\n\n\nWe shall compare the results obtained in some numerical simulations by using\n\na Machine-Learning Classifier trained to do most successful guesses\na prototype “Optimal Predictor Machine” trained to make the optimal decision\n\nFor the moment we treat both as “black boxes”, that is, we don’t study yet how they’re calculating their outputs (although you may already have a good guess at how the Optimal Predictor Machine works).\nTheir operation is implemented in this R script that we now load:\n\nsource('code/mlc_vs_opm.R')\n\nThis script simply defines the function hitsvsgain():\nhitsvsgain(ntrials, chooseAtrueA, chooseAtrueB, chooseBtrueB, chooseBtrueA, probsA)\nhaving six arguments:\n\nntrials: how many simulations of guesses to make\nchooseAtrueA: utility gained by guessing A when the successful guess is indeed A\nchooseAtrueB: utility gained by guessing A when the successful guess is B instead\nchooseBtrueB: utility gained by guessing B when the successful guess is indeed B\nchooseBtrueA: utility gained by guessing B when the successful guess is A instead\nprobsA: a tuple of probabilities (between 0 and 1) to be used in the simulations (recycling it if necessary), for the successful guess being A; the corresponding probabilities for B are therefore 1-probsA. If this argument is omitted it defaults to 0.5 (not very interesting)"
  },
  {
    "objectID": "0th_connection_ML.html#example-1-electronic-component",
    "href": "0th_connection_ML.html#example-1-electronic-component",
    "title": "4  First connection with machine learning",
    "section": "4.2 Example 1: electronic component",
    "text": "4.2 Example 1: electronic component\nLet’s apply our two classifiers to the Accept or discard? problem of §  1. We call A the alternative in which the element won’t fail before one year, and should therefore be accepted if this alternative were known at the time of the decision. We call B the alternative in which the element will fail within a year, and should therefore be discarded if this alternative were known at the time of the decision. Remember that the crucial point here is that the classifiers don’t have this information at the moment of making the decision.\nWe simulate this decision for 100 000 components (“trials”), assuming that the probabilities of failure can be 0.05, 0.20, 0.80, 0.95. The values of the arguments should be clear:\n\nhitsvsgain(ntrials=100000, chooseAtrueA=+1, chooseAtrueB=-11, chooseBtrueB=0, chooseBtrueA=0, probsA=c(0.05, 0.20, 0.80, 0.95))\n\n\nTrials: 100000\nMachine-Learning Classifier: successes 87613 ( 87.6 %) | total gain -23384\nOptimal Predictor Machine:   successes 72324 ( 72.3 %) | total gain 10457\n\n\nNote how the machine-learning classifier is the one that makes most successful guesses (around 88%), and yet it leads to a net loss! If the utility were in kroner, this classifier would cause the company producing the components a net loss of more than 20 000 kr.\nThe optimal predictor machine, on the other hand, makes fewer successful guesses overall (around 72%), and yet it leads to a net gain! It would earn the company a net gain of around 10 000 kr.\n\n\n\n\n\n\n Exercise\n\n\n\nHow is this possible? Try to understand what’s happening; feel free to research this by modifying the hitsvsgain() function, so that it prints additional outputs."
  },
  {
    "objectID": "0th_connection_ML.html#example-2-find-aladdin-image-recognition",
    "href": "0th_connection_ML.html#example-2-find-aladdin-image-recognition",
    "title": "4  First connection with machine learning",
    "section": "4.3 Example 2: find Aladdin! (image recognition)",
    "text": "4.3 Example 2: find Aladdin! (image recognition)\nA typical use of machine-learning classifiers is for image recognition: for instance, the classifier guesses whether a particular subject is present in the image or not.\nIntuitively one may think that “guessing successfully” should be the best goal here. But exceptions to this may be more common than one thinks. Consider the following scenario:\n\nBianca has a computer folder with 10 000 photos. Some of these include her beloved cat Aladdin, who sadly passed away recently. She would like to select all photos that include Aladdin and save them in a separate “Aladdin” folder. Doing this by hand would take too long, if at all possible; so Bianca wants to employ a machine-learning classifier.\nFor Bianca it’s important that no photo with Aladdin goes missing, so she would be very sad if any photo with him weren’t correctly recognized; on the other hand she doesn’t mind if some photos without him end up in the “Aladdin” folder – she can delete them herself afterwards.\n\nLet’s apply and compare our two classifiers to this image-recognition problem, using again the hitsvsgain() function. We call A the case where Aladdin is present in a photo, and B where he isn’t. To reflect Bianca’s preferences, let’s use these “emotional utilities”:\n\nchooseAisA = +2: Aladdin is correctly recognized\nchooseBisA = -2: Aladdin is not recognized and photo goes missing\nchooseBisB = +1: absence of Aladding is correctly recognized\nchooseAisB = -1: photo without Aladding end up in “Aladding” folder\n\nand let’s say that the photos may have probabilities 0.3, 0.4, 0.6, 0.7 of including Aladding:\n\nhitsvsgain(ntrials=10000, chooseAtrueA=+2, chooseAtrueB=-1, chooseBtrueB=1, chooseBtrueA=-2, probsA=c(0.3, 0.4, 0.6, 0.7))\n\n\nTrials: 10000\nMachine-Learning Classifier: successes 6444 ( 64.4 %) | total gain 4324\nOptimal Predictor Machine:   successes 6000 ( 60 %) | total gain 5548\n\n\nAgain we see that the machine-learning classifier makes more successful guesses than the optimal predictor machine, but the latter yields a higher “emotional utility”.\nYou may sensibly object that this result could depend on the peculiar utilities or probabilities chosen for this example. The next exercise helps answering your objection.\n\n\n\n\n\n\n Exercise\n\n\n\n\nIs there any case in which the optimal predictor machine yields a strictly lower utility than the machine-learning classifier?\n\nTry using different utilities, for instance using ±5 instead of ±2, or whatever other values you please.\nTry using different probabilities as well.\n\nAs in the previous exercise, try to understand what’s happening. Consider this question: how many photos including Aladdin did each classifier miss?\nModify the hitsvsgain() function to output this result.\nDo the comparison using the following utilities: chooseAtrueA=+1, chooseAtrueB=-1, chooseBtrueB=1, chooseBtrueA=-1. What’s the result? what does this tell you about the relationship between the machine-learning classifier and the optimal predictor machine?"
  },
  {
    "objectID": "inference.html#sec-inference-scenarios",
    "href": "inference.html#sec-inference-scenarios",
    "title": "5  What is an inference?",
    "section": "5.1 The wide scope and characteristics of inferences",
    "text": "5.1 The wide scope and characteristics of inferences\nLet’s see a couple more informal examples of inference problems. For some of them an underlying decision-making problem is also alluded to:\n\nLooking at the weather we try to assess if it’ll rain today, to decide whether to take an umbrella.\nConsidering a patient’s symptoms, test results, and medical history, a clinician tries to assess which disease affects a patient, so as to decide on the optimal treatment.\nLooking at the present game position  the X-player, which moves next, wonders whether placing the next X on the mid-right position leads to a win.\nThe computer of a self-driving car needs to assess, from the current set of camera frames, whether a particular patch of colours in the frames is a person, so as to slow down the car and stop.\nGiven that \\(G=6.67 \\cdot 10^{-11}\\,\\mathrm{m^3\\,s^{-2}\\,kg^{-1}}\\), \\(M = 5.97 \\cdot 10^{24}\\,\\mathrm{kg}\\) (mass of the Earth), and \\(r = 6.37 \\cdot 10^{6}\\,\\mathrm{m}\\) (radius of the Earth), a rocket engineer needs to know how much is \\(\\sqrt{2\\,G\\,M/r\\,}\\).\nWe’d like to know whether the rolled die is going to show .\nAn aircraft’s autopilot system needs to assess how much the aircraft’s roll will change if the right wing’s angle of attack is increased by \\(0.1\\,\\mathrm{rad}\\).\nBy looking at the dimensions, shape, texture of a newly dug-out fossil bone, an archaeologist wonders whether it belonged to a Tyrannosaurus rex.\nA voltage test on a newly produced electronic component yields a reading of \\(100\\,\\mathrm{mV}\\). The electronic component turns out to be defective. An engineer wants to assess whether the voltage-test reading could have been \\(100\\,\\mathrm{mV}\\), if the component had not been defective.\nSame as above, but the engineer wants to assess whether the voltage-test reading could have been \\(80\\,\\mathrm{mV}\\), if the component had not been defective.\n\n\n\n\nFrom measurements of the Sun’s energy output and of concentrations of various substances in the Earth’s atmosphere over the past 500 000 years, and of the emission rates of various substances in the years 1900–2022, climatologists and geophysicists try to assess the rate of mean-temperature increase in the years 2023–2100.\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nCh. 10 in A Survival Guide to the Misinformation Age.\n\n\n\n\n\n\n\n\n\n\n Exercises\n\n\n\n\nFor each example above, pinpoint what has to be inferred, and also the agent interested in the inference.\nPoint out which of the examples above explicitly give data or information that should be used for the inference.\nFor the examples that do not give explicit data or information, speculate what information could be implicitly assumed. For those that do give explicit data, speculate which other additional information could be implicitly assumed.\nCan any of the inferences above be done perfectly, that is, without any uncertainty, based the data given explicitly or implicitly?\nFind the examples that explicitly involve a decision. In which of them does the decision affect the results of the inference? In which it does not?\nAre any of the inferences “one-time only” – that is, their object or the data on which they are based have never happened before and will never happen again?\nAre any of the inferences based on data and information that come chronologically after the object of the inference?\nAre any of the inferences about something that is actually already known to the agent that’s making the inference?\nAre any of the inferences about something that actually did not happen?\nDo any of the inferences use “data” or “information” that are actually known (within the scenario itself) to be fictive, that is, not real?\n\n\n\nFrom the examples and from your answers to the exercise we observe some very important characteristics of inferences:\n\nSome inferences can be made exactly, that is, without uncertainty: it is possible to say whether the object of the inference is true or false. Other inferences, instead, involve an uncertainty.\nAll inferences are based on some data and information, which may be explicitly expressed or only implicitly understood.\nAn inference can be about something past, but based on present or future data and information: inferences can show all sorts of temporal relations.\nAn inference can be essentially unrepeatable, because it’s about something unrepeatable or based on unrepeatable data and information.\nThe data and information on which an inference is based can actually be unknown; that is, they can be only momentarily contemplated as real. Such an inference is said to be based on hypothetical reasoning.\nThe object of an inference can actually be something already known to be false or not real: the inference tries to assess it in the case that some data or information had been different. Such an inference is said to be based on counterfactual reasoning."
  },
  {
    "objectID": "inference.html#sec-inference-origin",
    "href": "inference.html#sec-inference-origin",
    "title": "5  What is an inference?",
    "section": "5.2 Where are inferences drawn from?",
    "text": "5.2 Where are inferences drawn from?\nThis question is far from trivial. In fact it has connections with the earth-shaking development and theorems in the foundations of mathematics of the 1900s.\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nMathematics: The Loss of Certainty.\n\n\nThe proper answer to this question will take up the next sections. But a central point can be emphasized now:\n\n\n\n\n\n\n\n\n\n\n\nInferences can only be drawn from other inferences.\n\n\n\nIn order to draw an inference – calculate a probability – we usually go up a chain: we must first draw other inferences, and for drawing those we must draw yet other inferences, and so on.\nAt some point we must stop at inferences that we take for granted without further proof. These typically concern direct experiences and observations. For instance, you see a tree in front of you, so you can take “there’s a tree here” as a true fact. Yet, notice that the situation is not so clear-cut: how do you know that you aren’t hallucinating, for example, and there’s actually no tree there? That is taken for granted. If you analyse the possibility of hallucination, you realize that you are taking other things for granted, and so on. Probably most philosophical research in the history of humanity has been about grappling with this runaway process – which is also a continuous source of sci-fi films. In logic and mathematical logic, this corresponds to the fact that to prove some theorem, we must always start from some axioms. There are “inferences” – tautologies – that can be drawn without requiring others; but they are all trivial, such as “this component failed early, or it didn’t”. They are of little use in a real problem, although they have a deep theoretical importance.\n\n\n\n\n\nSci-fi films like The Matrix ultimately draw on the fact that we must take some inferences for granted without further proof.\n\n\n\n\nIn concrete applications we start from many inferences upon which everyone, luckily, agrees. But sometimes we must also use starting inferences that are more dubious or not agreed upon by anyone. In this case the final inference has a somewhat contingent character, and we accept it (as well as the solution of any underlying decision problem) as the best available for the moment. This is partly the origin of the term “model”."
  },
  {
    "objectID": "inference.html#basic-elements-of-an-inference",
    "href": "inference.html#basic-elements-of-an-inference",
    "title": "5  What is an inference?",
    "section": "5.3 Basic elements of an inference",
    "text": "5.3 Basic elements of an inference\nLet us start to introduce some mathematical notation and more precise terminology for inferences.\nEvery inference has an “object” – what is to be assessed – as well as data, information, hypotheses, or hypothetical scenarios on which it is based. We call proposal the object of the inference, and conditional what the inference is based upon. We separate them with a vertical bar  “ \\(\\pmb{\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}}}\\) ”, which can be pronounced “given” or “conditional on”. Finally, we put parentheses around this and a “\\(\\mathrm{P}\\)” in front:\n\n\nProposal is Johnson’s (1924) terminology; Keynes (1921) uses “conclusion”; modern textbooks do not seem to use any specialized term. Conditional is modern terminology; other terms used: “evidence”, “premise”, “supposal”. The vertical bar, originally a solidus, was introduced by Keynes (1921).\n\\[\n\\mathrm{P}( \\underbracket[1px]{\\color[RGB]{34,136,51}\\boldsymbol{\\cdots}}_{\\textit{proposal}}\n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\underbracket[1px]{\\color[RGB]{68,119,170}\\boldsymbol{\\cdots}}_{\\textit{conditional}}\n) = {\\color[RGB]{238,102,119}\\boldsymbol{\\cdots}\\%}\n\\]\nthis means “the probability that [proposal], supposing [conditional], is . . . %”. Or also: “supposing [conditional], we can infer [proposal] with . . . % probability”.\nWe have seen that to calculate the probability for an inference, we must use the probabilities of other inferences, which in turn are calculated by using the probabilities of other inferences, and so on, until we arrive at probabilities that are taken for granted. A basic inference process could therefore be schematized like this:\n\n\n\n\n\n\n\n\nThe next important task ahead of us is to introduce a flexible and enough general mathematical representation for the objects and the bases of an inference. Then we shall finally study the rules for drawing correct inferences."
  },
  {
    "objectID": "sentences.html#sec-central-comps",
    "href": "sentences.html#sec-central-comps",
    "title": "6  Sentences",
    "section": "6.1 The central components of knowledge representation",
    "text": "6.1 The central components of knowledge representation\nWhen speaking of “data”, what comes to mind to many people is basically numbers or collections of numbers. Maybe numbers, then, could be used to represent all the variety of items exemplified above. This option, however, turns out to be too restrictive.\nI give you this number: “\\(8\\)”, saying that it is “data”. But what is it about? You, as an agent, can hardly call this number a piece of information, because you have no clue what to do with it.\nInstead, if I tell you: “The number of official planets in the solar system is 8”, then we can say that I’ve given you data. You can do different things with this piece of information (for instance: if you had decided to send one probe to each official planet, now you know you have to build eight probes).\nSo, “data” is not just numbers: a number is not “data” unless there’s an additional verbal and non-numeric context accompanying it – even if only implicitly. Sure, we could represent this meta-data information as numbers too; but this move would only shift the problem one level up: we would need an auxiliary verbal context explaining what the meta-data numbers are about.\nData can, moreover, be completely non-numeric. A clinician saying “The patient has fully recovered from the disease” (we imagine to know who’s the patient and what was the disease) is giving us a piece of information that we could further use, for instance, to make prognoses about other, similar patients. The clinician’s statement surely is “data”, but essentially non-numeric data. Sure, in some situations we could represent it as “1”, while “0” would represent “not recovered”; but the opposite convention could also be used, or the numbers “0.3” and “174” could be used. These numbers have intrinsically nothing to do with the clinician’s “recovery” data.\n\n\nBut the examples above actually reveal the answer to our needs! In the examples we expressed the data by means of sentences. Clearly any measurement result, decision outcome, hypothesis, not-real event, assumption, data, and any piece of information can be expressed by a sentence.\nWe shall therefore use sentences, also called propositions or statements,1 to represent and communicate all the kinds of “items” that can be the proposal or conditional of an inference. In some cases we can of course summarize a sentence by a number, as a shorthand, when the full meaning of the sentence is understood.\n\n1 These three terms are not always equivalent in formal logic, but here we’ll use them as synonyms.\nSentences are the central components of knowledge representation in AI agents. For example they appear at the heart of automated control programs and fault-management systems in NASA spacecrafts.\n\n\n (From the SMART paper)\n\n\n\n\n\n\n Study reading\n\n\n\n\n§ 7.1 in Artificial Intelligence.\nTake a quick look at these:\n\nSMART: A propositional logic-based trade analysis and risk assessment tool for a complex mission\naround p. 22 in No More Band-Aids: Integrating FM into the Onboard Execution Architecture \npart IV in Model-based programming of intelligent embedded systems and robotic space explorers"
  },
  {
    "objectID": "sentences.html#identifying-and-working-with-sentences",
    "href": "sentences.html#identifying-and-working-with-sentences",
    "title": "6  Sentences",
    "section": "6.2 Identifying and working with sentences",
    "text": "6.2 Identifying and working with sentences\nBut what is a sentence, more exactly? The everyday meaning of this word will work for us, even though there are more precise definitions – and still a lot of research in logic an artificial intelligence on how to define and use sentences. We shall adopt this useful definition:\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nPropositions\n\n\nA sentence is a verbal message for which an agent can determine, at least in principle, whether it is true or false.\nLet’s make this definition clearer with some remarks:\n\n\n A sentence doesn’t have to contain only words. It can contain pictures, sounds, and other non-verbal items. For example, the following:\n“This:  is an animated picture of Saitama.”\nis a sentence, even if it contains animated graphics, because we can say that it is true. Likewise, the following:\n“This link leads to a song by Pink Floyd.”\nis also a sentence, even if it contains links and audio, because we can say that it is false (that’s a song by Monty Python).\n A meaningful phrase may not be a sentence. For instance, a phrase like “Apples are much tastier than pears” may not be a sentence, because it’s a matter of personal taste whether it’s true or false; moreover, an agent’s opinion about apples and pears might change from time to time.\nThe phrase “Jenny right now finds apples tastier than pears”, on the other hand, could be a sentence; its truth being found by asking Jenny at that moment.\nIn an engineering context, the phrase “This valve will operate for at least two months” is a sentence, even if its truth is unknown at the moment: one has to wait two months, and then its truth will be unambiguously known.\n\n\n\n An expression involving technical terms may not be a sentence (and not meaningful either). For instance, in a data-science context the phrase “This neural-network algorithm has better performance than that random-forest one” is not a sentence unless we have objectively specified what “better” means (higher accuracy? higher true-positive rate? faster?), for example by adopting a particular comparison metric.\nSome expressions involving technical terms may appear to be sentences at first; but a deeper analysis then reveals that they are not. A famous example is the sentence “The two events (at different spatial locations) are simultaneous”. Einstein showed that there’s no physical way to determine whether such an expression is true or false. Its truth turns out to be a matter of convention (also in Newtonian mechanics). The Theory of Relativity was born from this observation.\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nOn the electrodynamics of moving bodies.\n\n\n\n\n\n\n\n\n\n\n\n\n Be particularly careful when reading scientific and engineering papers with a lot of technical terms and phrases: technical jargon often makes it especially difficult to understand whether something true or at least meaningful is being said, or not!\n\n\n\n A sentence can be expressed in different ways by different phrases and in different languages. For instance, “The temperature is 248.15 K”, “Temperaturen ligger på minus 25 grader”, and “25 °C is the value of the temperature” all represent the same sentence.\n\n\nThere are many advantages in working with sentences (rather than just numbers), and in keeping in mind that inference is about sentences:\nFirst, it leads to clarity in engineering problems and makes them more goal-oriented. A data engineer must acquire information and convey information. “Acquiring information” does not simply consist in making measurements or counting something: the engineer must understand what is being measured and why. If data is gathered from third parties, the engineer must ask what exactly the data mean and how they were acquired. In designing and engineering a solution, it is important to understand what information or outcomes the end user exactly wants. The “what”, “why”, “how” are expressed by sentences. A data engineer will often ask “wait, what do you mean by that?”. This question is not just an unofficial parenthesis in the official data-transfer workflow between the engineer and someone else. It is an integral part of that workflow: it means that some information has not been completely transferred yet.\nSecond, it is extremely important in AI and machine-learning design. A (human) engineer may proceed informally when drawing inferences, without worrying about “sentences” unless a need for disambiguation arises. A data engineer who’s designing or programming an algorithm that will do inferences automatically, must instead be unambiguous and cover beforehand all possible cases that the algorithm will face.\n\n\nWe therefore agree that the proposal and the conditional of an inference have to be sentences. This means that the proposal of the inference must be something that can be true or false. Many inferences, especially when they concern numerical measurements, involve several sentences. For example, an inference about the result of rolling a die actually consists of the probabilities for six separate proposals:\n\\[\n\\begin{aligned}\n&\\textsf{\\small`The result of the roll is 1'}\n\\\\\n&\\textsf{\\small`The result of the roll is 2'}\n\\\\\n&\\dotso\n\\\\\n&\\textsf{\\small`The result of the roll is 6'}\n\\end{aligned}\n\\]\nLater on we shall see how to work with more complex inferences of this kind. In real applications it can be useful, on some occasions, to pause and reduce an inference to its basic set of true/false sentences; this analysis may reveal contradictions in our inference problem. A simple way to do this is to reduce the complex inference into a set of yes/no questions.\nThis kind of analysis is also important in information-theoretic situations: the information content provided by an inference, when measured in Shannons, is related to the minimal amount of yes/no questions that the inference answers.\n\n\n\n\n\n\n Exercise\n\n\n\nRewrite each inference scenario of §  5.1 in a formal way, as one or more inferences\n\\[\n\\textit{[proposal]}\\ \\pmb{\\nonscript\\:\\Big\\vert\\nonscript\\:\\mathopen{}}\\ \\textit{[conditional]}\n\\]\nwhere proposal and conditional are well-defined sentences.\nIn ambiguous cases, use your judgement and motivate your choices."
  },
  {
    "objectID": "sentences.html#sec-sentence-notation",
    "href": "sentences.html#sec-sentence-notation",
    "title": "6  Sentences",
    "section": "6.3 Notation and abbreviations",
    "text": "6.3 Notation and abbreviations\nWriting full sentences would take up a lot of space. Even an expression such as “The speed is 10 m/s” is not a sentence, strictly speaking, because it leaves unspecified the speed of what, when it was measured and in which frame of reference, what we mean by “speed”, how the unit “m/s” is defined, and so on.\nTypically we leave the full content of a sentence to be understood from the context, and we denote the sentence by a simple expression such as\n\\[\n\\textsf{\\small The speed is 10\\,m/s}\n\\]\nor even more compactly introducing physical symbols:\n\\[\nv \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}10\\,\\mathrm{m/s}\n\\]\nwhere \\(v\\) is a physical variable denoting the speed; or sometimes even writing simply\n\\[\n10\\,\\mathrm{m/s}\n\\]\nIn some problems it’s useful to introduce symbols to denote sentences. As mentioned before, in these notes we’ll use sans-serif italic letters: \\(\\mathsfit{A},\\mathsfit{B},\\mathsfit{a},\\mathsfit{b},\\dotsc\\), possibly with sub- or super-scripts. For instance, the sentence “The speed is 10 m/s” could be denoted by the symbol \\(\\mathsfit{S}_{10}\\). We abbreviate such a definition like this:\n\\[\n\\mathsfit{S}_{10} \\coloneqq\\textsf{\\small`The speed is 10\\,m/s'}\n\\]\nwhich means “the symbol \\(\\mathsfit{S}_{10}\\) is defined to be the sentence \\(\\textsf{\\small`The speed is 10\\,m/s'}\\)”.\n\n\n\n\n\n\n We must be wary of how much we shorten sentences\n\n\n\nConsider these three:\n\\[\n\\begin{aligned}\n&\\textsf{\\small`The speed is measured to be 10\\,m/s'}\n\\\\\n&\\textsf{\\small`The speed is set to 10\\,m/s'}\n\\\\\n&\\textsf{\\small`The speed is reported, by a third party, to be 10\\,m/s'}\n\\end{aligned}\n\\]\nThe quantity “10 m/s” is the same in all three sentences, but their meanings are very different. They represent different kinds of data. These differences greatly affect any inference about or from these data. For instance, in the third case an engineer may not take the indirectly-reported speed “10 m/s” at face value, unlike the first case. In a scenario where all three sentences can occur, it would be ambiguous to simply write “\\(v = 10\\,\\mathrm{m/s}\\)”: would the equal-sign mean “measured”, “set”, or “indirectly reported”?\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nHow would you denote the three sentences above, to make their differences clear?\n\n\n\n\n\n\n\n\n\n\nGet familiar with abbreviations of sentences\n\n\n\nTo summarize, a sentence like\n\\[\n\\textsf{\\small`The temperature $T$ has value $x$'}\n\\]\ncould be abbreviated in these different ways:\n\nA symbol for the sentence (note the sans-serif font):\n\\[\n\\mathsfit{S}\n\\]\nSome key word appearing in the sentence:\n\\[\n\\textsf{\\small temperature}\n\\]\nAn equality:\n\\[\nT\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x\n\\]\nThe quantity appearing in the sentence:\n\\[\nT\n\\]\nThe value appearing in the sentence:\n\\[\nx\n\\]\n\nGet familiar with these kinds of abbreviations because they’re all very common. Some texts may even jump from one abbreviation to another in the same page or paragraph!"
  },
  {
    "objectID": "sentences.html#sec-connecting-sentences",
    "href": "sentences.html#sec-connecting-sentences",
    "title": "6  Sentences",
    "section": "6.4 Connecting sentences",
    "text": "6.4 Connecting sentences\n\nAtomic sentences\nIn analysing the measurement results, decision outcomes, hypotheses, assumptions, data and information that enter into an inference problem, it is convenient to find a collection of basic sentences or, using a more technical term, atomic sentences out of which all other sentences of interest can be constructed. These atomic sentences often represent elementary pieces of information in the problem.\nConsider for instance the following composite sentence, which could appear in our assembly-line scenario:\n\n“The electronic component is still whole after the shock test and the subsequent heating test. The voltage reported in the final power test is either 90 mV or 110 mV.”\n\nIn this statement we can identify at least four atomic sentences, which we denote by these symbols:\n\\[\\begin{aligned}\n\\mathsfit{s} &\\coloneqq\\textsf{\\small`The component is whole after the shock test'}\n\\\\\n\\mathsfit{h} &\\coloneqq\\textsf{\\small`The component is whole after the heating test'}\n\\\\\n\\mathsfit{v}_{90} &\\coloneqq\\textsf{\\small`The power-test voltage reading is 90\\,mV'}\n\\\\\n\\mathsfit{v}_{110} &\\coloneqq\\textsf{\\small`The power-test voltage reading is 110\\,mV'}\n\\end{aligned}\n\\]\nThe inference may actually require additional atomic sentences. For instance, it might become necessary to consider atomic sentences with other values for the reported voltage, such as\n\\[\\begin{aligned}\n\\mathsfit{v}_{110} &\\coloneqq\\textsf{\\small`The power-test voltage reading is 100\\,mV'}\n\\\\\n\\mathsfit{v}_{80} &\\coloneqq\\textsf{\\small`The power-test voltage reading is 80\\,mV'}\n\\end{aligned}\\]\nand so on.\n\n\nConnectives\nHow do we construct composite sentences, like the one above, out of atomic sentences?\nWe consider three ways: one operation to change a sentence into another related to it, and two operations to combine two or more sentences together. These operations are called connectives; you may have encountered them already in Boolean algebra. Our natural language offers many more operations to combine sentences, but these three connectives turn out to be all we need in virtually all engineering and data-science problems:\n\n\n\n\n\n\n\n\n\n\n\n\nNot (symbol  \\(\\lnot\\) )\n\nexample:\n\n\n\\[\\begin{aligned}\n\\mathsfit{s} &\\coloneqq\\textsf{\\small`The component is whole after the shock test'}\n\\\\[1ex]\n\\lnot \\mathsfit{s} &= \\textsf{\\small`The component is broken after the shock test'}\n\\end{aligned}\\]\n\nAnd (symbols  \\(\\land\\)  also  \\(\\mathbin{\\mkern-0mu,\\mkern-0mu}\\) )\n\nexample:\n\n\n\\[\n\\begin{aligned}\n\\mathsfit{s} &\\coloneqq\\textsf{\\small`The component is whole after the shock test'}\n\\\\\n\\mathsfit{h} &\\coloneqq\\textsf{\\small`The component is whole after the heating test'}\n\\\\[1ex]\n\\mathsfit{s} \\land \\mathsfit{h} &= \\textsf{\\small`The component is whole after the shock and heating tests'}\n\\\\\n\\mathsfit{s} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{h} &= \\textsf{\\small`The component is whole after the shock and heating tests'}\n\\end{aligned}\n\\]\n\nOr (symbol  \\(\\lor\\) )\n\nexample:\n\n\n\\[\\begin{aligned}\n\\mathsfit{v}_{90} &\\coloneqq\\textsf{\\small`The power-test voltage reading is 90\\,mV'}\n\\\\\n\\mathsfit{v}_{110} &\\coloneqq\\textsf{\\small`The power-test voltage reading is 110\\,mV'}\n\\\\[1ex]\n\\mathsfit{v}_{90} \\lor \\mathsfit{v}_{110} &= \\textsf{\\small`The power-test voltage reading is 90\\,mV, or 110\\,mV, or both'}\n\\end{aligned}\\]\n\n\n\nThese connectives can be applied multiple times, to form increasingly composite sentences.\nThe and connective appears very frequently in probability formulae. Using its standard symbol “\\(\\land\\)” would consume a lot of horizontal space. For this reason a comma “\\(\\mathbin{\\mkern-0mu,\\mkern-0mu}\\)” is often used as an alternative symbol. So the expressions \\(\\mathsfit{s} \\land \\mathsfit{h}\\) and \\(\\mathsfit{s} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{h}\\) are completely equivalent.\n\n\n\n\n\n\n Important subtleties of the connectives:\n\n\n\n\nThere is no strict correspondence between the words “not”, “and”, “or” in natural language and the three connectives. For instance the and connective could correspond to the words “but” or “whereas”, or just to a comma “ , ”.\nNot means not some kind of complementary quality, but the denial. For instance,  \\(\\lnot\\textsf{\\small`The chair is black'}\\)  generally does not mean  \\(\\textsf{\\small`The chair is white'}\\) ,   (although in some situations these two sentences could amount to the same thing).\nIt’s always best to declare explicitly what the not of a sentence concretely means. In our example we take\n\\[\n  \\lnot\\textsf{\\small`The component is whole'} \\coloneqq\\textsf{\\small`The component is broken'}\n  \\]\nBut in other examples the negation of “being whole” could comprise several different conditions. A good guideline is to always state the not of a sentence in positive terms.\nOr does not exclude that both the sentences it connects can be true. So in our example  \\(\\mathsfit{v}_{90} \\lor \\mathsfit{v}_{110}\\)  does not exclude, a priori, that the reported voltage could be both 90 mV and 110 mV. (There is a connective for that: “exclusive-or”, but it can be constructed out of the three we already have.)\n\n\n\nFrom the last remark we see that the sentence\n\\[\n\\textsf{\\small`The power-test voltage reading is 90\\,mV or 110\\,mV'}\n\\]\ndoes not correspond to   \\(\\mathsfit{v}_{90} \\lor \\mathsfit{v}_{110}\\) .  It is implicitly understood that a voltage reading cannot yield two different values at the same time. Convince yourself that the correct way to write that sentence is this:\n\\[\n(\\mathsfit{v}_{90} \\lor \\mathsfit{v}_{110})\n\\land\n\\lnot(\\mathsfit{v}_{90} \\land \\mathsfit{v}_{110})\n\\]\nFinally, the full composite sentence of the present example can be written in symbols as follows:\n\n“The electronic component is still whole after the shock test and the subsequent heating test. The voltage reported in the final power test is either 90 mV or 110 mV.”\n\n\\[\n\\textcolor[RGB]{102,204,238}{\\mathsfit{s}} \\land \\textcolor[RGB]{34,136,51}{\\mathsfit{h}} \\land\n(\\textcolor[RGB]{238,102,119}{\\mathsfit{v}_{90}} \\lor \\textcolor[RGB]{170,51,119}{\\mathsfit{v}_{110}})\n\\land\n\\lnot\n(\\textcolor[RGB]{238,102,119}{\\mathsfit{v}_{90}} \\land \\textcolor[RGB]{170,51,119}{\\mathsfit{v}_{110}})\n\\]\n\n\n\n\n\n\n\n\n Study reading\n\n\n\nJust take a quick look at § 7.4.1 in Artificial Intelligence and note the similarities with what we’ve just learned. In these notes we follow a faster approach leading directly to probability logic."
  },
  {
    "objectID": "sentences.html#if-then",
    "href": "sentences.html#if-then",
    "title": "6  Sentences",
    "section": "6.5 “If… then…”",
    "text": "6.5 “If… then…”\nSentences expressing data and information in natural language also appear connected with if… then…. For instance: “If the voltage reading is 200 mV, then the component is defective”. This kind of expression actually indicates that the following inference\n\\[\n\\textsf{\\small`The component is defective'} \\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{} \\textsf{\\small`The voltage reading is 200\\,mV'}\n\\]\nis true.\nThis kind of information is very important because it often is the starting point from which to arrive at the final inferences we’re interested in. We shall discuss it more in detail in the next sections.\n\n\n\n\n\n\n Careful\n\n\n\nThere is a connective in logic, called “material conditional”, which is also often translated as “if… then…”. But it is not the same as the inference relation discussed above. “If… then…” in natural language usually denotes an inference rather than a material conditional.\nResearch is still ongoing on these topics. If you are curious and in for a headache, look over The logic of conditionals.\n\n\n\n\n\nWe are now equipped with all the notions and symbolic notation to deal with our next task: learning the rules for drawing correct inferences.\n@@ TODO: add connections to impossibility of large language models to learn maths (Gödel & Co.)."
  },
  {
    "objectID": "truth_inference.html#sec-trivial-inference",
    "href": "truth_inference.html#sec-trivial-inference",
    "title": "7  Truth inference",
    "section": "7.1 A trivial inference",
    "text": "7.1 A trivial inference\nConsider again the assembly-line scenario of §  1, and suppose that an inspector has the following information about an electric component:\n\nThis electric component had an early failure (within a year of use). If an electric component fails early, then at production it didn’t pass either the heating test or the shock test. This component passed the shock test.\n\nThe inspector wants to assess whether the component did not pass the heating test.\nFrom the data and information given, the conclusion is that the component for sure did not pass the heating test. This conclusion is certain and somewhat trivial. But how did we obtain it? Which rules did we follow to arrive at it from the given data?\nFormal logic, with its deduction systems, is the huge field that formalizes and makes rigorous the rules that a rational person or an artificial intelligence should use in drawing sure inferences like the one above. We’ll now get a glimpse of it, as a trampoline for jumping towards more general and uncertain inferences."
  },
  {
    "objectID": "truth_inference.html#analysis-and-representation-of-the-problem",
    "href": "truth_inference.html#analysis-and-representation-of-the-problem",
    "title": "7  Truth inference",
    "section": "7.2 Analysis and representation of the problem",
    "text": "7.2 Analysis and representation of the problem\nFirst let’s analyse our simple problem and represent it with more compact symbols.\n\nAtomic sentences\nWe can introduce the following atomic sentences and symbols:\n\\[\n\\begin{aligned}\n\\mathsfit{h}&\\coloneqq\\textsf{\\small`The component passed the heating test'}\n\\\\\n\\mathsfit{s}&\\coloneqq\\textsf{\\small`The component passed the shock test'}\n\\\\\n\\mathsfit{f}&\\coloneqq\\textsf{\\small`The component had an early failure'}\n\\\\\n\\mathsfit{I}&\\coloneqq\\textsf{\\small (all other implicit background information)}\n\\end{aligned}\n\\]\n\n\nProposal\nThe proposal is \\(\\lnot\\mathsfit{h}\\), but in the present case we could also have chosen \\(\\mathsfit{h}\\).\n\n\nConditional\nThe bases for the inference are two known facts in the present case: \\(\\mathsfit{s}\\) and \\(\\mathsfit{f}\\). There may also be other obvious facts implicitly assumed in the inference, which we denote by \\(\\mathsfit{I}\\).\n\n\nStarting inferences\nLet us emphasize again that any inference is drawn from other inferences, which are either taken for granted, or drawn in turn from others. In the present case we are told that if an electric component fails early, then at production it didn’t pass either the heating test or the shock test. We write this as\n\\[\n\\lnot\\mathsfit{h}\\lor \\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I}\n\\]\nand we shall take this to be true (that is, to have probability \\(100\\%\\)).\nBut our scenario actually has at least one more, hidden, inference. We said that the component failed early, and that it did pass the shock test. This means, in particular, that it must be possible for the component to pass the shock test, even if it fails early. This means that\n\\[\n\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I}\n\\]\ncannot be false.\n\n\nTarget inference\nThe inference that the inspector wants to draw can be compactly written:\n\n\\[\n\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{s}\\land \\mathsfit{f}\\land \\mathsfit{I}\n\\]"
  },
  {
    "objectID": "truth_inference.html#truth-inference-rules",
    "href": "truth_inference.html#truth-inference-rules",
    "title": "7  Truth inference",
    "section": "7.3 Truth-inference rules",
    "text": "7.3 Truth-inference rules\n\nDeduction systems; a specific choice\nFormal logic gives us a set of rules for correctly drawing sure inferences, when such inferences are possible. These rules can be formulated in different ways, leading to a wide variety of deduction systems (each one with a wide variety of possible notations). The picture on the margin, for instance, shows how a proof of how our inference would look like, using the so-called sequent calculus, which consists of a dozen or so inference rules.\n\n\n\n\n\nThe bottom formula is the target inference. Each line denotes the application of an inference rule, from one or more inferences above the line, to one below the line. The two formulae with no line above are our starting inference, and a tautology.\n\n\n\n\nWe choose to compactly encode all truth-inference rules in the following way.\nFirst, represent true by the number \\(\\mathbf{1}\\), and false by \\(\\mathbf{0}\\).\nSecond, symbolically write that a proposal \\(\\mathsfit{Y}\\) is true, given a conditional \\(\\mathsfit{X}\\), as follows:\n\\[\n\\mathrm{T}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}) = 1\n\\]\nor “\\(=0\\)” if it’s false.\nThe rules of truth-inference are then encoded by the following equations, which must always hold for any atomic or composite sentences \\(\\mathsfit{X},\\mathsfit{Y},\\mathsfit{Z}\\):\n\n\n\n\n\n\n\n\n\n\n\n\n\nRule for “not”:\n\n\\[\\mathrm{T}(\\lnot \\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n+ \\mathrm{T}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= 1 \\tag{7.1}\\]\n\nRule for “and”:\n\n\\[\n\\mathrm{T}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= \\mathrm{T}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}\\land \\mathsfit{Z}) \\cdot\n\\mathrm{T}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= \\mathrm{T}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}\\land \\mathsfit{Z}) \\cdot\n\\mathrm{T}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\tag{7.2}\\]\n\nRule for “or”:\n\n\\[\\mathrm{T}(\\mathsfit{X}\\lor \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= \\mathrm{T}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) +\n\\mathrm{T}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n- \\mathrm{T}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\tag{7.3}\\]\n\nRule for truth:\n\n\\[\\mathrm{T}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}\\land \\mathsfit{Z})\n= 1\n\\tag{7.4}\\]\n\n\n\nHow to use the rules: Each equality can be rewritten in different ways according to the usual rules of algebra. Then the resulting left side can be replaced by the right side, and vice versa. The numerical values of starting inferences can be replaced in the corresponding expressions.\n\n\n\nLet’s see two examples:\n\nfrom one rule for “and” we can obtain the equality\n\\[\n{\\color[RGB]{102,204,238}\\mathrm{T}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}\\land \\mathsfit{Z})}\n=\\color[RGB]{204,187,68}\\frac{\\mathrm{T}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})}{\\mathrm{T}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})}\n\\]\nprovided that \\(\\mathrm{T}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) \\ne 0\\). Then wherever we see the left side, we can replace it with the fraction on the right side, and vice versa.\nfrom the rule for “or” we can obtain the equality\n\\[\n{\\color[RGB]{102,204,238}\n\\mathrm{T}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) - \\mathrm{T}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})}\n=\\color[RGB]{204,187,68}\n\\mathrm{T}(\\mathsfit{X}\\lor \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) - \\mathrm{T}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\]\nAgain wherever we see the left side, we can replace it with the sum on the right side, and vice versa.\n\n\n\nTarget inference in our scenario\nLet’s see how these rules allow us to arrive at our target inference,\n\\[\n\\color[RGB]{238,102,119}\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{s}\\land \\mathsfit{f}\\land \\mathsfit{I})\n\\]\nstarting from the given ones\n\\[\n\\color[RGB]{34,136,51}\\mathrm{T}(\\lnot\\mathsfit{h}\\lor \\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I}) = 1\n\\ ,\n\\qquad\n\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I}) \\ne 0\n\\]\nOne possibility is to work backwards from the target inference:\n\n\\[\n\\begin{aligned}\n&\\color[RGB]{238,102,119}\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{s}\\land \\mathsfit{f}\\land \\mathsfit{I})&&\n\\\\[1ex]\n&\\qquad=\\frac{\\mathrm{T}(\\lnot\\mathsfit{h}\\land \\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n{\\color[RGB]{34,136,51}\\underbracket[1pt]{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}_{\\ne 0}}\n&&\\text{\\small ∧-rule and starting inference}\n\\\\[1ex]\n&\\qquad=\\frac{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\lnot\\mathsfit{h}\\land \\mathsfit{f}\\land \\mathsfit{I})\\cdot\n\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})\n}\n{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n&&\\text{\\small ∧-rule}\n\\\\\n&\\qquad=\\frac{\\bigl[1-\\mathrm{T}(\\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\lnot\\mathsfit{h}\\land \\mathsfit{f}\\land \\mathsfit{I})\\bigr]\\cdot\n\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})\n}\n{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n&&\\text{\\small ¬-rule}\n\\\\\n&\\qquad=\\frac{\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})-\n\\mathrm{T}(\\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\lnot\\mathsfit{h}\\land \\mathsfit{f}\\land \\mathsfit{I})\\cdot\n\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})\n}\n{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n&&\\text{\\small algebra}\n\\\\\n&\\qquad=\\frac{\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})-\n\\mathrm{T}(\\lnot\\mathsfit{s}\\land \\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})\n}\n{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n&&\\text{\\small  ∧-rule}\n\\\\\n&\\qquad=\\frac{{\\color[RGB]{34,136,51}\\mathrm{T}(\\lnot\\mathsfit{h}\\land \\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}-\n\\mathrm{T}(\\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})\n}\n{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n&&\\text{\\small  ∨-rule}\n\\\\\n&\\qquad=\\frac{{\\color[RGB]{34,136,51}1} -\n\\mathrm{T}(\\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})\n}\n{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n&&\\text{\\small starting inference}\n\\\\\n&\\qquad=\\frac{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n&&\\text{\\small ¬-rule}\n\\\\[1ex]\n&\\qquad=\\color[RGB]{238,102,119}1\n&&\\text{\\small algebra}\n\\end{aligned}\n\\]\n\nTherefore \\(\\color[RGB]{238,102,119}\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{s}\\land \\mathsfit{f}\\land \\mathsfit{I}) = 1\\). We find that, indeed, the electronic component must for sure have failed the heating test!\n\n\n\n\n\n\n Exercise\n\n\n\nRetrace the proof above step by step. At each step, how was its particular rule (indicated on the right) used?\n\n\n\n\nThe way in which the rules can be applied to arrive at the target inference is not unique. In fact, in some concrete applications it can require a lot of work to find how to connect target inference with starting ones via the rules. The result, however, will always be the same:\n\n\n\n\n\n\n\n\n\n\n\nThe rules of truth-inference are self-consistent: even if applied in different sequences of steps, they always lead to the same final result.\n\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nProve the target inference \\(\\color[RGB]{238,102,119}\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{s}\\land \\mathsfit{f}\\land \\mathsfit{I}) = 1\\) using the rules of truth-inference, but beginning from the starting inference \\(\\color[RGB]{34,136,51}\\mathrm{T}(\\lnot\\mathsfit{h}\\land \\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})=1\\).\n\n\n\n\n[Optional] Equivalence with truth-tables\nIf you have studied Boolean algebra, you may be familiar with truth-tables; for instance the one for “and” displayed on the side. The truth-inference rules (7.1)–(7.4) contain the truth-tables that you already know as special cases.\n\n\n\n\n\n\\(\\mathsfit{X}\\)\n\\(\\mathsfit{Y}\\)\n\\(\\mathsfit{X}\\land \\mathsfit{Y}\\)\n\n\n\n\n1\n1\n1\n\n\n1\n0\n0\n\n\n0\n1\n0\n\n\n0\n0\n0\n\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nUse the truth-inference rules for “or” and “and” to build the truth-table for “or”. Check if it matches the one you already knew.\n\n\nThe truth-inference rules (7.1)–(7.4) are more complicated than truth-tables, but have two important advantages First, they allow us to work with conditionals, and to move sentences between proposals and conditionals. Second, they provide a smoother transition to the rules for probability-inference."
  },
  {
    "objectID": "truth_inference.html#logical-ai-agents-and-their-limitations",
    "href": "truth_inference.html#logical-ai-agents-and-their-limitations",
    "title": "7  Truth inference",
    "section": "7.4 Logical AI agents and their limitations",
    "text": "7.4 Logical AI agents and their limitations\nThe truth-inference discussed in this section are also the rules that a logical AI agent should follow. For example, the automated control and fault-management programs in NASA spacecrafts, mentioned in §  6.1, are programmed according to these rules.\n\n\n\n\n\n\n Study reading\n\n\n\nLook over Ch. 7 in Artificial Intelligence.\n\n\nMany – if not most – inference problems that human and AI agents must face are, however, of the uncertain kind: it is not possible to surely infer the truth of some outcome, and the truth of some initial data or initial inferences may not be known either. We shall now see how to generalize the truth-inference rules to uncertain situations.\n\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nOur cursory visit of formal logic only showed a microscopic part of this vast field. The study of truth-inference rules continues still today, with many exciting developments and applications. Feel free take a look at\n\nLogic in Computer Science\nMathematical Logic for Computer Science\nNatural Deduction Systems in Logic"
  },
  {
    "objectID": "probability_inference.html#sec-probability-def",
    "href": "probability_inference.html#sec-probability-def",
    "title": "8  Probability inference",
    "section": "8.1 When truth isn’t known: probability",
    "text": "8.1 When truth isn’t known: probability\nWhen we cross a busy city street we look left and right to check whether any cars are approaching. We typically don’t look up to check whether something is falling from the sky. Yet, couldn’t it be false that cars are approaching at that moment? and couldn’t it be true that some object is falling from the sky? Of course both events are possible. Then why do we look left and right, but not up?\nThe main reason is that we believe strongly that cars might be approaching, and believe very weakly that some object might be falling from the sky. In other words, we consider the first occurrence to be very probable, and the second extremely improbable.\nWe shall take the notion of probability as intuitively understood (just as we did with the notion of truth). Terms equivalent to “probability” are degree of belief, plausibility, credibility1.1 credibility literally means “believability” (from Latin credo = to believe).\n\n\n\n\n\n\n Beware of likelihood as a synonym for probability\n\n\n\nIn everyday language, “likelihood” is synonym with “probability”. In technical writings about probability or statistics, however, “likelihood” means something different and is not a synonym of “probability”, as we explain below (§  8.8.1).\n\n\nProbabilities are quantified between \\(0\\) and \\(1\\), or equivalently between \\(0\\%\\) and \\(100\\%\\). Assigning to a sentence a probability 1 is the same as saying that it is true; and a probability 0, that it is false. A probability of \\(0.5\\) represents a belief completely symmetric with respect to truth and falsity.\nLet’s emphasize and agree on some important facts about probabilities:\n\n Probabilities are assigned to sentences. We already discussed this point in §  6.3, but let’s reiterate it. Consider an engineer working on a problem of electric-power distribution in a specific geographical region. At a given moment the engineer may believe with \\(75\\%\\) probability that the measured average power output in the next hour will be 100 MW. The \\(75\\%\\) probability is assigned not to the quantity “100 MW”, but to the sentence\n\\[\n\\textsf{\\small`The measured average power output in the next hour will be 100\\,MW'}\n\\]\nThis difference is extremely important. Consider the alternative sentence\n\\[\n\\textsf{\\small`The average power output in the next hour will be \\emph{set} to 100\\,MW'}\n\\]\nthe numerical quantity is the same, but the meaning is very different. The probability can therefore be very different (if the engineer is the person deciding how to set that output, then the probability is \\(100\\%\\), because has decided and knows what the output is). The probability depends not only on a number, but on what it’s being done with that number – measuring, setting, third-party reporting, and so on. Often we write simply “\\(O \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}10\\,\\mathrm{W}\\)” provided that the full sentence behind this kind of shorthand is understood.\n Probabilities are agent- and knowledge-dependent. A coin is tossed, comes down heads, and is quickly hidden from view. Alice sees that it landed heads-up. Bob instead doesn’t manage to see the outcome and has no clue. Alice considers the sentence \\(\\textsf{\\small`Coin came down heads'}\\) to be true, that is, to have \\(100\\%\\) probability. Bob considers the same sentence to have \\(50\\%\\) probability.\nNote how Alice and Bob assign two different probabilities to the same sentence; yet both assignments are completely rational. If Bob assigned \\(100\\%\\) to \\(\\textsf{\\small`heads'}\\), we would suspect that he had seen the outcome after all; if he assigned \\(0\\%\\) to \\(\\textsf{\\small`heads'}\\), we would consider that unreasonable (he didn’t see the outcome, so why exclude \\(\\textsf{\\small`heads'}\\)?). At the same time we would be baffled if Alice assigned only \\(50\\%\\) to \\(\\textsf{\\small`heads'}\\), because she actually saw that the outcome was heads; maybe we would hypothesize that she feels unsure about what she saw.\nAn omniscient agent would know the truth or falsity of every sentence, and assign only probabilities 0 or 1. Some authors speak of “actual (but unknown) probabilities”. But if there were “actual” probabilities, they would be all 0 or 1, and it would be pointless to speak about probabilities at all – every inference would be a truth-inference.\n Probabilities are not frequencies. Consider the fraction of defective mechanical components to total components produced per year in some factory. This quantity can be physically measured and, once measured, would be agreed upon by every agent. It is a frequency, not a degree of belief or probability.\nIt is important to understand the difference between probability and frequency: mixing them up may lead to sub-optimal decisions. Later we shall say more about the difference and the precise relations between probability and frequency.\nFrequencies can be unknown to some agents. Probabilities cannot be “unknown”: they can only be difficult to calculate. Be careful when you read authors speaking of an “unknown probability”: they actually mean either “unknown frequency”, or a probability that has to be calculated (it’s “unknown” in the same sense that the value of \\(1-0.7 \\cdot 0.2/(1-0.3)\\) is “unknown” to you right now).\n Probabilities are not physical properties. Whether a tossed coin lands heads up or tails up is fully determined by the initial conditions (position, orientation, momentum, rotational momentum) of the toss and the boundary conditions (air velocity and pressure) during the flight. The same is true for all macroscopic engineering phenomena (even quantum phenomena have never been proved to be non-deterministic, and there are deterministic and experimentally consistent mathematical representations of quantum theory). So we cannot measure a probability using some physical apparatus; and the mechanisms underlying any engineering problem boil down to physical laws, not to probabilities.\n\n\n\n\n\n\n\n Study reading\n\n\n\nDynamical Bias in the Coin Toss. \n\n\n\n\nThese points listed above are not just a matter of principle. They have important practical consequences. A data scientist who is not attentive to the source of the data (measured? set? reported, and so maybe less trustworthy?), or who does not carefully assess the context of a probability, or who mixes a probability with a frequency, or who does not take advantage (when possible) of the physics involved in the a problem – such data scientist will design systems with sub-optimal performance2 – or even cause deaths.2 This fact can be mathematically proven."
  },
  {
    "objectID": "probability_inference.html#sec-uncertain-inference",
    "href": "probability_inference.html#sec-uncertain-inference",
    "title": "8  Probability inference",
    "section": "8.2 An unsure inference",
    "text": "8.2 An unsure inference\nConsider now the following variation of the trivial inference problem of §  7.1.\n\nThis electric component had an early failure. If an electric component fails early, then at production it either didn’t pass the heating test or didn’t pass the shock test. The probability that it didn’t pass both tests is 10%. There’s no reason to believe that the component passed the heating test, more than it passed the shock test.\n\nThe inspector wants to assess, also in this case, whether the component did not pass the heating test.\nFrom the data and information given, what would you say is the probability that the component didn’t pass the heating test?\n\n\n\n\n\n\n Exercises\n\n\n\n\nTry to argue why a conclusion cannot be drawn with certainty in this case. One way to argue this is to present two different scenarios that fit the given data but have opposite conclusions.\nTry to reason intuitively and assess the probability that the component didn’t pass the heating test. Should it be larger or smaller than 50%? Why?"
  },
  {
    "objectID": "probability_inference.html#probability-notation",
    "href": "probability_inference.html#probability-notation",
    "title": "8  Probability inference",
    "section": "8.3 Probability notation",
    "text": "8.3 Probability notation\nFor this inference problem we can’t find a true or false final value. The truth-inference rules (7.1)–(7.4) therefore cannot help us here. In fact even the “\\(\\mathrm{T}(\\dotso \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\dotso)\\)” notation is unsuitable, because it only admits the values \\(1\\) (true) and \\(0\\) (false).\nLet us first generalize this notation in a straightforward way:\nFirst, let’s represent the probability or degree of belief of a sentence by a number in the range \\([0,1]\\), that is, between \\(\\mathbf{1}\\) (certainty or true) and \\(\\mathbf{0}\\) (impossibility or false). The value \\(0.5\\) represents that the belief in the truth of the sentence is as strong as that in its falsity.\nSecond, let’s symbolically write that the probability of a proposal \\(\\mathsfit{Y}\\), given a conditional \\(\\mathsfit{X}\\), is some number \\(p\\), as follows:\n\\[\n\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}) = p\n\\]\nNote that this notation includes the notation for truth-values as a special case:\n\\[\n\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}) = 0\\text{ or }1\n\\quad\\Longleftrightarrow\\quad\n\\mathrm{T}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}) = 0\\text{ or }1\n\\]"
  },
  {
    "objectID": "probability_inference.html#sec-fundamental",
    "href": "probability_inference.html#sec-fundamental",
    "title": "8  Probability inference",
    "section": "8.4 Inference rules",
    "text": "8.4 Inference rules\nExtending our truth-inference notation to probability-inference notation has been straightforward. But which rules should we use for drawing inferences when probabilities are involved?\nThe amazing result is that the rules for truth-inference, formulae (7.1)–(7.4), extend also to probability-inference. The only difference is that they now hold for all values in the range \\([0,1]\\), rather than for \\(0\\) and \\(1\\) only.\nThis important result was taken more or less for granted at least since Laplace in the 1700s. But was formally proven for the first time in the 1946 by R. T. Cox. The proof has been refined since then. What kind of proof is it? It shows that if we don’t follow the rules we are doomed to arrive at illogical conclusions; we’ll show some examples later.\n\n\nFinally, here are the fundamental rules of all inference. They are encoded by the following equations, which must always hold for any atomic or composite sentences \\(\\mathsfit{X},\\mathsfit{Y},\\mathsfit{Z}\\):\n\n\n\n\n\n\n\n   THE FUNDAMENTAL LAWS OF INFERENCE   \n\n\n\n\n\n\\(\\boldsymbol{\\lnot}\\) “Not” rule\n\n\\[\\mathrm{P}(\\lnot \\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n+ \\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= 1\\]\n\n\n\\(\\boldsymbol{\\land}\\) “And” rule\n\n\\[\n\\mathrm{P}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= \\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}\\land \\mathsfit{Z}) \\cdot\n\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= \\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}\\land \\mathsfit{Z}) \\cdot\n\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\]\n\n\n\\(\\boldsymbol{\\lor}\\) “Or” rule\n\n\\[\\mathrm{P}(\\mathsfit{X}\\lor \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= \\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) +\n\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n- \\mathrm{P}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\]\n\n\nTruth rule\n\n\\[\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}\\land \\mathsfit{Z})\n= 1\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to use the rules: Each equality can be rewritten in different ways according to the usual rules of algebra. Then the resulting left side can be replaced by the right side, and vice versa. The numerical values of starting inferences can be replaced in the corresponding expressions.\n\n\n\nIt is amazing that ALL inference is nothing else but a repeated application of these four rules – billions of times or more in some cases. All machine-learning algorithms are just applications or approximations of these rules. Methods that you may have heard about in statistics are just specific applications of these rules. Truth inferences are also special applications of these rules. Most of this course is, at bottom, just a study of how to apply these rules in particular kinds of problems.\n\n\n\n\n\n\n Study reading\n\n\n\n\nSkim through Probability, Frequency and Reasonable Expectation. Try to get the ideas behind the reasoning, even if you can’t follow the mathematical details.\nCh. 2 of Bayesian Logical Data Analysis for the Physical Sciences\nCh. 1 of Probability\n§§ 1.0–1.2 of Data Analysis\nSkim through Chs–1–2 of Probability Theory\n\n\n\n \nThe fundamental inference rules are used in the same way as their truth-inference special case: Each equality can be rewritten in different ways according to the usual rules of algebra. Then left and right side of the equality thus obtained can replace each other in a proof."
  },
  {
    "objectID": "probability_inference.html#solution-of-the-uncertain-inference-example",
    "href": "probability_inference.html#solution-of-the-uncertain-inference-example",
    "title": "8  Probability inference",
    "section": "8.5 Solution of the uncertain-inference example",
    "text": "8.5 Solution of the uncertain-inference example\nArmed with the fundamental rules of inference, let’s solve our earlier inference problem. As usual we first analyse it, find what are its proposal and conditional, and which starting inferences are given in the problem.\n\nAtomic sentences\n\\[\n\\begin{aligned}\n\\mathsfit{h}&\\coloneqq\\textsf{\\small`The component passed the heating test'}\n\\\\\n\\mathsfit{s}&\\coloneqq\\textsf{\\small`The component passed the shock test'}\n\\\\\n\\mathsfit{f}&\\coloneqq\\textsf{\\small`The component had an early failure'}\n\\\\\n\\mathsfit{J}&\\coloneqq\\textsf{\\small (all other implicit background information)}\n\\end{aligned}\n\\]\nThe background information in this example is different from the previous, truth-inference one, so we use the different symbol \\(\\mathsfit{J}\\) for it.\n\n\nProposal, conditional, and target inference\nThe proposal is \\(\\lnot\\mathsfit{h}\\), just like in the truth-inference example.\nThe conditional is different now. We know that the component failed early, but we don’t know whether it passed the shock test. Hence the conditional is \\(\\mathsfit{f}\\land \\mathsfit{J}\\).\nThe target inference is therefore\n\\[\n\\color[RGB]{238,102,119}\\mathrm{P}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\n\\]\n\n\nStarting inferences\nWe are told that if an electric component fails early, then at production it didn’t pass either the heating test or the shock test. Let’s write this as\n\\[\n\\color[RGB]{34,136,51}\\mathrm{P}(\\lnot\\mathsfit{h}\\lor \\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J}) = 1\n\\]\nWe are also told that there is a \\(10\\%\\) probability that both tests fail\n\\[\n\\color[RGB]{34,136,51}\\mathrm{P}(\\lnot\\mathsfit{h}\\land \\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J}) = 0.1\n\\]\nFinally the problem says that there’s no reason to believe that the component didn’t pass the heating test, more than it didn’t pass the shock test. This can be written as follows:\n\\[\n\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J}) = \\mathrm{P}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\n\\]\nNote this interesting situation: we are not given the numerical values of these two probabilities, we are only told that they are equal. This is an example of application of the principle of indifference, which we’ll discuss more in detail later.\n\n\nFinal inference\nAlso in this case there is no unique way of applying the rules to reach our target inference, but all ways lead to the same result. Let’s try to proceed backwards:\n\n\\[\n\\begin{aligned}\n&\\color[RGB]{238,102,119}\\mathrm{P}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})&&\n\\\\[1ex]\n&\\qquad= {\\color[RGB]{34,136,51}\\mathrm{P}(\\lnot\\mathsfit{s}\\lor \\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})}\n+ {\\color[RGB]{34,136,51}\\mathrm{P}(\\lnot\\mathsfit{s}\\land \\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})}\n- \\mathrm{P}(\\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\n&&\\text{\\small ∨-rule}\n\\\\[1ex]\n&\\qquad= {\\color[RGB]{34,136,51}1}\n+ {\\color[RGB]{34,136,51}0.1}\n- \\mathrm{P}(\\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\n&&\\text{\\small starting inferences}\n\\\\[1ex]\n&\\qquad= 0.1 + \\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\n&&\\text{\\small ¬-rule}\n\\\\[1ex]\n&\\qquad= 0.1 + \\mathrm{P}(\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\n&&\\text{\\small starting inference}\n\\\\[1ex]\n&\\qquad= 0.1 + 1 -\\color[RGB]{238,102,119}\\mathrm{P}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\n&&\\text{\\small ¬-rule}\n\\end{aligned}\n\\]\n\nThe target probability appears on the left and right side with opposite signs. We can solve for it:\n\\[\n\\begin{aligned}\n2\\,{\\color[RGB]{238,102,119}\\mathrm{P}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})} &= 0.1 + 1\n\\\\[1ex]\n{\\color[RGB]{238,102,119}\\mathrm{P}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})} &= 0.55\n\\end{aligned}\n\\]\nSo the probability that the component didn’t pass the heating test is \\(55\\%\\).\n\n\n\n\n\n\n Exercises\n\n\n\n\nTry to find an intuitive explanation of why the probability is 55%, slightly larger than 50%. If your intuition says this probability is wrong, then\n\nCheck the proof of the inference for mistakes, or try to find a proof with a different path.\nExamine your intuition critically and educate it.\n\nCheck how the target probability \\(\\mathrm{P}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\\) changes if we change the value of the probability \\(\\mathrm{P}(\\lnot\\mathsfit{s}\\land \\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\\) from \\(0.1\\).\n\nWhat result do we obtain if \\(\\mathrm{P}(\\lnot\\mathsfit{s}\\land \\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})=0\\)? Can it be intuitively explained?\nWhat if \\(\\mathrm{P}(\\lnot\\mathsfit{s}\\land \\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})=1\\)? Does the result make sense?"
  },
  {
    "objectID": "probability_inference.html#how-the-inference-rules-are-used",
    "href": "probability_inference.html#how-the-inference-rules-are-used",
    "title": "8  Probability inference",
    "section": "8.6 How the inference rules are used",
    "text": "8.6 How the inference rules are used\nIn the solution above you noticed that the equations of the fundamental rules are not only used to obtain some of the probabilities appearing in them from the remaining probabilities.\nThe rules represent, first of all, constraints of logical consistency3 among probabilities. For instance, if we have probabilities  \\(\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{X}\\land \\mathsfit{Z})=0.1\\),  \\(\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z})=0.7\\),  and \\(\\mathrm{P}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z})=0.2\\),  then there’s an inconsistency somewhere, because these values violate the and-rule:  \\(0.2 \\ne 0.1 \\cdot 0.7\\).  In this case we must find the inconsistency and solve it. However, since probabilities are quantified by real numbers, it’s possible and acceptable to have slight discrepancies within numerical round-off errors.3 The technical term is coherence.\nThe rules also imply more general constraints. For example we must always have\n\\[\n\\begin{gathered}\n\\mathrm{P}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}) \\le \\min\\set[\\big]{\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}),\\  \\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z})}\n\\\\\n\\mathrm{P}(\\mathsfit{X}\\lor \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}) \\ge \\max \\set[\\big]{\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}),\\  \\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z})}\n\\end{gathered}\n\\]\n\n\n\n\n\n\n Exercise\n\n\n\nTry to prove the two constraints above."
  },
  {
    "objectID": "probability_inference.html#consequences-of-not-following-the-rules",
    "href": "probability_inference.html#consequences-of-not-following-the-rules",
    "title": "8  Probability inference",
    "section": "8.7 Consequences of not following the rules",
    "text": "8.7 Consequences of not following the rules\nThe fundamental rules of inference guarantee that the agent’s uncertain reasoning is self-consistent, and that it follows logic when there’s no uncertainty. Breaking the rules means that the resulting inference has some logical or irrational inconsistencies.\nThere are many examples of inconsistencies that appear when the rules are broken. Imagine for instance an agent that gives an 80% probability that it rains (above 1 mm) in the next hour; and it also gives a 90% probability that it rains (above 1 mm) and the average wind is above 3⋅m/s in the next hour. This is clearly unreasonable, because the raining scenario alone would be true with wind above 3 m/s and also below 3⋅m/s – so it should be more probable than the scenario where the wind is above 3 m/s. In fact those two probabilities break the and-rule.\n\n\n\n\n\n\n Exercise\n\n\n\nProve that the two probabilities in the example above break the and-rule. (Hint: you must use the fact that probabilities are numbers between 0 and 1, and that multiplying a number by something between 0 and 1 can only yield a smaller number.)\n\n\n\n\n\n\n\n\n Study reading\n\n\n\n\n§ 12.2.3 in Artificial Intelligence\nAs you continue your studies, go through chapters 4–8 of Rational Choice in an Uncertain World, just to get the main messages and an overview of curious psychological phenomena."
  },
  {
    "objectID": "probability_inference.html#remarks-on-terminology-and-notation",
    "href": "probability_inference.html#remarks-on-terminology-and-notation",
    "title": "8  Probability inference",
    "section": "8.8 Remarks on terminology and notation",
    "text": "8.8 Remarks on terminology and notation\n\nLikelihood\nIn everyday language, “likely” is often a synonym of “probable”; and “likelihood”, of “probability”. But in technical writings about probability, inference, and decision-making, “likelihood” has a very different meaning. Beware of this important difference in definition:\n\\(\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{X})\\) is:\n\nthe probability of \\(\\mathsfit{Y}\\) given \\(\\mathsfit{X}\\) (or conditional on \\(\\mathsfit{X}\\)),\nthe likelihood of \\(\\mathsfit{X}\\) in view of \\(\\mathsfit{Y}\\).\n\nLet’s express this also in a different way:\n\nthe probability of \\(\\mathsfit{Y}\\) given \\(\\mathsfit{X}\\), is \\(\\mathrm{P}({\\color[RGB]{68,119,170}\\mathsfit{Y}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{X})\\).\nthe likelihood of \\(\\mathsfit{Y}\\) in view of \\(\\mathsfit{X}\\), is \\(\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}{\\color[RGB]{68,119,170}\\mathsfit{Y}})\\).\n\n\n\n\n\n\n\n\n\n\n\nA priori there is no relation between the probability and the likelihood of a sentence \\(\\mathsfit{Y}\\): this sentence could have very high probability and very low likelihood, and vice versa.\n\n\nIn these notes we’ll avoid the possibly confusing term “likelihood”. All we need to express can be phrased in terms of probability.\n\n\nOmitting background information\nIn the analyses of the inference examples of §  7.1 and §  8.2 we defined sentences (\\(\\mathsfit{I}\\) and \\(\\mathsfit{J}\\)) expressing all background information, and always included these sentences in the conditionals of the inferences – because those inferences obviously depended on that background information.\nIn many concrete inference problems the background information usually stays there in the conditional from beginning to end, while the other sentences jump around between conditional and proposal as we apply the rules of inference. For this reason the background information is often omitted from the notation, being implicitly understood. For instance, if the background information is denoted \\(\\mathsfit{I}\\), one writes\n\n“\\(\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{X})\\)”  instead of  \\(\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{X}\\land \\mathsfit{I})\\)\n“\\(\\mathrm{P}(\\mathsfit{Y})\\)”  instead of  \\(\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\\)\n\nThis is what’s happening when you see in books probabilities “\\(P(x)\\)” without conditional.\nSuch practice may be convenient, but be wary of it, especially in particular situations:\n\nIn some inference problems we suddenly realize that we must distinguish between cases that depend on hypotheses, say \\(\\mathsfit{H}_1\\) and \\(\\mathsfit{H}_2\\), that were buried in the background information \\(\\mathsfit{I}\\). If the background information \\(\\mathsfit{I}\\) is explicitly reported in the notation, this is no problem: we can rewrite it as\n\\[ \\mathsfit{I}= (\\mathsfit{H}_1 \\lor \\mathsfit{H}_2) \\land \\mathsfit{I}'\\]\nand proceed, for example using the rule of extension of the conversation. If the background information was not explicitly written, this may lead to confusion and mistakes. For instance there may suddenly appear two instances of \\(\\mathrm{P}(\\mathsfit{X})\\) with different values, just because one of them is invisibly conditional on \\(\\mathsfit{I}\\), the other on \\(\\mathsfit{I}'\\).\nIn some inference problems we are considering several different instances of background information – for example because more than one agent is involved. It’s then extremely important to write the background information explicitly, lest we mix up the different agents’s degrees of belief.\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nA once-famous paper published in the quantum-theory literature arrived at completely wrong results simply by omitting background information, mixing up probabilities having different conditionals.\n\n\nThis kind of confusion from poor notation happens more often than one thinks, and even appears in scientific literature.\n\n\n“Random variables”\nSome texts speak of the probability of a “random variable”, or more precisely of the probability “that a random variable takes on a particular value”. As you notice, we have just expressed that idea by means of a sentence. The viewpoint and terminology of random variables is therefore a special case of that based on sentences, which we use here.\nThe dialect of “random variables” does not offer any advantages in concepts, notation, terminology, or calculations, and it has several shortcomings:\n\n\n\n\n\nJames Clerk Maxwell is one of the main founders of statistical mechanics and kinetic theory (and electromagnetism). Yet he never used the word “random” in his technical writings. Maxwell is known for being very clear and meticulous with explanations and terminology.\n\n\n\nAs discussed in §  8.1, in concrete applications it is important to know how a quantity “takes on” a value: for example it could be directly measured, indirectly reported, or purposely set to that specific value. Thinking and working in terms of sentences, rather than of random variables, allows us to account for these important differences.\nWe want a general AI agent to be able to deal with uncertainty and probability even in situations that do not involve mathematical sets.\nVery often the object (proposal) of a probability is not a “variable”: it is actually a constant value that is simply unknown (simple example: we are uncertain about the mass of a particular block of concrete, so we speak of the probability of some mass value; this doesn’t mean that the mass of the block is changing).\nWhat does “random” (or “chance”) mean? Good luck finding an understandable and non-circular definition in texts that use that word; strangely enough, they never define it. In these notes, if the word “random” is ever used, it stands for “unpredictable” or “unsystematic”.\n\nIt’s a question for sociology of science why some people keep on using less flexible points of view or terminologies. Probably they just memorize them as students and then a fossilization process sets in.\n\n\nFinally, some texts speak of the probability of an “event”. For all purposes an “event” is just what’s expressed in a sentence."
  },
  {
    "objectID": "derived_rules.html#sec-truth-stable",
    "href": "derived_rules.html#sec-truth-stable",
    "title": "9  Shortcut rules",
    "section": "9.1 Falsity and truth cannot be altered by additional knowledge",
    "text": "9.1 Falsity and truth cannot be altered by additional knowledge\nSuppose that sentence \\(\\mathsfit{X}\\) is judged to be completely impossible conditional on sentence \\(\\mathsfit{Z}\\):\n\\[\n\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) = 0\n\\]\nIt can then be proved, from the fundamental rules, that \\(\\mathsfit{X}\\) is also completely impossible if we add information to \\(\\mathsfit{Z}\\). That is, for any sentence \\(\\mathsfit{Y}\\) we’ll also have\n\\[\n\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}\\land \\mathsfit{Z}) = 0\n\\]\n\n\n\n\n\n\n Exercise\n\n\n\nTry to prove this. (Hint: try using the and-rule one or more times.)\n\n\n What if we use \\(\\lnot\\mathsfit{X}\\) for \\(\\mathsfit{Y}\\), that is, what if we acquire knowledge that \\(\\mathsfit{X}\\) is actually true? Then it can be proved that all probability calculations break down. The problem is that \\(\\lnot\\mathsfit{X}\\) and \\(\\mathsfit{Z}\\) turn out to be mutually contradictory, so all inferences are starting from contradictory premises. Just as in formal logic, from contradictory premises we can obtain any conclusion whatsoever.\nNote that this problem does not arise, however, if \\(\\mathsfit{X}\\) is only extremely improbable conditional on \\(\\mathsfit{Z}\\) – say with a probability of \\(10^{-100}\\) – rather than flat-out impossible. In practical applications we often approximate extremely small probabilities by \\(0\\), or extremely large ones by \\(1\\). If the probability calculations break down, we must then step back and correct the approximation.\n\n\nBy using the not-rule it is possible to prove that full certainty about a sentence behaves in a similar manner. If sentence \\(\\mathsfit{X}\\) is judged to be completely certain conditional on sentence \\(\\mathsfit{Z}\\):\n\\[\n\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) = 1\n\\]\nthen, from the fundamental rules, \\(\\mathsfit{X}\\) is also completely certain if we add information to \\(\\mathsfit{Z}\\). That is, for any sentence \\(\\mathsfit{Y}\\) we’ll also have\n\\[\n\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}\\land \\mathsfit{Z}) = 1\n\\]"
  },
  {
    "objectID": "derived_rules.html#sec-boolean",
    "href": "derived_rules.html#sec-boolean",
    "title": "9  Shortcut rules",
    "section": "9.2 Boolean algebra",
    "text": "9.2 Boolean algebra\nFirst, it is possible to show that all rules you may know from Boolean algebra are a consequence of the fundamental rules of §  8.4. So we can always make the following convenient replacements anywhere in a probability expression:\n\n\n\n\n\n\nDerived rules: Boolean algebra\n\n\n\n\\[\n\\begin{gathered}\n\\lnot\\lnot \\mathsfit{X}= \\mathsfit{X}\n\\\\[1ex]\n\\mathsfit{X}\\land \\mathsfit{X}= \\mathsfit{X}\n\\\\[1ex]\n\\mathsfit{X}\\lor \\mathsfit{X}= \\mathsfit{X}\n\\\\[1ex]\n\\mathsfit{X}\\land \\mathsfit{Y}= \\mathsfit{Y}\\land \\mathsfit{X}\n\\\\[1ex]\n\\mathsfit{X}\\lor \\mathsfit{Y}= \\mathsfit{Y}\\lor \\mathsfit{X}\n\\\\[1ex]\n\\mathsfit{X}\\land (\\mathsfit{Y}\\lor \\mathsfit{Z}) = (\\mathsfit{X}\\land \\mathsfit{Y}) \\lor (\\mathsfit{X}\\land \\mathsfit{Z})\n\\\\[1ex]\n\\mathsfit{X}\\lor (\\mathsfit{Y}\\land \\mathsfit{Z}) = (\\mathsfit{X}\\lor \\mathsfit{Y}) \\land (\\mathsfit{X}\\lor \\mathsfit{Z})\n\\\\[1ex]\n\\lnot (\\mathsfit{X}\\land \\mathsfit{Y}) = \\lnot \\mathsfit{X}\\lor \\lnot \\mathsfit{Y}\n\\\\[1ex]\n\\lnot (\\mathsfit{X}\\lor \\mathsfit{Y}) = \\lnot \\mathsfit{X}\\land \\lnot \\mathsfit{Y}\n\\end{gathered}\n\\]\n\n\nFor example, a partial proof of the rule \\(\\mathsfit{X}\\land \\mathsfit{X}= \\mathsfit{X}\\), called “and-idempotence”, from the four fundamental rules goes as follows:\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\mathsfit{X}\\land \\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})&&\n\\\\[1ex]\n&\\qquad= \\mathrm{P}(\\mathsfit{X}| \\mathsfit{X}\\land \\mathsfit{Z}) \\cdot \\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})&&\n&&\\text{\\small ∧-rule}\n\\\\[1ex]\n&\\qquad= 1 \\cdot \\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})&&\n&&\\text{\\small truth-rule}\n\\\\[1ex]\n&\\qquad= \\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\end{aligned}\n\\]\nand with a similar procedure it can be shown that \\(\\mathsfit{X}\\land \\mathsfit{X}\\) can be replaced with \\(\\mathsfit{X}\\) no matter where it appears. The above proof shows that the and-idempotence rule is tightly connected with the truth-rule of inference."
  },
  {
    "objectID": "derived_rules.html#sec-idempotent",
    "href": "derived_rules.html#sec-idempotent",
    "title": "9  Shortcut rules",
    "section": "9.3 Subtle importance of apparently trivial rules",
    "text": "9.3 Subtle importance of apparently trivial rules\nSome of the fundamental or derived rules may seem obvious or unimportant, but actually are of extreme importance in data science. For instance the and-idempotence rule effectively asserts that whenever we draw inferences, redundant information or data is automatically counted only once.\nThis amazing feature saves us from a lot of headaches. Imagine that an AI decision agent at the assembly line has been given background information equivalent to saying that if an electronic component passes the heating test (\\(\\mathsfit{h}\\)), then its probability of early failure (\\(\\mathsfit{f}\\)) is only 10%:\n\\[\\mathrm{P}(\\mathsfit{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{h}\\land \\mathsfit{Z}) = 0.1\\]\nA new voltage test has also been devised, and if a component passes this test (\\(\\mathsfit{v}\\)) then its probability of early failure is also only 10%:\n\\[\\mathrm{P}(\\mathsfit{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{v}\\land \\mathsfit{Z}) = 0.1\\]\nHowever, it is discovered that the voltage test works in exactly the same way as the heating test – they’re basically the same test: \\(\\mathsfit{v}=\\mathsfit{h}\\). This means that if an element passes the heating test, then it has automatically passed the voltage test as well:\n\\[\\mathrm{P}(\\mathsfit{v}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{h}\\land \\mathsfit{Z}) = 1\\]\nor equivalently, \\(\\mathsfit{v}\\land \\mathsfit{h}= \\mathsfit{h}\\land \\mathsfit{h}= \\mathsfit{h}\\).\nNow suppose that inadvertently we give our AI decision agent the redundant information that the element has passed the heating test and (therefore) the voltage test. What will the agent say about the probability of early failure, given this duplicate information? Let’s calculate:\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\mathsfit{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{v}\\land \\mathsfit{h}\\land \\mathsfit{Z})&&\n\\\\[1ex]\n&\\qquad= \\frac{\\mathrm{P}(\\mathsfit{f}\\land \\mathsfit{v}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{h}\\land \\mathsfit{Z})}{\n\\mathrm{P}(\\mathsfit{v}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{h}\\land \\mathsfit{Z})\n}\n&&\\text{\\small ∧-rule}\n\\\\[1ex]\n&\\qquad= \\frac{\\mathrm{P}(\\mathsfit{f}\\land \\mathsfit{v}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{h}\\land \\mathsfit{Z})}{1}\n=\\mathrm{P}(\\mathsfit{f}\\land \\mathsfit{v}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{h}\\land \\mathsfit{Z})\n&&\\text{\\small initial probability}\n\\\\[1ex]\n&\\qquad= \\mathrm{P}(\\mathsfit{v}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{h}\\land \\mathsfit{Z}) \\cdot\n\\mathrm{P}(\\mathsfit{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{h}\\land \\mathsfit{Z})\n&&\\text{\\small ∧-rule}\n\\\\[1ex]\n&\\qquad= 1 \\cdot\n\\mathrm{P}(\\mathsfit{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{h}\\land \\mathsfit{Z})\n&&\\text{\\small truth cannot be altered}\n\\\\[1ex]\n&\\qquad= 0.1\n&&\\text{\\small initial probability}\n\\end{aligned}\n\\]\nLuckily the agent correctly detected the redundancy of the sentence \\(\\mathsfit{v}\\) (“the element passed the voltage test”) and automatically discarded it, thank to the truth-rule, or equivalently the and-idempotence rule.\n This feature is of paramount importance in machine learning and data-driven engineering: the “features” that we give as an input to a machine-learning classifier could contain redundancies that we don’t recognize owing to the complexity of the data space. But if the classifier makes inferences according to the four fundamental rules, it will automatically discard any redundant features."
  },
  {
    "objectID": "derived_rules.html#sec-extension-conversation",
    "href": "derived_rules.html#sec-extension-conversation",
    "title": "9  Shortcut rules",
    "section": "9.4 Law of total probability or “extension of the conversation”",
    "text": "9.4 Law of total probability or “extension of the conversation”\nSuppose we have a set of \\(n\\) sentences \\(\\set{\\mathsfit{Y}_1, \\mathsfit{Y}_2, \\dotsc, \\mathsfit{Y}_n}\\) having these two properties:\n\nThey are mutually exclusive, meaning that the “and” of any two of them is false, given some background knowledge \\(\\mathsfit{Z}\\):\n\\[\n  \\mathrm{P}(\\mathsfit{Y}_1\\land\\mathsfit{Y}_2\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}) = 0\\ , \\quad\n  \\mathrm{P}(\\mathsfit{Y}_1\\land\\mathsfit{Y}_3\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}) = 0\\ , \\quad\n\\dotsc \\ , \\quad\n  \\mathrm{P}(\\mathsfit{Y}_{n-1}\\land\\mathsfit{Y}_n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}) = 0\n  \\]\nThey are exhaustive, meaning that the “or” of all of them is true, given the background knowledge \\(\\mathsfit{Z}\\):\n\\[\n  \\mathrm{P}(\\mathsfit{Y}_1\\lor \\mathsfit{Y}_2 \\lor \\dotsb \\lor \\mathsfit{Y}_n \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}) = 1\n  \\]\n\nThen the probability of a sentence \\(\\mathsfit{X}\\), conditional on \\(\\mathsfit{Z}\\), is equal to a combination of probabilities conditional on \\(\\mathsfit{Y}_1,\\mathsfit{Y}_2,\\dotsc\\):\n\n\n\n\n\n\nDerived rule: extension of the conversation\n\n\n\n\n\\[\n\\begin{aligned}\n\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) =\n\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}_1 \\land \\mathsfit{Z})\\cdot \\mathrm{P}(\\mathsfit{Y}_1 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) +\n\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}_2 \\land \\mathsfit{Z})\\cdot \\mathrm{P}(\\mathsfit{Y}_2 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) +{}&\n\\\\[2ex]\n\\dotsb + \\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}_n \\land \\mathsfit{Z})\\cdot \\mathrm{P}(\\mathsfit{Y}_n \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})&\n\\end{aligned}\n\\]\n\n\n\nThis rule is useful when it is difficult to assess the probability of a sentence conditional on the background information, but it is easier to assess the probabilities of that sentence conditional on several auxiliary sentences – often representing hypotheses that exclude one another, and of which we know at least one is true. The name extension of the conversation for this derived rule comes from the fact that we are able to call the additional sentences into play.\nThis situation occurs very often in concrete applications, especially in problems where the probabilities of several competing hypotheses have to be assessed."
  },
  {
    "objectID": "derived_rules.html#sec-bayes-theorem",
    "href": "derived_rules.html#sec-bayes-theorem",
    "title": "9  Shortcut rules",
    "section": "9.5 Bayes’s theorem",
    "text": "9.5 Bayes’s theorem\nThe probably most famous – or infamous – rule derived from the laws of inference is Bayes’s theorem. It allows us to relate the probability where a sentence \\(\\mathsfit{Y}\\) appear in the proposal and another \\(\\mathsfit{X}\\) in the conditional, with the probability where they are exchanged:\n\n\n\n\n\nBayes’s theorem guest-starring in The Big Bang Theory\n\n\n\n\n\n\n\n\nDerived rule: Bayes’s theorem\n\n\n\n\n\\[\n\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}\\land \\mathsfit{Z}) =\n\\frac{\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}\\land \\mathsfit{Z})\\cdot \\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})}{\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})}\n\\]\n\n\n\nObviously this rule can only be used if \\(\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) &gt; 0\\), that is, if the sentence \\(\\mathsfit{X}\\) is not false conditional on \\(\\mathsfit{Z}\\).\n\n\n\n\n\n\n Exercise\n\n\n\nProve Bayes’s theorem from the fundamental rules of inference.\n\n\nBayes’s theorem is extremely useful when we want to assess the probability of a sentence, typically a hypothesis, given some conditional, typically data; and we can easily assess the probability of the data conditional on the hypothesis. Note, however, that the sentences \\(\\mathsfit{Y}\\) and \\(\\mathsfit{X}\\) in the theorem can be about anything whatsoever: \\(\\mathsfit{Y}\\) does not always need to be a “hypothesis”, and \\(\\mathsfit{X}\\) “data”.\n\n\n\n\n\n\n Study reading\n\n\n\n§ 8.8 of Rational Choice in an Uncertain World"
  },
  {
    "objectID": "derived_rules.html#sec-bayes-extension",
    "href": "derived_rules.html#sec-bayes-extension",
    "title": "9  Shortcut rules",
    "section": "9.6 Bayes’s theorem & extension of the conversation",
    "text": "9.6 Bayes’s theorem & extension of the conversation\nBayes’s theorem is often with several sentences \\(\\set{\\mathsfit{Y}_1, \\mathsfit{Y}_2, \\dotsc, \\mathsfit{Y}_n}\\) that are mutually exclusive and exhaustive. Typically these represent competing hypotheses. In this case the probability of the sentence \\(\\mathsfit{X}\\) in the denominator can be expressed using the rule of extension of the conversation:\n\n\n\n\n\n\n\nDerived rule: Bayes’s theorem with extension of the conversation\n\n\n\n\n\\[\n\\mathrm{P}(\\mathsfit{Y}_1 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}\\land \\mathsfit{Z}) =\n\\frac{\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}_1 \\land \\mathsfit{Z})\\cdot \\mathrm{P}(\\mathsfit{Y}_1 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})}{\n\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}_1 \\land \\mathsfit{Z})\\cdot \\mathrm{P}(\\mathsfit{Y}_1 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) +\n\\dotsb + \\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}_n \\land \\mathsfit{Z})\\cdot \\mathrm{P}(\\mathsfit{Y}_n \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n}\n\\]\n\nand similarly for \\(\\mathsfit{Y}_2\\) and so on.\n\n\n\nWe will use this form of Bayes’s theorem very frequently."
  },
  {
    "objectID": "derived_rules.html#the-many-facets-of-bayess-theorem",
    "href": "derived_rules.html#the-many-facets-of-bayess-theorem",
    "title": "9  Shortcut rules",
    "section": "9.7 The many facets of Bayes’s theorem",
    "text": "9.7 The many facets of Bayes’s theorem\nBayes’s theorem is a very general result of the fundamental rules of inference, valid for any sentences \\(\\mathsfit{X},\\mathsfit{Y},\\mathsfit{Z}\\). This generality leads to many uses and interpretations.\nThe theorem is often proclaimed to be the rule according to which we “update our beliefs”. The meaning of this proclamation is the following. Let’s say that at some point \\(\\mathsfit{Z}\\) represents all your knowledge. Your degree of belief about some sentence \\(\\mathsfit{Y}\\) is then (at least in theory) the value of \\(\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\\). At some later point, let’s say that you get to know – maybe thanks to an observation you made – that the sentence \\(\\mathsfit{X}\\) is true. Your whole knowledge at that point is represented no longer by \\(\\mathsfit{Z}\\), but by \\(\\mathsfit{X}\\land \\mathsfit{Z}\\). Your degree of belief about \\(\\mathsfit{Y}\\) is then given by the value of \\(\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}\\land\\mathsfit{Z})\\). Bayes’s theorem allows you to find your degree of belief about \\(\\mathsfit{Y}\\) conditional on your new state of knowledge, from the one conditional on your old state of knowledge.\nThis chronological element, however, comes only from this particular way of using Bayes’s theorem. The theorem can more generally be used to connect any two states of knowledge \\(\\mathsfit{Z}\\) and \\(\\mathsfit{X}\\land\\mathsfit{Z}\\), no matter their temporal order, even if they happen simultaneously, and even if they belong to two different agents.\n\n\n\n\n\n\n Exercise\n\n\n\nUsing Bayes’s theorem and the fundamental laws of inference, prove that if \\(\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})=1\\), that is, if you already know that \\(\\mathsfit{X}\\) is true in your current state of knowledge \\(\\mathsfit{Z}\\), then\n\\[\n\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}\\land \\mathsfit{Z}) = \\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\]\nthat is, your degree of belief about \\(\\mathsfit{Y}\\) doesn’t change.\nIs this result reasonable?\n\n\n\n\n\n\n\n\n Study reading\n\n\n\n\n§§ 4.1–4.3 in Medical Decision Making give one more point of view on Bayes’s theorem.\nCh. 3 of Probability\nA graphical explanation of how Bayes’s theorem works mathematically (using a specific interpretation of the theorem):"
  },
  {
    "objectID": "monty.html#sec-monty-motivation",
    "href": "monty.html#sec-monty-motivation",
    "title": "10  Monty Hall and related inference problems",
    "section": "10.1 Motivation: calculation vs intuition",
    "text": "10.1 Motivation: calculation vs intuition\nThe “Monty Hall problem”, inspired by the TV show Let’s make a deal! hosted by Monty Hall, was proposed in the Parade magazine in 1990 (the numbers of the doors are changed here):\n\nSuppose you are on a game show and given a choice of three doors. Behind one is a car; behind the others are goats. You pick door No. 1, and the host, who knows what is behind them [and wouldn’t open the door with the car], opens No. 2, which has a goat. He then asks if you want to pick No. 3. Should you switch?\n\n\n\n\nThe web is full of insightful intuitive solutions and of informal probability discussions about this inference problem. Our purpose here is different: we want to solve it mechanically, just by applying the fundamental rules of inference (§  8.4) and the shortcut rules (§  9) derived from them. This purpose has two main reasons:\n\nWe want to be able to implement or encode the procedure algorithmically in an AI agent.\nWe generally cannot ground inferences on intuition. Intuition is shaky ground, and hopeless in data-science inference problems involving millions of data with thousands of numbers. To solve such complex problems we need to use a more mechanical procedure mathematically guaranteed to be self-consistent. That’s the probability calculus. Intuition is useful either for arriving at a method which we can eventually prove, by mathematical and logical means, to be correct; or for approximately explaining a method that we already know, again by mathematical and logical means, to be correct.\n\n\n\n\n\n\n\n Misleading intuition in high dimensions\n\n\n\nAs an example of our intuition can be completely astray in problems involving many data dimensions, consider the following fact.\nTake a one-dimensional Gaussian distribution of probability. You have probably heard that the probability that a data point is within a distance of three standard deviations from the peak is approximately \\(99.73\\%\\). If we take a two-dimensional (symmetric) Gaussian distribution, the probability that a data point (two real numbers) is within three standard deviations from the peak is \\(98.89\\%\\) – slightly less that the one-dimensional case. For a three-dimensional Gaussian, the probability that a data point (three real numbers) is within three standard deviations from the peak is \\(97.07\\%\\) – slightly smaller yet.\nNow try to answer this question: for a 100-dimensional Gaussian, what is the probability that a data point is within three standard deviations from the peak? The answer is \\(\\boldsymbol{(1.83 \\cdot 10^{-32})\\%}\\). This probability is so small that you would never observe a data point within three standard deviations from the peak, even if you checked one data point every second for the same duration as the present age of the universe – which is “only” around \\(4\\cdot 10^{17}\\) seconds.\n\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nFor further examples of our intuition leads us astray in high dimensions see\n\nCounterintuitive Properties of High Dimensional Space\nExercise 2.20 (and its solution) in Information Theory, Inference, and Learning Algorithms\n\n\n\nIt is instructive, however, if you also check what your intuition told you about the problem:\n\n\n\n\n\n\n Exercise\n\n\n\nExamine what your intuition tells you the answer should be, without spending too much time thinking, just as if you were on the game show. Examine which kind of heuristics your intuition uses. If you already know the solution to this puzzle, try to remember what your intuition told you the first time you faced it. Keep your observations in mind for later on."
  },
  {
    "objectID": "monty.html#sec-monty-agent",
    "href": "monty.html#sec-monty-agent",
    "title": "10  Monty Hall and related inference problems",
    "section": "10.2 Which agent? whose knowledge?",
    "text": "10.2 Which agent? whose knowledge?\nA sentence can be assigned different probabilities by different agents having different background information, although in some cases different background information can still lead to numerically equal probabilities.\nIn the present case, who’s the agent solving the inference problem? And what background information does it have?\nFrom the problem statement it sounds like you are the agent; but we can imagine that you have programmed an AI agent having your same background information, and ready to make the decision for you.\nWe must agree on which background information \\(\\mathsfit{K}\\) to give to this agent. Let’s define \\(\\mathsfit{K}\\) as the knowledge you have right before picking door 1. We make this choice so that we can add your door pick as additional information."
  },
  {
    "objectID": "monty.html#sec-monty-sentences",
    "href": "monty.html#sec-monty-sentences",
    "title": "10  Monty Hall and related inference problems",
    "section": "10.3 Define the atomic sentences relevant to the problem",
    "text": "10.3 Define the atomic sentences relevant to the problem\nThe following sentences seem sufficient:\n\\[\n\\begin{aligned}\n\\mathsfit{K}&\\coloneqq\\text{\\small[the background knowledge discussed in the previous section]}\n\\\\[1ex]\n\\mathsfit{\\small car1} &\\coloneqq\\textsf{\\small`The car is behind door 1'}\n\\\\\n\\mathsfit{\\small you1} &\\coloneqq\\textsf{\\small`You initially pick door 1'}\n\\\\\n\\mathsfit{\\small host2} &\\coloneqq\\textsf{\\small`The host opens door 2'}\n\\\\\n&\\text{\\small and similarly for the other door numbers}\n\\end{aligned}\n\\]\nWe could have used other symbols for the sentences, for instance “\\(C_1\\)” instead of “\\(\\mathsfit{\\small car1}\\)”; the specific symbol choice doesn’t matter. We could also have stated the sentences slightly differently, for instance “You choose door 1 at the beginning of the game”; what’s important is that we understand and agree on the meaning of the atomic sentences above."
  },
  {
    "objectID": "monty.html#sec-monty-goal",
    "href": "monty.html#sec-monty-goal",
    "title": "10  Monty Hall and related inference problems",
    "section": "10.4 Specify the desired inference",
    "text": "10.4 Specify the desired inference\nWe want the probability of the sentences \\(\\mathsfit{\\small car1}\\), \\(\\mathsfit{\\small car2}\\), \\(\\mathsfit{\\small car3}\\), given the knowledge that you picked door 1 (\\(\\mathsfit{\\small you1}\\)), that the host opened door 2 (\\(\\mathsfit{\\small host2}\\)), and the remaining background knowledge (\\(\\mathsfit{K}\\)). So in symbols we want the values of the following probabilities:\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K})\n\\\\\n&\\mathrm{P}(\\mathsfit{\\small car2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K})\n\\\\\n&\\mathrm{P}(\\mathsfit{\\small car3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K})\n\\end{aligned}\n\\]\nYou may object: “but we already know that there’s no car behind door 2, the one opened by the host; so that probability is 0%”. That’s correct, but how did you arrive at that probability value? Remember our goal: to solve this inference mechanically. That intuitive probability therefore must either appear as an initial probability, or be derived via the inference rules. No intuitive shortcuts."
  },
  {
    "objectID": "monty.html#sec-monty-prior",
    "href": "monty.html#sec-monty-prior",
    "title": "10  Monty Hall and related inference problems",
    "section": "10.5 Specify all initial probabilities",
    "text": "10.5 Specify all initial probabilities\nAs discussed in §  5.2, any inference – logical or uncertain – can only be derived from other inferences, or taken for granted as a starting point (“initial probability”, or “axiom” in logic). The only inferences that don’t need any initial probabilities are tautologies. We must write down explicitly the initial probabilities implicit in the present inference problem:\n\nThe car is for sure behind one of the three doors, and cannot be behind more than one door:\n\\[\n\\begin{gathered}\n\\mathrm{P}(\\mathsfit{\\small car1} \\lor \\mathsfit{\\small car2} \\lor \\mathsfit{\\small car3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) = 1\n\\\\[1ex]\n\\mathrm{P}(\\mathsfit{\\small car1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small car2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) =\n\\mathrm{P}(\\mathsfit{\\small car1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small car3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) =\n\\mathrm{P}(\\mathsfit{\\small car2} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small car3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) = 0\n\\end{gathered}\n\\]\nRemember from the shortcut rule for the stability of truth and falsity (§  9.1) that the \\(1\\) and \\(0\\) probabilities above do not change if we and additional information to \\(\\mathsfit{K}\\).\nThe host cannot open the door you picked or the door with the car. This translates in several initial probabilities; here are some:\n\\[\\begin{gathered}\n\\mathrm{P}(\\mathsfit{\\small host2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car2} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K}) = 0\n\\\\[1ex]\n\\mathrm{P}(\\mathsfit{\\small host1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K}) =\n\\mathrm{P}(\\mathsfit{\\small host3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K}) = 0\n  \\end{gathered}\n  \\]\nThe host must open one door, and cannot open more than one door:\n\\[\n\\begin{gathered}\n\\mathrm{P}(\\mathsfit{\\small host1} \\lor \\mathsfit{\\small host2} \\lor \\mathsfit{\\small host3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) = 1\n\\\\[1ex]\n\\mathrm{P}(\\mathsfit{\\small host1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small host2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) =\n\\mathrm{P}(\\mathsfit{\\small host1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small host3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) =\n\\mathrm{P}(\\mathsfit{\\small host2} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small host3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) = 0\n\\end{gathered}\n\\]\n\n\n\nThe probabilities above are all quite clear from the description of the inference problem. But implicit in that description are some more probabilities that will be needed in our inference. The values of these probabilities can be more open to debate, because the problem, as stated, provides ambiguous information. Later you shall explore possible alternative values for these probabilities.\n\nIt is equally probable that the car is behind any of the three doors, and your initial pick doesn’t change this uncertainty:\n\\[\\begin{aligned}\n\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) &= \\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K}) = 1/3\n\\\\\n\\mathrm{P}(\\mathsfit{\\small car2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) &= \\mathrm{P}(\\mathsfit{\\small car2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K}) = 1/3\n\\\\\n\\mathrm{P}(\\mathsfit{\\small car3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) &= \\mathrm{P}(\\mathsfit{\\small car3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K}) = 1/3\n  \\end{aligned}\n  \\]\nIf the host can choose between two doors (because the car is behind the door you picked initially), we are equally uncertain about the choice:\n\\[\n\\mathrm{P}(\\mathsfit{\\small host2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K}) =\n\\mathrm{P}(\\mathsfit{\\small host3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K}) = 1/2\n  \\]\n\nThis probability could be analysed into further hypotheses. Maybe the host, out of laziness, could more probably open the door that’s closer. But from the problem it isn’t fully clear which one is closer. The host could also more probably open the door that’s further from the one you choose. The host could have a predetermined scheme on which door to open. The hypotheses are endless; we can imagine some making \\(\\mathsfit{\\small host2}\\) more probable, and some making \\(\\mathsfit{\\small host3}\\) more probable, conditional on \\(\\mathsfit{\\small you1} \\land \\mathsfit{\\small car1} \\land \\mathsfit{K}\\). The probability of 50% seems like a good compromise. Later we shall examine the effects of changing this probability.\n\nSome peculiar probabilities\nWe defined the background knowledge \\(\\mathsfit{K}\\) as the one you have right before choosing door 1. We did this so that the sentence \\(\\mathsfit{\\small you1}\\), expressing your door pick, can be added as additional information: \\(\\mathsfit{\\small you1}\\land \\mathsfit{K}\\).\nThen it is legitimate to ask: what is the probability that you pick door 1, given only the background information \\(\\mathsfit{K}\\):\n\\[\\mathrm{P}(\\mathsfit{\\small you1}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\\ ?\\]\nTo answer this question we would need to specify \\(\\mathsfit{K}\\) more in detail. For instance it is possible that you planned to pick door 1 already the day before. In this case we would have \\(\\mathrm{P}(\\mathsfit{\\small you1}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) = 1\\) or very nearly so. Or you could pick door 1 right on the spot, with no clear conscious thought process behind your choice. In this case we would have \\(\\mathrm{P}(\\mathsfit{\\small you1}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) = 1/3\\) or a similar value.\nLuckily in the present problem these probabilities are not needed or, if they are used, their numerical values turn out not to matter (they “cancel out”).\n\n\n\n\n\n\n Silly literature\n\n\n\nSome texts on probability say that if you have decided something and therefore know it in advance for certain, then the probability of that something is undefined “because it is not random”. Obviously this is nonsense. If you already know something, then the probability of that something is well-defined and its value is \\(100%\\) – or something short of this value, if you want to make allowance for the occurrence of unplanned events."
  },
  {
    "objectID": "monty.html#sec-monty-solution",
    "href": "monty.html#sec-monty-solution",
    "title": "10  Monty Hall and related inference problems",
    "section": "10.6 Solution",
    "text": "10.6 Solution\nLet’s try first to calculate \\(\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K})\\), that is, the probability that the car is behind the door you picked.\nSeeing that we have several initial probabilities of the “\\(\\mathrm{P}(\\mathsfit{\\small host} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small car} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K})\\)” form, we can use Bayes’s theorem together with the “extension of the conversation” (§  9.6) to swap the positions of “\\(\\mathsfit{\\small car}\\)” and “\\(\\mathsfit{\\small host}\\)” sentences between supposal and conditional. In the present case the exhaustive and mutually exclusive sentences are \\(\\mathsfit{\\small car1}\\), \\(\\mathsfit{\\small car2}\\), \\(\\mathsfit{\\small car3}\\):\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K})\n\\\\[1ex]\n&\\qquad=\\frac{\n{\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{\\small host2}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K})} \\cdot\n{\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K})}\n}{\n\\enspace\\left[\\,\\begin{gathered}\n{\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{\\small host2}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K})} \\cdot\n{\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K})} +{}\\\\\n{\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{\\small host2}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car2} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K})} \\cdot\n{\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{\\small car2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K})} +{}\\\\\n\\mathrm{P}(\\mathsfit{\\small host2}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K}) \\cdot\n{\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{\\small car3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K})}\n\\end{gathered}\\,\\right]\\enspace\n}\n\\\\[1ex]\n&\\qquad=\\dotso\n\\end{aligned}\n\\]\nAll probabilities in green are initial probabilities discussed in the previous steps. Let’s substitute their values:\n\\[\n\\begin{aligned}\n&\\qquad=\\frac{\n{\\color[RGB]{34,136,51}1/2} \\cdot\n{\\color[RGB]{34,136,51}1/3}\n}{\n\\enspace\\left[\\,\\begin{gathered}\n{\\color[RGB]{34,136,51}1/2} \\cdot\n{\\color[RGB]{34,136,51}1/3} +{}\\\\\n{\\color[RGB]{34,136,51}0} \\cdot\n{\\color[RGB]{34,136,51}1/3} +{}\\\\\n\\mathrm{P}(\\mathsfit{\\small host2}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K}) \\cdot\n{\\color[RGB]{34,136,51}1/3}\n\\end{gathered}\\,\\right]\\enspace\n}\n\\\\[1ex]\n&\\qquad=\\frac{ 1/6\n}{\n1/6 +\n\\mathrm{P}(\\mathsfit{\\small host2}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K}) \\cdot\n1/3\n}\n\\\\[1ex]\n&\\qquad=\\dotso\n\\end{aligned}\n\\]\nAll that’s left is to find \\(\\mathrm{P}(\\mathsfit{\\small host2}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K})\\). It’s intuitively clear that this probability is 100%, because the host is forced to choose door 2 if you picked door 1 and the car is behind door 3. But our purpose is to make a full derivation starting from the initial probabilities only. We can find this probability by applying the or-rule and the and-rule to the probabilities that the host opens at least one door and cannot open more than one:\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\mathsfit{\\small host2}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K})\n\\\\[1ex]\n&\\qquad=\n\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{\\small host2} \\lor \\mathsfit{\\small host1} \\lor \\mathsfit{\\small host3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K})\n\\\\\n&\\qquad\\quad{}-\n\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{\\small host1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K})\n\\\\\n&\\qquad\\quad{}-\n\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{\\small host3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K})\n\\\\\n&\\qquad\\quad{}+\n\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{\\small host1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small host2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K})\n\\\\\n&\\qquad\\quad{}+\n\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{\\small host1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small host3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K})\n\\\\\n&\\qquad\\quad{}+\n\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{\\small host2} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small host3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K})\n\\\\\n&\\qquad\\quad{}-\n\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{\\small host1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small host2} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small host3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K})\n\\\\[1ex]\n&\\qquad= 1 - 0 - 0 + 0 + 0 + 0 - 0 = 1\n\\end{aligned}\n\\]\nas expected.\nFinally, using this probability in our previous calculation we find\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K})\n\\\\[1ex]\n&\\qquad=\\frac{ 1/6\n}{\n1/6 +\n\\mathrm{P}(\\mathsfit{\\small host2}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K}) \\cdot\n1/3\n}\n\\\\[1ex]\n&\\qquad=\\frac{ 1/6\n}{\n1/6 +\n1 \\cdot\n1/3\n}\n= \\frac{1/6}{3/6} = \\boldsymbol{\\frac{1}{3}}\n\\end{aligned}\n\\]\nthat is, there’s a 1/3 probability that the car is behind the door we picked!\n\n\nWhat about door 3, that is, the probability \\(\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K})\\)? Also in this case we can use Bayes’s theorem with the extension of the conversation. The calculation is immediate, because we have already calculated all the relevant pieces:\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\mathsfit{\\small car3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K})\n\\\\[1ex]\n&\\qquad=\\frac{\n\\mathrm{P}(\\mathsfit{\\small host2}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K}) \\cdot\n\\mathrm{P}(\\mathsfit{\\small car3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K})\n}{\n\\enspace\\left[\\,\\begin{gathered}\n\\mathrm{P}(\\mathsfit{\\small host2}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K}) \\cdot\n\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K}) +{}\\\\\n\\mathrm{P}(\\mathsfit{\\small host2}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car2} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K}) \\cdot\n\\mathrm{P}(\\mathsfit{\\small car2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K}) +{}\\\\\n\\mathrm{P}(\\mathsfit{\\small host2}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K}) \\cdot\n\\mathrm{P}(\\mathsfit{\\small car3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K})\n\\end{gathered}\\,\\right]\\enspace\n}\n\\\\\n&\\qquad=\\frac{\n1 \\cdot\n1/3\n}{\n\\enspace\\left[\\,\\begin{gathered}\n1/2 \\cdot\n1/3 +{}\\\\\n0 \\cdot\n1/3 +{}\\\\\n1\n\\cdot\n1/3\n\\end{gathered}\\,\\right]\\enspace\n}\n\\\\[1ex]\n&\\qquad=\\frac{1/3}{1/2} = \\boldsymbol{\\frac{2}{3}}\n\\end{aligned}\n\\]\nthat is, there’s a 2/3 probability that the car is behind door 3. If we’d like to win the car we should then switch doors.\n\n\n\n\n\n\n Exercise\n\n\n\nPerform a similar calculation to find \\(\\mathrm{P}(\\mathsfit{\\small car2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K})\\)"
  },
  {
    "objectID": "monty.html#sec-monty-remarks",
    "href": "monty.html#sec-monty-remarks",
    "title": "10  Monty Hall and related inference problems",
    "section": "10.7 Remarks on the use of Bayes’s theorem",
    "text": "10.7 Remarks on the use of Bayes’s theorem\nThe previous calculations may have been somewhat boring; but, again, the purpose was to see with our own eyes that the final result comes from the application of the four fundamental laws of inference to the initial probabilities – and from nothing else.\nYou notice that at several points our calculations could have taken a different path. For instance, in order to find \\(\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K})\\) we applied Bayes’s theorem to swap the sentences \\(\\mathsfit{\\small car1}\\) and \\(\\mathsfit{\\small host2}\\) in their supposal and conditional positions. Couldn’t we have swapped \\(\\mathsfit{\\small car1}\\) and \\(\\mathsfit{\\small host2}\\land \\mathsfit{\\small you1}\\) instead? That is, couldn’t we have made a calculation starting with\n\\[\n\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K})\n=\\frac{\n\\mathrm{P}(\\mathsfit{\\small host2} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small you1}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K}) \\cdot\n\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n}{\\dotso} \\enspace ?\n\\]\nafter all this is also a legitimate application of Bayes’s theorem.\nThe answer is: yes, we could have; and the final result would have been the same. The self-consistency of the probability calculus guarantees that there are no “wrong steps”, as long as every step is an application of one of the four fundamental rules (or of their shortcuts). The worst that can happen is that we take a longer route – but to exactly the same result. In fact it’s possible that there’s a shorter calculation route to arrive at the probabilities that we found in the previous section. But it doesn’t matter, because it would lead to the same result."
  },
  {
    "objectID": "monty.html#sec-monty-sensitivity",
    "href": "monty.html#sec-monty-sensitivity",
    "title": "10  Monty Hall and related inference problems",
    "section": "10.8 Sensitivity analysis",
    "text": "10.8 Sensitivity analysis\nIn §  10.5 we briefly discussed possible interpretations or variations of the Monty Hall problem, for which the probability that the host chooses among the available doors 2 and 3 (if the car is behind the door you picked) is different from 50%.\nWhen we are curious to know how an initial probability value can affect the final probabilities, we can leave its value as a variable, and check how the final probabilities change as we change this variable. This procedure is often called sensitivity analysis. Try to do a sensitivity analysis for the Monty Hall problem:\n\n\n\n\n\n\n Exercise\n\n\n\nInstead of assuming\n\\[\\mathrm{P}(\\mathsfit{\\small host2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K}) =\n\\mathrm{P}(\\mathsfit{\\small host3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K}) = 1/2\\]\nassign a generic variable value \\(p\\)\n\\[\\mathrm{P}(\\mathsfit{\\small host2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K}) = p\n\\qquad\n\\mathrm{P}(\\mathsfit{\\small host3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K}) = 1-p\\]\nwhere \\(p\\) could be any value between \\(0\\) and \\(1\\).\n\nCalculate \\(\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K})\\) as done in the previous sections, but keeping \\(p\\) as a generic variable. You therefore well find a probability \\(\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K})\\) that depend numerically on \\(p\\); it could be considered as a function of \\(p\\)\nPlot how the value of \\(\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K})\\) depends on \\(p\\), as the latter ranges from \\(0\\) to \\(1\\).\nFor which range of values of \\(p\\) is it convenient to switch door, that is, \\(\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K}) &lt; 1/2\\) ?\nImagine and describe alternative scenarios or background information that would lead to specific values of \\(p\\) different from \\(0.5\\)."
  },
  {
    "objectID": "monty.html#sec-monty-variations",
    "href": "monty.html#sec-monty-variations",
    "title": "10  Monty Hall and related inference problems",
    "section": "10.9 Variations and further exercises",
    "text": "10.9 Variations and further exercises\n\n\n\n\n\n\n Exercise: other variations\n\n\n\n\nIn §  10.2 we decided that the agent in this inference was you, with the knowledge \\(\\mathsfit{K}\\) right before you picked door 1. Try to change the agent: do you arrive at different probabilities?\n\nConsider a person in the audience, right before you picked door 1, as the agent, and re-solve the problem, adjusting all initial probabilities as needed.\nConsider the host as the agent, right before you picked door 1, and re-solve the problem, adjusting all initial probabilities as needed. Note that the host knows for certain where the car is, so you need to provide this additional, secret information. Consider the cases where the car is behind door 1 and behind door 3.\n\n\n\n\n\nSuppose a friend of yours backstage gave you partial information about the location of the car (you cheater!), which makes you believe that the car should be closer to door 1, and assign the probabilities\n\\[\\begin{aligned}\n\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}') &= \\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K}') = 1/3 + q\n\\\\\n\\mathrm{P}(\\mathsfit{\\small car2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}') &= \\mathrm{P}(\\mathsfit{\\small car2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K}') = 1/3\n\\\\\n\\mathrm{P}(\\mathsfit{\\small car3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}') &= \\mathrm{P}(\\mathsfit{\\small car3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you1} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{K}') = 1/3 - q\n  \\end{aligned}\n  \\]\nwith \\(0 \\le q \\le 1/3\\) (this is different background information, so we denote \\(\\mathsfit{K}'\\)). Re-solve the problem keeping the variable \\(q\\), and find if there’s any value for \\(q\\) for which it’s best to keep door 1.\n\n\n\n\n\n\n\n\n\n\n\n Exercise: making decisions\n\n\n\nIn this chapter we only solved the inference problem for the Monty Hall scenario: we calculated the probabilities of various outcomes. No decision has been made yet.\n\nAssign utilities to winning the car or winning the goat from the point of view of an agent who values the car more. The available decisions are, of course, “keep door 1” vs “switch to door 3”. Then solve the decision-making problem according to the procedure of §  3.3. What’s the optimal decision?\nNow assign utilities from the point of view of an agent who values the goat more than the car. Then solve the decision-making problem according to the usual procedure. What’s the optimal decision?\n\n\n\n\n\n\n\n\n\n\n\n Exercise: the Sleeping Beauty problem\n\n\n\nTake a look at the inference problem presented in this video:\n\nand try to solve it, using not intuition but the mechanical procedure and steps as in the Monty Hall solution above.\nNote that the video asks “What do you believe is the probability that the coin came up heads?”. Since probability and degree of belief are the same thing, that is like asking “What do you believe is your belief that the coin came up heads?” which is a redundant or quirky question. Instead, simply answer the question “What is your degree of belief (that is, probability) that the coin came up heads?”."
  },
  {
    "objectID": "1st_connection_ML.html#sec-1stconn-inference",
    "href": "1st_connection_ML.html#sec-1stconn-inference",
    "title": "11  Second connection with machine learning",
    "section": "11.1 “Learning” and “output” from the point of view of inference & decision",
    "text": "11.1 “Learning” and “output” from the point of view of inference & decision\nThe remarks above reveal similarities with what an agent does when drawing an inference: it uses known pieces of information, expressed by sentences \\({\\color[RGB]{34,136,51}\\mathsfit{D}_1}, {\\color[RGB]{34,136,51}\\mathsfit{D}_2}, {\\color[RGB]{34,136,51}\\dots}, {\\color[RGB]{34,136,51}\\mathsfit{D}_N}\\), together with some background or built-in information \\(\\color[RGB]{204,187,68}\\mathsfit{I}\\), in order to calculate the probability of a piece of information of a similar kind, expressed by a sentence \\(\\color[RGB]{238,102,119}\\mathsfit{D}_{N+1}\\):\n\\[\n\\mathrm{P}(\\color[RGB]{238,102,119}\\mathsfit{D}_{N+1}\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\\mathsfit{D}_{N} \\land \\dotsb \\land \\mathsfit{D}_2 \\land \\mathsfit{D}_1 \\color[RGB]{0,0,0}\\land {\\color[RGB]{204,187,68}\\mathsfit{I}})\n\\]\nWe can thus consider a first tentative correspondence:\n\\[\n\\mathrm{P}(\\underbracket[0ex]{\\color[RGB]{238,102,119}\\mathsfit{D}_{N+1}}_{\\mathclap{\\color[RGB]{238,102,119}\\text{outcome?}}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\\underbracket[0ex]{\\mathsfit{D}_N \\land \\dotsb \\land \\mathsfit{D}_2 \\land \\mathsfit{D}_1}_{\\mathclap{\\color[RGB]{34,136,51}\\text{training data?}}}\n\\color[RGB]{0,0,0}\\land \\underbracket[0ex]{\\color[RGB]{204,187,68}\\mathsfit{I}}_{\\mathrlap{\\color[RGB]{204,187,68}\\uparrow\\ \\text{architecture?}}})\n\\]\nThis correspondence seems convincing with regard to architecture and training data: in both cases we’re speaking about the use of pre-existing or built-in information, combined with additional one.\nBut it is less convincing with regarding to the outcome, because an agent gives the probabilities for several possible “outputs”, it doesn’t just yield one. This indicates that there must be also be some decision involved among the possible outcomes.\nWe’ll return to this tentative connection later."
  },
  {
    "objectID": "1st_connection_ML.html#sec-1stconn-outputs",
    "href": "1st_connection_ML.html#sec-1stconn-outputs",
    "title": "11  Second connection with machine learning",
    "section": "11.2 Why different outputs?",
    "text": "11.2 Why different outputs?\nIn the past chapters we have seen, over and over, what was claimed in the introduction to the present lecture notes: that an inference & decision problem has only one optimal solution. Once we specify the utilities and the initial probabilities of the problem, the fundamental rules of inference and the principle of maximal expected utility leads to one unique answer (unless, of course, there are several optimal ones with equal expected utilities).\nDifferent machine-learning algorithms, trained with the same training data, often give different answers or outputs to the same problem. Where do these differences come from? From the point of view of decision theory there are three possibilities, which don’t exclude one another:\n\nThe initial probabilities given to the algorithms are different. Since the training data are the same, this means that the background information built into one machine-learning algorithm is different from those built into another.\nIt is therefore important to understand what are the built-in background information and initial probabilities of different machine-learning algorithms. The built-in assumptions of an algorithm must match those of the real problem as closely as possible, in order to avoid sub-optimal or even disastrously wrong answers and outputs.\nThe utilities built into one machine-learning algorithm are different from those built into another.\nIt is therefore also important to understand what are the built-in utilities of different machine-learning algorithms. The built-in utilities must also match those of the real problem as closely as possible.\nThe calculations made by the algorithms are approximate, and different algorithms make different kinds of approximations. This means that the algorithms don’t arrive at the unique answer determined by decision theory, but to some other answers which may be approximately close to the correct one and to one another – or not.\nIt is therefore important to understand what are the calculation approximations made by different machine-learning algorithms. Some approximations may be too crude for some real problems, and may again lead to sub-optimal or even disastrously wrong answers and outputs."
  },
  {
    "objectID": "1st_connection_ML.html#sec-1stconn-preprocess",
    "href": "1st_connection_ML.html#sec-1stconn-preprocess",
    "title": "11  Second connection with machine learning",
    "section": "11.3 Data pre-processing and the data-processing inequality",
    "text": "11.3 Data pre-processing and the data-processing inequality\n“Data pre-processing” is a collective name given to very different operations on data before they are used in some algorithm to solve a decision or inference problem. Some of these operations are often said to be “essential” or “crucial” for the solution of these problems. This statement in not completely true, and needs qualification.\nWe can divide pre-processing procedures in roughly three categories:\n\nInconsistency checks\n\nProcedures in this category make sure that the data are what they were intended to be. For instance, if data should consist of the power outputs of several engines, but one datapoint is the physical weight of an engine, then that “datapoint” is actually no data at all for the present problem. It’s something included by mistake and should be removed. Similarly if, say, data about distances should be expressed in metres, but some turn out to be expressed in kilometres (this case overlaps with formatting procedures described below). Such procedures are necessary and useful, but they are just consistency checks and do not change the information contained in the proper data.\n\n\n\n\n\n\n\n\n\n\n\n\nIn later chapters we shall say more about some often erroneous procedures, like “tail trimming”, that actually remove proper data, lead to sub-optimal or completely erroneous solutions.\n\n\n\nFormatting\n\nThese procedures make sure that data are in the correct format to be inputted into the algorithm. They may also include rescaling of numerical values for avoiding numerical overflow or underflow errors during computation. Such procedures are often necessary and useful, but they just change the way data are encoded, possibly including the zero and unit of measurement; they do not actually change the information contained in the data.\n\n“Mutilation” or information-alteration\n\nProcedures of this kind alter the content of proper data. For instance, such a procedure may replace, in a data set of temperatures, a datapoint having value 20 °C with one having value 25 °C; this is not just a simple rescaling. These procedures include “de-noising”, “de-biasing”, “de-trending”, “filtering”, “dimensionality reduction” and similar procedures (often having noble-sounding names). We must state, clearly and strongly, that within Decision Theory and Probability Theory, such information-altering pre-processing is not necessary , and is in fact detrimental; this is why we call it “mutilation” here.\n\n\nIt is important that you understand that such data pre-processing is not something that one has to do in data science in general – quite the opposite, in principle you should not do it, because it is a destructive procedure. Such pre-processing is done in order to correct deficiencies of the algorithms currently in use, as discussed below.\nIf we build an “optimal predictor machine” that fully operates according to the four rules of inference (§  8.4) and of maximization of expected utility, then the data fed into this machine should not be pre-processed with any information-altering procedures. The reason is that the four fundamental rules automatically take care of factors such as noise, bias, systematic errors, redundancy in the optimal way. We briefly discussed in §  9.3 and saw a simple example of how redundancy is accounted for by the four rules.\nIf we have information about noise or other factors affecting the data, then we should include this information in the background information provided to the “optimal predictor machine”, rather than altering the data given to it. The reason, in intuitive terms, is that the machine does the adjustments while fully exploring the data themselves, so it can more deeply “see” how to make optimal adjustments given the “inner structure” of the data. In the pre-processing phase – as the prefix “pre-” indicates – we don’t have the full picture about the data, so any adjustment risks eliminating actually useful information and is always sub-optimal.\nMore formally, this is the content of the data-processing inequality from information theory:\n\n\n\n\n\n\nData-processing inequality\n\n\n\n\n“No clever manipulation of the data can improve the inferences that can be made from the data”\n(Elements of Information Theory § 2.8)\nor, from the complementary point of view:\n“Data processing can only destroy information”\n(Information Theory, Inference, and Learning Algorithms exercise 8.9)\n\n\n\n\n\n\n\n\n\n Study reading\n\n\n\n\nSkim through § 2.8 of Elements of Information Theory\nTake a look Exercise 8.9 and its solution in Information Theory, Inference, and Learning Algorithms\n\n\n\n\n\nThere are two main, partially connected reasons why one performs “mutilation” pre-processing of data:\n\nThe algorithm used is non-optimal: it’s only using approximations of the four fundamental rules, and therefore cannot remove noise, bias, redundancies, and similar in an optimal way – or at all. In this case, pre-processing is an approximate way of correcting this deficiency of the non-optimal algorithm.\nFull optimal processing is computationally too expensive. In this case we try to simplify the optimal calculation by doing in advance and in a cruder, faster way some of the “cleaning” that the full calculation would otherwise spend time doing in an optimal way."
  },
  {
    "objectID": "quantities_types.html#motivation-for-the-data-i-part",
    "href": "quantities_types.html#motivation-for-the-data-i-part",
    "title": "12  Quantities and data types",
    "section": "Motivation for the “Data I” part",
    "text": "Motivation for the “Data I” part\nIn the “Inference I” part we surveyed the four fundamental rules of inference, which determine how the degrees of belief of an agent should propagate and be mutually consistent, and we explored some of their consequences and applications. The rules can be used with any sentences whatsoever, so their application can be developed in detail in a wide variety of directions, with applications ranging from robotics to psychology. Each of these possible developments would require a full university course by itself.\nWe shall now restrict our attention to applications typical of engineering, data science, and machine learning, such as classification, forecast, prognosis, and hypothesis testing, in situations that involve quantifiable and measurable phenomena. For this purpose we focus on sentences of particular kinds, which can express such quantification and measurement. In a sense, we develop a specialized “language” for this kind of situations.\nStill, since we’re dealing with sentences, the probability calculus and inference rules apply without changes of any kind."
  },
  {
    "objectID": "quantities_types.html#quantities",
    "href": "quantities_types.html#quantities",
    "title": "12  Quantities and data types",
    "section": "12.1 Quantities",
    "text": "12.1 Quantities\n\nQuantities, values, domains\nMost decisions and inferences in engineering and data science involve things or properties of things that we can measure. We represent them by mathematical objects – most often, collections of numbers – with particular mathematical properties and operations.\nThe mathematical properties reflect the kind of activities that we can do with these things. For instance, colours are represented by particular tuples of numbers, and these tuples can be multiplied by some numeric weights and added, to obtain another tuple. This mathematical operation represents the fact that colours can be obtained by mixing other colours in different proportions. Physics and engineering are founded on this approach.\n\n\n25% #FF0000 + 75% #0000FF = #4000C0\nIt’s difficult to find a general term to denote any instance of such “things” and their mathematical representation. Yet it’s convenient if we find one, so we can discuss the general theory without getting bogged down in individual cases. To this purpose we’ll borrow the term quantity from physics and engineering.\n\n\n\n\n\n\n\n\n\n\nThe definition of “quantity” we are using here is similar to the one having the maximum specific level as defined in § 1.1 of the International vocabulary of metrology by the Joint Committee for Guides in Metrology.\nUsing the word “quantity” this way is just a convention between us. Other texts and scientists may use other words – for example “variable”, “event”, “state”. When you read a text or listen to a scientist, try to grasp the general idea behind the words.\nAs a general term we prefer the word “quantity” to a word like “variable”, because the latter word may give the idea of something changing in time – which may very well not be the case (think of the mass of a block of concrete). Same goes with a word like “state” for the opposite reason.\n\n\n\n\nWe distinguish between a quantity and its value. For instance, a quantity could be “The temperature at the point with coordinates  60.3775029, 5.3869233, 643,  at time 1895-10-04T10:03:14Z”; and its value could be \\(24\\,\\mathrm{°C}\\).\nThis distinction is necessary in inference and decision problems, because we may not know the value of a particular quantity. We then consider every possible value that quantity could have, and we can assign a probability to each. The set of possible values is called the domain of the quantity. In our temperature example, the domain is the set of all possible temperatures from \\(0\\,\\mathrm{K}\\) and above.\nAnother example:\n\nquantity: the image taken by a particular camera at a particular time, represented by a specific collection of numbers (say 128 × 128 × 3$ integers between \\(0\\) and \\(255\\))\nvalues: one possible value is this:  (corresponding to a grid of 128 × 128 × 3 specific numbers), another possible value is this: , and there are many other possible values\ndomain: the collection of \\(256^{3\\times128\\times128} \\approx 10^{118 370}\\) possible images (corresponding to the collection of possible grids of numeric values)\n\nOther examples of quantities and domains:\n\nThe distance between two objects in the Solar System at a specific time. The domain could be, say, all values from \\(0\\,\\mathrm{m}\\) to \\(6\\cdot10^{12}\\,\\mathrm{m}\\) (Pluto’s average orbital distance).\nThe number of total views of some online video (at a specific time), with a domain, say, from 0 to 20 billions.\nThe force on an object (at a specific time and place). The domain could be, say, 3D vectors with components in \\([-100\\,\\mathrm{N},\\,+100\\,\\mathrm{N}]\\).\nThe degree of satisfaction in a customer survey, with five possible values Not at all satisfied, Slightly satisfied, Moderately satisfied, Very satisfied, Extremely satisfied.\nThe graph representing a particular social network. Individuals are represented by nodes, and different kinds of relationships by directed or undirected links between nodes, possibly with numbers indicating their strength. The domain consists of all possible graphs with, say, \\(0\\) to \\(10000\\) nodes and all possible combinations of links and weights between the nodes.\nThe relationship between the input voltage and output current of an electric component. The domain could be all possible continuous curves from \\([0\\,\\mathrm{V}, 10\\,\\mathrm{V}]\\) to \\([0\\,\\mathrm{A}, 1\\,\\mathrm{A}]\\).\nA 1-minute audio track recorded by a device with a sampling frequency of 48 kHz (that is, 48 000 audio samples per second). The domain could be all possible sequences of 2 880 000 numbers in \\([0,1]\\).\nThe subject of an image, with domain of three possible values cat, dog, something else.\nThe roll, pitch, yaw of a rocket (at a specific time and place), with domain \\((-180°,+180°]\\times(-90°,+90°]\\times(-180°,+180°]\\).\n\n\n\nThe vague term “data” typically means the values of a collection of quantities.\nIn these notes we agree that a quantity has one, and only one, value.\n\n\n\n\n\n\n Quantity vs variate or variable\n\n\n\nWe can consider something that changes with time, or with space, or from individual to individual, or from unit to unit. This “something” is then not a quantity, according to our present terminology, but a collection of quantities: one for each time, or space, or individual. Later we shall call this collection a variate, especially when it refers to individuals or unit; or a variable.\nFor instance, your height at this exact moment is a quantity, but your height throughout your life is a variable, and the height (at this moment) across all Norwegian people is a variate.\nThese are just terminological conventions adopted in these notes. As mentioned before, different scientists often adopt different terms. What matters is not the terms, but that you have a clear understanding of the difference between the two notions that we here call “quantity” and “variate”.\n\n\n\n\nNotation\nWe shall denote quantities by italic letters, such as \\(X\\), or \\(U\\), or \\(A\\). The sentences that appear in decision-making and inferences are therefore often of the kind “the quantity \\(X\\) was observed to have value \\(x\\)”, where “\\(x\\)” stands for a specific value, for instance . This kind of sentences are often abbreviated in the form “\\(X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x\\)”.\n\n\n\n\n\n\n\n\n\n\n Keep in mind our discussion from §  6.3: we must make clear what that “\\(\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\)” means; it could mean “observed”, “set”, “reported”, and so on.\n Note the subtle difference between \\(X\\), in italics, and \\(\\mathsfit{X}\\), in so-called sans-serif typographic family. The first denotes a quantity, the second denotes a sentence. Usually we don’t have to worry too much about these symbol differences, because the meaning of the symbol is clear from the context. But just in case, you know the convention."
  },
  {
    "objectID": "quantities_types.html#sec-basic-types",
    "href": "quantities_types.html#sec-basic-types",
    "title": "12  Quantities and data types",
    "section": "12.2 Basic types of quantities",
    "text": "12.2 Basic types of quantities\nAs the examples above show, quantities and data come in all sorts and with different degrees of complexity. There is no clear-cut divide between different sorts of quantities. The same quantity can moreover be viewed and represented in many different ways, depending on the specific context, problem, purpose, and background information.\nIt is possible, however, to roughly differentiate between a handful of basic types of quantities, from which more complex types are built. Here is one kind of differentiation that is useful for inference problems about quantities:\n\nNominal\nA nominal or categorical quantity has a domain with a discrete and usually finite number of values. The values are not related by any mathematical property, and do not have any specific order.\nThis means that it does not make sense to say, for instance, that some value is “twice” or “1.5 times” another, or “larger” or “later” than another one. Nor does it make sense to “add” two quantities. In particular, there is no notion of cumulative probability, quantile, median, average, or standard deviation for a nominal quantity.\nExamples: the possible breeds of a dog, or the characters of a film.\nIt is of course possible to represent the values of a nominal quantity with numbers; say 1 for Dachshund, 2 for Labrador, 3 for Dalmatian, and so on. But that doesn’t mean that\nDalmatian\\({}-{}\\)Labrador\\({}={}\\)Labrador\\({}-{}\\)Dachshund\n(just because \\(3-2=2-1\\)) or similar nonsense.\n\n\nOrdinal\nAn ordinal quantity has a domain with a discrete and usually finite number of values. The values are not related by any mathematical property, but they do have a specific order.\nThis means that it does not make sense to say that some value is “twice” or “1.5 times” another, and we cannot add or subtract two values. But it does make sense to say, for any two values, which one has higher rank, for example “stronger”, or “later”, “larger”, and similar. Owing to the ordering property, it does make sense to speak of cumulative probability, quantile, and median of an ordinal quantity; but there is no notion of average or standard deviation for an ordinal quantity.\nExample: a pain-intensity scale. A patient can say whether some pain is more severe than another, but it isn’t clear what a pain “twice as severe” as another would mean (although there’s a lot of research on more precise quantification of pain). Another example: the “strength of friendship” in a social network. We can say that we have a “stronger friendship” with a person than with another; but it doesn’t make sense to say that we are “four times stronger friends” with a person than with another.\nIt is possible to represent the values of an ordinal quantity with numbers which reflect the order of the values. But it’s important to keep in mind that differences or averages of such numbers do not make sense. For this reason the use of numbers can be misleading at times; a less misleading possibility is to represent ordered values by alphabet letters, for example.\n\n\nBinary\nA binary or dichotomous quantity has only two possible values. It can be seen as a special case of a nominal or ordinal quantity, but the fact of having only two values lends it some special properties in inference problems. This is why we list it separately.\nObviously it doesn’t make much sense to speak of the difference or average of the two values; and their ranking is trivial even if it makes sense.\nThere’s an abundance of examples of binary quantities: yes/no answers, presence/absence of something, and so on.\n\n\nInterval\nAn interval quantity has a domain that can be discrete or continuous, finite or infinite. The values do admit some mathematical operations, at least convex combination and subtraction. They also admit an ordering.\nThis means that we can say, at the very least, whether the interval or “distance” between a pair of values is the same, or larger, or smaller than the interval between another pair. For this reason we can also say whether a value is larger than another. We can also take weighted sums of values, called convex combinations (keep in mind that simple addition of values may be meaningless for some quantities).\nOwing to these mathematical properties, it does make sense to speak of the cumulative probability, quantile, median, and also average and standard deviation for an interval quantity.\nThe number of electronic components produced in a year by an assembly line is an example of a discrete interval quantity. The power output of a nuclear plant at a given time is an example of a continuous interval quantity.\nIt is also possible to speak of ratio quantities, which are a special case of interval quantities, but we won’t have use of this distinction in the present notes.\n\n\nHow to decide the basic type of a quantity?\nTo attribute a basic type to a quantity we must ultimately check how that quantity is defined, obtained, and used. In some cases the values of the quantity may give some clue; for example, if we see values “\\(2.74\\)”, “\\(8.23\\)”, “\\(3.01\\)”, then the quantity is probably of the interval kind. But if we see values “\\(1\\)”, “\\(2\\)”, “\\(3\\)”, it’s unclear whether the quantity is interval, ordinal, nominal, or maybe of yet some other kind.\nThe type of a quantity also depends on its use in the specific problem. A quantity of a more complex type can be treated as a simpler type if needed. For instance, the response time of some device is in principle an interval quantity (measured, say, in seconds, as precisely as we want); but in a specific situation we could simply label its values as slow, medium, fast, thus turning it into an ordinal quantity.\n@@ TODO: add examples for image spaces\n\n\n\n\n\n\n Exercises\n\n\n\n\nFor each example at the beginning of the present section, assess whether that quantity can be considered as being of a basic type, and which type.\nFor each basic type discussed above, find two more concrete examples of that type of quantity\n\n\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nOn the theory of scales of measurement"
  },
  {
    "objectID": "quantities_types.html#other-attributes-of-basic-types",
    "href": "quantities_types.html#other-attributes-of-basic-types",
    "title": "12  Quantities and data types",
    "section": "12.3 Other attributes of basic types",
    "text": "12.3 Other attributes of basic types\nIt is useful to consider other basic aspects of quantities that are somewhat transversal to “type”. These aspects are also important when drawing inferences.\n\nDiscrete vs continuous\nNominal and ordinal quantities have discrete domains. The domain of an interval quantity can be discrete or continuous. In practice all domains are discrete, since we cannot observe, measure, report, or store values with infinite precision. In a modern computer, for example, a real number can “only” take on \\(2^{64} \\approx 20 000 000 000 000 000 000\\) possible values. In many situations the available precision is so high that we can consider the quantity as continuous for all practical purposes and use the mathematics of continuous sets – derivation, integration, and so on – to our advantage.\n\n\nBounded vs unbounded\nOrdinal and interval quantities may have domains with no minimum value, or no maximum value, or neither. Typical terms for these situations are lower- or upper-bounded, or left- or right-bounded, and unbounded; or similar terms.\nWhether to treat a quantity domain as bounded or unbounded depends on the quantity, the specific problem, and the computational resources. For example, the number of times a link on a webpage has been clicked can in principle be (right-)unbounded. Another example is the distance between two objects: we can consider it unbounded, but in concrete problems might be bounded, say, by the size of a laboratory, or by Earth’s circumference, or the Solar System’s extension, and so on.\n\n\n\n\n\n\n Exercises\n\n\n\n\nIf you had to set a maximum number of times a web link can be clicked, what number would you choose? Try to find a reasonable number, considering factors such as how fast a person can repeatedly click on a link, how long can a website (or the Earth?) last, and how many people can live in such an extent of time.\nWhat about the age of a person? What bound would you set, if you had to treat it as a bounded quantity?\n\n\n\n\n\nFinite vs infinite\nThe domain of a discrete quantity can consist of a finite or – at least in theory – an infinite number of possible values (the domain of a continuous quantity always has an infinite number of values). A domain can be infinite and yet bounded: consider the numbers in the range \\([0,1]\\).\nWhether to treat a domain as finite or infinite depends on the quantity, the specific problem, and the computational resources. For example, the intensity of a base colour in a pixel of a particular image might really take on 256 discrete steps between \\(0\\) and \\(1\\): \\(0, 0.0039215686, 0.0078431373, \\dotsc, 1\\). But in some situations we can treat this domain as practically infinite, with any possible value between \\(0\\) and \\(1\\).\n\n\nRounded\nA continuous interval quantity may be rounded, owing to the way it’s measured. In this case the quantity could be considered discrete rather than continuous. Rounding can impact the way we do inferences about such a quantity.\n\n\n The Iris dataset from its original paper\nThe famous Iris dataset, for instance, consists of several lengths – continuous interval quantities – of parts of flowers. All values are rounded to the millimetre, even if in reality the lengths could have intermediate values, of course. The age of a person is another frequent example of an in-principle continuous quantity which is rounded, say to the year or the month.\nIn some situations it’s important to be aware of rounding, because it can lead to quantities with different unrounded values to have identical rounded ones.\n\n\nCensored\nThe measurement procedure of a quantity may have an artificial lower or upper bound. A clinical thermometer, for instance, could have a maximum reading of \\(45\\,\\mathrm{°C}\\). If we measure with it the temperature of a \\(50\\,\\mathrm{°C}\\)-hot body, we’ll read “\\(45\\,\\mathrm{°C}\\)”, not the real temperature.\nA quantity with this characteristic is called censored, more specifically left-censored or right-censored when there’s only one artificial bound. The bound is called the censoring value.\nA censoring value denotes an actual value that could also be greater or less. This is important when we draw inferences about this kind of quantities.\n\n\n\n\n\n\n\n\n\n Exercises\n\n\n\nExplore datasets in a database such as the UC Irvine Machine Learning Repository:\n\nRead the description of the quantities listed in the dataset (sometimes in a readme file included with the dataset download)\nAnalyse the values of some of the quantities in the dataset: check if they can be considered continuous, discrete, or rounded; bounded or unbounded; uncensored or censored; and so on."
  },
  {
    "objectID": "quantities_types.html#sec-true-quantities",
    "href": "quantities_types.html#sec-true-quantities",
    "title": "12  Quantities and data types",
    "section": "12.4 “True” vs “measured” values",
    "text": "12.4 “True” vs “measured” values\nA difference is often drawn, especially in physics and engineering, between the “true” value of a quantity and the value “measured” or “observed” with a particular measuring instrument. What’s the difference? and how is the “true” value defined?\nThere are deep philosophical questions and choices underlying this distinction, and it would take a whole university course to do them justice.\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nThe Logic of Modern Physics\n\n\nIntuitively we define the “true” value as the value that would be measured with an instrument that is perfectly calibrated and as precise as theoretically possible. If we make a distinction between such value and the currently measured value then we’re implying that the current measurement is made with a less precise instrument, and the true and measured values could be different.\nIn some circumstances this distinction is unimportant: an agent can use the “measured” value without worries, and consider it as the “true” one. Typically this is the case when the possible discrepancy between measured and true value is enough small to have no consequences. In other circumstances the discrepancy is important: slightly different values lead to quite different consequences. Then it is necessary for the agent to try to infer – using the probability calculus – the true value, using the measured one as “data” or “evidence”. Said otherwise, the agent doesn’t use the measured value directly, but only as an intermediate step to guess the true value. The latter, in turn, can be used for further inferences.\nFrom the point of view of inference and decision-making, this distinction doesn’t lead to anything methodologically new: it just means that an agent has to do a chain of inferences instead of just one, using the four rules of inference as usual. This situation often requires the definition of two distinct quantities, the “true” and the “observed”, which can have slightly different domains. For instance we could have a voltage \\(V_\\text{obs}\\) measured with rounding to \\(1\\,\\mathrm{V}\\) and therefore with discrete domain \\(\\set{10\\,\\mathrm{V}, 11\\,\\mathrm{V}, 12\\,\\mathrm{V}, \\dotsc}\\); while needing the “true” voltage \\(V_\\text{true}\\) with a precision of at least \\(0.01\\,\\mathrm{V}\\), so this latter quantity could have a continuous domain.\nIn solving data-science and engineering problems it’s important to make clear whether a particular quantity value can be considered “true” and used as-is, or only “observed” with insufficient precision and used as data to infer the true value."
  },
  {
    "objectID": "quantities_types_multi.html#sec-data-multiv",
    "href": "quantities_types_multi.html#sec-data-multiv",
    "title": "13  Joint quantities and complex data types",
    "section": "13.1 Joint quantities",
    "text": "13.1 Joint quantities\nSome sets of basic quantities are just that: simple collections of quantities, in the sense that they do not have new properties or allow for new kinds of operations. We shall call these joint quantities when we need to distinguish them from quantities of a basic kind; but usually they are also simply called quantities.\nThe values of a joint quantity are just tuples of values of its basic component quantities. Their domain is the Cartesian product of the domains of the basic quantities.\nConsider for instance the age, sex1, and nationality of a particular individual. They can be represented as an interval-continuous quantity \\(A\\), a binary one \\(S\\), and a nominal one \\(N\\). We can join them together to form the joint quantity  “(age, sex, nationality)”  which can be denoted by  \\((A,S,N)\\).  One value of this joint quantity is, for example, \\((25\\,\\mathrm{y}, {\\small\\verb;F;}, {\\small\\verb;Norwegian;})\\). The domain could be1 We define sex by the presence of at least one Y chromosome or not. It is different from gender, which involves how a person identifies.\n\\[\n[0,+\\infty)\\times\n\\set{{\\small\\verb;F;}, {\\small\\verb;M;}} \\times\n\\set{{\\small\\verb;Afghan;}, {\\small\\verb;Albanian;}, \\dotsc, {\\small\\verb;Zimbabwean;}}\n\\]\n\nDiscreteness, boundedness, continuity\nA joint quantity may not be simply characterized as “discrete”, or “bounded”, or “infinite”, and so on. Usually we must specify these characteristics for each of its basic component quantities instead. Sometimes a joint quantity is called, for instance, “continuous” if all its basic components are continuous; but other conventions are also used.\n\n\n\n\n\n\n Exercises\n\n\n\nConsider again the examples of §  12.1.1. Do you find any examples of joint quantities?"
  },
  {
    "objectID": "quantities_types_multi.html#sec-data-complex",
    "href": "quantities_types_multi.html#sec-data-complex",
    "title": "13  Joint quantities and complex data types",
    "section": "13.2 Complex quantities",
    "text": "13.2 Complex quantities\nSome complex quantities can be represented as sets of quantities of basic types. These sets, however, are “more than the sum of their parts”: they possess new physical and mathematical properties and operations that do not apply or do not make sense for the single components.\nFamiliar examples are vectorial quantities from physics and engineering, such as location, velocity, force, torque. Another example are images, when represented as grids of basic quantities.\nConsider for example a 4 × 4 monochrome image, represented as a grid of 16 binary quantities \\(0\\) or \\(1\\). Three possible values could be these:\n    \nrepresented by the numeric matrices   \\(\\begin{psmallmatrix}1&0&0&0\\\\0&0&0&0\\\\0&0&0&0\\\\0&0&0&0\\end{psmallmatrix}\\), \\(\\begin{psmallmatrix}0&1&0&0\\\\0&0&0&0\\\\0&0&0&0\\\\0&0&0&0\\end{psmallmatrix}\\), \\(\\begin{psmallmatrix}0&0&0&0\\\\0&0&0&0\\\\0&0&0&0\\\\0&0&0&1\\end{psmallmatrix}\\).\nFrom the point of view of the individual binary quantities, these three “values” are equally different from one another: where one of them has grid value \\(1\\), the others have \\(0\\). But properly considered as images, we can say that the first and the second are somewhat more “similar” or “closer” to each other than the first and the third. This similarity can be represented and quantified by a metric over the domain of all such images. This metric involves all basic binary quantities at once; it is not a property of each of them individually.\nMore generally, complex quantities have additional, peculiar properties, represented by mathematical structures, which distinguish them from simpler joint quantities; although there is not a clear separation between the two.\nThese properties and structures are very important for inference problems, and usually make them computationally very hard. The importance of machine-learning methods lies to a great extent in the fact that they allow us to do approximate inference on these kinds of complex data. The peculiar structures of these data, however, are often also the cause of striking failures of some machine-learning methods, for example the reason why they may classify incorrectly, or correctly but for the wrong reasons."
  },
  {
    "objectID": "probability_distributions.html#motivation-for-the-inference-ii-part",
    "href": "probability_distributions.html#motivation-for-the-inference-ii-part",
    "title": "14  Probability distributions",
    "section": "Motivation for the “Inference II” part",
    "text": "Motivation for the “Inference II” part\nIn the “Data I” part we developed a “language”, that is, particular kinds of sentences, to approach inferences and probability calculations typical of data-science and engineering problems.\nIn the present part we focus on probability calculations that often occur with this kind of sequences, and on useful visual representation of such probabilities. Still, at bottom we’re just using the four fundamental rules of inference over and over again – nothing more than that."
  },
  {
    "objectID": "probability_distributions.html#distribution-of-probabilities-among-values",
    "href": "probability_distributions.html#distribution-of-probabilities-among-values",
    "title": "14  Probability distributions",
    "section": "14.1 Distribution of probabilities among values",
    "text": "14.1 Distribution of probabilities among values\nWhen an agent is uncertain about the value of a quantity, its uncertainty is expressed and quantified by assigning a degree of belief, conditional on the agent’s knowledge, to all the possible cases regarding the true value.\nFor a temperature measurement, for instance, the cases could be “The temperature is measured to have value 271 K”, “The temperature is measured to have value 272 K”, and so on up to 275 K. These cases are expressed by mutually exclusive and exhaustive sentences. Denoting the temperature with \\(T\\), these sentences can be abbreviated as\n\\[\n{\\color[RGB]{34,136,51}T = 271\\,\\mathrm{K}} \\ , \\quad\n{\\color[RGB]{34,136,51}T = 272\\,\\mathrm{K} \\ ,} \\quad\n{\\color[RGB]{34,136,51}T = 273\\,\\mathrm{K} \\ ,} \\quad\n{\\color[RGB]{34,136,51}T = 274\\,\\mathrm{K} \\ ,} \\quad\n{\\color[RGB]{34,136,51}T = 275\\,\\mathrm{K}} \\ .\n\\]\nThe agent’s belief about the quantity is then expressed by the probabilities about these sentences, conditional on the agent’s state of knowledge \\({\\color[RGB]{204,187,68}\\mathsfit{I}}\\): \n\\[\\begin{aligned}\n\\mathrm{P}({\\color[RGB]{34,136,51}T \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}271\\,\\mathrm{K}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{204,187,68}\\mathsfit{I}}) &= {\\color[RGB]{170,51,119}0.04} \\\\[1ex]\n\\mathrm{P}({\\color[RGB]{34,136,51}T \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}272\\,\\mathrm{K}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{204,187,68}\\mathsfit{I}}) &= {\\color[RGB]{170,51,119}0.10} \\\\[1ex]\n\\mathrm{P}({\\color[RGB]{34,136,51}T \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}273\\,\\mathrm{K}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{204,187,68}\\mathsfit{I}}) &= {\\color[RGB]{170,51,119}0.18} \\\\[1ex]\n\\mathrm{P}({\\color[RGB]{34,136,51}T \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}274\\,\\mathrm{K}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{204,187,68}\\mathsfit{I}}) &= {\\color[RGB]{170,51,119}0.28} \\\\[1ex]\n\\mathrm{P}({\\color[RGB]{34,136,51}T \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}275\\,\\mathrm{K}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{204,187,68}\\mathsfit{I}}) &= {\\color[RGB]{170,51,119}0.40}\n\\end{aligned}\n\\]\nthat sum up to one:\n\\[\n\\begin{aligned}\n&\\mathrm{P}({\\color[RGB]{34,136,51}T \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}271\\,\\mathrm{K}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{204,187,68}\\mathsfit{I}}) +\n\\dotsb +\n\\mathrm{P}({\\color[RGB]{34,136,51}T \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}275\\,\\mathrm{K}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{204,187,68}\\mathsfit{I}}) \\\\\n&\\quad{}=\n{\\color[RGB]{170,51,119}0.04}+{\\color[RGB]{170,51,119}0.10}+{\\color[RGB]{170,51,119}0.18}+{\\color[RGB]{170,51,119}0.28}+{\\color[RGB]{170,51,119}0.40} \\\\\n&\\quad{}=\n{\\color[RGB]{170,51,119}1}\n\\end{aligned}\\]\nThis collection of probabilities is called a probability distribution.\n\n\n\n\n\n\n What’s “distributed”?\n\n\n\nThe probability is distributed among the possible values, not the quantity, as illustrated in the side picture. The quantity cannot be “distributed”: it has one, definite value, which is however unknown to us.\n\n\n\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nConsider three sentences \\(\\mathsfit{X}_1, \\mathsfit{X}_2, \\mathsfit{X}_3\\) that are mutually exclusive and exhaustive on conditional \\(\\mathsfit{I}\\), that is:\n\\[\n\\begin{gathered}\n\\mathrm{P}(\\mathsfit{X}_1 \\land \\mathsfit{X}_2 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) =\n\\mathrm{P}(\\mathsfit{X}_1 \\land \\mathsfit{X}_3 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) =\n\\mathrm{P}(\\mathsfit{X}_2 \\land \\mathsfit{X}_3 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = 0\n\\\\\n\\mathrm{P}(\\mathsfit{X}_1 \\lor \\mathsfit{X}_2 \\lor \\mathsfit{X}_3 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = 1\n\\end{gathered}\n\\]\nProve, using the fundamental rules of inferences and any derived rules from §  8, that we must then have\n\\[\n\\mathrm{P}(\\mathsfit{X}_1 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) + \\mathrm{P}(\\mathsfit{X}_2 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) + \\mathrm{P}(\\mathsfit{X}_3 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = 1\n\\]\n\n\nLet’s see how probability distributions can be represented and visualized for the basic types of quantities discussed in §  12.\nWe start with probability distributions over discrete domains."
  },
  {
    "objectID": "probability_distributions.html#sec-discr-prob-distr",
    "href": "probability_distributions.html#sec-discr-prob-distr",
    "title": "14  Probability distributions",
    "section": "14.2 Discrete probability distributions",
    "text": "14.2 Discrete probability distributions\n\nTables and functions\nA probability distribution over a discrete domain can obviously be displayed as a table of values and their probabilities. For instance\n\n\n\n\n\n\n\n\n\n\n\nvalue\n271 K\n272 K\n273 K\n274 K\n275 K\n\n\n\n\nprobability\n0.04\n0.10\n0.18\n0.28\n0.40\n\n\n\nIn the case of ordinal or interval quantities it is sometimes possible to express the probability as a function of the value. For instance, the probability distribution above could be summarized by this function of the value \\({\\color[RGB]{34,136,51}t}\\):\n\\[\n\\mathrm{P}({\\color[RGB]{34,136,51}T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}t} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{204,187,68}\\mathsfit{I}}) =\n{\\color[RGB]{170,51,119}\\frac{({\\color[RGB]{34,136,51}t}/\\textrm{\\small K} - 269)^2}{90}}\n\\quad\\text{\\small (rounded to two decimals)}\n\\]\n\n\nA graphical representation is often helpful to detect features, peculiarities, and even inconsistencies in one or more probability distributions.\n\n\nHistograms and area-based representations\nA probability distribution for a nominal, ordinal, and discrete-interval quantity can be neatly represented by a histogram.\n\n\n\n\n\nHistogram for the probability distribution over possible component failures\n\n\nThe possible values are placed on a line. For an ordinal or interval quantity, the sequence of values on the line should correspond to their natural order. For a nominal quantity the order is irrelevant.\nA rectangle is then drawn above each value. Typically the rectangles are contiguous. The bases of the rectangles are all equal, and the areas of the rectangles are proportional to the probabilities. Since the bases are equal, this implies that the heights of the rectangles are also proportional to the probabilities.\nSuch kind of drawing can of course be horizontal, vertical, upside-down, and so on, depending on convenience.\nSince the probabilities must sum to one, the total area of the rectangles represents the unit of area. So in principle there is no need of writing probability values on some vertical axis, or grid, or similar visual device, because the probability value can be visually read as the ratio of a rectangle area to the total area. An axis or grid can nevertheless be helpful. Alternatively the probabilities can be reported above or below each rectangle.\nNominal quantities do not have any specific order, so their values do not need to be ordered on a line. Other area-based representations, such as pie charts, can also be used for these quantities.\n\n\nLine-based representations\nHistograms give faithful representations of discrete probability distributions. Their graphical bulkiness, however, can be a disadvantage in some situations; for instance when we want to have a clearer idea of how the probability distribution varies across values for ordinal or interval quantities; or when we want to compare several probability distributions over the same values.\nIn these cases we can use standard line plots, or variations thereof. Compare the following example.\nA technician wonders which component of a laptop failed first (only one can fail at a time), with seven possible alternatives (this is a nominal quantity): \\(\\set{{\\small\\verb;hard-drive;}, {\\small\\verb;motherboard;}, {\\small\\verb;CPU;}, {\\small\\verb;keyboard;}, {\\small\\verb;screen;}, {\\small\\verb;graphics-card;}, {\\small\\verb;PCI;}}\\). Before examining the laptop, the technician’s belief about which component failed first is distributed among the seven alternatives as in the blue histogram with solid borders. After a first inspection of the laptop, the technician’s belief has a new distribution, as in the red histogram with dashed borders:\n\nIt requires some concentration to tell the two probability distributions apart, and for example understand where their peaks are. Let us represent them by two line plots instead: solid blue with circles for the pre-inspection belief distribution, and dashed red with squares for the post-inspection one:\n\nthis line plot displays more cleanly the differences between the two distributions. We see that at first the technician most strongly believed the \\({\\small\\verb;keyboard;}\\) to be the faulty candidate, followed by the \\({\\small\\verb;PCI;}\\). After the preliminary inspection, the technician most strongly believes the \\({\\small\\verb;PCI;}\\) to be the faulty candidate, followed by the \\({\\small\\verb;graphics card;}\\)."
  },
  {
    "objectID": "probability_distributions.html#sec-prob-densities",
    "href": "probability_distributions.html#sec-prob-densities",
    "title": "14  Probability distributions",
    "section": "14.3 Probability densities",
    "text": "14.3 Probability densities\nDistributions of probability over continuous domains present several counter-intuitive aspects, which essentially arise because we are dealing with uncountable infinities – while often still using linguistic expressions that make at most sense for countable infinities. Here we follow a practical and realistic approach for working with such distributions.\nConsider a quantity \\(X\\) with a continuous domain. When we say that such a quantity has some value \\(x\\) we really mean that it has a value somewhere in the range   \\(x -\\epsilon/2\\)  to  \\(x+\\epsilon/2\\),   where the width \\(\\epsilon\\) is usually extremely small. For example, for double-precision values stored in a computer, the width1 must be at least \\(\\epsilon \\approx 2\\cdot 10^{-16}\\) :1 more precisely the relative width\n## R code\n&gt; 1.234567890123456 == 1.234567890123455\n[1] FALSE\n\n&gt; 1.2345678901234567 == 1.2345678901234566\n[1] TRUE\nand a value 1.3 really represents a range between 1.29999999999999982236431605997495353221893310546875 and 1.300000000000000266453525910037569701671600341796875, this range coming from the internal binary representation of 1.3. Often the width \\(\\epsilon\\) is much larger than the computer’s precision, and comes from the precision with which the value is experimentally measured.\nProbabilities are therefore assigned to such small ranges, not to single values. Since these ranges are very small, they are also very numerous. The total probability assigned to all of them must still amount to \\(1\\); therefore each small range receives an extremely small amount of probability. A standard Gaussian distribution for a real quantity, for instance, assigns a probability of approximately \\(8\\cdot 10^{-17}\\), or \\(0.00000000000000008\\), to a range of width \\(2\\cdot 10^{-16}\\) around the value \\(0\\). All other ranges are assigned even smaller probabilities.\nIn would be impractical to work with such small probabilities. We use probability densities instead. As implied by the term “density”, a probability density is the amount of probability \\(P\\) assigned to a standard range of width \\(\\epsilon\\), divided by that width. For example, if the probability assigned to a range of width  \\(\\epsilon=2\\cdot10^{-16}\\)  around \\(0\\) is  \\(P=7.97885\\cdot10^{-17}\\),  then the probability density around \\(0\\) is\n\\[\n\\frac{P}{\\epsilon} =\n\\frac{7.97885\\cdot10^{-17}}{2\\cdot10^{-16}} = 0.398942\n\\]\nwhich is a simpler number to work with.\nProbability densities are convenient because they usually do not depend on the range width \\(\\epsilon\\), if it’s small enough. Owing to physics reasons, we don’t expect a situation where \\(X\\) is between \\(0.9999999999999999\\) and \\(1.0000000000000001\\) to be very different from one where \\(X\\) is between \\(1.0000000000000001\\) and \\(1.0000000000000003\\). The probabilities assigned to these two small ranges of width \\(\\epsilon=2\\cdot 10^{-16}\\) each will therefore be approximately equal, let’s say \\(P\\) each. Now if we use a small range of width \\(\\epsilon\\) around \\(X=1\\), the probability is \\(P\\), and the probability density is \\(P/\\epsilon\\). If we consider a range of double width \\(2\\,\\epsilon\\) around \\(X=1\\), then the probability is \\(P+P\\) instead, but the probability density is still\n\\[\\frac{P+P}{2\\,\\epsilon} =\n\\frac{1.59577\\cdot10^{-16}}{4\\cdot10^{-16}}\n= 0.398942 \\ .\n\\]\nAs you see, even if we consider a range with double the width as before, the probability density is still the same.\n\n\nIn these notes we’ll denote probability densities with a lowercase \\(\\mathrm{p}\\), with the following notation:\n\\[\n\\underbracket[0pt]{\\mathrm{p}}_{\\mathrlap{\\color[RGB]{119,119,119}\\!\\uparrow\\ \\textit{lowercase}}}(X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) \\coloneqq\n\\frac{\n\\overbracket[0pt]{\\mathrm{P}}^{\\mathrlap{\\color[RGB]{119,119,119}\\!\\downarrow\\ \\textit{uppercase}}}(\\textsf{\\small`\\(X\\) has value between \\(x-\\epsilon/2\\) and \\(x+\\epsilon/2\\)'} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}{\\epsilon}\n\\]\nThis definition works even if we don’t specify the exact value of \\(\\epsilon\\), as long as it’s small enough.\n\n\n\n\n\n\n Probability densities are not probabilities\n\n\n\nIf \\(X\\) is a continuous quantity, the expression “\\(\\mathrm{p}(X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}2.5 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})=0.3\\)” does not mean “There is a \\(0.3\\) probability that \\(X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}2.5\\)”. The probability that \\(X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}2.5\\) exactly is, if anything, zero.\nThat expression means “There is a  \\(0.3\\cdot \\epsilon\\)   probability that \\(X\\) is between \\(2.5-\\epsilon/2\\) and \\(2.5+\\epsilon/2\\), for any \\(\\epsilon\\) small enough”.\nIn fact, probability densities can be larger than 1, because they are obtained by dividing by a number, the range width, that is in principle arbitrary. This fact shows that they cannot be probabilities.\nIt is important not to mix up probability and probability densities: we shall see later that densities have very different properties, for example with respect to maxima and averages.\n\n\nA helpful practice (though followed by few texts) is to always write a probability density as\n\\[\\mathrm{p}(X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\,\\mathrm{d}X\\]\nwhere “\\(\\mathrm{d}X\\)” stands for the width of a small range around \\(x\\). This notation is also helpful with integrals. Unfortunately it becomes a little cumbersome when we are dealing with more than one quantity.\n\nPhysical dimensions and units\nIn the International System of Units (SI)2, “Degree of belief” is considered to be a dimensionless quantity, or more precisely a quantity of dimension “1”. This is why we don’t write units such as “metres” (\\(\\mathrm{m}\\)), “kilograms” (\\(\\mathrm{kg}\\)) or similar together with a probability value.2 see also the material at the International Bureau of Weights and Measures (BIPM)\nA probability density, however, is defined as the ratio of a probability amount and an interval \\(\\epsilon\\) of some quantity. This latter quantity might well have physical dimensions, say “metres” \\(\\mathrm{m}\\). Then the ratio, which is the probability density, has dimensions \\(1/\\mathrm{m}\\). So probability densities in general have physical dimensions.\nAs another example, suppose that an agent with background knowledge \\(\\mathsfit{I}\\) assigns a degree of belief \\(0.00012\\) to an interval of temperature of width \\(0.0001\\,\\mathrm{°C}\\), around the temperature \\(T = 20\\,\\mathrm{°C}\\). Then the probability density at \\(20\\,\\mathrm{°C}\\) is equal to\n\\[\n\\mathrm{p}(T \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}20\\mathrm{°C} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) =\n\\frac{0.00012}{0.0001\\,\\mathrm{°C}} = 1.2\\,\\mathrm{°C^{-1}}\n\\]\nIt is an error to report probability densities without their correct physical units. In fact, keeping track of these units is often useful for consistency checks and finding errors in calculations, just like in other engineering or physics calculations.\nOn the other hand, if we write probability densities as previously suggested, in this case as “\\(\\mathrm{p}(T \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}20\\mathrm{°C} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\,\\mathrm{d}T\\)”, then the density written this way is a pure probability: the units “\\(\\mathrm{°C^{-1}}\\)” disappear because multiplied by \\(\\mathrm{d}T\\), which has the inverse units \\(\\mathrm{°C}\\)."
  },
  {
    "objectID": "probability_distributions.html#sec-represent-dens",
    "href": "probability_distributions.html#sec-represent-dens",
    "title": "14  Probability distributions",
    "section": "14.4 Representation of probability densities",
    "text": "14.4 Representation of probability densities\n\nLine-based representations\nThe histogram and the line representations become indistinguishable for a probability density.\nIf we represent the probability \\(P\\) assigned to a small range of width \\(\\epsilon\\) as the area of a rectangle, and the width of the rectangle is equal to \\(\\epsilon\\), then the height \\(P/\\epsilon\\) of the rectangle is numerically equal to the probability density. The difference from histograms for discrete quantities lies in the values reported on the vertical axis: for discrete quantities the values are probabilities (the areas of the rectangles), but for continuous quantities they are probability densities (the heights of the rectangles). This is also evident from the fact that the values reported on the vertical axis can be larger than 1, as in the plots on the side.\nThe rectangles, however, are so thin (usually thinner than a pixel on a screen) that they appear just as vertical lines, and together they look just like a curve delimiting a coloured area. If we don’t colour the area underneath we just have a line-based, or rather curve-based, representation of the probability density.\n\n\n   As the width \\(\\epsilon\\) of the small ranges is decreased, a histogram based on these widths become indistinguishable from a line plot\nIt can be good to keep in mind that the curve representing the probability density is not quite a function – in fact it’s best to call it a “density” or a “density function”. There are important reasons for keeping this distinction, which have also consequences for probability calculations, but we shall not delve into them in this course.\n\n\nScatter plots\nLine plots of a probability density are very informative, but they can also be slightly deceiving. Try the following experiment.\nConsider a continuous quantity \\(X\\) with the following probability density:\n\nWe want to represent the amount of probability in any small range, say between \\(X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}0\\) and \\(X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}0.1\\), by drawing in that range a number of short thin lines, the number being proportional to the probability. So a range with 10 lines has twice the probability of a range with 5 lines. The probability density around a value is therefore roughly represented by the density of the lines around that value.\nSuppose that we have 50 lines available to distribute this way. Where should we place them?\n\n\n\n\n\n\n\n Exercise\n\n\n\n\nWhich of these plots shows the correct placement of the 50 lines? (NB: the position of the correct answer is determined by a pseudorandom-number generator.)\n\n\n\n\n\n\n(A)\n\n\n\n\n\n\n\n(B)\n\n\n\n\n\n\n\n\n\n(C)\n\n\n\n\n\n\n\n(D)\n\n\n\n\n\n\n\n\n\nIn a scatter plot, the probability density is (approximately) represented by density of lines, or points, or similar objects, as in the examples above (only one above, though, correctly matches the density represented by the curve).\nAs the experiment and exercise above may have demonstrated, line plots sometimes give us slightly misleading ideas of how the probability is distributed across the domain; for example, peaks at some values make us overestimate the probability density around those values. Scatter plots often give a less misleading representation of the probability density.\nScatter plots are also useful for representing probability densities in more than one dimension – sometimes even in infinite dimensions! They can moreover be easier to produce computationally than line plots.\n\n\n\n\n\n\n\n\n Study reading\n\n\n\n\n§§ 5.3.0–5.3.1 of Risk Assessment and Decision Analysis with Bayesian Networks"
  },
  {
    "objectID": "probability_distributions.html#sec-combined-probs",
    "href": "probability_distributions.html#sec-combined-probs",
    "title": "14  Probability distributions",
    "section": "14.5 Combined probabilities",
    "text": "14.5 Combined probabilities\nA probability distribution is defined over a set of mutually exclusive and exhaustive sentences. In some inference problems, however, we do not need the probability of those sentences, but of some other sentence that can be obtained from them by an or operation. The probability of this sentence can then be obtained by a sum, according to the or-rule of inference. We can call this a combined probability. Let’s explain this procedure with an example.\nBack to our initial assembly-line scenario from §  1, the inference problem was to predict whether a specific component would fail within a year or not. Consider the time when the component will fail (if sold), and represent it by the quantity \\(T\\) with the following 24 different values, where “\\(\\mathrm{mo}\\)” stands for “months”:\n\\[\\begin{aligned}\n&\\textsf{\\small`The component will fail during it 1st month of use'}\\\\\n&\\textsf{\\small`The component will fail during it 2nd month of use'}\\\\\n&\\dotsc \\\\\n&\\textsf{\\small`The component will fail during it 23rd month of use'}\\\\\n&\\textsf{\\small`The component will fail during it 24th month of use or after'}\n\\end{aligned}\\]\nwhich we can shorten to  \\(T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}1 \\mathbin{\\mkern-0mu,\\mkern-0mu}T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}2 \\mathbin{\\mkern-0mu,\\mkern-0mu}\\dotsc \\mathbin{\\mkern-0mu,\\mkern-0mu}T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}24\\); note the slightly different meaning of the last value.\n\n\n\n\n\n\n Exercise\n\n\n\nWhat is the basic type of the quantity \\(T\\)? Which other characteristics does it have? for instance discrete? unbounded? rounded? uncensored?\n\n\nSuppose that the inspection device – our agent – has internally calculated a probability distribution for \\(T\\), conditional on its internal programming and the results of the tests on the component, collectively denoted \\(\\mathsfit{I}\\). The probabilities, compactly written, are\n\\[\n\\mathrm{P}(T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}1 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}), \\quad\n\\mathrm{P}(T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}2 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}), \\quad\n\\dotsc, \\quad\n\\mathrm{P}(T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}24 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\n\\]\nTheir values are stored in this csv file and plotted in the histogram on the side.\n\n\n\nWhat’s important for the agent’s decision about rejecting or accepting the component, is not the exact time when it will fail, but only whether it will fail within the first year or not. That is, the agent needs the probability of the sentence \\(\\textsf{\\small`The component will fail within a year of use'}\\). But this sentence is just the or of the first 12 sentences expressing the values of \\(T\\):\n\\[\n\\begin{aligned}\n&\\textsf{\\small`The component will fail within a year of use'}\n\\\\&\\qquad{}\\equiv\n\\textsf{\\small`The component will fail during it 1st month of use'}\n\\lor{}\n\\\\&\\qquad\\qquad\n\\textsf{\\small`The component will fail during it 2nd month of use'}\n\\lor \\dotsb\n\\\\&\\qquad\\qquad\n\\dotsb \\lor\n\\textsf{\\small`The component will fail during it 12th month of use'}\n\\\\[1ex]&\\qquad{}\\equiv\n(T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}1) \\lor (T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}2) \\lor \\dotsb \\lor (T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}12)\n\\end{aligned}\n\\]\nThe probability needed by the agent is therefore\n\\[\n\\mathrm{P}(T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}1 \\lor T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}2 \\lor \\dotsb \\lor T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}12\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\]\nwhich can be calculated using the or-rule, considering that the sentences involved are mutually exclusive:\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\textsf{\\small`The component will fail within a year of use'} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\\\[1ex]&\\qquad{}=\n\\mathrm{P}(T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}1 \\lor T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}2 \\lor \\dotsb \\lor T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}12\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\\\[1ex]&\\qquad{}=\n\\mathrm{P}(T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}1 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}) + \\mathrm{P}(T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}2 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}) + \\dotsb + \\mathrm{P}(T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}12 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\n\\\\[1ex]&\\qquad{}= \\sum_{t=1}^{12} \\mathrm{P}(T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}t \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\end{aligned}\n\\]\n\n\n\n\n\n\nSum notation\n\n\n\nWe shall often use the \\(\\sum\\)-notation for sums, as in the example above. A notation like “\\(\\displaystyle\\sum_{i=5}^{20}\\)” means: write multiple copies of what’s written on its right side, and in each copy replace the symbol “\\(i\\)” with values from \\(5\\) to \\(20\\), in turn; then sum up these copies. The symbol “\\(i\\)” is called the index of the sum. Sometimes the initial and final values, \\(5\\) and \\(20\\) in the example, are omitted if they are understood from the context, and the sum is written simply “\\(\\displaystyle\\sum_{i}\\)”.\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nUsing your favourite programming language:\n\nLoad the csv file containing the probabilities.\nInspect this file, find the headers of its columns and so on.\nCalculate the probability that the component will fail within a year of use.\nCalculate the probability that the component will fail “within two months of use, or after a year of use”."
  },
  {
    "objectID": "joint_probability.html#joint-probability-distributions",
    "href": "joint_probability.html#joint-probability-distributions",
    "title": "15  Joint probability distributions",
    "section": "15.1 Joint probability distributions",
    "text": "15.1 Joint probability distributions\nA joint quantity is just a collection or set of quantities of basic types. Saying that a joint quantity has a particular value means that each basic component quantity has a particular value in its specific domain. This is expressed by an and of sentences.\nConsider for instance the joint quantity \\(X\\) consisting of the age \\(\\color[RGB]{102,204,238}A\\) and sex \\(\\color[RGB]{34,136,51}S\\) of a specific person. The fact that \\(X\\) has a particular value is expressed by a composite sentence such as\n\\[\n\\textsf{\\small`The person's age is 25 years and the person's sex is female'}\n\\]\nwhich we can compactly write with an and:\n\\[\n{\\color[RGB]{102,204,238}A\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}25\\,\\mathrm{y}} \\land {\\color[RGB]{34,136,51}S\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\mathrm{f}}\n\\]\nAll the possible composite sentences of this kind are mutually exclusive and exhaustive.\nAn agent’s uncertainty about \\(X\\)’s true value is therefore represented by a probability distribution over all and-ed sentences of this kind, representing all possible joint values:\n\\[\n\\mathrm{P}\\bigl({\\color[RGB]{102,204,238}A \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}25\\,\\mathrm{y}} \\land {\\color[RGB]{34,136,51}S\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\mathrm{f}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}\\bigr) \\ , \\qquad\n\\mathrm{P}\\bigl({\\color[RGB]{102,204,238}A \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}31\\,\\mathrm{y}} \\land {\\color[RGB]{34,136,51}S\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\mathrm{m}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}\\bigr) \\ , \\qquad\n\\dotsc\n\\]\nwhere \\(\\mathsfit{I}\\) is the agent’s state of knowledge, and the probabilities sum up to one. We call each of these probabilities a joint probability, and their collection a joint probability distribution. Usually these probabilities are written in much abbreviated form, and a comma “\\(\\mathbin{\\mkern-0mu,\\mkern-0mu}\\)” is used instead of “\\(\\land\\)” (§  6.4); for instance you can commonly find the following notation:\n\\[\n\\mathrm{P}(A\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}25 \\mathbin{\\mkern-0mu,\\mkern-0mu}S\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\mathrm{f} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\]\nor even just\n\\[\n\\mathrm{P}(25, \\mathrm{f} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\]"
  },
  {
    "objectID": "joint_probability.html#sec-repr-joint-prob",
    "href": "joint_probability.html#sec-repr-joint-prob",
    "title": "15  Joint probability distributions",
    "section": "15.2 Representation of joint probability distributions",
    "text": "15.2 Representation of joint probability distributions\nThere is a wide variety of ways of representing joint probability distributions, and new ways are invented (and rediscovered) all the time. In some cases, especially when the quantity has more than three component quantities, it can become impossible to graphically represent the probability distribution in a faithful way. Therefore one often tries to represent only some aspects or features of interest from the full distribution. Whenever you see a plot of a joint probability distribution, you should carefully read what the plot shows and how it was made. Here we only illustrate some examples and ideas for representations.\n\nTables\nWhen a joint quantity consists of two, discrete and finite component quantities, the joint probabilities can be reported as a table, sometimes called a contingency table1.1 this term is most often used for joint distributions of frequencies rather than probability\nExample: Consider the next patient that will arrive at a particular hospital. There’s the possibility of arrival by \\({\\small\\verb;ambulance;}\\), \\({\\small\\verb;helicopter;}\\), or \\({\\small\\verb;other;}\\) transportation means; and the possibility that the patient will need \\({\\small\\verb;urgent;}\\) \\({\\small\\verb;non-urgent;}\\) care. These can be seen as two quantities \\(T\\) (nominal) and \\(U\\) (binary). When these two quantities are taken together; their joint probability distribution is as follows, conditional on the hospital’s data \\(\\mathsfit{I}_{\\text{H}}\\):\n\n\nTable 15.1: Joint probability distribution for transportation and urgency\n\n\n\n\n\n\n\n\n\n\\(\\mathrm{P}(U\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}u \\mathbin{\\mkern-0mu,\\mkern-0mu}T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}t\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}_{\\text{H}})\\)\n\ntransportation at arrival \\(T\\)\n\n\n\n\nambulance\nhelicopter\nother\n\n\nurgency \\(U\\)\nurgent\n0.11\n0.04\n0.03\n\n\nnon-urgent\n0.17\n0.01\n0.64\n\n\n\n\nWe see for instance that the most probable possibility is that the next patient will arrive by transportation means other than ambulance and helicopter, and won’t require urgent care:\n\\[\\mathrm{P}(U\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;other;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}_{\\text{H}}) =\n0.64\\]\nIt is also possible to replace the numerical probability values with graphical representations; for example as shades of a colour, or squares with different areas.\n\n\n\n\n\n\n Exercise – never forget the agent!\n\n\n\nWho could be the agent whose degrees of belief are represented in the table above? What could be the background information leading to such beliefs?\n\n\n\n\nScatter plots and similar\nProbability distributions for nominal, ordinal, or discrete-interval quantities can be represented by histograms or line plots, as we saw in §  14.2. Histograms could be generalized to quantities consisting of two joint discrete quantities: a probability could be represented by a cuboid or rectangular prism (a sort of small tower with rectangular section), or cylinder, or similar. This representation, even if it can look flamboyant, is often inconvenient because some of the three-dimensional objects can be hidden from view, as in the generic example on the side.\n\n\n Example of generalized histogram (from Mathematica)\nAlternatively, one can replace the numerical values of the probabilities in the tabular representation of the previous section with some graphical encoding.\nAn example is some colour scheme, say white for probability \\(0\\), black for probability \\(1\\), and grey levels for intermediate probabilities; or some other colour scheme. This is sometimes called a “density histogram”; see the generic example on the side. This representation can be useful for qualitative or semi-quantitative assessments, for example seeing which joint values have highest probabilities.\n\n\n Example of density histogram (from Mathematica)\nAnother example, similar to the scatter plot (§  14.4.2), is to encode the probability values with a proportional number of points or other shapes, as illustrated here for the probabilities of table 15.1:\n\n\n\nFigure 15.1: Scatter plot for the urgency-transportation joint probability distribution\n\n\nthe points do not need to be scattered in regular fashion as long as it’s clear which quantity value they are associated with. The scatter plot above has 100 points, and therefore we can see for instance that \\(\\mathrm{P}(U\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\textrm{\\small urgent} \\mathbin{\\mkern-0mu,\\mkern-0mu}T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\textrm{\\small helicopter}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}_{\\text{H}}) = 0.03\\), since the corresponding region has 3 points out of 100."
  },
  {
    "objectID": "joint_probability.html#sec-joint-prob-densities",
    "href": "joint_probability.html#sec-joint-prob-densities",
    "title": "15  Joint probability distributions",
    "section": "15.3 Joint probability densities",
    "text": "15.3 Joint probability densities\nIf a joint quantity consists in several continuous interval quantities, then its joint probability distribution is usually represented by a joint probability density, which generalize the one-dimensional discussion of §  14.3 to several dimensions.\nFor instance, if \\(X\\) and \\(Y\\) are two continuous interval quantity, then the notation\n\\[\n\\mathrm{p}(X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x \\mathbin{\\mkern-0mu,\\mkern-0mu}Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = 0.001\n\\]\nmeans that the joint sentence “\\(X\\) has value between \\(x-\\epsilon/2\\) and \\(x+\\epsilon/2\\), and \\(Y\\) between \\(y-\\delta/2\\) and \\(y+\\delta/2\\)”, or in symbols\n\\[\n\\bigl(x-\\tfrac{\\epsilon}{2} \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small&lt;}\\nonscript\\mkern 1mu}\\mathopen{} X \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small&lt;}\\nonscript\\mkern 1mu}\\mathopen{} x+\\tfrac{\\epsilon}{2}\\bigr)\n\\land\n\\bigl(y-\\tfrac{\\delta}{2} \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small&lt;}\\nonscript\\mkern 1mu}\\mathopen{} Y \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small&lt;}\\nonscript\\mkern 1mu}\\mathopen{} y+\\tfrac{\\delta}{2}\\bigr)\n\\]\nhas probability \\(0.001\\cdot\\epsilon\\cdot\\delta\\), conditional on the background knowledge \\(\\mathsfit{I}\\). Said otherwise, the rectangular region of values around \\((x,y)\\) with widths \\(\\epsilon\\) and \\(\\delta\\) is assigned a probability \\(0.001\\cdot\\epsilon\\cdot\\delta\\).\nRemember that a density typically has physical units, as in the one-dimensional case (§  14.3). For instance, if \\(X\\) above is a temperature measured in kelvin (\\(\\mathrm{K}\\)) and \\(Y\\) a resistance measured in ohm (\\(\\Omega\\)), then we would write  \\(\\mathrm{p}(X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x \\mathbin{\\mkern-0mu,\\mkern-0mu}Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = \\frac{0.001}{\\mathrm{K}\\,\\Omega}\\)."
  },
  {
    "objectID": "joint_probability.html#sec-repr-joint-dens",
    "href": "joint_probability.html#sec-repr-joint-dens",
    "title": "15  Joint probability distributions",
    "section": "15.4 Representation of joint probability densities",
    "text": "15.4 Representation of joint probability densities\nFor one-dimensional densities we discussed line-based representations and scatter plots (§  14.4). The first of these representations can be generalized to two-dimensional densities, leading to a surface plot. Below is the surface density plot for the probability density given by the formula\n\n\\[\n\\mathrm{p}(X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x \\mathbin{\\mkern-0mu,\\mkern-0mu}Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) =\n\\tfrac{3}{8\\,\\pi}\\, \\mathrm{e}^{-\\frac{1}{2} (x-1)^2-(y-1)^2}+\n\\tfrac{3}{64\\,\\pi}\\,\\mathrm{e}^{-\\frac{1}{32} (x-2)^2-\\frac{1}{2} (y-4)^2}+ \\tfrac{1}{40\\,\\pi}\\,\\mathrm{e}^{-\\frac{1}{8} (x-5)^2-\\frac{1}{5} (y-2)^2}\n\\]\n\n\nThis kind of representation can be very neat, but it has three drawbacks: it sometimes hides features of the density from view, it cannot be extended to three-dimensional densities, and sometimes the analytical expression for the probability density (like the formula above) is not available.\nThe scatter plot instead does not hides features, can also be used for three-dimensional densities, and can be used in cases where we can at least obtain “representative” points from the probability density, even if the analytical expression of the latter is too complicated or not available. This representation is, however, quantitatively more imprecise. Here is a scatter plot, using 10 000 points, for the probability density given above:\n\n\n\nFigure 15.2: Scatter-plot representation of the joint probability density \\(\\mathrm{p}(X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x \\mathbin{\\mkern-0mu,\\mkern-0mu}Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\)\n\n\nAs usual, the probability of a small region is proportional to the density of points in that region. If we had a joint density for three continuous quantities, its scatter plot would consist of three-dimensional clouds of points instead.\nClearly both kinds of representation have advantages and disadvantages."
  },
  {
    "objectID": "joint_probability.html#sec-joint-mix-distr",
    "href": "joint_probability.html#sec-joint-mix-distr",
    "title": "15  Joint probability distributions",
    "section": "15.5 Joint mixed discrete-continuous probability distributions",
    "text": "15.5 Joint mixed discrete-continuous probability distributions\nFrequently occurring in engineering and data-science problems are joint quantities composed by some discrete and some continuous quantities. Their joint probability distribution is a density with respect to the continuous component quantity.\nSuppose for instance that \\(Z\\) is a binary quantity with domain \\(\\set{{\\small\\verb;low;}, {\\small\\verb;high;}}\\), \\(X\\) a continuous quantity with all real numbers \\(\\mathbf{R}\\) as domain, and together they form the joint quantity \\((Z,X) \\in \\set{{\\small\\verb;low;}, {\\small\\verb;high;}} \\times \\mathbf{R}\\). Then the probability expression\n\\[\n\\mathrm{p}(Z\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;low;} \\mathbin{\\mkern-0mu,\\mkern-0mu}X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}3 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = 0.07\n\\]\nmeans that the agent with background information \\(\\mathsfit{I}\\) gives a \\(0.07\\cdot \\epsilon\\) degree of belief to the joint sentence “\\(Z\\) has value \\({\\small\\verb;low;}\\) and \\(X\\) has value between \\(3-\\epsilon/2\\) and \\(3+\\epsilon/2\\)”, for any small \\(\\epsilon\\). (As usual, if \\(X\\) has physical dimensions, say metres \\(\\mathrm{m}\\), then the probability density above has value \\(0.07\\,\\mathrm{m^{-1}}\\).)"
  },
  {
    "objectID": "joint_probability.html#sec-repr-mix-distr",
    "href": "joint_probability.html#sec-repr-mix-distr",
    "title": "15  Joint probability distributions",
    "section": "15.6 Representation of mixed probability distributions",
    "text": "15.6 Representation of mixed probability distributions\nMixed discrete-continuous probability distributions can be somewhat tricky to represent graphically. Here we consider line-based representations and scatter plots. We take as example the probability that the next patient who arrives at a particular hospital has a given age (positive continuous quantity) and arrives by \\({\\small\\verb;ambulance;}\\), \\({\\small\\verb;helicopter;}\\), or \\({\\small\\verb;other;}\\) transportation means.\n\nMulti-line plots\nA line plot can be used to represent the probability density for the continuous quantity and each specific value of the discrete quantity:\n\n\n\nFigure 15.3: [Line plot for the age-transportation joint probability]\n\n\nWith the plot above it’s important to keep in mind that the three curves are three pieces of the same probability density, not three different densities. This is also clear seeing that the three areas under them (which partly overlap) cannot each be \\(1\\), as would instead be expected for a probability density. The probability density is separated into three curves owing to the presence of the discrete quantity having three possible values.\nThe area under the solid blue curve is equal to \\(0.55\\), the area under the dashed red curve is \\(0.25\\), and the area under the dotted green curve is \\(0.20\\) . The total area under the three curves (counting also the overlapping regions) is equal to \\(1\\), as it should be.\nA possible disadvantage of this kind of plots is that some details, such as peaks, of the densities for some values of the discrete quantity may be barely discernible.\n\n\nScatter plots\nAs discussed before, in a scatter plot we represent the probability density by a cloud of “representative” objects, such as points, obtained from it: the density of these objects is approximately proportional to the probability density.\nAn example of scatter plot for the probability density of our example is the following (note that the blue colour is here no longer associated with \\({\\small\\verb;ambulance;}\\)):\n\nIn the plot above, the probability density is reflected by the density of vertical lines. Using points instead of vertical lines, the density would have been difficult to discern, since all points would be on one of three vertical positions.\nWe can use points if we give some variation to their vertical coordinate – keeping in mind that such vertical variation has no meaning. The idea is similar to the one of fig.  15.1. We obtain a plot like this:\n\n\n\nFigure 15.4: [Point-scatter plot for the age-transportation joint probability]\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nCompare the line plot of fig.  15.3 and the point-scatter plot of fig.  15.4, which represent the same joint probability density. Do some introspection, and analyse the contrasting impressions that the two kinds of representations may give you. For instance, does the line plot give you a wrong intuition about the sharpness of the peaks in the density?\nCompare with what you did in the exercise of §  14.4."
  },
  {
    "objectID": "joint_probability.html#sec-repr-general-distr",
    "href": "joint_probability.html#sec-repr-general-distr",
    "title": "15  Joint probability distributions",
    "section": "15.7 Representation of more general probability distributions and densities",
    "text": "15.7 Representation of more general probability distributions and densities\nProbability distributions for complex types of quantity can be quite tricky to represent in an informative way. They typically require a case-by-case approach.\nOften the idea behind the scatter plot works also in these complex cases: the distribution or density is represented by a “representative” sample of objects; and the objects can even depict the quantity itself.\nFor instance, imagine the quantity \\(L\\) defined as “the linear relationship between input voltage and output current of a specific electronic component”. The possible values of this quantity are not just simple numbers or categories, but straight lines, that is, functions of the form “\\(y=m\\,x + q\\)”, where \\(x\\) is the input voltage and \\(y\\) the output current. The possible values – straight lines – can differ in their angular coefficient \\(m\\) or in their intercept \\(q\\): one value could be the straight line \\(y= (2\\,\\mathrm{A/V})\\, x - 3\\,\\mathrm{A}\\), and another value could be straight line \\(y= (-1\\,\\mathrm{A/V})\\, x + 5\\,\\mathrm{A}\\), and so on. This is a continuous quantity, but it isn’t a quantity of a basic type.\n\n\n A voltage-current converter\nAn agent may be uncertain about which is the actual value of \\(L\\), that is, which is the straight line that correctly expresses the voltage-current relationship of the electronic component. The agent therefore assign a probability density over all possible values – over all possible straight lines. How to visually represent such a probability density?\nOne way is to use a scatter plot: the probability distribution is represented by a cloud of straight lines, whose density is approximately proportional to the probability density. Here is an example using 360 representative straight lines:\n\n\n\nFigure 15.5: [Scatter plot for the probability density over the voltage-current relationship]\n\n\nFrom this plot we can read some important semi-quantitative information about the agent’s probability density. For example, it’s most probable that the voltage-current relationship has a positive angular coefficient \\(m\\) around \\(0.5\\,\\mathrm{A/V}\\), and an intercept \\(q\\) around \\(3\\,\\mathrm{A}\\); it improbable, but not impossible, that the voltage-current relationship has a negative angular coefficient (the output current decreases as the input voltage is increased); and it’s practically impossible that the voltage-current relationship is almost vertical (say, changes in current larger than \\(\\sim 5\\,\\mathrm{A}\\) with changes in voltage smaller than \\(\\sim 0.2\\,\\mathrm{V}\\)).\n\n\n\n\n\n\n\n\n\n\n\n Exercises\n\n\n\n\nExplore datasets in a database such as the UC Irvine Machine Learning Repository, for example\n\nThe adult-income dataset\nThe heart-disease dataset\n\nAssume that the data given are representative “points” of a probability distribution or density (of which we don’t know the analytic formula). Plot the probability distributions and probability densities as scatter plots using some of these representative points.\nLook for the analytic formulae of some probability distributions and densities of simple and joint quantities, and plot them using different representations.\n\n\n\n\n\n\n\n\n\n\n\n Study reading\n\n\n\n\n§ 5.3.2 of Risk Assessment and Decision Analysis with Bayesian Networks\n§ 12.2.2 of Artificial Intelligence"
  },
  {
    "objectID": "marginal_probability.html#sec-marginal-probs",
    "href": "marginal_probability.html#sec-marginal-probs",
    "title": "16  Marginal probability distributions",
    "section": "16.1 Marginal probability: neglecting some quantities",
    "text": "16.1 Marginal probability: neglecting some quantities\nIn some situations an agent has a joint distribution of degrees of belief for the possible values of a joint quantity, but it needs to consider its belief in the value of one component quantity alone, irrespective of what the values for the other components quantities might be.\nConsider for instance the joint probability for the next-patient arrival scenario of table  15.1 from §  15.2, with joint quantity \\((U,T)\\). We may be interested in the probability that the next patient will need \\({\\small\\verb;urgent;}\\) care, independently of how the patient is transported to the hospital. This probability can be found, as usual, by analysing the problem in terms of sentences and using the basic rules of inference from §  8.4.\nThe sentence of interest is “The next patient will require urgent care”, or in symbols\n\\[U \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\]\nwhich is equivalent to “The next patient will require urgent care, and will arrive by ambulance, helicopter, or other means”, or in symbols\n\\[\nU \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\land\n(\nT \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;ambulance;}\\lor\nT \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;helicopter;}\\lor\nT \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;other;}\n)\n\\]\nUsing the derived rules of Boolean algebra of §  9.2 we can rewrite this sentence in yet another way:\n\\[\n(U \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\land T \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;ambulance;}) \\lor\n(U \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\land T \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;helicopter;}) \\lor\n(U \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\land T \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;other;})\n\\]\nThis last sentence is an or of mutually exclusive sentences. Its probability is therefore given by the or rule without the and terms (we now use the comma “\\(\\mathbin{\\mkern-0mu,\\mkern-0mu}\\)” for and):\n\n\\[\n\\begin{aligned}\n&\\mathrm{P}(U \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}})\n\\\\[1ex]\n&\\quad{}=\n\\mathrm{P}\\bigl[\n(U \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}T \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;ambulance;}) \\lor\n(U \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}T \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;helicopter;}) \\lor\n(U \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}T \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;other;})\n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}} \\bigr]\n\\\\[1ex]\n&\\quad{}=\\begin{aligned}[t]\n&\\mathrm{P}(U \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}T \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;ambulance;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}}) +{}\n\\\\\n&\\quad\\mathrm{P}(U \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}T \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}}) +{}\n\\\\\n&\\quad\\mathrm{P}(U \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}T \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;other;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}})\n\\end{aligned}\n\\end{aligned}\n\\]\n\nWe have found that the probability for a value of the urgency quantity \\(U\\), independently of the value of the transportation quantity \\(T\\), can be calculated by summing all joint probabilities with all possible \\(T\\) values. Using the \\(\\sum\\)-notation we can write this compactly:\n\\[\n\\mathrm{P}(U \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}}) =\n\\sum_{t}\n\\mathrm{P}(U \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}t \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}})\n\\]\nwhere it’s understood that the sum index \\(t\\) runs over the values \\(\\set{{\\small\\verb;ambulance;}, {\\small\\verb;helicopter;}, {\\small\\verb;other;}}\\).\nThis is called a marginal probability.\n\n\n\n\n\n\n Exercise\n\n\n\nUsing the values from table 15.1, calculate:\n\nthe marginal probability that the next patient will need urgent care\nthe marginal probability that the next patient will arrive by helicopter\n\n\n\n\n\nConsidering now a more generic case of a joint quantity with component quantities \\(\\color[RGB]{34,136,51}X\\) and \\(\\color[RGB]{238,102,119}Y\\), the probability for a specific value of \\(\\color[RGB]{34,136,51}X\\), conditional on some information \\(\\mathsfit{I}\\) and irrespective of what the value of \\(\\color[RGB]{238,102,119}Y\\) might be, is given by\n\\[\n\\mathrm{P}({\\color[RGB]{34,136,51}X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = \\sum_{\\color[RGB]{238,102,119}y} \\mathrm{P}({\\color[RGB]{238,102,119}Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y} \\mathbin{\\mkern-0mu,\\mkern-0mu}{\\color[RGB]{34,136,51}X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\]\nYou may notice the similarity with the expression for a combined probability from §  14.5. Indeed a marginal probability is just a special case of a combined probability: we are combining all probabilities that exhaust the possibilities for the sentence \\(Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y\\).\n\n\n\n\n\n\n Exercise: test your understanding\n\n\n\nUsing again the values from table 15.1, calculate the probability that the next patient will need urgent care and will be transported either by ambulance or by helicopter."
  },
  {
    "objectID": "marginal_probability.html#sec-marginal-dens",
    "href": "marginal_probability.html#sec-marginal-dens",
    "title": "16  Marginal probability distributions",
    "section": "16.2 Marginal density distributions",
    "text": "16.2 Marginal density distributions\nIn the example of the previous section, suppose now that the quantities \\(\\color[RGB]{34,136,51}X\\) and \\(\\color[RGB]{238,102,119}Y\\) are continuous. Then the joint probability is expressed by a density:\n\\[\\mathrm{p}({\\color[RGB]{238,102,119}Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y} \\mathbin{\\mkern-0mu,\\mkern-0mu}{\\color[RGB]{34,136,51}X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\]\nwith the usual meaning. The marginal probability density for \\(\\color[RGB]{34,136,51}X\\) is still given by a sum, but this sum occurs over intervals of values of \\(\\color[RGB]{238,102,119}Y\\), intervals with very small widths; as a consequence the sum will have a very large number of terms. To remind ourselves of this fact, which can be very important in some situations, we use a different notation in terms of integrals:\n\\[\n\\mathrm{p}({\\color[RGB]{34,136,51}X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = \\int_{\\color[RGB]{238,102,119}\\varUpsilon} \\mathrm{p}({\\color[RGB]{238,102,119}Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y} \\mathbin{\\mkern-0mu,\\mkern-0mu}{\\color[RGB]{34,136,51}X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\, \\mathrm{d}{\\color[RGB]{238,102,119}y}\n\\]\nwhere \\(\\color[RGB]{238,102,119}\\varUpsilon\\) represents the domain of the quantity \\(\\color[RGB]{238,102,119}Y\\).\nThe fact that integrals appear is in some cases extremely useful, because it allows us to use the theory of integration to calculate marginal probabilities quickly and precisely, instead of having to compute sums of a large numbers of small terms – a procedure that can be computationally expensive and lead to numerical errors owing to underflow or similar phenomena."
  },
  {
    "objectID": "marginal_probability.html#sec-marginal-scatter",
    "href": "marginal_probability.html#sec-marginal-scatter",
    "title": "16  Marginal probability distributions",
    "section": "16.3 Marginal probabilities and scatter plots",
    "text": "16.3 Marginal probabilities and scatter plots\nIn the previous chapters we have often discussed scatter plots for representing probability distributions of various kinds: discrete, continuous, joint, mixed, and so on.\nOne more advantage of the scatter plots for a joint distribution is that it can be quickly modified to represent any marginal, again with a scatter plot; whereas the use of a surface plot would require analytical calculations or approximations thereof.\nConsider for instance the joint probability density from §  15.4, represented by the formula\n\n\\[\n\\mathrm{p}(X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x \\mathbin{\\mkern-0mu,\\mkern-0mu}Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) =\n\\tfrac{3}{8\\,\\pi}\\, \\mathrm{e}^{-\\frac{1}{2} (x-1)^2-(y-1)^2}+\n\\tfrac{3}{64\\,\\pi}\\,\\mathrm{e}^{-\\frac{1}{32} (x-2)^2-\\frac{1}{2} (y-4)^2}+ \\tfrac{1}{40\\,\\pi}\\,\\mathrm{e}^{-\\frac{1}{8} (x-5)^2-\\frac{1}{5} (y-2)^2}\n\\]\n\nand suppose we would like to visualize the marginal density for \\(X\\),  \\(\\mathrm{p}(X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\). In order to represent it with a line plot, we would first have to calculate the integral of the formula above over all possible values of \\(Y\\):\n\n\\[\n\\mathrm{p}(X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) =\n\\int_{-\\infty}^{\\infty}\n\\Bigl[\n\\tfrac{3}{8\\,\\pi}\\, \\mathrm{e}^{-\\frac{1}{2} (x-1)^2-(y-1)^2}+\n\\tfrac{3}{64\\,\\pi}\\,\\mathrm{e}^{-\\frac{1}{32} (x-2)^2-\\frac{1}{2} (y-4)^2}+ \\tfrac{1}{40\\,\\pi}\\,\\mathrm{e}^{-\\frac{1}{8} (x-5)^2-\\frac{1}{5} (y-2)^2}\n\\Bigr]\n\\, \\mathrm{d}y\n\\]\n\nBut suppose, instead, that we have stored the points used to represent the density as a scatter plot, as in fig.  15.2. Each of these points is a pair of coordinates \\((x,y)\\), representing an \\(X\\)-value and a \\(Y\\)-value. It turns out that these same points can be used to make a scatter-plot of the marginal density for \\(X\\), by considering their \\(x\\)-coordinates only. Often we use a subsample (unsystematically chosen) of them, so that the resulting one-dimensional scatter plot doesn’t become too congested and difficult to read.\nAs an example, here is a scatter plot for the marginal density \\(\\mathrm{p}(X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\) above, obtained by selecting a subset of 400 points from the scatter plot (fig.  15.2) for \\(\\mathrm{p}(X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x \\mathbin{\\mkern-0mu,\\mkern-0mu}Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\); the points are replaced by vertical lines for better visibility:\n\n\n\nFigure 16.1: Scatter-plot representation of the marginal probability density \\(\\mathrm{p}(X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\)\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nThe points for the scatter plot of fig.  15.2 (§  15.4) are saved in the file scatterXY_samples.csv. Use them to represent the marginal probability density \\(\\mathrm{p}(Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\), for the other quantity \\(Y\\), as a scatter plot."
  },
  {
    "objectID": "marginal_probability.html#sec-use-pitfall-marginal",
    "href": "marginal_probability.html#sec-use-pitfall-marginal",
    "title": "16  Marginal probability distributions",
    "section": "16.4 Uses and pitfalls of marginal probability distributions",
    "text": "16.4 Uses and pitfalls of marginal probability distributions\nAn agent’s probability distribution for a multi-dimensional joint quantity is not easily – or at all – visualizable. This shortcoming is even worse because, as discussed in §  10.1, our intuition often fails us badly in multi-dimensional problems.\nMarginal probability distributions for one or two of the component quantities are useful because they offer us a little glimpse of the multi-dimensional “monster”. In concrete engineering and data-science problem, when we need to discuss a multi-dimensional distribution it is good practice to visually report at least its one-dimensional marginal distributions.\nIn the machine-learning literature, one frequent application of this low-dimensional glimpse is for qualitatively assessing whether two multi-dimensional distributions are similar. Their one-dimensional marginals are visually compared and, if they overlap, one hopes (and some works in the literature even conclude) that the multi-dimensional distributions are somewhat similar as well.\n\n\nUnfortunately marginal distributions can also be quite deceiving:\n\n\n\n\n\n\n Exercise\n\n\n\nHere are three different joint probability densities for the joint quantity \\((X,Y)\\), each density represented by a scatter plot with 200 points. the files containing the coordinates of the scatter-plot points are also given:\nA. File scatterXY_A.csv:\n\nB. File scatterXY_B.csv:\n\nC. File scatterXY_C.csv:\n\n\n\n\nReproduce the three scatter plots above using the points from the three files, just to confirm that they are correct.\nFor each density, plot the marginal density for the quantity \\(X\\) as a scatter plot. Use the method described in §  16.3; do not subsample the points.\nWhat can you say about the three marginal densities you obtain?\n\nDo the same, but for the marginal densities for \\(Y\\).\nWhat can you say about the three marginal densities you obtain?\nIf two joint probability distributions have the same marginals, can we conclude that they are identical, or at least similar?\n\n\n\n\n\n\n\n\n\n\n\n Study reading\n\n\n\n\n§§ 5.3.2–5.3.3 of Risk Assessment and Decision Analysis with Bayesian Networks\n§ 12.3 of Artificial Intelligence\n§§ 5.1–5.5 of Probability"
  },
  {
    "objectID": "conditional_probability.html#sec-conditional-probs",
    "href": "conditional_probability.html#sec-conditional-probs",
    "title": "17  Conditional probability and learning",
    "section": "17.1 Conditional probability: augmenting knowledge",
    "text": "17.1 Conditional probability: augmenting knowledge\nWhen we introduced the notion of degree of belief – a.k.a. probability – in chapter  8, we stressed the fact that every probability is conditional on some state of knowledge or information. So the term “conditional probability” sounds like a pleonasm, just like saying “round circle”.\nThis term must be understood in a way analogous to “marginal probability”: it applies in situations where we have two or more sentences of interest. We speak of a “conditional probability” when we want to emphasize that additional sentences appear in the conditional (right side of “\\(\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\)”) of that probability, as compared to other probabilities. For instance, in a scenario in which these two probabilities appear:\n\\[\n\\mathrm{P}(\\mathsfit{A} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{B} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I})\n\\qquad\n\\mathrm{P}(\\mathsfit{A} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\]\nwe call the first conditional probability of \\(\\mathsfit{A}\\) (given \\(\\mathsfit{B}\\)) to emphasize or point out that its conditional includes an additional sentence (\\(\\mathsfit{B}\\)), whereas the conditional of the second probability doesn’t.\nSuch emphasis is important because it also means that the “conditional” probability is based on some additional knowledge, information, or hypothesis with respect to the “non-conditional” one. This has obvious connections with the idea of “learning”. Indeed the calculation of “conditional” probabilities enters in all situations (even if hypothetical or counterfactual, see §  5.1) in which some knowledge is augmented by new knowledge. This can happens in several ways, which we now examine."
  },
  {
    "objectID": "conditional_probability.html#sec-conditional-joint-dis",
    "href": "conditional_probability.html#sec-conditional-joint-dis",
    "title": "17  Conditional probability and learning",
    "section": "17.2 Conditional from joint probability: dissimilar quantities",
    "text": "17.2 Conditional from joint probability: dissimilar quantities\nConsider once more the next-patient arrival scenario of §  15.2, with joint quantity \\((U,T)\\) and an agent’s joint probability distribution as in table  15.1. Suppose that the agent must forecast whether the next patient will require \\({\\small\\verb;urgent;}\\) or \\({\\small\\verb;non-urgent;}\\) care, so it needs to calculate the probability distribution for \\(U\\) (that is, the probabilities for \\(U\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\) and \\(U\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;non-urgent;}\\)).\nIn the first exercise of §  16.1 you found that the marginal probability that the next patient will need urgent care is\n\\[\\mathrm{P}(U\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}}) = 18\\%\\]\nthis is the agent’s degree of belief if it has the knowledge encoded in the sentence \\(\\mathsfit{I}_{\\text{H}}\\), nothing more and nothing less.\nBut now let’s imagine that the agent receives a new piece of information: it is told that the next patient is being transported by helicopter. In other words, the agent now knows that the sentence \\(T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;helicopter;}\\) is true. The agent’s complete knowledge is then encoded in the anded sentence\n\\[T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;helicopter;}\\land \\mathsfit{I}_{\\text{H}}\\]\nwhich should therefore appear in the conditional. The agent’s belief that the next patient requires urgent care is therefore\n\\[\\mathrm{P}(U\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;helicopter;}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}_{\\text{H}})\\]\nCalculation of this probability can be done by just one application of the and-rule:\n\n\\[\n\\begin{aligned}\n&\\mathrm{P}(U\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}}) =\n\\mathrm{P}(U\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;helicopter;}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}_{\\text{H}}) \\cdot\n\\mathrm{P}(T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}})\n\\\\[3ex]\n&\\quad\\implies\\quad\n\\mathrm{P}(U\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;helicopter;}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}_{\\text{H}})\n=\n\\frac{\n\\mathrm{P}(U\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}})\n}{\n\\mathrm{P}(T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}})\n}\n\\end{aligned}\n\\]\n\nWe do have the joint probability for \\(U\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\land T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;helicopter;}\\) that appears in the numerator of the fraction above. The probability in the denominator is just a marginal probability for \\(T\\), and we know how to calculate that too from §  16.1. Finally we find\n\\[\n\\mathrm{P}(U\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;helicopter;}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}_{\\text{H}})\n=\\frac{\n\\mathrm{P}(U\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}})\n}{\n\\sum_u\\mathrm{P}(U\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}u \\mathbin{\\mkern-0mu,\\mkern-0mu}T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}})\n}\n\\]\nwhere it’s understood that the sum index \\(u\\) runs over the values \\(\\set{{\\small\\verb;urgent;}, {\\small\\verb;non-urgent;}}\\).\nThis is called a conditional probability; in this case, the conditional probability of  \\(U\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\)  given  \\(T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;helicopter;}\\).\nThe collection of probabilities for all possible values of the quantity \\(U\\), given a specific value of the quantity \\(T\\), say \\({\\small\\verb;helicopter;}\\):\n\\[\n\\mathrm{P}(U\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;helicopter;}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}_{\\text{H}}) \\ ,\n\\qquad\n\\mathrm{P}(U\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;non-urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;helicopter;}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}_{\\text{H}})\n\\]\nis called the conditional probability distribution for \\(U\\)  given  \\(T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;helicopter;}\\). It is indeed a probability distribution because the two probabilities sum up to \\(1\\).\n\n\n\n\n\n\n\n\n\n\nNote that the collection of probabilities for, say, \\(U\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\), but for different values of the conditional quantity \\(T\\), that is\n\\[\n\\begin{aligned}\n&\\mathrm{P}(U\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}_{\\text{H}}) \\ ,\n\\\\[1ex]\n&\\mathrm{P}(U\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;helicopter;}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}_{\\text{H}}) \\ ,\n\\\\[1ex]\n&\\mathrm{P}(U\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;other;}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}_{\\text{H}})\n\\end{aligned}\n\\]\nis not a probability distribution. Calculate the three probabilities above and check that indeed they do not sum up to one.\n\n\n\n\n\n\n\n\n Exercise\n\n\n\n\nUsing the values from table 15.1 and the formula for marginal probabilities, calculate:\n\nThe conditional probability that the next patient needs urgent care, given that the patient is being transported by helicopter.\nThe conditional probability that the next patient is being transported by helicopter, given that the patient needs urgent care.\n\nNow discuss and find an intuitive explanation for these comparisons:\n\nThe two probabilities you obtained above. Are they equal? why or why not?\nThe marginal probability that the next patient will be transported by helicopter, with the conditional probability that the patient will be transported by helicopter given that it’s urgent. Are they equal? if not, which is higher, and why?"
  },
  {
    "objectID": "conditional_probability.html#sec-conditional-joint-sim",
    "href": "conditional_probability.html#sec-conditional-joint-sim",
    "title": "17  Conditional probability and learning",
    "section": "17.3 Conditional from joint probability: similar quantities",
    "text": "17.3 Conditional from joint probability: similar quantities\nIn the previous section we examined how knowledge about one quantity of a particular kind can change an agent’s degree of belief about a quantity of a different kind, for example “transportation” about “urgency” or vice versa. This change is reflected in the value of the corresponding conditional probability.\nThis kind of change can also occur with quantities of a “similar kind”, that is, quantities that represent the same kind of phenomenon and have exactly the same domain. The maths and calculations are identical to those we have explored, but the interpretation and application can be somewhat different.\nAs an example, imagine a scenario similar to the next-patient one above, but now consider the next three patients to arrive, and their urgency. Define the following three quantities:\n\\(U_1\\) : urgency of the next patient\n\\(U_2\\) : urgency of the second future patient from now\n\\(U_3\\) : urgency of the third future patient from now\n\nevery one of these quantities has the same domain: \\(\\set{{\\small\\verb;urgent;},{\\small\\verb;non-urgent;}}\\).\nThe joint quantity \\((U_1, U_2, U_3, U_4)\\) has a domain with 23 = 8 possible values:\n\n\\(U_1\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}U_2\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}U_3\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\)\n\\(U_1\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}U_2\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}U_3\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;non-urgent;}\\)\n. . .\n\\(U_1\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}U_2\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}U_3\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\)\n\\(U_1\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}U_2\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}U_3\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;non-urgent;}\\)\n\nSuppose that an agent, with background information \\(\\mathsfit{I}\\), has a joint probability distribution for the joint quantity \\((U_1, U_2, U_3)\\); the distribution is implicitly given as follows: \n\nIf \\({\\small\\verb;urgent;}\\) appears 0 times out of 3: probability = \\(53.6\\%\\)\nIf \\({\\small\\verb;urgent;}\\) appears 1 times out of 3: probability = \\(11.4\\%\\)\nIf \\({\\small\\verb;urgent;}\\) appears 2 times out of 3: probability = \\(3.6\\%\\)\nIf \\({\\small\\verb;urgent;}\\) appears 3 times out of 3: probability = \\(1.4\\%\\)\n\nexamples:\n\\[\n\\begin{aligned}\n&\\mathrm{P}(U_1\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}U_2\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}U_3\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n= 0.036\n\\\\[1ex]\n&\\mathrm{P}(U_1\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}U_2\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}U_3\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;non-urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n= 0.114\n\\\\[1ex]\n&\\mathrm{P}(U_1\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}U_2\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}U_3\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;non-urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n= 0.036\n\\\\[1ex]\n\\end{aligned}\n\\]\n\n\n\n\n\n\n Exercise\n\n\n\n\nCheck that the joint probability distribution as defined above indeed sums up to \\(1\\).\nCalculate the marginal probability for \\(U_1\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\), that is,  \\(\\mathrm{P}(U_1\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\\). \nCalculate the marginal probability that the second and third patients are non-urgent cases, that is\n\n\\[\\mathrm{P}(U_2\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}U_3\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;non-urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}) \\ .\\] \n\n\nFrom this joint probability distribution the agent can calculate, among other things, its degree of belief that the third patient from now will require urgent care, regardless of the urgency of the preceding two patients. It’s the marginal probability\n\\[\n\\begin{aligned}\n\\mathrm{P}(U_3\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})  &=\n\\sum_{u_1}\\sum_{u_2}\n\\mathrm{P}(U_1\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}u_1 \\mathbin{\\mkern-0mu,\\mkern-0mu}U_2\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}u_2 \\mathbin{\\mkern-0mu,\\mkern-0mu}U_3\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\\\[1ex]\n&= 0.114 + 0.036 + 0.036 + 0.014\n\\\\[1ex]\n&= \\boldsymbol{20.0\\%}\n\\end{aligned}\n\\]\nwhere the first term \\(0.114\\) in the sum corresponds to \\(U_1\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}U_2\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}U_3\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\), the second term \\(0.036\\) to \\(U_1\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}U_2\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}U_3\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\), and so on.\nTherefore the agent, right now, has a \\(20\\%\\) degree of belief that the third patient from now will require urgent care.\n\n\nNow fast-forward in time, after two patients have arrived and been taken good care of. Suppose that both were non-urgent cases, and the agent knows this. The agent needs to forecast whether the next (third) patient will require urgent care.\nIt wouldn’t be sensible to use  \\(\\mathrm{P}(U_3\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\\),  calculated above, because this degree of belief represents an agent having only the background knowledge \\(\\mathsfit{I}\\). Now, instead, the agent has additional information about the first two patients, encoded in this anded sentence:\n\\[\nU_1\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}U_2\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;non-urgent;}\n\\]\nThe relevant degree of belief is therefore the conditional probability\n\\[\n\\begin{aligned}\n&\\mathrm{P}(U_3\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} U_1\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}U_2\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I})\n\\\\[2ex]\n&\\qquad{}=\n\\frac{\n\\mathrm{P}(U_1\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}U_2\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}U_3\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}{\n\\mathrm{P}(U_1\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}U_2\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;non-urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}\n\\\\[1ex]\n&\\qquad{}=\\frac{0.114}{0.65}\n\\\\[2ex]\n&\\qquad{}\\approx\n\\boldsymbol{17.5\\%}\n\\end{aligned}\n\\]\nThis conditional probability of \\(17.5\\%\\) for \\(U_3\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;non-urgent;}\\) is lower than the marginal one \\(20.0\\%\\) calculated previously. Observation of two patients has thus affected the agent’s degree of belief.\n\n\nLet’s also check how the agent’s belief changes in the case where the first two patients are both urgent. The calculation is completely analogous:\n\\[\n\\begin{aligned}\n&\\mathrm{P}(U_3\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} U_1\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}U_2\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I})\n\\\\[2ex]\n&\\qquad{}=\n\\frac{\n\\mathrm{P}(U_1\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}U_2\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}U_3\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}{\n\\mathrm{P}(U_1\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}U_2\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}\n\\\\[1ex]\n&\\qquad{}=\\frac{0.030}{0.107}\n\\\\[2ex]\n&\\qquad{}\\approx\n\\boldsymbol{28.0\\%}\n\\end{aligned}\n\\]\nIn this case the conditional probability \\(28.0\\%\\) for \\(U_3\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\) is higher than the marginal one \\(20.0\\%\\).\nOne possible intuitive explanation of these probability changes, in the present scenario, is that observation of two non-urgent cases makes the agent slightly more confident that “this is a day with few urgent cases”; whereas observation of two urgent cases makes the agent more confident that “this is a day with many urgent cases”.\n\n\n\n\n\n\n\n\n\n\nIn general we cannot say that the probability of a particular value (such as \\({\\small\\verb;urgent;}\\) in the scenario above) will decrease or increase as similar or dissimilar values are observed, nor how much the increase or decrease will be.\nIn a different situation the probability of \\({\\small\\verb;urgent;}\\) could actually increase as more and more \\({\\small\\verb;non-urgent;}\\) cases are observed. Imagine, for instance, a scenario where the agent initially knows that there are 10 urgent and 90 non-urgent cases ahead. Having observed 90 non-urgent cases, the agent will give a much higher probability – 100% – that the next case will be an urgent one.\nThe differences among such scenarios are reflected in differences of the joint probabilities, from which the conditional probabilities are calculated.\nAll these situations are correctly handled with the four fundamental rules of inference and the formula for conditional probability derived from them.\n\n\n\n\n\n\n\n\n Exercises\n\n\n\n\nUsing the same joint distribution above, calculate\n\\[\\mathrm{P}(U_1\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} U_2\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}U_3\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I})\\]\nthat is, the probability that the next patient will require urgent care given that the agent knows the second and third patients will not-require urgent care.\n\nWhy is the value obtained different from  \\(\\mathrm{P}(U_1\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\\) ?\nDescribe a scenario in which the calculation above makes sense (and patients 2 and 3 still arrive after patient 1).\n\n\n\n\n\nDo an analysis completely analogous to the previous, three-patient one, but with different background information \\(\\mathsfit{J}\\) that gives a joint probability distribution for \\((U_1, U_2, U_3)\\) as follows:\n• If \\({\\small\\verb;urgent;}\\) appears 0 times out of 3: probability = \\(0\\%\\)\n• If \\({\\small\\verb;urgent;}\\) appears 1 times out of 3: probability = \\(24.5\\%\\)\n• If \\({\\small\\verb;urgent;}\\) appears 2 times out of 3: probability = \\(7.8\\%\\)\n• If \\({\\small\\verb;urgent;}\\) appears 3 times out of 3: probability = \\(3.1\\%\\)\n\nCalculate\n\\[\\mathrm{P}(U_3\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{J})\\] \nand\n\\[\\mathrm{P}(U_3\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} U_1\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}U_2\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{J})\\] and compare them.\nExplain why this particular change in degree of belief occurs, in this situation."
  },
  {
    "objectID": "conditional_probability.html#sec-conditional-joint-general",
    "href": "conditional_probability.html#sec-conditional-joint-general",
    "title": "17  Conditional probability and learning",
    "section": "17.4 General case: conditional from joint",
    "text": "17.4 General case: conditional from joint\nTake the time to review the two sections above, focusing on the application and meaning of the two scenarios and calculations, and noting the similarities and differences:\n\n The calculations were completely analogous; in particular, the conditional probability was obtained as the quotient of a joint probability and a marginal one.\n In the first (next-patient) scenario, information about one aspect of the situation changed the agent’s belief about another aspect; the two aspects were somewhat different (transportation and urgency). Whereas in the second (three-patient) scenario, information about analogous occurrences of an aspect of the situation changed the agent’s belief about a further occurrence.\n\n\n\nA third scenario is also possible, which combines the two above. Consider the case with three patients, where each patient can require \\({\\small\\verb;urgent;}\\) care or not, and can be transported by \\({\\small\\verb;ambulance;}\\), \\({\\small\\verb;helicopter;}\\), or \\({\\small\\verb;other;}\\) means. To describe this situation, introduce three pairs of quantities, which together form the joint quantity\n\\[\n(U_1, T_1, \\ U_2, T_2, \\ U_3, T_3)\n\\]\nwhose meaning should now be obvious. This joint quantity has \\((2\\cdot 3)^3 = 216\\) possible values, corresponding to all urgency & transportation combinations for the three patients.\nGiven the joint probability distribution for this joint quantity, it is possible to calculate all kinds of conditional probabilities, which reflect the knowledge that the agent may have acquired. For instance, suppose the agent has observed that\n\nthe first two patients have not required urgent care\nthe first patient was transported by ambulance\nthe second patient was transported by other means\nthe third patient is arriving by ambulance\n\nand needs to infer whether the third patient will require urgent care. The required probability is\n\n\\[\n\\begin{aligned}\n&\\mathrm{P}(U_3\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T_3\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0mu,\\mkern-0mu}\nU_1\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}T_1\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0mu,\\mkern-0mu}\nU_2\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}T_2\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;other;}\\mathbin{\\mkern-0mu,\\mkern-0mu}\n\\mathsfit{I})\n\\\\[2ex]\n&\\qquad{}=\n\\frac{\n\\mathrm{P}(U_3\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}T_3\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0mu,\\mkern-0mu}\nU_1\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}T_1\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0mu,\\mkern-0mu}\nU_2\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}T_2\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;other;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\mathsfit{I})\n}{\n\\mathrm{P}(T_3\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0mu,\\mkern-0mu}\nU_1\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}T_1\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0mu,\\mkern-0mu}\nU_2\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}T_2\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;other;}\\mathbin{\\mkern-0mu,\\mkern-0mu}\n\\mathsfit{I})\n}\n\\end{aligned}\n\\]\n\nand is calculated in a way completely analogous to the ones already seen.\n\n\nAll three kinds of inference scenarios frequently occur in data science and engineering. In machine learning, the second scenario is connected to “unsupervised learning”; the third, mixed one to “supervised learning”. As you just saw, the probability calculus “sees” all of the them as analogous: information about something changes the agent’s belief about something else. And the handling of all three cases is perfectly covered by the four fundamental rules of inference.\n\n\nLet’s now consider a more generic case of a joint quantity with component quantities \\(\\color[RGB]{34,136,51}X\\) and \\(\\color[RGB]{238,102,119}Y\\), whose joint probability distribution is given. The two quantities could be complicated joint quantities themselves. The conditional probability for \\(\\color[RGB]{238,102,119}Y\\), given that \\(\\color[RGB]{34,136,51}X\\) has some specific value \\(\\color[RGB]{34,136,51}x^*\\), is then\n\\[\n\\mathrm{P}({\\color[RGB]{238,102,119}Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x^*}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}) =\n\\frac{\n\\mathrm{P}({\\color[RGB]{238,102,119}Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y} \\mathbin{\\mkern-0mu,\\mkern-0mu}{\\color[RGB]{34,136,51}X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x^*}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\n}{\n\\sum_{\\color[RGB]{238,102,119}\\upsilon}\\mathrm{P}({\\color[RGB]{238,102,119}Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\upsilon}\\mathbin{\\mkern-0mu,\\mkern-0mu}{\\color[RGB]{34,136,51}X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x^*}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\n}\n\\tag{17.1}\\]\nfor all possible values \\(\\color[RGB]{238,102,119}y\\)."
  },
  {
    "objectID": "conditional_probability.html#sec-conditional-conditional",
    "href": "conditional_probability.html#sec-conditional-conditional",
    "title": "17  Conditional probability and learning",
    "section": "17.5 Conditional from conditional probability",
    "text": "17.5 Conditional from conditional probability\nAs emphasized in §  5.2, probabilities are either obtained from other probabilities, or taken as given probabilities, maybe determined by symmetry requirements. This is also true when we want to calculate conditional probabilities.\nUp to now we have calculated conditional probabilities starting from the joint distribution as given, using the derived formula (17.1). In some situations, however, an agent may have given conditional probabilities together with given marginal probabilities.\nAs an example let’s consider a variation of our next-patient scenario one more time. The agent has background information \\(\\mathsfit{I}_{\\text{S}}\\) that provides the following set of probabilities:\n\nTwo conditional probability distributions  \\(\\mathrm{P}(T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\dotso \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} U\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\dotso \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}_{\\text{S}})\\) for transportation \\(T\\) given urgency \\(U\\), as reported in the following table:\n\n\n\nTable 17.1: Probability distributions for transportation given urgency\n\n\n\n\n\n\n\n\n\n\\(\\mathrm{P}(T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}t \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} U\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}u\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}_{\\text{S}})\\)\n\ntransportation at arrival  \\(T\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}{}\\)\n\n\n\n\nambulance\nhelicopter\nother\n\n\ngiven urgency  \\({}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}U\\)\nurgent\n0.61\n0.22\n0.17\n\n\nnon-urgent\n0.21\n0.01\n0.78\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis table has two probability distributions: on the first row, one conditional on \\(U\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\); on the second row, one conditional on \\(U\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;non-urgent;}\\). Check that the probabilities on each row indeed sum up to one.\n\n\n\n\n\nMarginal probability distribution  \\(\\mathrm{P}(U\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\dotso \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{S}})\\) for urgency \\(U\\):\n\n\\[\n\\mathrm{P}(U\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{S}}) = 0.18 \\ ,\n\\quad\n\\mathrm{P}(U\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;non-urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{S}}) = 0.82\n\\tag{17.2}\\]\n\n\nWith this background information, the agent can also compute all joint probabilities simply using the and-rule. For instance\n\\[\n\\begin{aligned}\n&P(U\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{S}})\n\\\\[1ex]\n&\\quad{}=\nP(T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} U\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}_{\\text{S}}) \\cdot\nP(U\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{S}})\n\\\\[1ex]\n&\\quad{}= 0.22 \\cdot 0.18 = \\boldsymbol{3.96\\%}\n\\end{aligned}\n\\]\n\n\nNote that the joint probabilities are slightly different compared with those from the previous background information \\(\\mathsfit{I}_{\\text{H}}\\).\nAnd from the joint probabilities, the marginal ones for transportation \\(T\\) can also be calculated. For instance\n\\[\n\\begin{aligned}\n&P(T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{S}})\n\\\\[1ex]\n&\\quad{}=\n\\sum_u P(T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;helicopter;}\\mathbin{\\mkern-0mu,\\mkern-0mu}U\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}u \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{S}})\n\\\\[1ex]\n&\\quad{}=\n\\sum_u P(T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} U\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}u \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}_{\\text{S}}) \\cdot\nP(U\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}u \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{S}})\n\\\\[1ex]\n&\\quad{}=\n0.22 \\cdot 0.18 +\n0.01 \\cdot 0.82\n\\\\[1ex]\n&\\quad{}= \\boldsymbol{4.78\\%}\n\\end{aligned}\n\\]\nSuppose that the agent knows that the next patient is being transported by \\({\\small\\verb;helicopter;}\\), and needs to forecast whether \\({\\small\\verb;urgent;}\\) care will be needed. This inference is the conditional probability  \\(\\mathrm{P}(U\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;helicopter;}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}_{\\text{S}})\\), which can also be rewritten in terms of the set of probabilities initially given:\n\\[\n\\begin{aligned}\n&\\mathrm{P}(U\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;helicopter;}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}_{\\text{H}})\n\\\\[2ex]\n&\\quad{}=\\frac{\n\\mathrm{P}(U\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}})\n}{\n\\mathrm{P}(T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}})\n}\n\\\\[1ex]\n&\\quad{}=\\frac{\nP(T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} U\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}_{\\text{S}}) \\cdot\nP(U\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{S}})\n}{\n\\sum_u P(T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} U\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}u \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}_{\\text{S}}) \\cdot\nP(U\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}u \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{S}})\n}\n\\\\[1ex]\n&\\quad{}=\\frac{0.0396}{0.0478}\n\\\\[2ex]\n&\\quad{}=\\boldsymbol{82.8\\%}\n\\end{aligned}\n\\]\nThis calculation has been slightly more involved than the one in §  17.2 because the joint probabilities were not directly available. Our calculation involved the steps  “ \\(T\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}U \\longrightarrow T\\land U \\longrightarrow U\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}T\\) ”.\n\n\nIf the agent were instead interested, say, in forecasting the transportation means knowing that the next patient requires urgent care, then the relevant degree of belief  \\(\\mathrm{P}(T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\dotso \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} U\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}_{\\text{S}})\\) would be immediately available and no calculations would be needed."
  },
  {
    "objectID": "conditional_probability.html#sec-conditional-conditional-general",
    "href": "conditional_probability.html#sec-conditional-conditional-general",
    "title": "17  Conditional probability and learning",
    "section": "17.6 General case: conditional from conditional",
    "text": "17.6 General case: conditional from conditional\nThe example from the previous section can be easily generalized. Consider a joint quantity with component quantities \\(\\color[RGB]{34,136,51}X\\) and \\(\\color[RGB]{238,102,119}Y\\). The probabilities  \\(\\mathrm{P}({\\color[RGB]{34,136,51}X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\dotso} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{238,102,119}Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\dotso} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I})\\)  and  \\(\\mathrm{P}({\\color[RGB]{238,102,119}Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\dotso} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\)  are given.\nThe conditional probability for \\(\\color[RGB]{238,102,119}Y\\), given that \\(\\color[RGB]{34,136,51}X\\) has some specific value \\(\\color[RGB]{34,136,51}x^*\\), is then\n\\[\n\\mathrm{P}({\\color[RGB]{238,102,119}Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x^*}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}) =\n\\frac{\n\\mathrm{P}( {\\color[RGB]{34,136,51}X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x^*}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{238,102,119}Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}) \\cdot\n\\mathrm{P}( {\\color[RGB]{238,102,119}Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\n}{\n\\sum_{\\color[RGB]{238,102,119}\\upsilon}\n\\mathrm{P}( {\\color[RGB]{34,136,51}X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x^*}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{238,102,119}Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\upsilon} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}) \\cdot\n\\mathrm{P}( {\\color[RGB]{238,102,119}Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\upsilon} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\n}\n\\tag{17.3}\\]\nfor all possible values \\(\\color[RGB]{238,102,119}y\\).\nIn the above formula we recognize Bayes’s theorem from §  9.5.\nThis formula is often exaggeratedly emphasized in the literature; some texts even present it as an “axiom” to be used in situations such as the present one. But we see that it is simply a by-product of the four fundamental rules of inference in a specific situation. An AI agent who knows the four fundamental inference rules, and doesn’t know what “Bayes’s theorem” is, will nevertheless arrive at this very formula."
  },
  {
    "objectID": "conditional_probability.html#sec-conditional-dens",
    "href": "conditional_probability.html#sec-conditional-dens",
    "title": "17  Conditional probability and learning",
    "section": "17.7 Conditional densities",
    "text": "17.7 Conditional densities\nThe discussion so far about conditional probabilities extends to conditional probability densities, in the usual way explained in §§ 15.3 and 16.2.\nIf \\(\\color[RGB]{34,136,51}X\\) and \\(\\color[RGB]{238,102,119}Y\\) are continuous quantities, the notation\n\\[\n\\mathrm{p}({\\color[RGB]{238,102,119}Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}) = {\\color[RGB]{68,119,170}q}\n\\]\nmeans that, given background information \\(\\mathsfit{I}\\) and given the sentence “\\(\\color[RGB]{34,136,51}X\\) has value between \\(\\color[RGB]{34,136,51}x-\\delta/2\\) and \\(\\color[RGB]{34,136,51}x+\\delta/2\\)”, the sentence “\\(\\color[RGB]{238,102,119}Y\\) has value between \\(\\color[RGB]{238,102,119}y-\\epsilon/2\\) and \\(\\color[RGB]{238,102,119}y+\\epsilon/2\\)” has probability \\({\\color[RGB]{68,119,170}q}\\cdot{\\color[RGB]{238,102,119}\\epsilon}\\), as long as \\(\\color[RGB]{34,136,51}\\delta\\) and \\(\\color[RGB]{238,102,119}\\epsilon\\) are small enough. Note that the small interval \\(\\color[RGB]{34,136,51}\\delta\\) for \\(\\color[RGB]{34,136,51}X\\) is not multiplied by the density \\(\\color[RGB]{68,119,170}q\\).\nThe relation between a conditional density and a joint density or a different conditional density is given by\n\\[\n\\begin{aligned}\n&\\mathrm{p}({\\color[RGB]{238,102,119}Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I})\n\\\\[1ex]\n&\\quad{}=\n\\frac{\\displaystyle\n\\mathrm{p}({\\color[RGB]{238,102,119}Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y} \\mathbin{\\mkern-0mu,\\mkern-0mu}{\\color[RGB]{34,136,51}X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}{\\displaystyle\n\\int_{\\color[RGB]{238,102,119}\\varUpsilon}\\mathrm{p}({\\color[RGB]{238,102,119}Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\upsilon} \\mathbin{\\mkern-0mu,\\mkern-0mu}{\\color[RGB]{34,136,51}X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) \\, \\mathrm{d}{\\color[RGB]{238,102,119}\\upsilon}\n}\n\\\\[1ex]\n&\\quad{}=\n\\frac{\\displaystyle\n\\mathrm{p}({\\color[RGB]{34,136,51}X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{238,102,119}Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}) \\cdot\n\\mathrm{p}({\\color[RGB]{238,102,119}Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}{\\displaystyle\n\\int_{\\color[RGB]{238,102,119}\\varUpsilon} \\mathrm{p}({\\color[RGB]{34,136,51}X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{238,102,119}Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\upsilon} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}) \\cdot\n\\mathrm{p}({\\color[RGB]{238,102,119}Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\upsilon} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\, \\mathrm{d}{\\color[RGB]{238,102,119}\\upsilon}\n}\n\\end{aligned}\n\\]\nwhere \\(\\color[RGB]{238,102,119}\\varUpsilon\\) is the domain of \\(\\color[RGB]{238,102,119}Y\\)."
  },
  {
    "objectID": "conditional_probability.html#sec-repr-conditional",
    "href": "conditional_probability.html#sec-repr-conditional",
    "title": "17  Conditional probability and learning",
    "section": "17.8 Graphical representation of conditional probability distributions and densities",
    "text": "17.8 Graphical representation of conditional probability distributions and densities\nConditional probability distributions and densities can be plotted in all the ways discussed in chapters 15 and 16. If we have two quantities \\(A\\) and \\(B\\), often we want to compare the different conditional probability distributions for \\(A\\) given different values of \\(B\\):\n\n\\(\\mathrm{P}(A\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\dotso \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} B\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;one-value;} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I})\\),\n\\(\\mathrm{P}(A\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\dotso \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} B\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;another-value;} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I})\\),\n\\(\\dotsc\\)\n\nand so on. This can be achieved by representing them by overlapping line plots, or side-by-side scatter plots, or similar ways.\n\n\nIn §  16.3 we saw that if we have the scatter plot for a joint probability density, then from its points we can often obtain a scatter plot for its marginal densities. Unfortunately no similar advantage exists for the conditional densities that can be obtained from a joint density. In theory, a conditional density for \\(Y\\), given that a quantity \\(X\\) has value in some small interval \\(\\delta\\) around \\(x\\), could be obtained by only considering scatter-plot points having \\(X\\) coordinate in a small interval between \\(x-\\delta/2\\) and \\(x+\\delta/2\\). But the number of such points is usually too small and the resulting scatter plot could be very misleading.\n\n\n\n\n\n\n\n\n Study reading\n\n\n\n\n§ 5.4 of Risk Assessment and Decision Analysis with Bayesian Networks\n§§ 12.2.1, 12.3, and 12.5 of Artificial Intelligence\n§§ 4.1–4.3 in Medical Decision Making\n§§ 5.1–5.5 of Probability – yes, once more!"
  },
  {
    "objectID": "information.html#sec-indep-sentences",
    "href": "information.html#sec-indep-sentences",
    "title": "18  Information, relevance, independence, association",
    "section": "18.1 Independence of sentences",
    "text": "18.1 Independence of sentences\nIn an ordinary situation represented by background information \\(\\mathsfit{I}\\), if you have to infer whether a coin will land heads, then knowing that it is raining outside has no impact on your inference: the information about rain is irrelevant for your inference. In other words, your degree of belief about the coin remains the same if you include the information about rain in the conditional.\nIn probability notation, representing “The coin lands heads” with \\(\\mathsfit{H}\\) and “It rains outside” with \\(\\mathsfit{R}\\), this irrelevance means\n\\[\n\\mathrm{P}(\\mathsfit{H} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = \\mathrm{P}(\\mathsfit{H} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{R} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I})\n\\]\n\n\nMore generally two sentences \\(\\mathsfit{A}\\), \\(\\mathsfit{B}\\) are said to be mutually irrelevant or logically independent given information \\(\\mathsfit{I}\\) if any one of these three conditions holds:\n\n\n“independEnt” is written with an E, not with an A.\n\n\\(\\mathrm{P}(\\mathsfit{A}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{B}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}) = \\mathrm{P}(\\mathsfit{A}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\)\n\\(\\mathrm{P}(\\mathsfit{B}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{A}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}) = \\mathrm{P}(\\mathsfit{B}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\)\n\\(\\mathrm{P}(\\mathsfit{A}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{B}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = \\mathrm{P}(\\mathsfit{A}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) \\cdot \\mathrm{P}(\\mathsfit{B}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\)\n\nThese three conditions are equivalent to one another. In the first condition, \\(\\mathrm{P}(\\mathsfit{A}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{B}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I})\\) is undefined if \\(\\mathrm{P}(\\mathsfit{B}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})=0\\), but in this case independence still holds; analogously in the second condition.\n\n\n\n\n\n\n\n\n\n\n\nIrrelevance or independence is not an absolute notion, but relative to some background knowledge. Two sentences may be independent given some background information, and not independent given another.\nIndependence as defined above is a logical, not physical, notion. It isn’t stating anything about physical dependence between phenomena related to the sentences \\(\\mathsfit{A}\\) and \\(\\mathsfit{B}\\). It’s simply stating that information about one does not affect an agent’s beliefs about the other."
  },
  {
    "objectID": "information.html#sec-indep-quantities",
    "href": "information.html#sec-indep-quantities",
    "title": "18  Information, relevance, independence, association",
    "section": "18.2 Independence of quantities",
    "text": "18.2 Independence of quantities\nThe notion of irrelevance of two sentences can be generalized to quantities. Take two quantities \\(X\\) and \\(Y\\). They are said to be mutually irrelevant or logically independent given information \\(\\mathsfit{I}\\) if any one of these three condition holds for all possible values \\(x\\) of \\(X\\) and \\(y\\) of \\(Y\\):\n\n\\(\\mathrm{P}(X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}) = \\mathrm{P}(X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\)   all \\(x,y\\)\n\\(\\mathrm{P}(Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}) = \\mathrm{P}(Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\)   all \\(x,y\\)\n\\(\\mathrm{P}(X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x \\mathbin{\\mkern-0mu,\\mkern-0mu}Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = \\mathrm{P}(X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) \\cdot \\mathrm{P}(Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\)   all \\(x,y\\)\n\n\n\nNote the difference between independence of two sentences and independence of two quantities. The latter independence involves not just two, but many sentences: as many as the values of \\(X\\) and \\(Y\\).\nIn fact it may happen that for some particular values \\(x^*\\) of \\(X\\) and \\(y^*\\) \\(Y\\) the probabilities become independent, say:\n\\[\n\\mathrm{P}(X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x^* \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y^* \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}) = \\mathrm{P}(X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x^* \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\n\\]\nbut this equality does not occur for other values. In this case the quantities \\(X\\) and \\(Y\\) are not independent given information \\(\\mathsfit{I}\\). The general idea is that two quantities are independent if knowledge about one of them cannot change an agent’s beliefs about the other, no matter what their values might be.\n\n\n\n\n\n\n\n\n\n\n\nAlso in this case, irrelevance or independence is not an absolute notion, but relative to some background knowledge. Two quantities may be independent given some background information, and not independent given another.\nAlso in this case, independence is a logical, not physical, notion. It isn’t stating anything about physical dependence between phenomena related to the quantities \\(X\\) and \\(Y\\). It’s simply stating that information about one does not affect an agent’s beliefs about the other.\n\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nConsider our familiar next-patient inference problem with quantities urgency \\(U\\) and transportation \\(T\\). Assume a different background information \\(\\mathsfit{J}\\) that leads to the following joint probability distribution:\n\n\n\n\n\n\n\n\n\n\n\n\\(\\mathrm{P}(U\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}u \\mathbin{\\mkern-0mu,\\mkern-0mu}T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}t\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{J})\\)\n\ntransportation at arrival \\(T\\)\n\n\n\n\nambulance\nhelicopter\nother\n\n\nurgency \\(U\\)\nurgent\n0.15\n0.08\n0.02\n\n\nnon-urgent\n0.45\n0.04\n0.26\n\n\n\n\nCalculate the marginal probability distribution \\(\\mathrm{P}(U\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{J})\\) and the conditional probability distribution \\(\\mathrm{P}(U \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{J})\\), and compare them. Is the value \\(T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;ambulance;}\\) relevant for inferences about \\(U\\)? \nCalculate the conditional probability distribution \\(\\mathrm{P}(U \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;helicopter;}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{J})\\), and compare it with the marginal \\(\\mathrm{P}(U\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{J})\\). Is the value \\(T\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;helicopter;}\\) relevant for inferences about \\(U\\)? \nAre the quantities \\(U\\) and \\(T\\) independent, given the background knowledge \\(\\mathsfit{J}\\)?"
  },
  {
    "objectID": "information.html#sec-info-uncertainty",
    "href": "information.html#sec-info-uncertainty",
    "title": "18  Information, relevance, independence, association",
    "section": "18.3 Information and uncertainty",
    "text": "18.3 Information and uncertainty\nThe definition of irrelevance given above appears to be very “black or white”: either two sentences or quantities are independent, or they aren’t. But in reality there is no such dichotomy. We can envisage some scenario \\(\\mathsfit{I}\\) where for instance the probabilities \\(\\mathrm{P}(Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I})\\) and \\(\\mathrm{P}(Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\) are extremely close in value, although not exactly equal:\n\\[\n\\mathrm{P}(Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I})\n= \\mathrm{P}(Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) + \\delta(x,y)\n\\]\nwith \\(\\delta(x,y)\\) very small. This would mean that knowledge about \\(X\\) modifies an agent’s belief just a little; and depending on the situation such modification could be unimportant. In this situation the two quantities would be “independent” for all practical purposes. Therefore there are degrees of relevance rather than a dichotomy “relevant vs irrelevant”.\nThis suggests that we try to quantify such degrees. This quantification would also give a measure of how “important” a quantity can be for inferences about another quantity.\nThis is the domain of Information Theory, which would require a course by itself to be properly explored. In this chapter we shall just get an overview of the main ideas and notions of this theory.\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\n\nInformation Theory, Inference, and Learning Algorithms\nElements of Information Theory\n\n\n\n\n\nThe notion of degree of relevance is important in data science and machine learning, because it rigorously quantifies two related, intuitive ideas often occurring in these fields:\n\n“Correlation” or “association”: in its general meaning, it’s the idea that if an agent’s knowledge about some quantity changes, then knowledge about another quantity may change as well.\n“Feature importance”: it’s the idea that knowledge about some aspects of a given problem may lead to improved inferences about other aspects.\n\nIn the next section we explore some tricky aspects and peculiarities of these ideas, which also tell us which kind of properties a quantitative measure for them should possess."
  },
  {
    "objectID": "information.html#sec-importance-scenarios",
    "href": "information.html#sec-importance-scenarios",
    "title": "18  Information, relevance, independence, association",
    "section": "18.4 Exploring “importance”: some scenarios",
    "text": "18.4 Exploring “importance”: some scenarios\nThe following examples are only meant to give an intuitive motivation of the characteristics that a metric for “importance” or “relevance” should have.\n\nFirst two characteristics: a lottery scenario\nA lottery comprises 1 000 000 tickets numbered from 000000 to 999999. One of these tickets is the winner. Its number is already known. You are allowed to choose any ticket you like, before the winner number is announced.\nBefore you choose, you have the possibility of getting (for free) some clues about the winning number. The clues are these:\n\nClue A:       \n\nThe first four digits of the winning number\n\nClue B:       \n\nThe 1st, 2nd, 3rd, and 5th digits of the winning number\n\nClue C:       \n\nThe last three digits of the winning number\n\n\nNow consider the following three “clue scenarios”.\n\nScenario 1: choose one clue\nYou have the possibility of choosing one of the three clues above. Which would you choose, in order of importance?\nObviously A or B are the most important, and equally important, because they increase your probability of winning from 1/1 000 000 to 1/100. C is the least important because it increases your probability of winning to 1/1000.\n\n\nScenario 2: discard one clue\nAll three clues are put in front of you (but you can’t see their digits). If you could keep all three, then you’d win for sure because they would give you all digits of the winning number.\nYou are instead asked to discard one of the three clues, keeping the remaining too. Which would you discard, in order of least importance?\nIf you discarded A, then B and C together would give you all digits of the winning number; so you would still win. Analogously if you discarded B. If you discarded C, then A and B together would give you all digits but the last; so you’d have a 1/10 probability of winning.\nObviously C is the most important clue to keep, and A and B least important.\n\n\nScenario 3: discard one more clue\nIn the previous Scenario 2, we saw that discarding A or B would not alter your 100% probability of winning. Either clue could therefore be said to have “importance = 0”.\nIf you had to discard both A and B, however, your situation would suddenly become worse, with only a 1/1000 probability of winning – like choosing C in Scenario 1. Clues A and B together can therefore be said to have high “importance &gt; 0”.\nNow let’s draw some conclusions from comparing the scenarios above.\n\n\nIn Scenario 1 we found that the “importance ranking” of the clues is\nA = B &gt; C\nwhereas in Scenario 2 we found the completely opposite ranking\nC &gt; A = B\nWe conclude that\n\n\n\n\n\n\nImportance is context-dependent\n\n\n\n\nIt doesn’t make sense to ask which aspect or feature is “most important” if we don’t specify the context of its use. Important if used alone? Important if used with others? and which others?\nDepending on the context, an importance ranking could be completely reversed. A quantitative measure of “importance” must therefore take the context into account.\n\n\n\n\n\nIn Scenario 3 we found that two clues may be completely unimportant if considered individually, but extremely important if considered jointly.\nWe conclude that\n\n\n\n\n\n\nImportance is non-additive\n\n\n\n\nA quantitative measure of importance cannot be additive, that is, quantify the importance of two or more features as the sum of their individual importance.\n\n\n\n\n\n\nThird characteristic: A two-quantity scenario\nSuppose we have a discrete quantities \\(X\\) with domain \\(\\set{1,2,3,4,5,6}\\) and another discrete quantity \\(Y\\) with domain \\(\\set{1,2,3,4}\\). We want to infer the value of \\(Y\\) after we are told the value of \\(X\\).\nThe conditional probabilities for \\(Y\\) given different values of \\(X\\) are as follows (each column sums up to \\(1\\))\n\n\nTable 18.1: Example conditional distribution for two discrete quantities\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\mathrm{P}(Y\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}X\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I})\\)\n\n  \\({}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}X\\)\n\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n  \\(Y\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}{}\\)\n1\n1.00\n0.00\n0.00\n0.00\n0.00\n0.50\n\n\n2\n0.00\n1.00\n0.00\n0.00\n0.50\n0.00\n\n\n3\n0.00\n0.00\n1.00\n0.50\n0.00\n0.00\n\n\n4\n0.00\n0.00\n0.00\n0.50\n0.50\n0.50\n\n\n\n\nLet’s see what kind of inferences could occur.\nIf we observe \\(X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}1\\), then we know for sure that \\(Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}1\\); similarly for \\(X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}2\\) and \\(X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}3\\). These three values of \\(X\\) are therefore “most important” for inference about \\(Y\\). If we instead observe \\(X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}4\\), then our uncertainty about \\(Y\\) is between two of its values; similarly for \\(X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}5\\) and \\(X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}6\\), These three values of \\(X\\) are therefore “least important” for inference about \\(Y\\).\nBut we usually speak of importance of a quantity or feature, not of a specific value. So what is the “overall importance” of \\(X\\)?\nConsider again three scenarios.\n\nIn the first, we have 33% probability each of observing values \\(1\\), \\(2\\), \\(3\\) of \\(X\\), for a total of 99%; and 0.33% probability of observing each of the remaining values, for a total of 1%.\nIn this scenario we expect to make an almost exact inference about \\(Y\\), and the quantity \\(X\\) has therefore great “overall importance”.\nIn the second, the reverse happens: we have 0.33% probability each of observing values \\(1\\), \\(2\\), \\(3\\) of \\(X\\), for a total of 1%; and 33% probability of observing each of the remaining values, for a total of 99%.\nIn this scenario we expect to uncertain roughly between two values of \\(Y\\), and the quantity \\(X\\) has therefore less “overall importance”.\nIn the third, we have around 16.7% probability of observing each of the values of \\(X\\).\nThis scenario is in between the first two: we expect to make an exact inference about \\(Y\\) half of the time, and to be undecided between two values half of the time. The quantity \\(X\\) has therefore some “overall importance”: neither zero, nor as much as in the first scenario.\n\nWhat determines the “overall importance” of the quantity or feature \\(X\\) is therefore its probability distribution.\nWe conclude that\n\n\n\n\n\n\nThe importance of a quantity depends on its probability distribution\n\n\n\n\nThe importance of a quantity is not only determined by the relation between its possible values and what we need to infer, but also by the probability with which its values can occur.\nA quantitative measure of “importance” of a quantity must therefore take the probability distribution for the latter into account."
  },
  {
    "objectID": "information.html#sec-entropy-mutualinfo",
    "href": "information.html#sec-entropy-mutualinfo",
    "title": "18  Information, relevance, independence, association",
    "section": "18.5 Entropies and mutual information",
    "text": "18.5 Entropies and mutual information\nThe thought-experiments above suggest that a quantitative measure of the importance of a quantity must have at least these three characteristics:\n\ntake the context somehow into account\nbe non-additive\ntake the probability distribution for the quantity into account\n\nDo measures with such properties exist?\nThey do. Indeed they are regularly used in Communication Theory and Information Theory, owing to the properties above. They even have international standards on their definition and measurement units. Before presenting them, let’s briefly present the mother of them all.\n\nShannon entropy\nAn agent with background knowledge \\(\\mathsfit{I}\\) has a belief distribution \\(\\mathrm{P}(Y\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\\) about the (finite) quantity \\(Y\\). The agent’s uncertainty about the value of \\(Y\\) can be quantified by the Shannon entropy:\n\\[\n\\mathrm{H}(Y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) \\coloneqq-\\sum_y \\mathrm{P}(Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\, \\log_2 \\mathrm{P}(Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\]\nwhose unit is the shannon (symbol \\(\\mathrm{Sh}\\)) when the logarithm is in base 2, as above.\nShannon entropy lies at the foundation of the whole fields of Information Theory and Communication Theory, and would require a lengthy discussion. Let’s just mention some of its properties and meanings:\n\nIt also quantifies the information gained by the agent, if it acquires knowledge about the value of \\(Y\\).\nIt is always positive or zero.\nIt is zero if, and only if, the agent knows the value of \\(Y\\), that is, if the probability distribution for \\(Y\\) gives 100% to one value and 0% to all others.\nIts maximum possible value is \\(\\log_2 N\\), where \\(N\\) is the number of possible values of \\(Y\\). This maximum is attained by the uniform distribution for \\(Y\\).\nThe value (in shannons) of the Shannon entropy can be interpreted as the number of binary digits that we lack for correctly identifying the value of \\(Y\\), if the possible values were listed as integers in binary format. Alternatively, a Shannon entropy equal to  \\(h\\,\\mathrm{Sh}\\)  is equivalent to being fully uncertain among \\(2^h\\) possible alternatives.\n\n\n\n Plot of the Shannon entropy for a binary quantity \\(Y\\in\\set{1,2}\\), for different distributions \\(\\mathrm{P}(Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}1\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\)\n\n\nHere are the two most useful measures of “relevance” or “importance”, both based on the Shannon entropy:\n\n\nConditional entropy\nThe conditional entropy1 of a quantity \\(Y\\) given a quantity \\(X\\) and additional knowledge \\(\\mathsfit{I}\\), is defined as1 or “equivocation” according to ISO standard\n\n\\[\n\\mathrm{H}(Y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}) \\coloneqq\n-\\sum_x \\sum_y\n\\mathrm{P}( X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\cdot\n\\mathrm{P}(Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I})\\,\n\\log_2 \\mathrm{P}(Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I})\n\\]\n\nand, as defined above, is measured in shannons (symbol \\(\\mathrm{Sh}\\)).\nIt satisfies the three requirements: (1) different additional knowledge \\(\\mathsfit{I}\\), corresponding to different contexts, leads to different probabilities; (2) if the quantity \\(X\\) can be split into two component quantities, it can be proved that the conditional entropy given them jointly is more than the sum of the conditional entropies given them individually; (3) the probability distribution for \\(X\\) explicitly appears in its definition.\nIt has several remarkable properties:\n\nIf knowledge of \\(Y\\) is completely determined by that of \\(X\\), that is, if \\(Y\\) is a function of \\(X\\), then the conditional entropy \\(\\mathrm{H}(Y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I})\\) is zero. Vice versa, if the conditional entropy is zero, then \\(Y\\) must be a function of \\(X\\).\nIf knowledge of \\(X\\) is irrelevant, in the sense of §  18.2, to knowledge of \\(Y\\), then the conditional entropy \\(\\mathrm{H}(Y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I})\\) takes on its maximal value, determined by the marginal probability for \\(Y\\). Vice versa, if the conditional entropy takes on its maximal value, then \\(X\\) is irrelevant to \\(Y\\).\nThe value (in shannons) of the conditional entropy has the same meaning as for the Shannon entropy: if the conditional entropy amounts to \\(h\\,\\mathrm{Sh}\\), then after knowing \\(X\\) our uncertainty about \\(Y\\) is the same as if we were fully uncertain among \\(2^h\\) possible alternatives.\n\nFor instance, in the case of table  18.1, the conditional entropy has the following values in the three scenarios:\n\n\\(\\mathrm{H}(Y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}_1) = 0.01\\,\\mathrm{Sh}\\) , almost zero; indeed \\(Y\\) can almost be considered a function of \\(X\\) in this case.\n\\(\\mathrm{H}(Y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}_2) = 0.99\\,\\mathrm{Sh}\\) , almost 1; indeed in this case we are approximately uncertain between two values of \\(Y\\).\n\\(\\mathrm{H}(Y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}_3) = 0.5\\,\\mathrm{Sh}\\) ; indeed this case is intermediate between the previous two.\n\n\n\nMutual information\nSuppose that, according to background knowledge \\(\\mathsfit{I}\\), for any value of \\(X\\) there’s a 100% probability that \\(Y\\) has one and the same value, say \\(Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}1\\). The conditional entropy \\(\\mathrm{H}(Y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I})\\) is then zero. In this case it is true that \\(Y\\) is formally a function of \\(X\\). But it is also true that we could perfectly predict \\(Y\\) without any knowledge of \\(X\\). The observed value of \\(X\\) didn’t really help us in forecasting \\(Y\\); in other words, \\(X\\) is not relevant for inference about \\(Y\\).22 There’s no contradiction with the second “remarkable property” previously discussed: in this case the maximal value that the conditional entropy can take is zero.\nIf we are interested in quantifying how much our knowledge about \\(X\\) “helped” in inferring \\(Y\\), we can subtract the conditional entropy for \\(Y\\) given \\(X\\) from the maximum value it would have if \\(X\\) were unavailable.\nThis is the mutual information3 of a quantity \\(Y\\) given a quantity \\(X\\) and additional knowledge \\(\\mathsfit{I}\\). It is defined as3 or “mean transinformation content” according to ISO standard\n\n\\[\n\\mathrm{H}(Y : X\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) \\coloneqq\n\\sum_x \\sum_y\n\\mathrm{P}(Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y \\mathbin{\\mkern-0mu,\\mkern-0mu}X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\,\n\\log_2 \\frac{\\mathrm{P}(Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y \\mathbin{\\mkern-0mu,\\mkern-0mu}X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})}{\n\\mathrm{P}(Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\cdot\n\\mathrm{P}(X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}\n\\]\n\nand, as defined above, is also measured in shannons. It also satisfies the three requirements for a measure of “importance”. Its properties are somehow complementary to those of the conditional entropy:\n\nIf \\(Y\\) and \\(X\\) are independent, in the sense of §  18.2, then their mutual information is zero. Vice versa, if it is zero, then \\(Y\\) and \\(X\\) are independent.\nIf knowledge of \\(Y\\) is completely determined by that of \\(X\\), that is, if \\(Y\\) is a function of \\(X\\), then their mutual information attains its maximal value (which could be zero). Vice versa, if the mutual information attains its maximal value, then \\(Y\\) must be a function of \\(X\\).\nIf the mutual information between \\(Y\\) and \\(X\\) amounts to \\(h\\,\\mathrm{Sh}\\), then knowledge of \\(X\\) reduces, on average, \\(2^h\\)-fold times the possibilities regarding the value of \\(Y\\).\nIt is symmetric in the roles of \\(X\\) and \\(Y\\), that is, \\(\\mathrm{H}(Y : X\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = \\mathrm{H}(X : Y\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\).\n\nIn the case of table  18.1, the mutual information has the following values in the three scenarios:\n\n\\(\\mathrm{H}(Y : X\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_1) = 1.61\\,\\mathrm{Sh}\\) , almost equal to the maximal value achievable in this scenario (\\(1.62\\,\\mathrm{Sh}\\)); indeed \\(Y\\) can almost be considered a function of \\(X\\) in this case. Since \\(2^{1.61}\\approx 2.1\\), knowledge of \\(X\\) roughly halves the number of possible values of \\(Y\\).\n\\(\\mathrm{H}(Y : X\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_2) = 0.81\\,\\mathrm{Sh}\\) ; this means that knowledge of \\(X\\) reduces by \\(2^{0.81}\\approx 1.8\\) or almost 2 times the number of possible values of \\(Y\\).\n\\(\\mathrm{H}(Y : X\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_3) = 1.5\\,\\mathrm{Sh}\\) ; knowledge of \\(X\\) reduces by \\(2^{1.5} \\approx 2.8\\) or almost 3 times the number of possible values of \\(Y\\).\n\n\n\nUses\nWhether to use the conditional entropy \\(\\mathrm{H}(Y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I})\\) or the mutual information \\(\\mathrm{H}(Y : X\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\) depends on the question we are asking.\nConditional entropy is the right choice if we want to quantify how much \\(Y\\) can be considered as a function of \\(X\\) – including the special case of a constant function. It is also the right choice if we want to know how many binary-search iterations it would take to find \\(Y\\), on average, once \\(X\\) is known.\nMutual information is the right choice if we want to quantify how much the knowledge of \\(X\\) helps, on average, on finding \\(Y\\), or equivalently how many additional binary-search iterations it would take to find \\(Y\\), if \\(X\\) were not known. This therefore includes quantifying “correlation” or “association”.\n\n\n\n\n\n\n\n\n\n\nThe widely used Pearson correlation coefficient is actually a very poor measure of correlation or association; it is more a measure of “linearity” than correlation. It can be very dangerous to rely on in data-science and machine-learning problems, where we can expect non-linearity and peculiar associations in wide dimensions. It is widely used not because it’s good, but because of intellectual inertia.\n\n\nIf we simply want to rank the relative importance of alternative quantities \\(X_1\\), \\(X_2\\), etc. in inferring \\(Y\\), then the two measures are equivalent and yield the same ranking, since they basically differ by a common zero point.\n\n\n\n\n\n\n Study reading\n\n\n\n\nChapter 8 of Information Theory, Inference, and Learning Algorithms\n§ 12.4 of Artificial Intelligence"
  },
  {
    "objectID": "information.html#sec-utility-importance",
    "href": "information.html#sec-utility-importance",
    "title": "18  Information, relevance, independence, association",
    "section": "18.6 Utility Theory to quantify relevance and importance",
    "text": "18.6 Utility Theory to quantify relevance and importance\nThe entropy-based measures discussed in the previous section originate from, have deep connections with, the problem of repeated communication or signal transmission. They do not require anything else beside joint probabilities. In a general decision problem – where an agent has probabilities and utilities – another approach is required, however.\nQuestions such as “What happens if I discard quantity \\(X\\) in my inference?” or “If I have to choose between quantity \\(U\\) and quantity \\(V\\) to condition my inference, which one should I choose?” are actually decision-making problems. They must therefore be solved using Decision Theory (this is an example of the recursive capabilities of Decision Theory, discussed in §  2.4).\nThe application of decision theory in these situations if often intuitively understandable. For example, if we need to rank the importance of quantities \\(U\\) and \\(V\\), we can calculate how much the expected utility would decrease if we discarded the one or the other.\nWe’ll come back to these questions toward the end of the course."
  },
  {
    "objectID": "2nd_connection_ML.html",
    "href": "2nd_connection_ML.html",
    "title": "19  Third connection with machine learning",
    "section": "",
    "text": "\\[\n\\DeclarePairedDelimiters{\\set}{\\{}{\\}}\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\\]\n\n\n\n\nIn chapter  11 we made a second tentative connection between the notions about probability explored until then, and notions from machine learning. We considered the possibility that a machine-learning algorithm is like an agent that has some built-in background information (corresponding to the algorithm’s architecture), has received pieces of information (corresponding to the data about perfectly known instances of the task), and is assessing a not-previously known piece of information (the outcome in a new task instance):\n\\[\n\\mathrm{P}(\\underbracket[0ex]{\\color[RGB]{238,102,119}\\mathsfit{D}_{N+1}}_{\\mathclap{\\color[RGB]{238,102,119}\\text{outcome?}}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\\underbracket[0ex]{\\mathsfit{D}_N \\land \\dotsb \\land \\mathsfit{D}_2 \\land \\mathsfit{D}_1}_{\\mathclap{\\color[RGB]{34,136,51}\\text{training data?}}}\n\\color[RGB]{0,0,0}\\land \\underbracket[0ex]{\\color[RGB]{204,187,68}\\mathsfit{I}}_{\\mathrlap{\\color[RGB]{204,187,68}\\uparrow\\ \\text{architecture?}}})\n\\]\nThe correspondence about training data and architecture seems somewhat convincing, the one about outcome will need more exploration, because it seems to involve some decision process – and we haven’t fully explored the machinery of decision-making yet.\nHaving introduced the notion of quantity in the latest chapters 12 and 13, we recognize that training data about a task instance concern some quantity and its value, so they can be expressed by a sentence like \\(D_i\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}d_i\\), where\n\n\\(i\\) is the instance: \\(1,2,\\dotsc,N\\)\n\\(D_i\\) describes the kind of data at instance \\(i\\), for example “128 × 128 image with 24-bit colour depth”\n\\(d_i\\) is the value of the data at instance \\(i\\), for example the one here at the margin\n\n\n\n\nAnd similarly for the outcome of a new task instance where the algorithm is applied for real, which we consider as instance \\(N+1\\). So we can rewrite the correspondence above as follows:\n\\[\n\\mathrm{P}(\\underbracket[0ex]{\\color[RGB]{238,102,119}D_{N+1} \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}d_{N+1}}_{\\mathclap{\\color[RGB]{238,102,119}\\text{outcome?}}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\\underbracket[0ex]{ D_N \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}d_N \\mathbin{\\mkern-0mu,\\mkern-0mu}\\dotsb \\mathbin{\\mkern-0mu,\\mkern-0mu}D_2 \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}d_2 \\mathbin{\\mkern-0mu,\\mkern-0mu}D_1 \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}d_1}_{\\mathclap{\\color[RGB]{34,136,51}\\text{training data?}}}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\underbracket[0ex]{\\color[RGB]{204,187,68}\\mathsfit{I}}_{\\mathrlap{\\color[RGB]{204,187,68}\\uparrow\\ \\text{architecture?}}})\n\\]\nThis kind of inference is what we explored in the “next-three-patients” scenario of §  17.3 and some of the following sections.\nLet’s extend this tentative connection even further.\n\n\nMachine-learning textbooks usually make a distinction between “supervised learning” and “unsupervised learning”. Unfortunately the explanation given for this distinction is sometimes misleading:\n\n  Some books say that in supervised learning the algorithm “learns a functional relationship between some kind of input and some kind of output”. This is usually not true: in the vast majority of applications there isn’t any functional relationship between input and output at all; at most only a statistical or probabilistic one. This is clear from the fact that two training datapoints can have identical inputs but different outputs (here is an example); and you remember from Calculus I that we can’t speak of a function in this case. It’s unclear how something that doesn’t exist can be learned.\nBooks that give this kind of explanation are unfortunately oversimplifying things, to the point of being incorrect1. The algorithm is actually doing something more complex – which we shall analyse in detail later.\n  Yet other books say that the distinction rests in the kind of data used for training: “input-output” pairs for supervised learning, and only “inputs” for unsupervised learning. It’s good that this description doesn’t mention “functions”, but it is still unsatisfactory, because it confuses the means with the purpose. It’s a little like saying that the difference between car and aeroplane is that the latter has wings. Sure – but why? This description misses the essential difference between these two means of transportation: they operate through different material media and exploit different kinds of physics; that’s why the second has wings.\nBooks that give this kind of explanation focus on a more “operational” kind of knowledge, which is sufficient for their specific goals. The very terms “supervised learning” and “unsupervised learning” indeed emphasize this operational side. But for our goal we need to look beyond operational differences and to understand their reasons.\n  More enlightening books explain that the distinction rests in what the algorithm needs for each new application: in supervised learning, it uses features – that is, additional information – available at each new application instance; whereas in unsupervised learning it doesn’t: no new instance-dependent information is given.\nFrom this point of view, we also see that the distinction between “supervised” and “unsupervised” becomes less sharp: we can imagine to increase the information that’s used at each new instance from zero (“unsupervised”) to larger and larger amounts (“supervised”).\n\n1 Paraphrasing M. W. Zemansky:\n“Teaching machine learning\nIs as easy as a song:\nYou think you make it simpler\nWhen you make it slightly wrong!”Going back to our tentative correspondence with inference and decision-making agents, we see a strong similarity between unsupervised & supervised learning and two kinds of inference:\n\nIn the unsupervised case, even if the quantities \\({\\color[RGB]{34,136,51}D_1}, {\\color[RGB]{34,136,51}D_2}, {\\color[RGB]{34,136,51}\\dotsc}, {\\color[RGB]{34,136,51}D_N}\\) in the known instances and in the new instance \\({\\color[RGB]{238,102,119}D_{N+1}}\\) might consist of joint or complex quantities (chapter  13), we are not interested in their possible decomposition into component quantities. So we still have the tentative connection above:\n\\[\n  \\mathrm{P}(\\underbracket[0ex]{\\color[RGB]{238,102,119}D_{N+1} \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}d_{N+1}}_{\\mathclap{\\color[RGB]{238,102,119}\\text{outcome?}}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n  \\color[RGB]{34,136,51}\\underbracket[0ex]{ D_N \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}d_N \\mathbin{\\mkern-0mu,\\mkern-0mu}\\dotsb  \\mathbin{\\mkern-0mu,\\mkern-0mu}D_1 \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}d_1}_{\\mathclap{\\color[RGB]{34,136,51}\\text{training data?}}}\n  \\color[RGB]{0,0,0}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\underbracket[0ex]{\\color[RGB]{204,187,68}\\mathsfit{I}}_{\\mathrlap{\\color[RGB]{204,187,68}\\uparrow\\ \\text{architecture?}}})\n  \\]\nThis is the kind of inference explored in the “next-three-patients” scenario of §  17.3. As another example or scenario, the agent may have been given information about a collection of images, and then tries to guess what the next image could be.\nIn the supervised case, the quantities in the known instances and in the new application are joint quantities:\n\\[\n  \\begin{gathered}\n  {\\color[RGB]{34,136,51}D_N} = ({\\color[RGB]{34,136,51}Y_N}, {\\color[RGB]{34,136,51}X_N}),\n  \\ {\\color[RGB]{34,136,51}\\dotsc},\\\n  {\\color[RGB]{34,136,51}D_1} = ({\\color[RGB]{34,136,51}Y_1}, {\\color[RGB]{34,136,51}X_1})\n  \\\\[1ex]\n  {\\color[RGB]{238,102,119}D_{N+1}} = ({\\color[RGB]{238,102,119}Y_{N+1}}, {\\color[RGB]{238,102,119}X_{N+1}})\n  \\end{gathered}\n  \\]\nand we are interested in the \\(X\\) and \\(Y\\) component quantities separately. For instance, the \\(X\\)-quantity could be an image, as in the example above, and the \\(Y\\)-quantity could be a label with domain \\(\\set{{\\small\\verb;Muppet;}, {\\small\\verb;non-Muppet;}}\\). The reason we make this separation is that, upon applying the algorithm in a new task instance, one of these component quantities, say \\(\\color[RGB]{238,102,119}X_{N+1}\\), can actually be observed by the agent; so it is known. It’s the other component quantity, \\(\\color[RGB]{238,102,119}Y_{N+1}\\), that the agent is uncertain about. The agent therefore needs to draw the following inference:\n\\[\n  \\mathrm{P}\\bigl(\n  {\\color[RGB]{238,102,119}Y_{N+1} \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y_{N+1}}\n  \\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}\n  {\\color[RGB]{238,102,119}X_{N+1} \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x_{N+1}}\\, \\mathbin{\\mkern-0mu,\\mkern-0mu}\\,\n  \\color[RGB]{34,136,51}Y_N \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y_N \\mathbin{\\mkern-0mu,\\mkern-0mu}X_N \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x_N \\mathbin{\\mkern-0mu,\\mkern-0mu}\n  \\dotsb \\mathbin{\\mkern-0mu,\\mkern-0mu}\n  Y_1 \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y_1 \\mathbin{\\mkern-0mu,\\mkern-0mu}X_1 \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x_1\n  \\color[RGB]{0,0,0}\\mathbin{\\mkern-0mu,\\mkern-0mu}{\\color[RGB]{204,187,68}\\mathsfit{I}} \\bigr)\n  \\]\nThis is the more complicated version of the “next-three-patients” scenario with “urgency & transportation”, discussed in §  17.4.\n\nAn interesting aspect of this new tentative correspondence is that there isn’t any essential difference between unsupervised and supervised cases. Consider the last probability above; using the formula for a conditional probability (§  17.1) we have\n\n\\[\n\\begin{aligned}\n    &\\mathrm{P}\\bigl(\n    {\\color[RGB]{238,102,119}Y_{N+1} \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y_{N+1}}\n    \\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}\n    {\\color[RGB]{238,102,119}X_{N+1} \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x_{N+1}}\\, \\mathbin{\\mkern-0mu,\\mkern-0mu}\\,\n    \\color[RGB]{34,136,51}Y_N \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y_N \\mathbin{\\mkern-0mu,\\mkern-0mu}X_N \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x_N \\mathbin{\\mkern-0mu,\\mkern-0mu}\n    \\dotsb \\mathbin{\\mkern-0mu,\\mkern-0mu}\n    Y_1 \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y_1 \\mathbin{\\mkern-0mu,\\mkern-0mu}X_1 \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x_1\n    \\color[RGB]{0,0,0}\\mathbin{\\mkern-0mu,\\mkern-0mu}{\\color[RGB]{204,187,68}\\mathsfit{I}} \\bigr)\n    \\\\[2ex]\n    &\\qquad{}=\n    \\frac{\n        \\mathrm{P}\\bigl(\n    \\color[RGB]{187,187,187}\\overbracket[1px]{\n    {\\color[RGB]{238,102,119}Y_{N+1} \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y_{N+1}} \\mathbin{\\mkern-0mu,\\mkern-0mu}\n    {\\color[RGB]{238,102,119}X_{N+1} \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x_{N+1}}\n    }^\\mathclap{{\\color[RGB]{238,102,119}D_{N+1}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}d_{N+1}}}\n    \\color[RGB]{0,0,0}\n        \\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}\n        \\color[RGB]{187,187,187}\\overbracket[1px]{\n    \\color[RGB]{34,136,51}Y_N \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y_N \\mathbin{\\mkern-0mu,\\mkern-0mu}X_N \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x_N\n        }^\\mathclap{{\\color[RGB]{34,136,51}D_{N}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}d_{N}}}\\color[RGB]{34,136,51}\n    \\mathbin{\\mkern-0mu,\\mkern-0mu}\n    \\dotsb \\mathbin{\\mkern-0mu,\\mkern-0mu}\n        \\color[RGB]{187,187,187}\\overbracket[1px]{\n    \\color[RGB]{34,136,51}Y_1 \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y_1 \\mathbin{\\mkern-0mu,\\mkern-0mu}X_1 \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x_1\n        }^\\mathclap{{\\color[RGB]{34,136,51}D_{1}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}d_{1}}}\n    \\color[RGB]{0,0,0}\\mathbin{\\mkern-0mu,\\mkern-0mu}{\\color[RGB]{204,187,68}\\mathsfit{I}} \\bigr)\n}{\n     \\sum_{\\color[RGB]{238,102,119}y_{N+1}} \\mathrm{P}\\bigl(\n    {\\color[RGB]{238,102,119}Y_{N+1} \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y_{N+1}} \\mathbin{\\mkern-0mu,\\mkern-0mu}\n    {\\color[RGB]{238,102,119}X_{N+1} \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x_{N+1}}\n        \\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}\n    \\color[RGB]{34,136,51}Y_N \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y_N \\mathbin{\\mkern-0mu,\\mkern-0mu}X_N \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x_N \\mathbin{\\mkern-0mu,\\mkern-0mu}\n    \\dotsb \\mathbin{\\mkern-0mu,\\mkern-0mu}\n    Y_1 \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y_1 \\mathbin{\\mkern-0mu,\\mkern-0mu}X_1 \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x_1\n    \\color[RGB]{0,0,0}\\mathbin{\\mkern-0mu,\\mkern-0mu}{\\color[RGB]{204,187,68}\\mathsfit{I}} \\bigr)\n}\n\\end{aligned}\n\\]\n\n\n\\[\n\\begin{aligned}\n    &\\mathrm{P}\\bigl(\n    {\\color[RGB]{238,102,119}Y_{N+1} \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y_{N+1}}\n    \\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}\n    {\\color[RGB]{238,102,119}X_{N+1} \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x_{N+1}}\\, \\mathbin{\\mkern-0mu,\\mkern-0mu}\\,\n    \\color[RGB]{34,136,51}Y_N \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y_N \\mathbin{\\mkern-0mu,\\mkern-0mu}X_N \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x_N \\mathbin{\\mkern-0mu,\\mkern-0mu}\n    \\dotsb \\mathbin{\\mkern-0mu,\\mkern-0mu}\n    Y_1 \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y_1 \\mathbin{\\mkern-0mu,\\mkern-0mu}X_1 \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x_1\n    \\color[RGB]{0,0,0}\\mathbin{\\mkern-0mu,\\mkern-0mu}{\\color[RGB]{204,187,68}\\mathsfit{I}} \\bigr)\n    \\\\[2ex]\n    &\\qquad{}=\n    \\frac{\n        \\mathrm{P}\\bigl(\n    {\\color[RGB]{238,102,119}Y_{N+1} \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y_{N+1}} \\mathbin{\\mkern-0mu,\\mkern-0mu}\n    {\\color[RGB]{238,102,119}X_{N+1} \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x_{N+1}}\n    \\color[RGB]{0,0,0}\n        \\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}\n    \\color[RGB]{34,136,51}Y_N \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y_N \\mathbin{\\mkern-0mu,\\mkern-0mu}X_N \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x_N\n    \\mathbin{\\mkern-0mu,\\mkern-0mu}\n    \\dotsb \\mathbin{\\mkern-0mu,\\mkern-0mu}\n    \\color[RGB]{34,136,51}Y_1 \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y_1 \\mathbin{\\mkern-0mu,\\mkern-0mu}X_1 \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x_1\n    \\color[RGB]{0,0,0}\\mathbin{\\mkern-0mu,\\mkern-0mu}{\\color[RGB]{204,187,68}\\mathsfit{I}} \\bigr)\n}{\n     \\sum_{\\color[RGB]{238,102,119}y_{N+1}} \\mathrm{P}\\bigl(\n    {\\color[RGB]{238,102,119}Y_{N+1} \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y_{N+1}} \\mathbin{\\mkern-0mu,\\mkern-0mu}\n    {\\color[RGB]{238,102,119}X_{N+1} \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x_{N+1}}\n        \\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}\n    \\color[RGB]{34,136,51}Y_N \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y_N \\mathbin{\\mkern-0mu,\\mkern-0mu}X_N \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x_N \\mathbin{\\mkern-0mu,\\mkern-0mu}\n    \\dotsb \\mathbin{\\mkern-0mu,\\mkern-0mu}\n    Y_1 \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y_1 \\mathbin{\\mkern-0mu,\\mkern-0mu}X_1 \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x_1\n    \\color[RGB]{0,0,0}\\mathbin{\\mkern-0mu,\\mkern-0mu}{\\color[RGB]{204,187,68}\\mathsfit{I}} \\bigr)\n}\n\\\\[2ex]\n&\\mathrm{P}\\bigl(\\color[RGB]{68,119,170}\n    {Y_{N+1} \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y_{N+1}}\n    \\mathbin{\\mkern-0mu,\\mkern-0mu}\n    {X_{N+1} \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x_{N+1}}\\, \\mathbin{\\mkern-0mu,\\mkern-0mu}\\,\n    Y_N \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y_N \\mathbin{\\mkern-0mu,\\mkern-0mu}X_N \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x_N \\mathbin{\\mkern-0mu,\\mkern-0mu}\n    \\dotsb \\mathbin{\\mkern-0mu,\\mkern-0mu}\n    Y_1 \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y_1 \\mathbin{\\mkern-0mu,\\mkern-0mu}X_1 \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x_1\n    \\color[RGB]{0,0,0}\\mathbin{\\mkern-0mu,\\mkern-0mu}{\\color[RGB]{204,187,68}\\mathsfit{I}} \\bigr)\n\\end{aligned}\n\\]\n\nThis is a fraction between the probability for the “unsupervised case” (which we wrote above compactly in terms of \\(D\\) rather than \\((Y,X)\\)), and the same probability summed up for all possible values of \\(\\color[RGB]{238,102,119}Y_{N+1}\\), which in the image-example above would be a sum over \\(\\color[RGB]{238,102,119}Y_{N+1}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;Muppet;}\\) and \\(\\color[RGB]{238,102,119}Y_{N+1}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;non-Muppet;}\\).\nSo if the agent has the probability for the “unsupervised case”, then with a simple computation it also has the probability for the “supervised case”, and many other cases that cannot be categorized as “unsupervised” or “supervised” (there is also a connection with “generative” vs “discriminative” algorithms here).\n\n\nWe seem to have found a formula that should lie at the core of any machine-learning algorithm: the formula for conditional probability, which covers all possible inference cases: with or without features, with or without previous data, and many other cases. If we want that an inference algorithm which “learns” from example data, and possibly use features, shall be optimal, then it must operate on the conditional-probability formula in one way or another – at least approximately.\nWhat does such an optimal algorithm require, in order to operate in full? From the discussion of chapter  17 we see that it requires the following:\n\n A joint probability distribution about all quantities representing features and instances; past, present, and future.\nThis joint probability distribution also expresses what kind of background knowledge the algorithm assumes, and therefore the kind of problems it can be applied to. It thus forms the “architecture” and “internal parameters” of the algorithm. For example, in §  17.3 and its exercises we examined a joint distribution expressing the belief that “the more often a specific case is seen, the more probable it is at the next instance”; and also another distribution expressing the opposite belief: “the more often a specific case is seen, the less probable it is at the next instance”.\n Data from previous instances, that is, “training” or “learning” data.\n Optionally, some quantities or “feature” data about the present instance.\n\nOf these three requirements, the first seems the most difficult to implement. How to encode a belief distribution about a possibly infinite number of quantities, some of which may refer to the future? And how to choose such a belief distribution in a reasonable way?\nThis is what we discuss in the next chapters."
  },
  {
    "objectID": "populations_variates.html#sec-collections",
    "href": "populations_variates.html#sec-collections",
    "title": "20  Populations and variates",
    "section": "20.1 Collections of similar quantities",
    "text": "20.1 Collections of similar quantities\nIn engineering and data science we often face inference problems involving a collection of similar quantities (each of which can be simple, joint, or complex). “Similar” means that all such quantities have the same domain – for instance, each of them has possible values \\(\\set{{\\small\\verb;low;}, {\\small\\verb;medium;}, {\\small\\verb;high;}}\\); or each of them has possible values between \\(0\\,\\mathrm{\\textcelsius}\\) and \\(100\\,\\mathrm{\\textcelsius}\\) – and they have a similar meaning and measurement procedure. They can be considered different “instances” of the same quantity, so to speak. We saw an example in the three-patient hospital scenario of §  17.3, with the three “urgency” quantities \\(U_1\\), \\(U_2\\), \\(U_3\\). Here are other examples:\n\n\nStock exchange\n\nWe are interested in the daily change in closing price of a stock, during 100 days. Each day the change can be positive (or zero), or negative.\n\n\nThe daily change on any day can clearly be considered as a binary quantity, say with domain \\(\\set{{\\color[RGB]{34,136,51}{\\small\\verb;+;}}, {\\color[RGB]{204,187,68}{\\small\\verb;-;}}}\\). The daily changes in 100 days are a set of 100 binary quantities with exactly the same domain – even if each one can have a different value.\n\n\n\n\n\n\n\n\nMars prospecting\n\nSome robot examines 100 similar-sized rocks in a large crater on Mars. Each rock either contains hematite, or it doesn’t.\n\n\nThe hematite-content of any rock can be considered as a binary quantity, say with domain \\(\\set{{\\color[RGB]{34,136,51}{\\small\\verb;Y;}}, {\\color[RGB]{204,187,68}{\\small\\verb;N;}}}\\). The hematite contents of the 100 rocks are again a set of 100 binary quantities with exactly the same domain – even if each one can have a different value.\n\n\n\n\n\n\n\n\nGlass forensics\n\nA criminal forensics department has 215 glass fragments collected from many different crime scenes. Each fragment is characterized by a refractive index (between \\(1\\) and \\(\\infty\\)), a percentage of Calcium (between \\(0\\%\\) and \\(100\\%\\)), a percentage of Silicon (ditto), and a type of origin (for example “from window of building”, “from window of car”, and similar).\n\n\nThe refractive index, Calcium percentage, Silicon percentage, and type of the 215 fragments are a set of 215 joint quantities with exactly the same joint domain.\n\n\n\n\nIt is easy to think of many other and very diverse examples, with even more complex variates, such as images or words. We shall now try to abstract and generalize this similarity."
  },
  {
    "objectID": "populations_variates.html#sec-variates-populations",
    "href": "populations_variates.html#sec-variates-populations",
    "title": "20  Populations and variates",
    "section": "20.2 Units, variates, statistical populations",
    "text": "20.2 Units, variates, statistical populations\nConsider a large collection of entities that are somehow similar to one another. We call these entities units. These units could be, for instance:\n\nphysical objects such as cars, windmills, planets, or rocks from a particular place;\ncreatures such as animals of a particular species, or human beings, maybe with something in common such as geographical region; or plants of a particular kind;\nautomatons having a particular application;\nsoftware objects such as photos;\nabstract objects such as functions or graphs;\nthe rolls of a particular die or the tosses of a particular coin;\nthe weather conditions in several days.\n\nThese units are similar to one another in that they have some set of attributes1 common to all. These attributes can present themselves in a specific number of mutually-exclusive guises, which can be different from unit to unit. For instance, the attributes could be:1 The term features is frequently used in machine learning\n\n“colour”, each unit being, say, green, blue, or yellow;\n“mass”, each unit having a mass between \\(0.1\\,\\mathrm{kg}\\) and \\(10\\,\\mathrm{kg}\\);\n“health condition”, each unit (an animal or human in this case) being healthy or ill; or maybe being affected by one of a specific set of diseases;\ncontaining something, for instance a particular chemical substance;\n“having a label”, each unit having one of the labels A, B, C;\na complex combination of several simpler attributes like the ones above.\n\nThe units also have additional attributes (they must, otherwise we wouldn’t be able to distinguish each unit from all others), which we simply don’t consider or can’t measure. We’ll discuss this possibility later.\nFrom this description it’s clear that the attributes of each unit are a (possibly joint) quantity, as defined in §  12.1.1. Once the units and their attributes are specified, we have a set of as many quantities as there are units. All these quantities have identical domains.\nA quantity which is a collection of attributes of a set of units is called a variate. So when we speak about a variate it is understood that this is a quantity that appears, replicated, in some set of units.\n\n\nWe call a collection of units so defined a statistical population, or just population when there’s no ambiguity. The number of units is called the size of the population.\nThe notion of statistical population is extremely general and encompassing: many different things can be thought of as a population. In speaking of “data”, what is often meant is a particular statistical population. The specification of a population requires precision, especially when it is used to draw inferences, as we shall see later. A statistical population has not been properly specified until two things are precisely specified:\n\na way to determine whether something is a unit or not: inclusion and exclusion criteria, means of collection, and so on\na definition of the variate considered, its possible values, and how it is measured\n\n\n\n\n\n\n\n Exercises\n\n\n\n\nWhich of the following descriptions does properly define a statistical population? explain why it does or does not.\n\nPeople.\nElectronic components produced in a specific assembly line, since the line became operational until its discontinuation, and measured for their electric resistance, with possible values in \\([0\\,\\mathrm{\\Omega}, \\infty\\,\\mathrm{\\Omega}]\\), and for their result on a shock test, with possible values \\(\\set{{\\small\\verb;pass;}, {\\small\\verb;fail;}}\\).\nPeople born in Norway between 1st January 1990 and 31st December 2010.\nThe words contained in all websites of the internet.\nRocks, of volume between 1 cm³ and 1 m³, found in the Schiaparelli crater (as defined by contours on a map), and tested to contain hematite, with possible values \\(\\set{{\\small\\verb;Y;}, {\\small\\verb;N;}}\\).\n\nBrowse some datasets at the UC Irvine Machine Learning repository. Each dataset is a statistical population. The variate in most of these populations is a joint variate (to be discussed soon), that is, a collection of several variates.\nExamine and discuss the specification of some of those datasets:\n\nIs it well-specified what constitutes a “unit”? Are the criteria for including or excluding datapoints, their origin, and so on, well explained?\nAre the variates well-defined? Is it explained what they mean, how they were measured, what is their domain, and so on?\n\n\n\n\n\n\n\n\n\n\n\n\n Subtleties in the notion of statistical population\n\n\n\n\nA statistical population is only a conceptual device for simplifying and facing some decision or inference problem. There is no objectively-defined population “out there”.\nAny entity, object, person, and so on has some characteristics that makes it completely unique (say, its space-time coordinates), otherwise we wouldn’t be able to distinguish it from others. From this point of view any entity is just a one-member population in itself. If we consider two or more entities as being “similar” and belonging to the same population, it’s because we have decided to disregard some characteristics and focus on some others. This decision is arbitrary, a matter of convention, and depends on the specific inference and decision problem.\nTo “test” whether an entity belong to a given population, means to check if that entity satisfies the agreed-upon definition of that population.\nAny physical entity, object, person, etc. can represent different units in different and even non-overlapping statistical populations. For instance, a 100 cm³ rock found in the Schiaparelli crater on Mars could be a unit in these populations:\n\nRocks, of volume between 1 cm³ and 1 m³, found in the Schiaparelli crater and tested for hematite\nRocks, of volume between 10 cm³ and 200 cm³, found in the Schiaparelli crater and tested for hematite\nRocks, of volume between 10 cm³ and 200 m³, found in any crater on any planet of the solar system, and tested for hematite\nRocks, of volume between 1 cm³ and 1 m³, found in the Schiaparelli crater and measured for the magnitude of their magnetic field.\n\nPopulations a. b. c. above have the same variate but differ in their definition of “unit”. Populations a. and d. have the same definition of unit but different variates. Population b. is a subset of population a.: they have the same variate, and any unit in b. is also a unit in a.; but not every unit in a. is also a unit in b.. Populations a. and c. have some overlap: they have the same variate, and some units of a. are also units of c., and vice versa."
  },
  {
    "objectID": "populations_variates.html#sec-joint-variates",
    "href": "populations_variates.html#sec-joint-variates",
    "title": "20  Populations and variates",
    "section": "20.3 Populations with joint variates",
    "text": "20.3 Populations with joint variates\nThe definition of statistical population (§  20.2) makes it clear that the quantity associated with each unit can be of arbitrary complexity. In particular it could be a joint quantity (§  13.1), that is, a collection of quantities of a simpler type.\nWe saw an example at the beginning of this chapter, with a population relevant for glass forensics:\n\n\n\n\nunits: glass fragments (collected at specific locations)\nvariate: the joint variate \\((\\mathit{RI}, \\mathit{Ca}, \\mathit{Si}, \\mathit{Type})\\) consisting of four simple variates:\n\n\\(\\mathit{R}\\)efractive \\(\\mathit{I}\\)ndex of the glass fragment (interval continuous variate), with domain from \\(1\\) (included) to \\(+\\infty\\)\nweight percent of \\(\\mathit{Ca}\\)lcium in the fragment (interval discrete variate), with domain from \\(0\\) to \\(100\\) in steps of 0.01\nweight percent of \\(\\mathit{Si}\\)licon in the fragment (interval discrete variate), with domain from \\(0\\) to \\(100\\) in steps of 0.01\n\\(\\mathit{Type}\\) of glass fragment (nominal variate), with seven possible values building_windows_float_processed, building_windows_non_float_processed, vehicle_windows_float_processed, vehicle_windows_non_float_processed, containers, tableware, headlamps\n\n\nHere is a table with the values of the joint variate \\((\\mathit{RI}, \\mathit{Ca}, \\mathit{Si}, \\mathit{Type})\\) for ten units:\n\n\nTable 20.1: Glass fragments\n\n\n\n\n\n\n\n\n\nunit\n\\(\\mathit{RI}\\)\n\\(\\mathit{Ca}\\)\n\\(\\mathit{Si}\\)\n\\(\\mathit{Type}\\)\n\n\n\n\n1\n\\(1.51888\\)\n\\(9.95\\)\n\\(72.50\\)\ntableware\n\n\n2\n\\(1.51556\\)\n\\(9.41\\)\n\\(73.23\\)\nheadlamps\n\n\n3\n\\(1.51645\\)\n\\(8.08\\)\n\\(72.65\\)\nbuilding_windows_non_float_processed\n\n\n4\n\\(1.52247\\)\n\\(9.76\\)\n\\(70.26\\)\nheadlamps\n\n\n5\n\\(1.51909\\)\n\\(8.78\\)\n\\(71.81\\)\nbuilding_windows_float_processed\n\n\n6\n\\(1.51590\\)\n\\(8.22\\)\n\\(73.10\\)\nbuilding_windows_non_float_processed\n\n\n7\n\\(1.51610\\)\n\\(8.32\\)\n\\(72.69\\)\nvehicle_windows_float_processed\n\n\n8\n\\(1.51673\\)\n\\(8.03\\)\n\\(72.53\\)\nbuilding_windows_non_float_processed\n\n\n9\n\\(1.51915\\)\n\\(10.09\\)\n\\(72.69\\)\ncontainers\n\n\n10\n\\(1.51651\\)\n\\(9.76\\)\n\\(73.61\\)\nheadlamps\n\n\n\n\nThe joint-variate value for unit 4, for instance, is\n\\[\n\\mathit{RI}_{\\color[RGB]{204,187,68}4}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}1.52247 \\land\n\\mathit{Ca}_{\\color[RGB]{204,187,68}4}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}9.76 \\land\n\\mathit{Si}_{\\color[RGB]{204,187,68}4}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}70.26 \\land\n\\mathit{Type}_{\\color[RGB]{204,187,68}4}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;headlamps;}\n\\]\n\n\n\n\n\n\n\n\n\n\n\nIn the terminology we agreed upon, a variate is not a quantity. Remember that something is a quantity if it makes sense to ask “Does it have value . . . ?”. Now consider the variate \\(\\mathit{Ca}\\) (Calcium percentage) above. I say “\\(\\mathit{Ca}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}8.1\\)”. Can you check if what I said is true? No, because you don’t know which unit I’m referring to; you don’t even know if “\\(\\mathit{Ca}\\)” was referring to some unit.\nThe variate for a specific unit is a quantity instead. We can indicate this by appending the unit label to the variate symbol, as we did with “\\(\\mathit{Ca}_{\\color[RGB]{204,187,68}4}\\)” above. If I tell you “\\(\\mathit{Ca}_{\\color[RGB]{204,187,68}4}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}8\\)”, you can check that what i said is false; so \\(\\mathit{Ca}_{\\color[RGB]{204,187,68}4}\\) is a quantity.\n\n\n\n\nThe units’ IDs don’t need to be consecutive numbers; in fact they don’t even need to be numbers: any label that completely distinguishes all units will do."
  },
  {
    "objectID": "populations_variates.html#sec-marginal-pop",
    "href": "populations_variates.html#sec-marginal-pop",
    "title": "20  Populations and variates",
    "section": "20.4 Marginal populations",
    "text": "20.4 Marginal populations\nWhen a population has a joint variate, we may be interested in only a subset of the simpler variates that constitute the joint one. In the glass-forensics population above, for instance, we might be interested only in the \\(\\mathit{Ca}\\)lcium and \\(\\mathit{Type}\\) variates. These two variates together are called marginal variates and define what we can call a marginal population of the original one. A marginal population has the same units as the original one, but only a subset of the variates of the original. It is a statistical population in its own right.\nThe notion of “marginalization” is a relative notion. Any population can often be considered as the marginal of a population with the same units but additional attributes.\n\n\n\n\n\n\n\n\n Exercises\n\n\n\n\nDownload the dataset2 income_nominal_data_all.csv (4 MB):\n\nHow many variates does this population have?\nWhat types of variate (binary, nominal, etc.) do they seem to be?\nWhat are their domains?\n\nExplore datasets from the UC Irvine Machine Learning Repository, answering the three questions above.\n\n2 This is an adapted version of the UCI Adult dataset"
  },
  {
    "objectID": "statistics.html#sec-diff-prob-stat",
    "href": "statistics.html#sec-diff-prob-stat",
    "title": "21  Statistics",
    "section": "21.1 What’s the difference between Probability Theory and Statistics?",
    "text": "21.1 What’s the difference between Probability Theory and Statistics?\n“Probability theory” and “statistics” are often mentioned together. We shall soon see why, and what are the relationship between them. But first let’s try to define them more precisely:\n\nProbability theory\n\nis the theory that describes and norms the quantification and propagation of uncertainty, as we saw in §  8.1.\n\nStatistics\n\nis the study of collective properties of the variates of populations or, more generally, of collections of data.\n\n\nThere are clear and crucial differences between the two:\n\nThe fact that we are uncertain about something doesn’t mean that there are populations or replicas involved. We can apply probability theory without doing any statistics.\nIf we have full information about a population – the value of each variate for each unit – then we can calculate summaries and other properties of the variates. And there’s no uncertainty involved: at all times we can exactly calculate any information we like about any variates. So we do statistics, but probability theory plays no role (except perhaps in the form of propositional logic).\n\nMany texts do not clearly distinguish between probability and statistics. The distinction is important for us because we will have to solve problems involving the uncertainty about particular statistics, so the two must be kept clearly separate. This distinction was observed by James Clerk Maxwell who used it to develop the theories of statistical mechanics and kinetic theory.\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nMaxwell explains the statistical method and its use in the molecular description of matter:\n\nIntroductory Lecture on Experimental Physics\nMolecules\n\n\n\nIn many concrete problems, however, these do theories do go hand in hand and interact. This happens mainly in two non-mutually exclusive ways:\n\nthe statistics of a population give information that can be used in the conditional of an inference\nwe want to draw inferences about some statistics of a population, whose values we don’t know.\n\n\n\nLet’s now discuss some important statistics."
  },
  {
    "objectID": "statistics.html#sec-freq-distr",
    "href": "statistics.html#sec-freq-distr",
    "title": "21  Statistics",
    "section": "21.2 Frequencies and frequency distributions",
    "text": "21.2 Frequencies and frequency distributions\nConsider a statistical population of \\(N\\) units, with a variate \\(X\\) having a finite set of \\(K\\) values as domain. To keep things simple let’s just say these values are \\(\\set{1, 2, \\dotsc, K}\\) (without any ordering implied); the discussion applies for any finite set. The variate \\(X\\) could be of any non-continuous type: nominal, ordinal, interval, binary (§  12.2), or of a joint or complex type (§  13). Let’s denote the variate associated with unit \\(i\\) by \\(X_i\\). For instance, we express that unit #3 has \\(X\\)-variate value \\(5\\) and unit #7 has \\(X\\)-variate value \\(1\\) by writing\n\\[\nX_3 \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}5 \\land X_7 \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}1\n\\quad\\text{\\small or more compactly}\\quad\nX_3 \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}5 \\mathbin{\\mkern-0mu,\\mkern-0mu}X_7 \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}1\n\\]\nFor each value \\(a\\) in the domain of the variate \\(X\\), we count how many units in the population have that particular value, let’s call the number we find \\(n_a\\). This is the absolute frequency of the value \\(a\\) in this population. Obviously \\(n_a\\) must be an integer between \\(0\\) (included) and \\(K\\) (included). The set of absolute frequencies of all values is called the absolute frequency distribution of the variate in the population. We must have\n\\[\\sum_{a=1}^K n_a = N \\ .\\]\nIt is often useful to give not the absolute count of how often the value \\(a\\) appears in a population, but the fraction with respect to the population size, which we denote by \\(f_a\\):\n\\[f_a \\coloneqq n_a/N\\]\nThis is called the relative frequency of the value \\(a\\). Obviously \\(0 \\le f_a \\le 1\\). The collection of relative frequencies for all values, \\(\\set{f_1, f_2, \\dotsc, f_K}\\), satisfies\n\\[\\sum_{a=1}^K f_a = 1 \\ .\\]\nWe call this collection the relative frequency distribution. We shall denote it with the boldface symbol \\(\\boldsymbol{f}\\) (boldface indicates that it is a tuple of numbers):\n\\(\\boldsymbol{f} \\coloneqq(f_1, f_2, \\dotsc, f_K)\\)\nwith an analogous convention if other letters are used instead of “\\(f\\)”.\n\n\n\n\n\n\n\n\n\n\nIn the following we shall call relative frequencies simply “frequencies”, and explicitly use the word “absolute” when we speak about absolute frequencies.\n\n\n\n\nThe frequency distribution of values in a population does not give us full information about the population, because it doesn’t tell which unit has which value. In many situations, however, this is all we need to know or we can hope to know.\nFrequencies and frequency distributions are quantities in the technical sense of §  12.1.1. In fact we can say, for instance, “The frequency of the value \\(7\\) is 0.3”, or “The frequency distribution for the values \\(1,2,3\\) is \\((0.2, 0.7, 0.1)\\)”. We shall denote the quantity, as separate from its value, by the corresponding capital letter, for example \\(F_1\\), so that we can write sentences about frequencies in our usual abbreviated form. For instance\n\\[\nF_3\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}f_3\n\\]\nmeans “The frequency of the variate value \\(3\\) is equal to \\(f_3\\)”, where \\(f_3\\) must be a specific number.\n\n\n\n\n\n\n Exercise\n\n\n\nConsider the statistical population defined as follows:\n\nunits: the bookings at a specific hotel during a specific time period\nvariate: the market segment of the booking\nvariate domain: the set of five values \\(\\set{{\\small\\verb;Aviation;}, {\\small\\verb;Complementary;}, {\\small\\verb;Corporate;}, {\\small\\verb;Offline;}, {\\small\\verb;Online;}}\\)\n\nThe population data is stored in the file hotel_bookings-market.csv. Each row of the file corresponds to a unit, and lists the unit id (this is not a variate in the present population) and the market segment.\nUse any method you like (a script in your favourite programming language, counting by hand, or whatever) to answer these questions:\n\nWhat is the size of the population?\nWhat are the absolute frequencies of the five values?\nWhat are their relative frequencies?\nWhich units have the value \\({\\small\\verb;Corporate;}\\)?\n\n\n\n\nDifferences between frequencies and probabilities\nThe fact that frequencies are non-negative and sum up to 1 makes them somewhat similar to probabilities, from a purely numerical point of view. The two notions, however, are completely different and have different uses. Here is a list of some important differences:\n\n\nNot few works in machine learning tend to call “probabilities” any set of positive numbers that sum up to one. Be careful when reading them. Mentally replace probability with degree of belief and see if the text mentioning “probabilities” still makes sense.\n\n\nA probability expresses a degree of belief.\nA frequency is just the count of how many times something appears.\n\n\nThe probability of a sentence depends on an agent’s state of knowledge and background information. Two agents can assign different probabilities to the same sentence.\nThe frequency of a value in a population is an objective physical quantity. All agents agree on the frequency (if they have the possibility to counting the units it).\n\n\n\n\nProbabilities refer to sentences.\nFrequencies refer to values in a population, not to sentences (unless we are speaking of how many times a sentence appears in, say, a book; but this is a completely different and peculiar case.)\n\n\nA probability can refer to a specific unit in a population. An agent can consider, for instance, the probability that a variate for unit #7 has value 3.\nA frequency cannot refer to a specific unit in a population. It is meaningless to “count how many times the value 3 appears in unit #7”."
  },
  {
    "objectID": "statistics.html#sec-joint-freq",
    "href": "statistics.html#sec-joint-freq",
    "title": "21  Statistics",
    "section": "21.3 Joint frequencies",
    "text": "21.3 Joint frequencies\nConsider the following population consisting of ten units with joint variate \\((\\mathit{age}, \\mathit{race}, \\mathit{sex}, \\mathit{income})\\), whose component variates have the following properties:\n\n\\(\\mathit{age}\\):   interval discrete with domain \\(\\set{17, 18, \\dotsc, 90+}\\)\n\\(\\mathit{race}\\):   nominal with domain \\(\\set{{\\small\\verb;Amer-Indian-Eskimo;}, {\\small\\verb;Asian-Pac-Islander;} , {\\small\\verb;Black;}, {\\small\\verb;Other;}, {\\small\\verb;White;}}\\)\n\\(\\mathit{sex}\\):   binary with domain \\(\\set{{\\small\\verb;F;}, {\\small\\verb;M;}}\\)\n\\(\\mathit{income}\\):   binary with domain \\(\\set{{\\small\\verb;`&lt;=50K';}, {\\small\\verb;`&gt;50K';}}\\)\n\n\n\nTable 21.1: Income\n\n\n\\(\\mathit{age}\\)\n\\(\\mathit{race}\\)\n\\(\\mathit{sex}\\)\n\\(\\mathit{income}\\)\n\n\n\n\n53\n\\({\\small\\verb;White;}\\)\n\\({\\small\\verb;M;}\\)\n\\({\\small\\verb;`&gt;50K';}\\)\n\n\n53\n\\({\\small\\verb;Black;}\\)\n\\({\\small\\verb;F;}\\)\n\\({\\small\\verb;`&lt;=50K';}\\)\n\n\n48\n\\({\\small\\verb;White;}\\)\n\\({\\small\\verb;M;}\\)\n\\({\\small\\verb;`&gt;50K';}\\)\n\n\n53\n\\({\\small\\verb;White;}\\)\n\\({\\small\\verb;F;}\\)\n\\({\\small\\verb;`&gt;50K';}\\)\n\n\n53\n\\({\\small\\verb;White;}\\)\n\\({\\small\\verb;M;}\\)\n\\({\\small\\verb;`&lt;=50K';}\\)\n\n\n26\n\\({\\small\\verb;White;}\\)\n\\({\\small\\verb;M;}\\)\n\\({\\small\\verb;`&lt;=50K';}\\)\n\n\n48\n\\({\\small\\verb;White;}\\)\n\\({\\small\\verb;F;}\\)\n\\({\\small\\verb;`&gt;50K';}\\)\n\n\n53\n\\({\\small\\verb;White;}\\)\n\\({\\small\\verb;M;}\\)\n\\({\\small\\verb;`&gt;50K';}\\)\n\n\n53\n\\({\\small\\verb;Black;}\\)\n\\({\\small\\verb;M;}\\)\n\\({\\small\\verb;`&lt;=50K';}\\)\n\n\n48\n\\({\\small\\verb;Amer-Indian-Eskimo;}\\)\n\\({\\small\\verb;M;}\\)\n\\({\\small\\verb;`&gt;50K';}\\)\n\n\n\n\nThe joint frequency distribution for the joint variate of the population above gives the frequencies of all possible joint variate values, for instance the value\n\\(\\mathit{age}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}53 \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathit{race}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;Black;}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathit{sex}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;F;}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathit{income}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;`&lt;=50K';}\\)\nIn this population, most joint values appear each only once, and the remaining values never appear; this is because of the population’s small size and the large number of possible variate values. A couple of joint values appear twice. We have for example\n\\[\\begin{aligned}\n&f(\\mathit{age}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}53 \\mathbin{\\mkern-0mu,\\mkern-0mu}\n\\mathit{race}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;White;}\\mathbin{\\mkern-0mu,\\mkern-0mu}\n\\mathit{sex}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;M;}\\mathbin{\\mkern-0mu,\\mkern-0mu}\n\\mathit{income}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;`&gt;50K';}\n) = \\frac{2}{10}\n\\\\[2ex]\n&f(\\mathit{age}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}53 \\mathbin{\\mkern-0mu,\\mkern-0mu}\n\\mathit{race}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;Black;}\\mathbin{\\mkern-0mu,\\mkern-0mu}\n\\mathit{sex}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;F;}\\mathbin{\\mkern-0mu,\\mkern-0mu}\n\\mathit{income}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;`&lt;=50K';}\n) = \\frac{1}{10}\n\\\\[2ex]\n&f(\\mathit{age}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}48 \\mathbin{\\mkern-0mu,\\mkern-0mu}\n\\mathit{race}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;Amer-Indian-Eskimo;}\\mathbin{\\mkern-0mu,\\mkern-0mu}\n\\mathit{sex}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;F;}\\mathbin{\\mkern-0mu,\\mkern-0mu}\n\\mathit{income}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;`&gt;50K';}\n) = 0\n\\end{aligned}\\]\n\n\n\n\n\n\n Exercise\n\n\n\nTry to write a function that takes as input a dataset with a small number of variates and outputs the joint frequency distribution for all combinations of variate values. The best output format is a multidimensional array having one dimension per variate, and for each dimension a length equal to the number of possible values of that variate. The value of the array in each cell is the corresponding frequency.\nFor instance, consider the case of the income dataset above but without the age variate. The output of the function would then be an array with \\(5 \\times 2 \\times 2\\) dimensions"
  },
  {
    "objectID": "statistics.html#sec-marginal-freq",
    "href": "statistics.html#sec-marginal-freq",
    "title": "21  Statistics",
    "section": "21.4 Marginal frequencies",
    "text": "21.4 Marginal frequencies\nWhen a population has a joint variate, we may be interested in only a subset of the simpler variates that constitute the joint one. In the population of the example above, for instance, we might be interested only in the \\(\\mathit{age}\\) and \\(\\mathit{income}\\) variates. These two variates together are then called marginal variates and define what we can call a marginal population of the original one. A marginal population has the same units as the original one, but only a subset of the variates of the original. It is a statistical population in its own right.\nThe notion of “marginalization” is a relative notion. Any population can often be considered as the marginal of a population with the same units but additional attributes.\n\n\nGiven a statistical population with joint variates \\({\\color[RGB]{34,136,51}X}, {\\color[RGB]{238,102,119}Y}\\), we define the marginal frequency of the value \\({\\color[RGB]{238,102,119}y}\\) of \\({\\color[RGB]{238,102,119}Y}\\) as the frequency of the value \\({\\color[RGB]{238,102,119}y}\\) in the marginal population with the variate \\({\\color[RGB]{238,102,119}Y}\\) alone. This frequency is simply written\n\\[\nf({\\color[RGB]{238,102,119}Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y})\n\\]\nA conditional frequency can be calculated as the sum of the joint frequencies for all values \\({\\color[RGB]{34,136,51}x}\\), in a way analogous to marginal probabilities (§  16.1):\n\\[\nf({\\color[RGB]{238,102,119}Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y}) = \\sum_{\\color[RGB]{238,102,119}y} f({\\color[RGB]{238,102,119}Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y} \\mathbin{\\mkern-0mu,\\mkern-0mu}{\\color[RGB]{34,136,51}X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x})\n\\]\nFor example, if from the population of table  21.1 we consider the marginal population with variates \\((\\mathit{age}, \\mathit{income})\\), some of the marginal frequencies are\n\\[\\begin{aligned}\n&f(\\mathit{age}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}53 \\mathbin{\\mkern-0mu,\\mkern-0mu}\n\\mathit{income}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;`&lt;=50K';}\n) = \\frac{3}{10}\n\\\\[2ex]\n&f(\\mathit{age}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}26 \\mathbin{\\mkern-0mu,\\mkern-0mu}\n\\mathit{income}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;`&lt;=50K';}\n) = \\frac{1}{10}\n\\\\[2ex]\n&f(\\mathit{age}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}48 \\mathbin{\\mkern-0mu,\\mkern-0mu}\n\\mathit{income}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;`&gt;50K';}\n) = \\frac{3}{10}\n\\end{aligned}\\]\n\n\n\n\n\n\n\n\n Exercises\n\n\n\n\nDownload again the dataset income_nominal_data_all.csv:\n\nCalculate the marginal frequencies of some of its variates.\nDoes any variate have a value appearing with marginal absolute frequency equal to 1?"
  },
  {
    "objectID": "statistics.html#sec-summary-stat",
    "href": "statistics.html#sec-summary-stat",
    "title": "21  Statistics",
    "section": "21.5 Summary statistics",
    "text": "21.5 Summary statistics\nIn communicating statistics about a population it is always best to report and, when possible, visually show (for instance as marginal distributions) the full joint frequency distribution of the population’s variates.\nSometimes one wants to share some sort of “summary” of the frequency distribution, emphasizing particular aspects of it; because these are also aspects of the population. Different kinds of aspects can be chosen; some of them are only defined for specific types of variates. They are often called “summary statistics” or “descriptive statistics”. Below we give a brief description of some common ones, emphasizing when they are appropriate and when they are not. These summaries can also be used for probability distributions.\n\nMode\nThe mode is the value having the highest frequency (or probability, if we’re speaking about an agent’s beliefs rather than a population). There can be more than one mode.\nThe mode is defined for any distribution over discrete values, also for nominal quantities.\n\n\n\n\n\n\n\n\n\n\nBe careful in relying too much on the “mode” for a continuous quantity. Continuous quantities can be transformed in a one-to-one way into other, equivalent ones; and such a transformation also give the equivalent frequency or probability density for the new quantity. There is no general relationship between the modes of the densities for the two equivalent quantities. In fact, the density for one quantity can have one mode, whereas the density for the equivalent quantity can have no mode, or many modes. This is true for all kinds of distributions represented by densities, for example a continuous distribution of energy.\n\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nSome paradoxes, errors, and resolutions concerning the spectral optimization of human vision\n\n\n\n\nMedian and quartiles\nRecall (§  12.2) that an ordinal or an interval quantity or variate have values that can be ranked in a specific order. If there is a value for which the sum of the frequencies of all values of rank lower than that value equals the sum of the frequencies of all values of rank higher than that value, then that value is called the median of the distribution. See the following histogram as an example:\n The value \\({\\small\\verb;e;}\\) is the median of this frequency distribution, because \\(f({\\small\\verb;c;})+f({\\small\\verb;d;}) = f({\\small\\verb;f;})+f({\\small\\verb;g;})+f({\\small\\verb;h;})+f({\\small\\verb;i;})= 47.6\\%\\)\nIf there is no such separating value, then sightly different definitions of median exist in the literature; but the approximate idea is the same: a value that somehow divides the domain into two parts of roughly equal (50%) total frequency. This idea can be also applied to continuous distributions represented by densities.\nThe notion of median can be generalized to that of a value that separates the domain into a lower-rank part with total frequency 1/4, and a higher-rank part with total frequency 3/4; and also to that of a value separating into a 3/4 vs 1/4 proportion instead. These values are called the first quartile and third quartile. The two quartiles and the median (also called second quartile) divide the domain into four parts of roughly equal 25% frequencies.\nIf the variate or quantity under consideration is of interval type, then it’s possible to take the difference between the third and first quartile, called the interquartile range.\n\n\nMean and standard deviation\nFor an interval quantity \\(X\\) with values \\(\\set{x_1, x_2, \\dotsc}\\) for which it makes sense to take the sum, it is possible to define the mean and standard deviation:\n\\[\n\\bar{X} \\coloneqq\\sum_i x_i\\cdot f(x_i)\n\\qquad\n\\sigma(X) \\coloneqq\\sqrt{\\sum_i (x_i-\\bar{X})^2\\cdot f(x_i)}\n\\]\nwe assume that their meaning is more or less familiar to you.\n\n\nUses and pitfalls\nFor a nominal variate or quantity it doesn’t make sense to speak of median, quartiles, mean, standard deviation, because its possible values cannot be ranked or added.\nFor an ordinal variate or quantity it doesn’t make sense to speak of mean or standard deviation, because its possible values cannot be added.\n\n\nThe mean and standard deviation can make sense and can be useful in some circumstances. But note that even if the values of a quantity can be summed, their mean (and standard deviation) may not quite make sense.\nConsider the number of patients visiting a hospital in 100 consecutive days. It is possible to consider the mean number of patients per day. This number has a meaning: if this number of patients visited the hospital every day for 100 days, then the total number of visits would be equal to the actual total. The same reasoning can be made for the number of nurses working in the hospital every day for 100 days, and their mean.\nNow consider the daily ratios of patients to nurses, for those 100 days. These ratios are numbers, so we can take their mean. But what does such a mean represent? if we multiply it by 100, we don’t obtain the total of anything. Also, if we consider the total number of patients and total number of nurses in 100 days, their ratio will not be equal to the “mean ratio” we calculated.\nThe example above is not meant to say that a mean of ratios never makes sense, but to point out that mean and standard deviation are often overused. In chapter  23 we will discuss other problems that may arise in using mean and standard deviation.\nIn general, when in doubt, we recommend to use median and quartiles or median and interquartile range, which are more generally meaningful and enjoy several other properties (for example so-called “robustness”) useful in doing statistics.\n\n\nNote, in any case, that the present discussion regards the question of how to provide summary information besides the full frequency (or probability) distribution. If our problem is to choose one value out of the possible ones, then that’s a decision-making problem, which must be solved by specifying utilities and maximizing the expected utility, as preliminary discussed in chapter  2 and as will be discussed more in detail towards the final chapters.\n\n\n\n\n\n\n Study reading\n\n\n\n\n§ 2.6 of Risk Assessment and Decision Analysis with Bayesian Networks\n§ “The median estimate” of Meaningful expression of uncertainty in measurement\nThe Median Isn’t the Message"
  },
  {
    "objectID": "statistics.html#sec-outliers",
    "href": "statistics.html#sec-outliers",
    "title": "21  Statistics",
    "section": "21.6 Outliers vs out-of-population units",
    "text": "21.6 Outliers vs out-of-population units\nThe term “outlier” frequently appears in problems related to statistics and probability, often in conjunction with some summary statistics described above. Unfortunately the definitions of this term can be confusing or misleading. With the notion of outlier often there also comes a barrage of “methods” or rules meant to “deal” with outliers. Some such rules, for instance the rule of discarding any datapoints lying at more than three standard deviations from the mean, are often mindless and dangerous.\nSo let’s avoid the term “outlier” for the moment, and let’s take a different perspective.\n\n\nOne reason why we consider a population of units is that we are interested in making inferences about some units in this population, for which we lack the values of some variates. As we shall see in the forthcoming chapters, such inferences can be made if we first try to infer the full joint frequency distribution for the variates of the population of interest.\nThis kind of inference becomes more difficult if we have reckoned into the population some units that actually don’t belong there.\nSuppose for instance that a hospital is interested in the age of female patients admitted in a year. In collecting data, some male patients are counted in. Then obviously the age frequencies obtained from the collected data will not reflect the age frequencies among females. The problem is that some out-of-population units have been counted in by mistake.\nThe way out-of-population units affect and distort the frequency count can be different from problem to problem.\nIn our example, suppose that the wast majority of female patients could have age between 45–55 years, and that the male patients erroneously counted in also have age in the same range. Then the bulk of the frequency distribution will appear more inflated than it should be. Or suppose instead that the male patients erroneously reckoned have age between 80–90 years. In this case the old-age tail of the distribution will appear more inflated. As you see we can’t a priori point to any “tail” or “bulk” as a problem.\nLow frequencies are relatively affected by out-of-population units more than high frequencies. Suppose 10 female patients out of 100 have age 52; frequency 10%. If one 52-year-old male patient is now included by mistake, the frequency becomes 11/101 ≈ 10.1%, or a 1% relative error. But if one female patient out of 100 has age 96 (1% frequency), and a male patient of the same age is now included by mistake, the frequency becomes 2/101 ≈ 1.98%, with a 98% relative error. This is the reason some people focus on distribution tails and “outliers”, defined as data having with low-frequency values. (Note that this reasoning would concern any regions of low frequency, for example among two modes; not just tails.)\nYet we cannot mindlessly attack low-frequency regions and data just because they could be more affected by out-of-population units. In many problems of data science, engineering, medicine, low-frequency cases are the most important ones (think of rare diseases, rare mineral elements, rare astronomical events, and so on). So if we alter or eliminate low-frequency data only because they might be out-of-population units, then we have dangerously affected all our inferences about such rare events.\nMoreover, how could we judge what the “correct” frequency should be? Many outlier methods assume that the true frequency of the population has a Gaussian shape, and alter or cut the tails based on this assumption. But how can we know if such an assumption is correct? It turns out that the tails of a distribution are important for checking such assumption. Then you see the full circularity behind such mindless methods.\n\n\n\n\n\n\n Study reading\n\n\n\n§ 2.1 of Risk Assessment and Decision Analysis with Bayesian Networks\n\n\nWhich method should one use to face this problem, then? – The answer is that there’s no universal method. The approach depends on the specific problem. A data scientist must carefully examine all possible sources of out-of-population units, make inferences about them, and integrate these inferences in the general inference about the population of interest.\nThere is literature discussing first-principle approaches of this kind for different scenarios, but we cannot discuss them in the present course.\n\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nCh. 21 of Probability Theory"
  },
  {
    "objectID": "subpopulations.html#sec-subpopulations",
    "href": "subpopulations.html#sec-subpopulations",
    "title": "22  Subpopulations and conditional frequencies",
    "section": "22.1 Subpopulations",
    "text": "22.1 Subpopulations\nWhen we have a statistical population with a joint variate, it is often of interest to focus on a subset of units that share the same value of a particular variate.\nConsider for instance the following population, related to the glass-forensics example we encountered before:\n\n\n\n\nunits: glass fragments (collected at specific locations)\nvariate: the joint variate \\((\\mathit{Ca}, \\mathit{Si}, \\mathit{Type})\\) consisting of three simple variates:\n\nweight fraction of \\(\\mathit{Ca}\\)lcium in the fragment (ordinal variate), with three possible values \\(\\set{{\\small\\verb;low;}, {\\small\\verb;medium;}, {\\small\\verb;high;}}\\)\nweight fraction of \\(\\mathit{Si}\\)licon in the fragment (ordinal variate), with three possible values \\(\\set{{\\small\\verb;low;}, {\\small\\verb;medium;}, {\\small\\verb;high;}}\\)\n\\(\\mathit{Type}\\) of glass fragment (nominal variate), with seven possible values \\(\\{{\\small\\verb;building_windows_float_processed;}\\), \\({\\small\\verb;building_windows_non_float_processed;}\\), \\({\\small\\verb;containers;}\\), \\({\\small\\verb;tableware;}\\), \\({\\small\\verb;headlamps;}\\}\\)\n\n\n\n\nTable 22.1: Simplified glass-fragment population data\n\n\n\n\n\n\n\n\nunit\n\\(\\mathit{Ca}\\)\n\\(\\mathit{Si}\\)\n\\(\\mathit{Type}\\)\n\n\n\n\n1\n\\({\\small\\verb;low;}\\)\n\\({\\small\\verb;high;}\\)\n\\({\\small\\verb;headlamps;}\\)\n\n\n2\n\\({\\small\\verb;low;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;building_windows_non_float_processed;}\\)\n\n\n3\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;tableware;}\\)\n\n\n4\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;building_windows_float_processed;}\\)\n\n\n5\n\\({\\small\\verb;low;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;headlamps;}\\)\n\n\n6\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;containers;}\\)\n\n\n7\n\\({\\small\\verb;low;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;building_windows_non_float_processed;}\\)\n\n\n8\n\\({\\small\\verb;low;}\\)\n\\({\\small\\verb;high;}\\)\n\\({\\small\\verb;tableware;}\\)\n\n\n9\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;tableware;}\\)\n\n\n10\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;tableware;}\\)\n\n\n\n\nLet’s say we are interested only in units that have the \\(\\mathit{Type}\\) variate equal to \\({\\small\\verb;tableware;}\\). Discarding all others we obtain a new, smaller population with four units:\n\n\nTable 22.2: Selection according to \\(\\mathit{Type}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;tableware;}\\)\n\n\n\n\n\n\n\n\nunit\n\\(\\mathit{Ca}\\)\n\\(\\mathit{Si}\\)\n\\(\\boldsymbol{\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}}\\mathit{Type}\\)\n\n\n\n\n3\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;tableware;}\\)\n\n\n8\n\\({\\small\\verb;low;}\\)\n\\({\\small\\verb;high;}\\)\n\\({\\small\\verb;tableware;}\\)\n\n\n9\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;tableware;}\\)\n\n\n10\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;tableware;}\\)\n\n\n\n\nwere a bar “\\(\\boldsymbol{\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}}\\)” indicates the variate used for the selection.\nAs another example, we could be interested instead in those units that have both \\(\\mathit{Ca}\\) and \\(\\mathit{Si}\\) variates equal to \\({\\small\\verb;medium;}\\). We obtain a smaller population with five units:\n\n\nTable 22.3: Selection according to \\(\\mathit{Ca}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;medium;}\\) and \\(\\mathit{Si}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;medium;}\\)\n\n\n\n\n\n\n\n\nunit\n\\(\\boldsymbol{\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}}\\mathit{Ca}\\)\n\\(\\boldsymbol{\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}}\\mathit{Si}\\)\n\\(\\mathit{Type}\\)\n\n\n\n\n3\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;tableware;}\\)\n\n\n4\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;building_windows_float_processed;}\\)\n\n\n6\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;containers;}\\)\n\n\n9\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;tableware;}\\)\n\n\n10\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;tableware;}\\)\n\n\n\n\nPopulations formed in this way are called subpopulations of the original one. They are statistical populations in their own right. The notion of “subpopulation” is a relative notion. Any population can often be considered as a subpopulation of some larger population having additional variates.\n\n\n\n\n\n\n Exercise\n\n\n\n\nFrom the population of table  22.1:\n\nConstruct the subpopulation with variate \\(\\mathit{Si}\\) equal to \\({\\small\\verb;high;}\\)\nConstruct the subpopulation with variate \\(\\mathit{Type}\\) equal to \\({\\small\\verb;headlamps;}\\) and the variate \\(\\mathit{Si}\\) equal to \\({\\small\\verb;medium;}\\)\n\n\nCheck your understanding of the reasoning behind the notions of marginal population and subpopulation with this exercise:\n\nFrom the population of table  22.1, construct the subpopulation with variate \\(\\mathit{Type}\\) equal to either \\({\\small\\verb;headlamps;}\\) or \\({\\small\\verb;tableware;}\\)."
  },
  {
    "objectID": "subpopulations.html#sec-conditional-freqs",
    "href": "subpopulations.html#sec-conditional-freqs",
    "title": "22  Subpopulations and conditional frequencies",
    "section": "22.2 Conditional frequencies",
    "text": "22.2 Conditional frequencies\nGiven a statistical population with joint variates \\({\\color[RGB]{34,136,51}X}, {\\color[RGB]{238,102,119}Y}\\) (and possibly others), we define the conditional frequency of the value \\({\\color[RGB]{238,102,119}y}\\) of \\({\\color[RGB]{238,102,119}Y}\\), given or “conditional on” the value \\({\\color[RGB]{34,136,51}x}\\) of \\({\\color[RGB]{34,136,51}X}\\), as the frequency of the value \\({\\color[RGB]{238,102,119}y}\\) in the subpopulation selected by \\({\\color[RGB]{34,136,51}X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x}\\). This frequency is usually written\n\\[\nf({\\color[RGB]{238,102,119}Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x})\n\\]\nwhere \\(f\\) is the symbol for the joint frequency of the population.\nConsider for instance the glass-fragment population of table  22.1. The conditional frequency of \\({\\color[RGB]{238,102,119}\\mathit{Ca}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;low;}}\\) given \\({\\color[RGB]{34,136,51}\\mathit{Type}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;tableware;}}\\) is the (marginal) frequency of \\(\\mathit{Ca}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;low;}\\) in the subpopulation of table  22.2, from which we find\n\\[\nf({\\color[RGB]{238,102,119}\\mathit{Ca}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;low;}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}\\mathit{Type}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;tableware;}}) = \\frac{1}{4}\n\\]\nThe collection of these conditional frequencies for all values of \\({\\color[RGB]{238,102,119}Y}\\) constitutes the conditional frequency distribution of \\({\\color[RGB]{238,102,119}Y}\\) conditional on \\({\\color[RGB]{34,136,51}X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x}\\). In our example this distribution has three conditional frequencies:\n\\[\\begin{aligned}\n&f({\\color[RGB]{238,102,119}\\mathit{Ca}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;low;}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}\\mathit{Type}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;tableware;}}) = \\frac{1}{4}\n\\\\\n&f({\\color[RGB]{238,102,119}\\mathit{Ca}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;medium;}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}\\mathit{Type}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;tableware;}}) = \\frac{3}{4}\n\\\\\n&f({\\color[RGB]{238,102,119}\\mathit{Ca}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;high;}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}\\mathit{Type}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;tableware;}}) = 0\n\\end{aligned}\\]\nwhich sum up to \\(1\\) as they should.\n\n\n\n\n\n\n Conditional on a value of a variate\n\n\n\nIt doesn’t make sense to speak of the conditional frequency distribution of \\(Y\\) “conditional on \\(X\\)”. Conditional frequencies and frequency distributions are always conditional on some value of a variate. If we consider all possible values of \\(Y\\) and of \\(X\\) we obtain a collection of frequencies that is not a distribution.\n\n\nA conditional frequency can be calculated as the ratio of a joint and a marginal frequencies, in a way analogous to conditional probabilities (§  17.1):\n\\[\nf({\\color[RGB]{238,102,119}Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x}) =\n\\frac{f({\\color[RGB]{238,102,119}Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y} \\mathbin{\\mkern-0mu,\\mkern-0mu}{\\color[RGB]{34,136,51}X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x})}{f({\\color[RGB]{34,136,51}X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x})} =\n\\frac{f({\\color[RGB]{238,102,119}Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y} \\mathbin{\\mkern-0mu,\\mkern-0mu}{\\color[RGB]{34,136,51}X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x})}{\n\\sum_{\\color[RGB]{238,102,119}y} f({\\color[RGB]{238,102,119}Y\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}y} \\mathbin{\\mkern-0mu,\\mkern-0mu}{\\color[RGB]{34,136,51}X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}x})}\n\\]\n\n\n\n\n\n\n Exercises\n\n\n\nCalculate the conditional frequency distributions corresponding to the subpopulations of tables 22.2 and 22.3. For example, for table  22.2 this means calculating\n\\[\\begin{aligned}\n&f(\\mathit{Ca}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;low;} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathit{Si}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;low;} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{Type}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;tableware;})\\ ,\n\\\\\n&f(\\mathit{Ca}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;low;} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathit{Si}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;medium;} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{Type}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;tableware;})\\ ,\n\\\\\n&\\dotsc\\ ,\n\\\\\n&f(\\mathit{Ca}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;high;} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathit{Si}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;high;} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{Type}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;tableware;})\n\\end{aligned}\\]"
  },
  {
    "objectID": "subpopulations.html#sec-association",
    "href": "subpopulations.html#sec-association",
    "title": "22  Subpopulations and conditional frequencies",
    "section": "22.3 Associations",
    "text": "22.3 Associations\nThe analysis of subpopulations and conditional frequencies is important because it often reveals peculiar associations1 among different variates and groups of variates. Let’s illustrate what we mean by “association” with an example.1 In everyday language this is the same as “correlation”. The term “association” is used in statistics to avoid confusion with the Pearson correlation coefficient (see §  18.5)\nExtract the subpopulation having variate \\(\\mathit{Type}\\) equal to \\({\\small\\verb;headlamps;}\\) from the population of table  22.1:\n\n\nTable 22.4: Selection according to \\(\\mathit{Type}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;headlamps;}\\)\n\n\n\n\n\n\n\n\nunit\n\\(\\mathit{Ca}\\)\n\\(\\mathit{Si}\\)\n\\(\\boldsymbol{\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}}\\mathit{Type}\\)\n\n\n\n\n1\n\\({\\small\\verb;low;}\\)\n\\({\\small\\verb;high;}\\)\n\\({\\small\\verb;headlamps;}\\)\n\n\n5\n\\({\\small\\verb;low;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;headlamps;}\\)\n\n\n\n\nwe notice that all units have variate \\(\\mathit{Ca}\\) equal to \\({\\small\\verb;low;}\\). In terms of conditional frequencies, this means\n\\[\n\\begin{aligned}\n&f(\\mathit{Ca}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;low;} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{Type}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;headlamps;}) = 1\n\\\\\n&f(\\mathit{Ca}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;medium;} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{Type}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;headlamps;}) = 0\n\\\\\n&f(\\mathit{Ca}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;high;} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{Type}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;headlamps;}) = 0\n\\end{aligned}\n\\]\nIt is therefore impossible to observe other values of \\(\\mathit{Ca}\\) in this new population.22 We are not claiming that this fact will be true if new units are considered; this important question will be discussed later.\nOn the other hand, if we extract the subpopulation having variate \\(\\mathit{Ca}\\) equal to \\({\\small\\verb;low;}\\) we obtain\n\n\nTable 22.5: Selection according to \\(\\mathit{Ca}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;low;}\\)\n\n\n\n\n\n\n\n\nunit\n\\(\\boldsymbol{\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}}\\mathit{Ca}\\)\n\\(\\mathit{Si}\\)\n\\(\\mathit{Type}\\)\n\n\n\n\n1\n\\({\\small\\verb;low;}\\)\n\\({\\small\\verb;high;}\\)\n\\({\\small\\verb;headlamps;}\\)\n\n\n2\n\\({\\small\\verb;low;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;building_windows_non_float_processed;}\\)\n\n\n5\n\\({\\small\\verb;low;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;headlamps;}\\)\n\n\n7\n\\({\\small\\verb;low;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;building_windows_non_float_processed;}\\)\n\n\n8\n\\({\\small\\verb;low;}\\)\n\\({\\small\\verb;high;}\\)\n\\({\\small\\verb;tableware;}\\)\n\n\n\n\nwith conditional frequencies such as\n\\[\n\\begin{aligned}\n&f(\\mathit{Type}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;headlamps;} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{Ca}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;low;}) = \\frac{2}{5}\n\\\\[1ex]\n&f(\\mathit{Type}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;tableware;} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{Ca}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;low;}) = \\frac{1}{5}\n\\end{aligned}\n\\]\nand so on. The reverse is therefore not true: if \\(\\mathit{Ca}\\) is equal to \\({\\small\\verb;low;}\\), that does not mean that it’s impossible to observe other \\(\\mathit{Type}\\) values besides \\({\\small\\verb;headlamps;}\\). Note especially how these frequencies differ:\n\\[\n\\begin{gathered}\nf(\\mathit{Ca}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;low;} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{Type}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;headlamps;}) = 1\n\\\\[1ex]\nf(\\mathit{Type}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;headlamps;} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{Ca}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;low;}) = \\frac{2}{5}\n\\end{gathered}\n\\]\nIn the original population we have, figuratively speaking, the following interesting association:\n\\[\n\\mathit{Type}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;headlamps;}\\ \\mathrel{\\color[RGB]{34,136,51}\\Rightarrow}\\\n\\mathit{Ca}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;low;}\n\\qquad\\text{\\small but}\\qquad\n\\mathit{Ca}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;low;}\\  \\mathrel{\\color[RGB]{238,102,119}\\nRightarrow}\\\n\\mathit{Type}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;headlamps;}\n\\]\nThis kind of associations is often useful. Suppose for instance that you are asked to pick a unit with \\(\\mathit{Ca}\\) equal to \\({\\small\\verb;low;}\\) in the original population; but it’s difficult to measure a unit’s \\(\\mathit{Ca}\\) value, while it’s easy to measure its \\(\\mathit{Type}\\) value. Then you could instead search for a unit having \\(\\mathit{Type}\\) equal to \\({\\small\\verb;headlamps;}\\) (easier search), and you would be sure that the unit you found also has \\(\\mathit{Ca}\\) equal to \\({\\small\\verb;low;}\\).\n\n\nThe example above, where some values of a variate completely exclude some values of another, is a special one. More often we find that there are small or large changes in the frequency distribution of some variate, depending on the subpopulation considered.\n\n\n\n\n\n\n Exercise\n\n\n\n\nCalculate the (marginal) frequency distribution for the \\(\\mathit{Ca}\\) variate for the glass-fragment population of table  22.1. Is the value \\({\\small\\verb;low;}\\) more frequent than \\({\\small\\verb;medium;}\\)? or vice versa?\nCalculate the frequency distribution for \\(\\mathit{Ca}\\), conditional on \\(\\mathit{Type}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\small\\verb;tableware;}\\) (see table  22.2). How does this frequency distribution differ from the one you calculated above? Come up with possible ways to exploit this difference in concrete applications.\n\n\n\n\nAssociations can be very counter-intuitive\nIt is usually best to assess associations by explicitly calculating all relevant conditional frequencies, rather than jumping to intuitive conclusions after having examined just a few. Here’s an example.\n\n\nConsider the statistical population defined as follows:\n\n\n\n\nunits: all reparations done by a repair company on a particular kind of electronic components, which is extremely delicate and usually very difficult to repair. The population has 26 units (every unit actually represents a batch 100 reparations, so the population really refers to 2600 reparations).\na joint variate, consisting in three binary ones:\n\n\\(\\mathit{\\color[RGB]{102,204,238}Location}\\) of the repair procedure, with values \\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\) and \\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\);\nrepair \\(\\mathit{\\color[RGB]{34,136,51}Method}\\), with values \\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\) and \\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\), representing a traditional reparation method and one introduced more recently;\n\\(\\mathit{\\color[RGB]{238,102,119}Outcome}\\) of the repair procedure, with values \\(\\color[RGB]{238,102,119}{\\small\\verb;success;}\\) and \\(\\color[RGB]{238,102,119}{\\small\\verb;fail;}\\).\n\n\n\n\nTable 22.6: Reparations (each row is one unit, representing 100 reparations).\nfile repair_data.csv\n\n\n\\(\\mathit{\\color[RGB]{238,102,119}Outcome}\\)\n\\(\\mathit{\\color[RGB]{34,136,51}Method}\\)\n\\(\\mathit{\\color[RGB]{102,204,238}Location}\\)\n\n\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;success;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;fail;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;fail;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;fail;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;fail;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;fail;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;success;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;success;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;success;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;success;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;success;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;success;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;fail;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;success;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;fail;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;fail;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;fail;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;fail;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;success;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;success;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;success;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;fail;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;fail;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;success;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;success;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;fail;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\)\n\n\n\n\nThe repair company claims that, in this population, the \\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\) repair method is more effective than the \\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\). Can you back up their claims?:\n\n\n\n\n\n\n Exercise (one of the most fun of the course!)\n\n\n\nUse the population data above. The calculations can be done with any tools you like.\n\nExamine the whole population first:\n\nCalculate the frequency distribution of the \\(\\mathit{\\color[RGB]{238,102,119}Outcome}\\) variate, conditional on \\(\\mathit{\\color[RGB]{34,136,51}Method}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\) (note that we are disregarding the \\(\\mathit{\\color[RGB]{102,204,238}Location}\\)).\nCalculate the frequency distribution of the \\(\\mathit{\\color[RGB]{238,102,119}Outcome}\\) variate, conditional on \\(\\mathit{\\color[RGB]{34,136,51}Method}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\).\nCompare the two conditional frequency distributions above. Which of the two repair methods seems more effective?\nAre the claims of the repair company justified?\n\nNow examine the reparations that have been done \\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\):\n\nBefore doing any calculations, what do you expect to find? should the \\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\) repair method be more effective than the \\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\) one, for onsite reparations?\nCalculate the frequency distribution of the \\(\\mathit{\\color[RGB]{238,102,119}Outcome}\\) variate, conditional on \\(\\mathit{\\color[RGB]{34,136,51}Method}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\) and \\(\\mathit{\\color[RGB]{102,204,238}Location}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\).\nCalculate the frequency distribution of the \\(\\mathit{\\color[RGB]{238,102,119}Outcome}\\) variate, conditional on \\(\\mathit{\\color[RGB]{34,136,51}Method}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\) and \\(\\mathit{\\color[RGB]{102,204,238}Location}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\).\nCompare the two conditional frequency distributions for this \\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\) case. Which of the two repair methods seems more effective?\nHow do you explain this result in the light of what you found in step 3.?\n\nNow examine the reparations that have been done \\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\)ly:\n\nBefore doing any calculations, what do you expect to find? should the \\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\) repair method be more effective than the \\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\) one, for reparations done remotely?\nCalculate the frequency distribution of the \\(\\mathit{\\color[RGB]{238,102,119}Outcome}\\) variate, conditional on \\(\\mathit{\\color[RGB]{34,136,51}Method}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\) and \\(\\mathit{\\color[RGB]{102,204,238}Location}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\).\nCalculate the frequency distribution of the \\(\\mathit{\\color[RGB]{238,102,119}Outcome}\\) variate, conditional on \\(\\mathit{\\color[RGB]{34,136,51}Method}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\) and \\(\\mathit{\\color[RGB]{102,204,238}Location}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\).\nCompare the two conditional frequency distributions for this \\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\) case. Which of the two repair methods seems more effective?\nHow do you explain this result in the light of what you found in steps 3. and 7.?\n\nSummarize and explain all your findings.\nCan the repair company claim that the \\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\) repair method is better than the \\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\)?\n\nSuppose you need to send an electronic component for repair to this company.\n\nIf you could choose both the \\(\\mathit{\\color[RGB]{102,204,238}Location}\\) and the \\(\\mathit{\\color[RGB]{34,136,51}Method}\\) of the repair, which would you choose? why?\nIf you could only choose the repair \\(\\mathit{\\color[RGB]{34,136,51}Method}\\), but have no control over the \\(\\mathit{\\color[RGB]{102,204,238}Location}\\), which method would you choose? why?\n\nIs there other information, missing from the description of the population, that should be known before answering the questions above?\n\n\n\n\n\n\n\n\n\n\n\n\n Study reading\n\n\n\n\n§§ 2.2–2.4 and 2.7–2.10 of Risk Assessment and Decision Analysis with Bayesian Networks\nThe role of exchangeability in inference This can be a difficult reading. Try to get the main message.\nSimpson’s paradox"
  },
  {
    "objectID": "samples.html#sec-infinite-populations",
    "href": "samples.html#sec-infinite-populations",
    "title": "23  Infinite populations and samples",
    "section": "23.1 Infinite populations",
    "text": "23.1 Infinite populations\nThe examples of populations that we explored so far comprised a small number of units, and all their data were exactly and fully known. In concrete inference and decision problems we usually deal with populations that are much larger, and often even potentially infinite; and some of their data might be unknown.\nIn the glass-forensic example (table  20.1), for instance, many more glass fragments could be examined beyond the 10 units reported there, with no clear bound on the total number. We could even extend that population considering glass fragments from past and future crime scenes:\n\n\n\nTable 23.1: Glass fragments, extended\n\n\n\n\n\n\n\n\n\n\nunit\n\\(\\mathit{RI}\\)\n\\(\\mathit{Ca}\\)\n\\(\\mathit{Si}\\)\n\\(\\mathit{Type}\\)\nnotes\n\n\n\n\n1\n\\(1.51888\\)\n\\(9.95\\)\n\\(72.50\\)\n\\({\\small\\verb;tableware;}\\)\n\n\n\n2\n\\(1.51556\\)\n\\(9.41\\)\n\\(73.23\\)\n\\({\\small\\verb;headlamps;}\\)\n\n\n\n3\n\\(1.51645\\)\n\\(8.08\\)\n\\(72.65\\)\n\\({\\small\\verb;building_windows_non_float_processed;}\\)\n\n\n\n4\n\\(1.52247\\)\n\\(9.76\\)\n\\(70.26\\)\n\\({\\small\\verb;headlamps;}\\)\n\n\n\n5\n\\(1.51909\\)\n\\(8.78\\)\n\\(71.81\\)\n\\({\\small\\verb;building_windows_float_processed;}\\)\n\n\n\n6\n\\(1.51590\\)\n\\(8.22\\)\n\\(73.10\\)\n\\({\\small\\verb;building_windows_non_float_processed;}\\)\n\n\n\n7\n\\(1.51610\\)\n\\(8.32\\)\n\\(72.69\\)\n\\({\\small\\verb;vehicle_windows_float_processed;}\\)\n\n\n\n8\n\\(1.51673\\)\n\\(8.03\\)\n\\(72.53\\)\n\\({\\small\\verb;building_windows_non_float_processed;}\\)\n\n\n\n9\n\\(1.51915\\)\n\\(10.09\\)\n\\(72.69\\)\n\\({\\small\\verb;containers;}\\)\n\n\n\n10\n\\(1.51651\\)\n\\(9.76\\)\n\\(73.61\\)\n\\({\\small\\verb;headlamps;}\\)\n\n\n\n…\n…\n…\n…\n…\n…\n\n\n351\n\\(1.52101\\)\n\\(8.75\\)\n\\(71.78\\)\n?\nfrom unsolved-crime scene in 1963\n\n\n…\n…\n…\n…\n…\n…\n\n\n1027\n\\(1.51761\\)\n\\(7.83\\)\n\\(72.73\\)\n?\ncrime scene in 2063\n\n\n…\n…\n…\n…\n…\n…\n\n\n\n\n\nthe imaginary example above also shows that the values of some variates for some units might be unknown; this is a situation we shall discuss in depth later.\n\n\nWe shall henceforth focus on statistical populations with a number of units that is in principle infinite, or so large that it can be considered practically infinite. “Practically” means that the number of units we’ll use as data or draw inferences about is a very small fraction, say less than 0.1%, of the total population size.\nThis is often the case. Consider for example (as in §  12.1.1) the collection of all possible 128 × 128 images with 24-bit colour depth. This collection has \\(2^{24 \\times 128 \\times 128} \\approx 10^{118 370}\\) units. Even if we used 100 billions of such images as data, and wanted to draw inferences on another 100 billions, these would constitute only \\(10^{-118 357}\\,\\%\\) of the whole collection. This collection is practically infinite.\nNote that we can’t say whether a population, per se, is “practically infinite” or not. It could be practically infinite for a particular inference problem, but not for another.\nWhen we use the term “population” it will often be understood that we’re speaking about a statistical population that is practically infinite with respect to the inference or decision problem under consideration.\n\nLimit frequencies\nIn §  21.2 we defined relative frequencies. Relative frequencies are ratios of two integers, the denominator being the population size \\(N\\). So a frequency \\(f\\) can only take on \\(N+1\\) rational values \\(0/N, \\dotsc, N/N\\) between \\(0\\) and \\(1\\). As the population size increases, the number of distinct, possible frequencies increases and eventually can be considered practically continuous. Frequencies in this case are sometimes called limit frequencies and they are treated as real numbers between \\(0\\) and \\(1\\)."
  },
  {
    "objectID": "samples.html#sec-samples",
    "href": "samples.html#sec-samples",
    "title": "23  Infinite populations and samples",
    "section": "23.2 Samples",
    "text": "23.2 Samples\nIn the glass-forensic example above (table  23.1), the units and data we had initially (table  20.1) have been considered as part or a much larger population. Such a part is called a population sample or “sample” for short. Almost all data considered in engineering and data-science problems can be considered to be population samples.\nIt is extremely important to specify how a sample is extracted or collected from a population. For instance, if we consider table  20.1 to be a full population, we could extract a sample in such a way that \\(\\mathit{Type}\\) only has value \\({\\small\\verb;headlamps;}\\) (similarly to when we construct a subpopulation, §  22.1, but for a subpopulation we would select all units having that variate value). The marginal frequency of the value \\({\\small\\verb;headlamps;}\\) in the sample would then be \\(1\\), whereas in the original population it is \\(3/10 \\approx 0.333\\) – two very different frequencies.\n\n“Representative” and biased samples\nIn inference and decision problems we would like to use samples for which the various frequencies don’t differ very much from those in the original population. Such a sample is called a “representative sample”. This is a difficult notion; the International Organization for Standardization for instance warns (item 3.1.14):\n\nThe notion of representative sample is fraught with controversy, with some survey practitioners rejecting the term altogether.\n\nIn many cases it is impossible for a sample of given size to be “representative”:\n\n\n\n\n\n\n Exercise\n\n\n\nConsider the following population of 16 units, with four binary variates \\(W,X,Y,Z\\), each with values \\(0\\) and \\(1\\):\n\n\n\n\nTable 23.2: Four-bit population\n\n\n\\(W\\)\n\\(X\\)\n\\(Y\\)\n\\(Z\\)\n\n\n\n\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n\n\n0\n1\n0\n0\n\n\n1\n1\n0\n0\n\n\n0\n0\n1\n0\n\n\n1\n0\n1\n0\n\n\n0\n1\n1\n0\n\n\n1\n1\n1\n0\n\n\n0\n0\n0\n1\n\n\n1\n0\n0\n1\n\n\n0\n1\n0\n1\n\n\n1\n1\n0\n1\n\n\n0\n0\n1\n1\n\n\n1\n0\n1\n1\n\n\n0\n1\n1\n1\n\n\n1\n1\n1\n1\n\n\n\n\n\n\nThe joint variate \\((W,X,Y,Z)\\) has 16 possible values, from \\((0,0,0,0)\\) to \\((1,1,1,1)\\). Each of these values appear exactly once in the population, so it has frequency \\(1/16\\). The marginal frequency distribution for each binary variate is also uniform, with frequencies of 50% for both \\(0\\) and \\(1\\).\n\nExtract a representative sample of size four units. In particular, the marginal frequency distributions of the four variates should be as close to 50%/50% as possible.\n\n\n\n\n\nObviously we cannot expect a population sample to exactly reflect all frequency distributions – joint, marginal, conditional – of the original population; some discrepancy is to be expected. How much discrepancy should be allowed? And what is the minimal size for a sample not to exceed such discrepancy?\nInformation Theory, briefly mentioned in chapter  18, can give reasonable answers to these questions. Let us summarize some examples here.\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\n\nChapters 1–10 of Information Theory, Inference, and Learning Algorithms\nVideo lectures 1–9 from the Course on Information Theory, Pattern Recognition, and Neural Networks\n\n\n\nFirst we need to introduce the Shannon entropy of a discrete frequency distribution. It is defined in a way analogous to the Shannon entropy for a discrete probability distribution, discussed in §  18.5. Lets say the distribution is \\(\\boldsymbol{f} \\coloneqq(f_1,f_2, \\dotsc)\\). Its Shannon entropy \\(\\mathrm{H}(\\boldsymbol{f})\\) is\n\\[\n\\mathrm{H}(\\boldsymbol{f}) \\coloneqq-\\sum_{i} f_i\\ \\log_2 f_i\n\\qquad\\text{\\color[RGB]{119,119,119}\\small(with \\(0\\cdot\\log 0 \\coloneqq 0\\))}\n\\]\nand is measured in bits when the base of the logarithm is 2.\nIf we have a population with joint frequency distribution \\(\\boldsymbol{f}\\), then a representative sample from it must have at least size\n\\[\n2^{\\mathrm{H}(\\boldsymbol{f})} \\equiv\n\\frac{1}{{f_1}^{f_1}\\cdot {f_2}^{f_2}\\cdot {f_3}^{f_3}\\cdot \\dotsb}\n\\] \nThis particular number has important practical consequences; for example it is related to the maximum rate at which a communication channel can send symbols (which can be considered as values of a variate) with an error as low as we please.\n\n\n\n\n\n\n Exercise\n\n\n\n\nCalculate the Shannon entropy of the joint frequency distribution for the four-bit population of table  23.2.\nCalculate the minimum representative-sample size according to the Shannon-entropy formula. Is the result intuitive?\n\n\n\n\n\nIf we are only interested in a smaller number of variates of a population, then the representative sample can be smaller as well: its size would be given by the entropy of the corresponding marginal frequency distribution of the variates of interest. In the example of table  23.2, if we are only interested in the variate \\(X\\), then any sample consisting of two units, one having \\(X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}0\\) and the other having \\(X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}1\\), would be a representative sample of the marginal frequency distribution \\(f(X)\\).\n\n\nBiases\nFor many populations it is difficult or impossible to obtain a “representative” sample, even when the sample is large. As the exercises above show, this impossibility remains even if we extract the sample in an unsystematic and unpredictable way (“random sample”), by shuffling or similar techniques.\nA sample that presents some aspects, such as frequency distributions, which are at variance with the original population, is sometimes called biased (this term is used in many different ways by different authors). Unfortunately, most samples are “biased” in this sense.\nThe only way to counteract the misleading information given by a biased sample is to specify appropriate background information, which comes not from data samples but from a general meta-analysis, often based on physical, medical, and similar principles, of the problem and population."
  },
  {
    "objectID": "recap.html#sec-recap",
    "href": "recap.html#sec-recap",
    "title": "24  A general inference problem",
    "section": "24.1 Recap",
    "text": "24.1 Recap\nOn the “inference side” we gave a concrete and operational explanation of what “drawing an inference” means: it is the calculation of the probabilities – the degrees of belief – of some sentences, from the probabilities of others. This means that in principle we can draw inferences and apply the probability calculus to literally anything that can be expressed by language.\nWe also saw the following points:\n\nThe calculation of probabilities only uses four fundamental – and mathematically quite simple – rules (§  8.4). Any inference, even those make by the most complex machine-learning algorithms, is just a repeated application of those four rules (sometimes with approximating shortcuts for the sake of speed).\nNo inference can be drawn unless some probabilities are first posited as a starting point. This is just another face of the formal-logic fact that no theorem (besides tautologies) can be derived by logic, unless some axioms are given first.\nThe four fundamental rules are determined by basic requirements of logical consistency. Modifying the rules would lead to inconsistent inferences and sub-optimal decisions.\n\n\n\n\n\n\n\n\n\nflowchart BT\n  A[quantity] --o B([sentences]) --&gt; C{{probability calculus}}\n  D[value] ---o B\n  E[population] --o B\n  F[...] -..-o B\n  G[(problem)] --x A & D & E\n  G[(problem)] -.-x F\n\n\n\n\n\nOn the “data side” we introduced a handful of notions, such as “quantity”, “having a value”, “domain”, “quantity type”, “population”, “variate”, “frequency”, and others. We learned how to use them, and paid attention to some of their counter-intuitive properties.\nThese notions allow us to speak, in a precise way, about typical situations and problems that arise in engineering and other scientific contexts. In essence we have introduced a particular kind of sentences to express engineering problems – so that we can apply the probability calculus to them and draw inferences about them.\nThis specific language has its roots in physics and mathematics, where it has been successfully used and refined for several centuries. But it might evolve in different directions in the future, to make allowance for new inferential problems. Later we shall indeed expand it in a couple of directions to cover particular operations that we do with data."
  },
  {
    "objectID": "population_inference.html#induction",
    "href": "population_inference.html#induction",
    "title": "25  Inference and populations",
    "section": "25.1 Induction",
    "text": "25.1 Induction\nIt is a fact of everyday experience that we expect things to happen in particular ways because of our previous experience with similar things. If we observe that ten electronic components in a row come out damaged from an assembly line, then we expect that the next one will be damaged as well. If you regularly go hiking in a particular area and often see deer, and sometimes hares, then you think it likely to see deer also at your next hike and, if you’re lucky, a hare. You would probably be surprised to see a moose if you had never seen one in those areas. This doesn’t only concern the future: if someone asks you whether you saw a deer during a hike from long time ago, which you don’t quite remember anymore, you’d say “probably I did”.\nThis familiar experience is called induction, especially in the philosophical literature. It has generated a lot of thought and research since the times of Hume (18th century), who apparently was the first to ask how and why this experience is possible.\nBut this kind of regularities does not have clear-cut circumstances. In some cases it is not clear what should be considered “similar” things – in fact, sometimes we circularly reverse the reasoning: if something does not respect an observed regularity then we say it wasn’t “similar”. Also the circumstances in which this regularity should occur are often not-well-defined: should you expect to see deer if you hike in a somewhat different area?\nAnd sometimes this kind of regularities simply fails. Something expected doesn’t happen any longer, even if the circumstances and the “similarity” are clearly the same. Jeffreys aptly said:\n\nA common argument for induction is that induction has always worked in the past and therefore may be expected to hold in the future. It has been objected that this is itself an inductive argumentand cannot be used in support of induction. What is hardly ever mentioned is that induction has often failed in the past and that progress in science is very largely the consequence of direct attention to instances where the inductive method has led to incorrect predictions.\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nChapter I of Scientific Inference (3rd ed.) is an extremely insightful reading about inference, probability, and science.\n\n\n\nInduction and data science\nInduction, with the mentioned caveats that accompany it, is at the basis of how we approach and solve many engineering problems, and at the heart of data science: from data about particular things or phenomena we try to infer new or old instances of “similar” things or phenomena.\nWe shall now see how this is done in a quantitative manner with the probability calculus. It is important to clarify what the probability calculus, and all approximate inference methods derived from it, can and cannot do:\n\n It allows us to make inductive inferences in a quantitative and guaranteed self-consistent way.\n It does not explain why there are regularities and why induction in some cases works.\n It does not tell us which things or phenomena should be considered “similar”. In fact, what’s similar and what’s not is something that we must input into the probability calculus.\n If we formulate the “similarity” and “non-similarity” of an instance as well-defined hypotheses, then the probability calculus allows us to calculate their probabilities (and make a decision as to accept, if necessary)."
  },
  {
    "objectID": "population_inference.html#sec-two-populations",
    "href": "population_inference.html#sec-two-populations",
    "title": "25  Inference and populations",
    "section": "25.2 Inferences and populations",
    "text": "25.2 Inferences and populations\n\nStock exchange and Mars prospecting, again\nConsider the following two sketches of inference problems, related to the scenarios of §  20.1:\n\n\nStock exchange\n\nIn 100 days, the daily change in closing price of a stock has been positive 74 times, and negative 26 times, according a particular sequence; for instance:\n\n\n\nIn which of the subsequent 3 days will the closing-price change be positive, and in which negative?\n\n\n\n\n \n\n\nMars prospecting\n\nOf the last 100 similar-sized rocks examined in a large crater on Mars, 74 contained hematite (Y), and 26 did not (N). For instance, the data could be:\n\n\n\nWhich, among the next 3 rocks that will be examined, will contain hematite, and which will be hematite-free?\n\n\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nDiscuss:\n\nWhich of the two inferences above seems more difficult?…\n…Why? Speculate on which factors make one inference more difficult than the other.\nWhich differences and similarities do you find between the two inferences?\nWhich additional information could be important for drawing more precise inferences?\nWhich type of quantities appear in the two inferences?\n\n\n\n\n\nTranslation into sentences\nLet us remember chapter 8 that in order to set up and solve an inference problem we must first define appropriate sentences which we’ll use in probability supposals and conditionals.\nFor the two problems above it looks obvious that we can use sentences involving the language of statistical populations. We must then try to define the units and the variates. Tentatively let’s try the following definitions:\n\nStock exchange\n\n\nUnits: the problem is only sketched, so let’s assume that a precise definition can be found somewhere. There are at least 100 + 3 units, but the problem suggests that we might consider an infinite population.\nVariates: the change in closing price seems an obvious binary variate for the population. Let’s denote it \\(C\\), with domain \\(\\set{\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}},\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}}\\).\n\n\nMars prospecting\n\n\nUnits: again the problem is only sketched, so let’s assume that a precise description exists of how the rocks to be examined should be chosen. There are at least 100 + 3 units, but the problem suggests that we might consider a practically infinite population.\nVariates: the presence of hematite is an obvious binary variate. Let’s denote it with \\(H\\), with domain \\(\\set{\\color[RGB]{34,136,51}{\\small\\verb;Y;},\\color[RGB]{238,102,119}{\\small\\verb;N;}}\\).\n\n\n\nThe population data can be represented as follows, where question marks ? indicate the units about which we want to draw inferences, and the ellipses “…” indicate that the populations possibly extend to infinite other units:\n\n\n\n\nTable 25.1: Stock exchange\n\n\nunit\n\\(C\\)\n\n\n\n\n…\n…\n\n\n1\n\\(\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\)\n\n\n2\n\\(\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\)\n\n\n3\n\\(\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\)\n\n\n4\n\\(\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\)\n\n\n5\n\\(\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\)\n\n\n6\n\\(\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\)\n\n\n(omitted)\n(omitted)\n\n\n95\n\\(\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\)\n\n\n96\n\\(\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\)\n\n\n97\n\\(\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\)\n\n\n98\n\\(\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\)\n\n\n99\n\\(\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\)\n\n\n100\n\\(\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\)\n\n\n101\n?\n\n\n102\n?\n\n\n103\n?\n\n\n…\n…\n\n\n\n\n\n\n\n\n\nTable 25.2: Mars prospecting\n\n\nunit\n\\(H\\)\n\n\n\n\n…\n…\n\n\n1\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n2\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n3\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\)\n\n\n4\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n5\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n6\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n(omitted)\n(omitted)\n\n\n95\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\)\n\n\n96\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n97\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\)\n\n\n98\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n99\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n100\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n101\n?\n\n\n102\n?\n\n\n103\n?\n\n\n…\n…\n\n\n\n\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nWould you define these two populations in a different way? Do you think other variates should be included, for example?\n\n\n\n\nDesired probabilities\nThe stock-exchange problem asks “In which of the subsequent 3 days will the closing-price change be positive, and in which negative?”. In terms of the populations just introduced, this can be translated into:\n\n“will the \\(C\\) variate for units #101, #102, #103 have value \\(\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\) or \\(\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\)?”\nIn other words, we are asking which of the two following mutually exclusive sentences, expressed symbolically, will be true:\n\\[\nC_{101}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\qquad C_{101}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\n\\]\nSince one of them must be true, their probabilities form a probability distribution (§  14), in which for the moment we omit the conditional:\n\\[\n\\mathrm{P}\\bigl(C_{101}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}  \\quad\\quad\n\\mathrm{P}\\bigl(C_{101}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}\n\\]\nAnalogously for units #102 and #103, leading to two more probability distributions.\nWhen we and together sentences for the three units we obtain a joint probability distribution (§  15) over 2³ = 8 mutually exclusive and exhaustive composite sentences:\n\\[\\begin{aligned}\n&\\mathrm{P}\\bigl(C_{101}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0mu,\\mkern-0mu}C_{102}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0mu,\\mkern-0mu}C_{103}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\n\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}\n\\\\[1ex]\n&\\mathrm{P}\\bigl(C_{101}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\mathbin{\\mkern-0mu,\\mkern-0mu}C_{102}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0mu,\\mkern-0mu}C_{103}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\n\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}\n\\\\[1ex]\n&\\dotso\n\\\\[1ex]\n&\\mathrm{P}\\bigl(C_{101}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\mathbin{\\mkern-0mu,\\mkern-0mu}C_{102}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\mathbin{\\mkern-0mu,\\mkern-0mu}C_{103}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\n\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}\n\\end{aligned}\\]\n\n\nNow let’s focus on the conditional. The information we are given consists of the values of the \\(C\\) variate for units #1 to #100, which we can and together. We must also and all other information implicit in the stock-exchange problem, which we denote \\(\\mathsfit{I}_{\\text{s}}\\):\n\\[\n\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{} C_{100}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0mu,\\mkern-0mu}C_{99}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\dotsc \\mathbin{\\mkern-0mu,\\mkern-0mu}\nC_{3}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0mu,\\mkern-0mu}C_{2}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\mathbin{\\mkern-0mu,\\mkern-0mu}C_{1}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}_{\\text{s}} \\bigr)\n\\]\nWe finally arrive at the eight probabilities (which constitute a probability distribution) that we want to calculate:\n\n\\[\\begin{aligned}\n&\\mathrm{P}\\bigl(C_{101}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0mu,\\mkern-0mu}C_{102}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0mu,\\mkern-0mu}C_{103}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\n\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{} C_{100}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0mu,\\mkern-0mu}C_{99}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\dotsc \\mathbin{\\mkern-0mu,\\mkern-0mu}\nC_{3}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0mu,\\mkern-0mu}C_{2}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\mathbin{\\mkern-0mu,\\mkern-0mu}C_{1}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}_{\\text{s}} \\bigr)\n\\\\[1ex]\n&\\mathrm{P}\\bigl(C_{101}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\mathbin{\\mkern-0mu,\\mkern-0mu}C_{102}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0mu,\\mkern-0mu}C_{103}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\n\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{} C_{100}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0mu,\\mkern-0mu}C_{99}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\dotsc \\mathbin{\\mkern-0mu,\\mkern-0mu}\nC_{3}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0mu,\\mkern-0mu}C_{2}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\mathbin{\\mkern-0mu,\\mkern-0mu}C_{1}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}_{\\text{s}} \\bigr)\n\\\\[1ex]\n&\\dotso\n\\\\[1ex]\n&\\mathrm{P}\\bigl(C_{101}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\mathbin{\\mkern-0mu,\\mkern-0mu}C_{102}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\mathbin{\\mkern-0mu,\\mkern-0mu}C_{103}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\n\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{} C_{100}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0mu,\\mkern-0mu}C_{99}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\dotsc \\mathbin{\\mkern-0mu,\\mkern-0mu}\nC_{3}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0mu,\\mkern-0mu}C_{2}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\mathbin{\\mkern-0mu,\\mkern-0mu}C_{1}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}_{\\text{s}} \\bigr)\n\\end{aligned}\\]\n\n\n\n\n\n\n\n Exercise\n\n\n\nDo a similar analysis for the Mars-prospecting problem, and write down the final probability distribution that we wish to calculate.\n\n\n\n\nThe next – fundamental – question is: how do we calculate these probabilities? Which other probabilities can we take as our starting point?"
  },
  {
    "objectID": "exchangeability.html#sec-exch-populations",
    "href": "exchangeability.html#sec-exch-populations",
    "title": "26  Exchangeability",
    "section": "26.1 Infinitely exchangeable populations",
    "text": "26.1 Infinitely exchangeable populations\n\nDefinition\nConsider a (practically) infinite statistical population with variate \\(X\\). This variate could be a joint one, consisting of variates \\(U,V,W,\\dots\\) of arbitrary types. For simplicity let’s assume that the domain of this variate is discrete. For instance, the variate could be of a binary type with domain \\(\\set{{\\small\\verb;Yes;}, {\\small\\verb;No;}}\\); or it could be the combination of such a binary variate and an another ordinal variate with domain \\(\\set{{\\small\\verb;low;}, {\\small\\verb;medium;}, {\\small\\verb;high;}}\\); so the domain of the joint variable would be the set of 2 × 3 values \\(\\set[\\big]{({\\small\\verb;Yes;}, {\\small\\verb;low;}),\\ ({\\small\\verb;No;}, {\\small\\verb;low;}),\\ \\dotsc,\\ ({\\small\\verb;No;},{\\small\\verb;high;})}\\).\nThis variate associates a quantity to each unit in the population. We denote by \\(X_1\\) the variate for unit #1, and so on.\nNow consider an agent, with background knowledge \\(\\mathsfit{I}\\), which must draw inferences about some population units.\nWe call the infinite population exchangeable if the agent’s inferences are unaffected by the units’ identities, and only depend on the units’ variate values. Said otherwise, exchanges among units that have the same values don’t matter for inference purposes.\n\n\nExample\nLet’s make this clear with a simplified version of the Mars-prospecting example.\nSuppose the agent knows the values of units #95–#99, and needs the probability that unit #101 has variate value \\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\), unit #102 value \\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\), and unit #103 value \\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\), as schematized below:\n\n\n\n\n\nagent’s 1st inference\n\n\nunit\n\\(H\\)\n\n\n\n\n…\n…\n\n\n1\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n2\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n3\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\)\n\n\n4\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n5\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n6\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n7\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n8\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\)\n\n\n…\n…\n\n\n95\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\) \\(\\mathrlap{\\quad\\qquad\\textbf{given}}\\)\n\n\n96\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\) \\(\\mathrlap{\\quad\\qquad\\textbf{given}}\\)\n\n\n97\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\) \\(\\mathrlap{\\quad\\qquad\\textbf{given}}\\)\n\n\n98\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\) \\(\\mathrlap{\\quad\\qquad\\textbf{given}}\\)\n\n\n99\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\) \\(\\mathrlap{\\quad\\qquad\\textbf{given}}\\)\n\n\n100\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n101\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\) \\(\\mathrlap{\\quad\\qquad\\textbf{?}}\\)\n\n\n102\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\) \\(\\mathrlap{\\quad\\qquad\\textbf{?}}\\)\n\n\n103\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\) \\(\\mathrlap{\\quad\\qquad\\textbf{?}}\\)\n\n\n…\n…\n\n\n\n\n\n\n\n\\[\n\\mathrm{P}\\bigl(H_{101}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\color[RGB]{238,102,119}{\\small\\verb;N;}\\mathbin{\\mkern-0mu,\\mkern-0mu}H_{102}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\mathbin{\\mkern-0mu,\\mkern-0mu}H_{103}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\color[RGB]{238,102,119}{\\small\\verb;N;}\n\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}\nH_{99}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\mathbin{\\mkern-0mu,\\mkern-0mu}H_{98}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\mathbin{\\mkern-0mu,\\mkern-0mu}\nH_{97}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\color[RGB]{238,102,119}{\\small\\verb;N;}\\mathbin{\\mkern-0mu,\\mkern-0mu}H_{96}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\mathbin{\\mkern-0mu,\\mkern-0mu}\nH_{95}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\color[RGB]{238,102,119}{\\small\\verb;N;}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}\\bigr)\n\\]\n\n\nIf the population is exchangeable, then the inference above is exactly the same – the probability values are identical – as the following one:\n\n\n\n\n\nagent’s 2nd inference\n\n\nunit\n\\(H\\)\n\n\n\n\n…\n…\n\n\n1\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\) \\(\\mathrlap{\\quad\\qquad\\textbf{given}}\\)\n\n\n2\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n3\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\) \\(\\mathrlap{\\quad\\qquad\\textbf{?}}\\)\n\n\n4\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n5\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\) \\(\\mathrlap{\\quad\\qquad\\textbf{given}}\\)\n\n\n6\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\) \\(\\mathrlap{\\quad\\qquad\\textbf{given}}\\)\n\n\n7\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\) \\(\\mathrlap{\\quad\\qquad\\textbf{?}}\\)\n\n\n8\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\)\n\n\n…\n…\n\n\n95\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\) \\(\\mathrlap{\\quad\\qquad\\textbf{given}}\\)\n\n\n96\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n97\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\) \\(\\mathrlap{\\quad\\qquad\\textbf{?}}\\)\n\n\n98\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n99\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n100\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n101\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\)\n\n\n102\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n103\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\) \\(\\mathrlap{\\quad\\qquad\\textbf{given}}\\)\n\n\n…\n…\n\n\n\n\n\n\n\n\\[\n\\mathrm{P}\\bigl(H_{3}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\color[RGB]{238,102,119}{\\small\\verb;N;}\\mathbin{\\mkern-0mu,\\mkern-0mu}H_{7}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\mathbin{\\mkern-0mu,\\mkern-0mu}H_{97}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\color[RGB]{238,102,119}{\\small\\verb;N;}\n\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}\nH_{1}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\mathbin{\\mkern-0mu,\\mkern-0mu}H_{5}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\mathbin{\\mkern-0mu,\\mkern-0mu}\nH_{95}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\color[RGB]{238,102,119}{\\small\\verb;N;}\\mathbin{\\mkern-0mu,\\mkern-0mu}H_{6}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\mathbin{\\mkern-0mu,\\mkern-0mu}\nH_{103}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\color[RGB]{238,102,119}{\\small\\verb;N;}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}\\bigr)\n\\]\nWhy? Because both inferences have one \\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\) and two \\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\) in their supposals, and both have three \\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\) and two \\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\) in their conditionals. Or, from a different point of view, both inferences look the same if we reorder the units (each keeping its value), as shown below:\n\n\n\n\n2nd inference\n\n\nunit\n\\(H\\)\n\n\n\n\n…\n…\n\n\n1\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\) \\(\\mathrlap{\\quad\\quad\\textbf{given}}\\)\n\n\n2\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n3\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\) \\(\\mathrlap{\\quad\\quad\\textbf{?}}\\)\n\n\n4\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n5\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\) \\(\\mathrlap{\\quad\\quad\\textbf{given}}\\)\n\n\n6\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\) \\(\\mathrlap{\\quad\\quad\\textbf{given}}\\)\n\n\n7\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\) \\(\\mathrlap{\\quad\\quad\\textbf{?}}\\)\n\n\n8\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\)\n\n\n…\n…\n\n\n95\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\) \\(\\mathrlap{\\quad\\quad\\textbf{given}}\\)\n\n\n96\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n97\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\) \\(\\mathrlap{\\quad\\quad\\textbf{?}}\\)\n\n\n98\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n99\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n100\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n101\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\)\n\n\n102\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n103\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\) \\(\\mathrlap{\\quad\\quad\\textbf{given}}\\)\n\n\n…\n…\n\n\n\n\n\n\n\n2nd inference reordered\n\n\nunit\n\\(H\\)\n\n\n\n\n…\n…\n\n\n99\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n2\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n101\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\)\n\n\n4\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n98\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n96\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n102\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n8\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\)\n\n\n…\n…\n\n\n95\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\) \\(\\mathrlap{\\quad\\quad\\textbf{given}}\\)\n\n\n6\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\) \\(\\mathrlap{\\quad\\quad\\textbf{given}}\\)\n\n\n103\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\) \\(\\mathrlap{\\quad\\quad\\textbf{given}}\\)\n\n\n5\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\) \\(\\mathrlap{\\quad\\quad\\textbf{given}}\\)\n\n\n1\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\) \\(\\mathrlap{\\quad\\quad\\textbf{given}}\\)\n\n\n100\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n3\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\) \\(\\mathrlap{\\quad\\quad\\textbf{?}}\\)\n\n\n7\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\) \\(\\mathrlap{\\quad\\quad\\textbf{?}}\\)\n\n\n97\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\) \\(\\mathrlap{\\quad\\quad\\textbf{?}}\\)\n\n\n…\n…\n\n\n\n\n\n\n\n1st inference\n\n\nunit\n\\(H\\)\n\n\n\n\n…\n…\n\n\n1\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n2\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n3\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\)\n\n\n4\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n5\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n6\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n7\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n8\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\)\n\n\n…\n…\n\n\n95\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\) \\(\\mathrlap{\\quad\\quad\\textbf{given}}\\)\n\n\n96\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\) \\(\\mathrlap{\\quad\\quad\\textbf{given}}\\)\n\n\n97\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\) \\(\\mathrlap{\\quad\\quad\\textbf{given}}\\)\n\n\n98\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\) \\(\\mathrlap{\\quad\\quad\\textbf{given}}\\)\n\n\n99\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\) \\(\\mathrlap{\\quad\\quad\\textbf{given}}\\)\n\n\n100\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n101\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\) \\(\\mathrlap{\\quad\\quad\\textbf{?}}\\)\n\n\n102\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\) \\(\\mathrlap{\\quad\\quad\\textbf{?}}\\)\n\n\n103\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\) \\(\\mathrlap{\\quad\\quad\\textbf{?}}\\)\n\n\n…\n…\n\n\n\n\n\n\n\n\n\n\\[\\begin{aligned}\n&\\mathrm{P}\\bigl(H_{101}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\color[RGB]{238,102,119}{\\small\\verb;N;}\\mathbin{\\mkern-0mu,\\mkern-0mu}H_{102}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\mathbin{\\mkern-0mu,\\mkern-0mu}H_{103}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\color[RGB]{238,102,119}{\\small\\verb;N;}\n\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}\nH_{99}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\mathbin{\\mkern-0mu,\\mkern-0mu}H_{98}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\mathbin{\\mkern-0mu,\\mkern-0mu}\nH_{97}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\color[RGB]{238,102,119}{\\small\\verb;N;}\\mathbin{\\mkern-0mu,\\mkern-0mu}H_{96}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\mathbin{\\mkern-0mu,\\mkern-0mu}\nH_{95}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\color[RGB]{238,102,119}{\\small\\verb;N;}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}\\bigr)\n\\\\[1ex]\n&=\\mathrm{P}\\bigl(H_{3}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\color[RGB]{238,102,119}{\\small\\verb;N;}\\mathbin{\\mkern-0mu,\\mkern-0mu}H_{7}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\mathbin{\\mkern-0mu,\\mkern-0mu}H_{97}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\color[RGB]{238,102,119}{\\small\\verb;N;}\n\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}\nH_{1}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\mathbin{\\mkern-0mu,\\mkern-0mu}H_{5}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\mathbin{\\mkern-0mu,\\mkern-0mu}\nH_{95}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\color[RGB]{238,102,119}{\\small\\verb;N;}\\mathbin{\\mkern-0mu,\\mkern-0mu}H_{6}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\mathbin{\\mkern-0mu,\\mkern-0mu}\nH_{103}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\color[RGB]{238,102,119}{\\small\\verb;N;}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}\\bigr)\n\\end{aligned}\\]\nThis equality under exchanges holds no matter how many units we consider in the supposal and in the conditional (the conditional could even be empty).\nAs two additional examples with the same population,\n\\[\\begin{gathered}\n\\mathrm{P}(H_{2}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} H_{101}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\color[RGB]{238,102,119}{\\small\\verb;N;}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I})\n= \\mathrm{P}(H_{98}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} H_{8}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\color[RGB]{238,102,119}{\\small\\verb;N;}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I})\n\\\\[1ex]\n\\mathrm{P}(H_{99}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\mathbin{\\mkern-0mu,\\mkern-0mu}H_{7}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\mathbin{\\mkern-0mu,\\mkern-0mu}H_{3}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\color[RGB]{238,102,119}{\\small\\verb;N;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n=\\mathrm{P}(H_{4}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\mathbin{\\mkern-0mu,\\mkern-0mu}H_{102}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\mathbin{\\mkern-0mu,\\mkern-0mu}H_{95}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\color[RGB]{238,102,119}{\\small\\verb;N;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\end{gathered}\\]\n\n\nThe definition of exchangeability extends in an obvious way to populations with variates having arbitrary discrete domains. As an example consider a population with an ordinal variate \\(X\\) having domain of three values \\(\\set{\\texttt{\\small\\color[RGB]{34,136,51}low},\\texttt{\\small\\color[RGB]{102,204,238}med},\\texttt{\\small\\color[RGB]{204,187,68}high}}\\). If this population is exchangeable for an agent with state of knowledge \\(\\mathsfit{J}\\), then we must have for instance\n\\[\\begin{aligned}\n&\\mathrm{P}( X_5\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\texttt{\\small\\color[RGB]{102,204,238}med}\\mathbin{\\mkern-0mu,\\mkern-0mu}X_2\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\texttt{\\small\\color[RGB]{204,187,68}high}\\mathbin{\\mkern-0mu,\\mkern-0mu}\nX_{14}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\texttt{\\small\\color[RGB]{102,204,238}med}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X_{7}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\texttt{\\small\\color[RGB]{34,136,51}low}\\mathbin{\\mkern-0mu,\\mkern-0mu}X_{1}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\texttt{\\small\\color[RGB]{102,204,238}med}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{J})\n\\\\[1ex]\n&=\n\\mathrm{P}( X_1\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\texttt{\\small\\color[RGB]{102,204,238}med}\\mathbin{\\mkern-0mu,\\mkern-0mu}X_{90}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\texttt{\\small\\color[RGB]{204,187,68}high}\\mathbin{\\mkern-0mu,\\mkern-0mu}\nX_{3}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\texttt{\\small\\color[RGB]{102,204,238}med}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X_{7}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\texttt{\\small\\color[RGB]{34,136,51}low}\\mathbin{\\mkern-0mu,\\mkern-0mu}X_{5}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\texttt{\\small\\color[RGB]{102,204,238}med}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{J})\n\\end{aligned}\\]\n\n\n\n\n\n\n Important points about exchangeability\n\n\n\n\nStrictly speaking, exchangeability is not an intrinsic property of a population. It is a property of an agent’s state of knowledge about the population. For instance, if an agent knows that the units’ indices reflect some temporal order, then that agent generally won’t consider that population as exchangeable. Another agent, oblivious of the fact that the units’ indices carry information, may instead consider that population as exchangeable.\nThe probability calculus doesn’t tell us if a population is exchangeable; in fact, it requires exchangeability (or non-exchangeability) as an input.\n…However, if the possibility of exchangeability and non-exchangeability are formulated as two well-defined hypotheses, the probability calculus can tell us their probabilities.\nThere isn’t any clear-cut dichotomy between exchangeable and non-exchangeable populations. Rather, the discrepancy between probabilities under exchanges of units gradually increases from practically acceptable levels to unacceptable ones. What’s acceptable depends on the particular problem.\n\n\n\n\n\n\n\n\n\n Exercise\n\n\n\n@@ TODO"
  },
  {
    "objectID": "inference_exchangeability.html#sec-inference-known-freq",
    "href": "inference_exchangeability.html#sec-inference-known-freq",
    "title": "27  Inference under exchangeability",
    "section": "27.1 Inference when population frequencies are known",
    "text": "27.1 Inference when population frequencies are known\nSuppose we have a practically infinite statistical population with variate \\(X\\) having a discrete and finite domain. For simplicity let’s assume the domain consists in the integers \\(\\set{1,2,\\dotsc,K}\\) (we can always rename the actual domain values so as to put them into correspondence with a set of integers).\nAn agent with state of knowledge \\(\\mathsfit{I}\\) needs to draw inferences about some units, given the values of other units, as in the examples of the previous §  25.2 and chapter  26.\nNow we suppose that the agent additionally knows the joint frequency distribution for the variate \\(X\\) in the population. We express this knowledge with the sentence \\(\\boldsymbol{F}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\boldsymbol{f}\\), where \\(\\boldsymbol{f}\\) is a \\(K\\)-tuple of numerical frequencies. We denote the frequencies of the values \\(1,2,\\dotsc, K\\) by \\(f_1, f_2, \\dotsc, f_K\\).\nIf someone gave you the frequencies above (their exact numerical values), and asked you your degree of belief that the variate for some unit, say unit #47, has value \\(2\\), what would you answer?\nThis scenario has a very strong symmetry. A fraction \\(f_2\\) of all the units have value \\(2\\). Unit #47 could be one among those in this fraction, or one among those in the remaining  \\(1-f_2\\)  fraction. We are compelled to give probability \\(f_2\\) that unit #47 has value \\(2\\):\n\\[\n\\mathrm{P}(X_{47}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}2 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\boldsymbol{F}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\boldsymbol{f} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}) = f_2\n\\]\nThis is just one way of looking at this scenario. Alternative ways show similar symmetries, which lead to the same degree of belief.\nMore generally, the agent’s degree of belief on some unit #\\(u\\) to have value \\(i\\), when the joint frequency distribution \\(f\\) is known, must be\n\\[\n\\mathrm{P}(X_u \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}i \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\boldsymbol{F}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\boldsymbol{f} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}) = f_i\n\\]\n\n\nWe can extend this reasoning to more than one unit. A crucial point here is that the population is practically infinite. If we know that the frequency of value \\(2\\) is \\(f_2\\), and then we observe a unit with that value, then we know that the total number of units having value \\(2\\) has slightly decreased. In a practically infinite population, however, this decrease is negligible, so we can think of \\(f_2\\) as remaining the same.\n\n\n\n\n\n\n Exercise\n\n\n\nSuppose the population is finite, with \\(N\\) units. The absolute frequency (§  21.2) of value \\(2\\) is then \\(Nf_2\\).\n\nCalculate the new absolute and relative frequencies of value \\(2\\) after we remove one unit with that value.\nCalculate the difference between the previous and new frequency.\nDo the same calculations also for the other variate values (\\(1,3,\\dotsc\\)).\n\n\n\nWe therefore find that the agent’s degree of belief on some unit #\\(u\\) to have value \\(i\\), and some other unit #\\(v\\) to have value \\(j\\), when the joint frequency distribution \\(f\\) is known, must be\n\\[\n\\mathrm{P}(X_u \\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}i \\mathbin{\\mkern-0mu,\\mkern-0mu}X_v\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}j \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\boldsymbol{F}\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}\\boldsymbol{f} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}) = f_i \\cdot f_j\n\\]\nAnd so on for more units.\n\n@@ TODO to be continued"
  },
  {
    "objectID": "missing_parts.html",
    "href": "missing_parts.html",
    "title": "28  To be written",
    "section": "",
    "text": "The final part, about population inference and the universal machine that does it, has yet to be written"
  },
  {
    "objectID": "glass_application_R.html#example-population-and-data",
    "href": "glass_application_R.html#example-population-and-data",
    "title": "29  The optimal predictor machine in action (for nominal quantities)",
    "section": "29.1 Example population and data",
    "text": "29.1 Example population and data\nConsider the following population, which we consider to be exchangeable:\n\nUnits: glass fragments collected at particularly defined crime scenes.\nVariates:\n\n\\(\\mathit{RI}\\), ordinal, domain \\(\\set{1,\\dotsc,4}\\): Refractive Index of the fragment\n\\(\\mathit{Na}\\), ordinal, domain \\(\\set{1,\\dotsc,4}\\): Natrium content of the fragment\n\\(\\mathit{Mg}\\), ordinal, domain \\(\\set{1,\\dotsc,4}\\): Magnesium content of the fragment\n\\(\\mathit{Al}\\), ordinal, domain \\(\\set{1,\\dotsc,4}\\): Aluminium content of the fragment\n\\(\\mathit{Si}\\), ordinal, domain \\(\\set{1,\\dotsc,4}\\): Silicon content of the fragment\n\\(\\mathit{K}\\), ordinal, domain \\(\\set{1,\\dotsc,4}\\): Potassium content of the fragment\n\\(\\mathit{Ca}\\), ordinal, domain \\(\\set{1,\\dotsc,4}\\): Calcium content of the fragment\n\\(\\mathit{Ba}\\), ordinal, domain \\(\\set{1,\\dotsc,4}\\): Barium content of the fragment\n\\(\\mathit{Fe}\\), ordinal, domain \\(\\set{1,\\dotsc,4}\\): Iron content of the fragment\n\\(\\mathit{Type}\\), nominal, domain \\(\\set{\\texttt{\\small T1},\\dotsc,\\texttt{\\small T7}}\\): Type or origin of the glass fragment\n\nThe values for the \\(\\mathit{RI}\\) and content variates represent ranges of numeric values or percentages, which you can find in this metadata file. In the same file you also find the description of the glass types.\n\nWe have a sample of 214 units from this population; their variate values are stored in this data file. Here are the first five units:\n\nprint(head(fread('datasets/glass_data-4_lev.csv'), 5))\n\n   Id RI Na Mg Al Si K Ca Ba Fe Type\n1:  1  2  2  4  2  3 1  2  1  1   T2\n2:  2  2  1  3  2  3 1  2  1  1   T2\n3:  3  2  2  4  2  2 1  2  1  1   T1\n4:  4  1  2  2  2  3 1  2  1  1   T4\n5:  5  2  2  3  2  3 1  2  1  1   T1\n\n\n\n\nNow we imagine to prepare an agent for drawing inferences about the full population of glass fragments – which also means fragments from future or unsolved crime scenes. The agent uses – or is the embodiment of – the universal exchangeable-inference machine."
  },
  {
    "objectID": "glass_application_R.html#the-agents-initial-state-of-knowledge",
    "href": "glass_application_R.html#the-agents-initial-state-of-knowledge",
    "title": "29  The optimal predictor machine in action (for nominal quantities)",
    "section": "29.2 The agent’s initial state of knowledge",
    "text": "29.2 The agent’s initial state of knowledge\n\nLoad the machine’s apparatus\nThe calls\nsource('code/tplotfunctions.R')\nsource('code/optimal_predictor_machine_nominal.R')\nat the beginning of this chapter loaded several R functions that implement the universal machine: they draw inferences, calculate marginal and conditional probabilities, and plot probability distributions.\n\n\nLearning the background information\nOur agent at the moment doesn’t know anything at all, not even about the existence of the population above. If we were to ask it anything, we would just get a blank stare back.\nLet us give it the basic background information about the population: the variates’ names and domains. We do this through the function finfo(): it has a data argument, which we omit for the moment, and a metadata argument. The latter can simply be the name of the file containing the metadata (NB: this file must have a specific format):\n\npriorknowledge &lt;- finfo(metadata='datasets/glass_metadata-4_lev.csv')\n\nThe agent now possesses this basic background knowlege, encoded in the priorknowledge object. The encoding uses a particular mathematical representation which, however, is of no interest to us1. Other representations could also be used, but the knowledge would be the same. Think of this as encoding an image into a png or other lossless format: the representation of the image would be different, but the image would be the same.1 If you’re curious you can have a glimpse at it with the command str(priorknowledge), which displays structural information about an object.\n\n\nPreliminary inferences about the population\nNow the agent knows about the population, variates, and domains. But it has not seen any data, that is, the variate values for some units. Yet we can ask it some questions and to draw some inferences. Remember that the answer to a question is not just a value: it is the collection of all possible values, with a probability assigned to each. If the actual value is known, then it will have probability 1, and all others probability 0.\nLet’s ask the agent: what is the marginal frequency distribution for the variate \\(\\mathit{Type}\\), in the full (infinite!) population? Obviously the agent doesn’t know what the actual distribution is, nor do we. It will calculate a probability distribution over all possible marginal frequency distributions.\nThis probability distribution for the \\(\\mathit{Type}\\) variate is calculated by the function fmarginal(). It has arguments finfo: the agent’s information; and variates: the names of the variates of which we want the marginal frequencies:\n\npriorknowledge_type &lt;- fmarginal(finfo=priorknowledge, variates='Type')\n\nThe answer is stored in the object priorknowledge_type, which now contains only information pertinent to the \\(\\mathit{Type}\\) variate.\nWe would like to visualize this probability distribution over marginal frequency distributions. A complication is that we would need infinite dimensions to visualize this faithfully. One approximate way to represent this probability distribution is by showing, say, 100 representative samples from it. The idea is the same as for a scatter plot (§  14.4). In this case we would then have 100 different frequency distributions for the variate \\(\\mathit{Type}\\).\nThe function plotsamples1D() does this kind of visual representation. It has arguments finfo: the object encoding the probability distribution; n (default 100): the number of samples to show; and predict, which for the moment we set to FALSE and discuss in a moment.\nHow do you think this probability distribution will look like? what kind of marginal frequencies we do expect in the full population?\n\nplotsamples1D(finfo=priorknowledge_type, n=100, predict=FALSE)\n\n\n\n\nYou see that anything goes: Some frequency distributions give frequency almost 1 to a specific value, and almost 0 to the others. Other frequency distributions spread out the frequencies more evenly, with some peaks here or there.\nThis is a meaningful answer, because the agent hasn’t seen any data. From its point of view, everything is possible in this population.\n@@ TODO: add representation as quantiles\n\n\nPreliminary inferences about units\nUp to now the agent has drawn an inference regarding the full population, not regarding any specific unit. Now let’s ask it: what will be the value of the \\(\\mathit{Type}\\) variate in the next unit, or glass fragment, we observe? As usual, an answer consists in a probability distribution over all possible values.\n\n\n\n\n\n\n Exercise\n\n\n\nBefore continuing, ask yourself the same question: which probabilities would you give to the \\(\\mathit{Type}\\) values for the next unit, given that you only know that this quantity has seven possible values?\n\n\nThe agent’s answer this time is a probability distribution over seven values, which we can draw faithfully. The function plotsamples1D() can draw this probability as well, if we give the argument predict=TRUE (default):\n\nplotsamples1D(finfo=priorknowledge_type)\n\n\n\n\nThis plot shows the probability distribution for the next unit in blue, together with a sample of 100 possible frequency distributions for the \\(\\mathit{Type}\\) variate over the full population. Note that samples are drawn anew every time, so they can look somewhat differently from time to time.22 To have reproducible plots, use set.seed(314) (or any integer you like) before calling the plot function.\nThe agent’s answer is that in the next unit we can observe any \\(\\mathit{Type}\\) value with equal probability. Do you think this is a reasonable answer?\nThe plot above and the information it represents are very useful for inference purposes: not only they give the probability for the next observation, but also an idea of how such a probability could change in the future, as more data are collected and knowledge of the population’s frequency distribution becomes more precise.\n\n\n\n\n\n\n Exercise\n\n\n\nInspect the agent’s inferences for other variates."
  },
  {
    "objectID": "glass_application_R.html#the-agent-learns-from-data",
    "href": "glass_application_R.html#the-agent-learns-from-data",
    "title": "29  The optimal predictor machine in action (for nominal quantities)",
    "section": "29.3 The agent learns from data",
    "text": "29.3 The agent learns from data\n\nLearning from the sample data\nNow let’s give the agent the data from the sample of 214 glass fragments. This is done again with the finfo() function, but providing the data argument, which can be the name of the data file:\n\npostknowledge &lt;- finfo(data='datasets/glass_data-4_lev.csv', metadata='datasets/glass_metadata-4_lev.csv')\n\nThe postknowledge object contains the agent’s knowledge from the metadata and the sample data. This object can be used in the same way as the object representing the agent’s background knowledge.\n\n\nInferences about the population\nNow that the agent has learned from the data, we can ask it again what is the marginal frequency distribution for the variate \\(\\mathit{Type}\\), in the full population.\nWe calculate the probability for the possible marginal frequency distributions, and then plot it as a set of 100 representative samples:\n\npostknowledge_type &lt;- fmarginal(finfo=postknowledge, variates='Type')\n\nplotsamples1D(finfo=postknowledge_type, predict=FALSE)\n\n\n\n\nThis plot shows two important aspects of this probability distribution and of the agent’s current state of knowledge:\n\nNot anything goes anymore. Some frequency distributions are clearly “excluded”, or more precisely they are extremely improbable. The most probable frequency distributions have a maximum at the \\(\\texttt{\\small T2}\\) value, another lower peak for the value \\(\\texttt{\\small T4}\\), and several other qualitative features that can be glimpsed from the plot.\nYet, the agent still has a degree of uncertainty, qualitatively shown by the width of the “bands” of frequency distributions. For example, the frequency for the \\(\\texttt{\\small T2}\\) value could be \\(0.40\\) as well as \\(0.30\\)\n\n@@ TODO: add code with function that reports the exact probabilities of the frequencies.\nAnd there are other more specific aspects that can be found by visual inspection. For instance:\n\nSome frequency distributions have their absolute maximum at \\(\\texttt{\\small T1}\\), and a lower value at \\(\\texttt{\\small T2}\\). Vice versa, others have their absolute maximum at \\(\\texttt{\\small T2}\\) and a lower value at \\(\\texttt{\\small T1}\\). So there’s still uncertainty as to which value is the most frequent in the full population.\nThe agent gives a very small but non-zero frequency to the value \\(\\texttt{\\small T7}\\). Yet, the data have no units at all with the \\(\\texttt{\\small T7}\\) value. Even if the agent has never seen this value in the data it was given, it knows nevertheless, from the metadata, that this is a possible value. So the agent doesn’t dogmatically say that its frequency in the full population should be zero (as in the sample); only that it should be extremely low.\n\n\n\n\n\n\n\n Exercise\n\n\n\nGiven that the value \\(\\texttt{\\small T7}\\) has not been observed in 214 units, what approximate upper bound to its frequency would you give? Does the agent’s inference agree with your intuition?\n\n\n\n\nInferences about units\nFinally we ask the agent what \\(\\mathit{Type}\\) value we should observe in the next glass fragment. The probability distribution answering this question is plotted by the same function with the argument predict=TRUE (default), as before:\n\nplotsamples1D(finfo=postknowledge_type)\n\n\n\n\nFigure 29.1: Frequency distributions for full population, and probability distribution for next unit"
  },
  {
    "objectID": "glass_application_R.html#conditional-discriminative-or-supervised-learning-inferences",
    "href": "glass_application_R.html#conditional-discriminative-or-supervised-learning-inferences",
    "title": "29  The optimal predictor machine in action (for nominal quantities)",
    "section": "29.4 Conditional, “discriminative” or “supervised-learning” inferences",
    "text": "29.4 Conditional, “discriminative” or “supervised-learning” inferences\nThe inferences about a new units that the agent has made so far were of an “unsupervised-learning” or “generative” kind ([§ sec-2nd-connection-ML]): the agent did not receive or use any partial information about a new unit. Let’s now try a “supervised-learning” or “discriminative” kind of inference.\nImagine that we are at a new crime scene, a glass fragment is recovered, and tests are made about its refractive index and chemical composition. The following values are found, referred to the levels of our variates:\n\n\n\n\nnewfragment &lt;- c(RI=2, Na=2,  Mg=3,  Al=2,  Si=3,  K=1, Ca=2,  Ba=1,  Fe=1)\n\nThe detectives would like to know what’s the possible origin of this fragment, that is, its \\(\\mathit{Type}\\). Our agent can draw this inference.\nFirst, the agent can calculate the probability distribution over the conditional frequencies (§  22.2) of the \\(\\mathit{Type}\\) values for the subpopulation (§  22.1) of units having the specific variate values above. This calculation is done with the function fconditional(), with arguments finfo: the agent’s current knowledge, and unitdata: the partial data obtained from the unit.\n\ncondknowledge &lt;- fconditional(finfo=postknowledge, unitdata=newfragment)\ncondknowledge_type &lt;- fmarginal(finfo=condknowledge, variates='Type')\n\nThe condknowledge object contains the agent’s knowledge conditional on the variates given; this knowledge is about the remaining variates, which in this case are the single variate \\(\\mathit{Type}\\) (so the fmarginal() calculation is actually redundant in this case).\nSecond, the agent can calculate the probability distribution for the \\(\\mathit{Type}\\) values of this particular glass fragment, given the above information.\nBoth inferences can be visualized in the usual way:\n\nplotsamples1D(finfo=condknowledge_type)\n\n\n\n\nFigure 29.2: Conditional frequency distributions for full population, and conditional probability distribution for next unit\n\n\n\n\nThe agent thus gives a probability around \\(80\\%\\) to the fragment’s being of \\(\\texttt{\\small T1}\\) type, around \\(10\\%\\) of being \\(\\texttt{\\small T2}\\) type, and around \\(5\\%\\) of being \\(\\texttt{\\small T5}\\) type. It also shows that further training data could change these probabilities by even \\(\\pm 10\\%\\) or even \\(\\pm 15\\%\\).\nNote how the possible conditional frequency distributions for \\(\\mathit{Type}\\) and the probability distribution differ from the unconditional ones shown in fig.  29.1. The global maximum, \\(80\\%\\), is now sharper than the one for the unconditional case, \\(35\\%\\). This means that knowledge of the other variates for the present fragment has decreased the agent’s uncertainty as regards its type.\nShould the crime investigation proceed on the assumption that the fragment is of \\(\\texttt{\\small T1}\\) type? No, not necessarily. The best decision depends on the gains and costs involved in making correct or wrong assumptions. To this last decision-making problem we turn next.\n\n\n\n\n\n\n Exercise\n\n\n\n\nPerform the “discriminative” inferences above, but omitting from the newfragment data one variate in turn; so first omit only \\(\\mathit{RI}\\) and do the inferences, then omit only \\(\\mathit{Na}\\) and do the inferences, and so on.\nIn each of these inferences you will find that the agent becomes more ore less “confident” about the fragment type. Which of the variates above seem to be most important for making a confident inference?"
  },
  {
    "objectID": "making_decisions.html#decisions-possible-situations-and-consequences",
    "href": "making_decisions.html#decisions-possible-situations-and-consequences",
    "title": "30  Making decisions",
    "section": "30.1 Decisions, possible situations, and consequences",
    "text": "30.1 Decisions, possible situations, and consequences"
  },
  {
    "objectID": "making_decisions.html#gains-and-losses-utilities",
    "href": "making_decisions.html#gains-and-losses-utilities",
    "title": "30  Making decisions",
    "section": "30.2 Gains and losses: utilities",
    "text": "30.2 Gains and losses: utilities\n\nFactors that enter utility quantification\nUtilities can rarely be assigned a priori."
  },
  {
    "objectID": "making_decisions.html#making-decisions-under-uncertainty-maximization-of-expected-utility",
    "href": "making_decisions.html#making-decisions-under-uncertainty-maximization-of-expected-utility",
    "title": "30  Making decisions",
    "section": "30.3 Making decisions under uncertainty: maximization of expected utility",
    "text": "30.3 Making decisions under uncertainty: maximization of expected utility"
  },
  {
    "objectID": "machine_learning_overview.html",
    "href": "machine_learning_overview.html",
    "title": "31  Machine learning - an introduction",
    "section": "",
    "text": "32 TODO discuss functional form\nAs engineers, when faced with the task of using data to model the behaviour of some system, our job is twofold. First, we need to select a suitable method to serve as the function \\(f\\), and second, we need to find the optimal values for the parameters \\(\\theta\\). In ADA501 we will see how to choose an analytical \\(f\\) that corresponds to certain physical systems, while in our course, we will look at methods where \\(f\\) can be practically anything. In fact, we will not even require \\(f\\) to be a deterministic function – consider for instance generative models like ChatGPT or DALL-E, which creates different outputs for the same input, each time it is run.\nFinding the parameters \\(\\theta\\) is what takes us from a general method, to the specific solution to a problem. As we will see, the way of finding them differs between the various machine learning methods, but the principle is always the same: We need to choose a loss function, and then iteratively adjust \\(\\theta\\) so that the loss, when computed on known data, becomes as small as possible. The loss function should represent the difference between what the model outputs, and the correct answer – the better the model, the smaller the difference. The choice of this loss function depends on what kind of problem we wish to solve, and we will look at the common ones shortly. But at this point we can already define the three main types of machine learning:\nHaving decided on a method to represent \\(f\\) and found a set of parameters \\(\\theta\\), we say that these combined constitute our model. The model should have internalised the important correlations in the data and thereby allows us to make predictions, i.e. do modelling. If we do decision-making based on the output of the model as well, we typically call in an agent, since there is now some level of autonomy involved. In this chapter, however, we will stick to modelling problems in a supervised fashion."
  },
  {
    "objectID": "machine_learning_overview.html#hyperparameters-and-model-complexity",
    "href": "machine_learning_overview.html#hyperparameters-and-model-complexity",
    "title": "31  Machine learning - an introduction",
    "section": "32.1 Hyperparameters and model complexity",
    "text": "32.1 Hyperparameters and model complexity\nYou may have heard the quote by statistician George Box:\n\nAll models are wrong, but some are useful.\n\nAlthough coming off as a somewhat negative view on things, the quote still captures an important point about statistical modelling – our goal is not to make a complete, 100% accurate description of reality, but rather a simplified description that is meaningful in the context of our task at hand. The “correct” level of simplicity, in other words the optimal number of parameters \\(\\theta\\), can be hard to find. Often it will be influenced by practical considerations such as time and computing power, but it is always governed by the amount of data we have available. We will look at the theory of model selection later, but let us first consider a visual example, from which we can define some important concepts.\nImagine that you don’t have a thermometer that shows the outside temperature. Never knowing whether you should wear a jacket or not when leaving the house, you finally have an idea: If you can construct a mathematical model for the outside temperature as a function of the time of day, then a look at the clock yould be enough to decide for or against bringing a jacket. You manage to borrow a thermometer for a day, and make ten measurements at different times, which will form the basis for the model:\n\n\n\n\n\n\nFigure 32.1: Temperature measurements over the course of 24 hours.\n\n\n\n\nSource: over_and_underfitting.ipynb\nThe next step is to choose a function \\(f\\). For one-dimensional data like this, we could for instance select among the group of polynomials, of the form \\[\n    y (x, \\mathbf{w}) = w_0 + w_1 x + w_2 x^2 + \\dots + w_M x^M = \\sum_{j=0}^{M} w_j x^j \\,,\n\\] where \\(M\\) is the order of the polynomial. Recall that a zero’th order polymonial is just a constant value, so such a model would be represented with one single parameter. A first-order polynomial is a linear function (two parameters), and the higher in order (and number of parameters) we go, the more “curves” the function can have. This already presents us with a problem. Which order polynomial (i.e. which value of \\(M\\)) should we choose? Let us try different ones, and for each case, fit the parameters, meaning we find the parameter values that yield the smallest difference from the measured data points:\n\n\n\n\n\n\n\n\n(a) Zeroth-order polymonial\n\n\n\n\n\n\n\n(b) First-order polymonial\n\n\n\n\n\n\n\n\n\n(c) Fourth-order polymonial\n\n\n\n\n\n\n\n(d) Ninth-order polymonial\n\n\n\n\nFigure 32.2: Fitting polymonials of different orders to the set of measurements.\n\n\nSource: over_and_underfitting.ipynb\nObviously, the constant function does a poor job of describing our data, and likewise for the linear function. A fourth-order polynomial, on the other hand, looks very reasonable. Now consider the ninth-order polynomial: it matches the measurements perfectly, but surely, this does not match our expectation for what the temperature should look like.\nWe say that the first and second model underfit the data. This can happen for two reasons: Either the model has too little complexity to be able to describe the data (which is the case in this example), or, potentially, the optimal parameter values have not been found. The opposite case is overfitting, as shown for the last model, where the complexity is too high and the model adapts to artifacts in the data.\nThis concept is also called the bias-variance tradeoff. We will not go into too much detail on this yet, but qualitatively, we can say that bias (used in this setting) is the difference between the predicted value and the true value, when averaging over different data sets. Variance (again when used in this setting) indicates how big the change in parameters, and hence in model predictions, we get from fitting to different data sets. Let us say you measure the temperature on ten different days, and for each day, you fit a model, like before. These may be the results:\n\n\n\n\n\n\n\n\n(a) Caption 1\n\n\n\n\n\n\n\n(b) Caption 2\n\n\n\n\n\n\n\n(c) ?(caption)\n\n\n\n\nFigure 32.3: Caption 1\n\n\nSource: over_and_underfitting.ipynb\nThe blue dots are our “original” data points, plotted for reference, while the red lines corresponds to models fitted for each day’s measurements. Due to fluctuations in the measurements, they are different, but note how the difference is related to the model complexity. Our linear models (\\(M=1\\)) are all quite similar, but neither capture the pattern in the data very well, so they all have high bias. The overly complex models (\\(M=9\\)) have zero bias on their repective dataset, but high variance. The optimal choice is likely somewhere in-between (hence the tradeoff), as for the \\(M=4\\) models, which perform well without being overly sensitive to fluctuations in data. Since the value of \\(M\\) is chosen by us, we call it a hyperparameter, to separate it from the regular parameters which are optimised by minimising the loss function."
  },
  {
    "objectID": "machine_learning_overview.html#model-selection-in-practice",
    "href": "machine_learning_overview.html#model-selection-in-practice",
    "title": "31  Machine learning - an introduction",
    "section": "32.2 Model selection in practice",
    "text": "32.2 Model selection in practice\nMove or delete ??"
  },
  {
    "objectID": "decision_trees.html#decision-trees-for-categorical-classification",
    "href": "decision_trees.html#decision-trees-for-categorical-classification",
    "title": "32  Decision trees",
    "section": "32.1 Decision trees for categorical classification",
    "text": "32.1 Decision trees for categorical classification\nFor a trivial example, lets us say you want to go and spend the day at the beach. There are certain criteria that should be fullfilled for that to be a good idea, and some of them depend on each other. A decision tree could look like this:\n\n\n\n\n\n\n\n\nG\n\n \n\nA\n\n   Go to the beach?    \n\nB\n\n Workday?  \n\nC\n\n Don’t go  \n\nB-&gt;C\n\n  Yes  \n\nD\n\n Sunny?  \n\nB-&gt;D\n\n  No  \n\nE\n\n Temp &gt; 18 C  \n\nD-&gt;E\n\n  Yes  \n\nF\n\n Temp &gt; 23 C  \n\nD-&gt;F\n\n  No  \n\nG\n\n  Go to the beach   \n\nE-&gt;G\n\n  Yes  \n\nH\n\n  Don’t go   \n\nE-&gt;H\n\n  No  \n\nI\n\n  Go to the beach   \n\nF-&gt;I\n\n  Yes  \n\nJ\n\n  Don’t go   \n\nF-&gt;J\n\n  No  \n\n\nFigure 32.1: A very simple decision tree.\n\n\n\n\nDepending on input data such as the weather, we end up following a certain path from the root node, along the branches, down to the leaf node, which returns the final decision for these given observations. The botany analogies are not strictly necessary, but at least we see where the name decision tree comes from.\nStudying the above tree structure more closely, we see that there are several possible ways of structuring it, that would lead to the same outcome. We can choose to first split on the Sunny node, and split on Workday afterwards. Drawing it out on paper, however, would show that this structure needs a larger total number of nodes, since we always need to split on Workday. Hence, the most efficient tree is the one that steps through the observables in order of descending importance.\nThe basic algorithm for buiding a decision tree (or growing it, if you prefer) on categorical data, can be written out quite compactly. Consider the following pseudo-code: (ref https://www.cs.cmu.edu/~tom/files/MachineLearningTomMitchell.pdf)\nTODO rename attributes into features?\n\nfunction BuildTree(examples, target_attribute, attributes)\n  tree ← a single node, so far without any label\n  if all examples are of the same classification then \n    give tree a label equal to the classification\n    return tree\n  else if attributes is empty then\n    give tree a label equal the most common value of target_attribute in examples\n    return tree\n  else\n    A ← the attribute from attributes with highest Importance(examples)\n\n    for each value v of A do\n      Examples_v ← the subset of examples where A has the value v\n      subtree ← BuildTree(Examples_v, Target_attributes, Attributes - {A})\n      add a branch with label (A = v) to tree, and below it, add the tree subtree\n    \n    return tree\n\nThis is the original ID3 algorithm (Quinlan 1986). Note how it works recursively – for each new feature, the function calls itself to build a subtree.\n\nQuinlan, J. Ross. 1986. “Induction of Decision Trees.” Machine Learning 1: 81–106.\n\nThe same algorithm is shown and explained in section 19.3 in Russell and Norvig, although they fail to specify that this is ID3.\n\nWe start by creating a node, which becomes a leaf node either if it classifies all examples correctly (no reason to split), or if there are no more features left (not possible to split). Otherwise, we find the most important feature by calling Importance(examples), and proceed to make all possible splits. Now, the magic happens in the Importance function. How can we quantify which feature is best to discriminate on? We have in Section 23.1 met a useful definition from information theory, which was the Shannon entropy: \\[\n    H(f) \\coloneqq-\\sum_{i} f_i\\ \\log_2 f_i\n    \\qquad\\text{\\color[RGB]{119,119,119}\\small(with \\(0\\cdot\\log 0 \\coloneqq 0\\))}\n\\]\nwhere the \\(f_i\\) are frequency distributions. If we stick to the simple example of our target features being “yes” or “no”, we can write out the summation like so:\n\\[\n    H() = - f_{\\mathrm{yes}} \\log_2 f_{\\mathrm{yes}} - f_{\\mathrm{no}} \\log_2 f_{\\mathrm{no}}\n\\]\nLet us compute the entropy for two different cases, to see how it works. In the first case, we have 10 examples: 6 corresponding to “yes”, and four corresponding to “no”. The entropy is then\n\\[\n    H() = - (6/10) \\log_2 (6/10) - (4/10) \\log_2 (4/10) = 0.97\n\\]\nIn the second case, we still have 10 examples, but nearly all of the same class: 9 examples are “yes”, and 1 is “no”:\n\\[\n    H() = - (9/10) \\log_2 (9/10) - (1/10) \\log_2 (1/10) = 0.47\n\\]\nInterpreting the entropy as a measure of impurity in the set of examples, we can guess (or compute, using \\(0\\cdot \\log_2 0 = 0\\)) that the lowest possible entropy occurs for a set where all are of the same class. When doing classification, this is of course what we aim for – separating all examples into those corresponding to “yes” and those corresponding to “no”. A way of selecting the most important feature is then to choose the one where we expect the highest reduction in entropy, caused by splitting on this feature. This is called the information gain, and is generally defined as\n\\[\n    Gain(A, S) \\coloneqq H(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} H(S_v) \\,,\n\\] where \\(A\\) is the feature under consideration, \\(Values(A)\\) are all the possible values that \\(A\\) can have. Futher, \\(S\\) is the set of examples, and \\(S_v\\) is the subset containing examples where \\(A\\) has the value \\(v\\). Looking again at the binary yes/no case, it looks a little simpler. Using the feature Sunny as \\(A\\), we get:\n\\[\n    Gain(\\texttt{Sunny}, S) = H(S) - \\left(\\frac{S_{\\texttt{Sunny=yes}}}{S} H(S_{\\texttt{Sunny=yes}}) + \\frac{S_{\\texttt{Sunny=no}}}{S} H(S_{\\texttt{Sunny=no}})\\right) \\,.\n\\]\nThis equation can be read as “gain equals the original entropy before splitting on Sunny, minus the weighted entropy after splitting”, which is what we were after."
  },
  {
    "objectID": "neural_networks.html",
    "href": "neural_networks.html",
    "title": "33  Neural networks",
    "section": "",
    "text": "\\[\n\\DeclarePairedDelimiters{\\set}{\\{}{\\}}\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\\]"
  }
]