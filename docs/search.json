[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ADA511: Data science and data-driven engineering",
    "section": "",
    "text": "Science is built up with facts, as a house is with stones. But a collection of facts is no more a science than a heap of stones is a house.     (H. Poincaré)\n\nPreface\n**WARNING: THIS IS A WORKING DRAFT. TEXT WILL CHANGE A LOT. MANY PASSAGES ARE JUST TEMPORARY, INCOHERENT, AND DISJOINTED.\nTo be written.\n\nDifference between car mechanic and automotive engineer\n“Engineering based on data” is just how engineering and science in general have been in the past 400 years or so. Nothing new there.\nThe amount of available data has changed. This may lead to a reduction – or in some cases an increase – in uncertainty, and therefore to different solutions.\nLuckily the fundamental theory to deal with large amount of data is exactly the same to deal with small amounts. So the foundations haven’t changed.\n\nThis course makes you acquainted with the foundations."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Accept or discard?",
    "section": "",
    "text": "Let’s start with a question that could arise in a particular engineering problem:\n\nA particular kind of electronic component is produced on an assembly line. At the end of the line, there is an automated inspection device that works as follows with every newly produced component coming out of the line.\nThe inspection device first makes some tests on the new component. The tests give an uncertain forecast of whether that component will fail within its first year of use, or after.\nThen the device decides whether the component is accepted and packaged for sale, or discarded and thrown away.\nWhen a new electronic component is sold, the manufacturer has a net gain of \\(1\\$\\). If the component fails within a year of use, however, the manufacturer incur net loss of \\(10\\$\\) (11$ loss, minus the 1$ gained at first), owing to warranty refunds and damage costs to be paid to the buyer. When a new electronic component is discarded, the manufacturer has \\(0\\$\\) net gain.\nFor a specific new electronic component, just come out of the assembly line, the tests of the automated inspection device indicate that there is a \\(10\\%\\) probability that the component will fail within its first year of use.\n\n\n\n\nShould the inspection device accept or discard the new component?\n\nFirst, try to give and motivate an answer.\nThis is not the real question of this exercise, however. In fact it doesn’t matter if you don’t get the correct answer; not even if you don’t manage to get an answer at all.\nThe purpose here is for you to do some introspection about your own reasoning. Then examine and discuss these points:\n\nWhich numerical elements in the problem seem to affect the answer?\nCan these numerical elements be clearly separated? How would you separate them?\nHow would the answer change, if these numerical elements were changed? Feel free to change them, also in extreme ways, and see how the answer would change.\nCould we solve the problem if we didn’t have the probabilities? Why?\nCould we solve the problem if we didn’t know the various gains and losses? Why?\nCan this problem be somehow abstracted, and then transformed into another one with completely different details? For instance, consider translating along these lines:\n\ninspection device → computer pilot of self-driving car\ntests → camera image\nfail within a year → pedestrian in front of car\naccept/discard → keep on going/ break"
  },
  {
    "objectID": "framework.html#what-does-the-intro-problem-tell-us",
    "href": "framework.html#what-does-the-intro-problem-tell-us",
    "title": "2  Framework",
    "section": "2.1 What does the intro problem tell us?",
    "text": "2.1 What does the intro problem tell us?\nLet’s approach the “accept or discard?” problem of the previous chapter 1 in an intuitive way.\n\n\nWe’re jumping the gun here, because we haven’t learned the method to solve this problem yet!\nFirst let’s say that we accept the component. What happens?\nWe must try to make sense of that \\(10\\%\\) probability that the component fails within a year. Different people do this with different imagination tricks. We can imagine, for instance, that this situation is repeated 100 times. In 10 of these repetitions the accepted electronic component is sold and fails within a year after selling. In the remaining 90 repetitions, the component is sold and works fine for at least a year.\nIn each of the 10 imaginary repetitions in which the component fails early, the manufacturer loses \\(10\\$\\). That’s a total loss of \\(10 \\cdot 10\\$ = 100\\$\\). In each of the 90 imaginary repetitions in which the component doesn’t fail early, the manufacturer gains \\(1\\$\\). That’s a total gain of \\(90\\$\\). So over all 100 imaginary repetitions the manufacturer gains \\[\n10\\cdot (-10\\$) + 90\\cdot 1\\$ = {\\color[RGB]{238,102,119} -10\\$} \\ ,\n\\] that is, the manufacturer has not gained, but lost \\(10\\$\\) ! That’s an average of \\(0.1\\$\\) lost per repetition.\nNow let’s say that we discard the component instead. What happens? In this case we don’t need to invoke imaginary repetitions, but even if we do, it’s clear that the manufacturer doesn’t gain or lose anything – that is, the “gain” is \\(0\\$\\) – in each and all of the repetitions.\nThe conclusion is that if in a situation like this we accept the component, then we’ll lose \\(1\\$\\) on average; whereas if we discard it, then on average we won’t lose anything or gain anything.\nObviously the best, or “least worst”, decision to make is to discard the component.\n\n\n\n\n\n\n Exercises\n\n\n\n\nNow that we have an idea of the general reasoning, check what happens with different values of the probability of failure and of the failure cost: is it still best to discard? For instance, try with\n\nfailure probability 10% and failure cost 5$;\nfailure probability 5% and failure cost 10$;\nfailure probability 10%, failure cost 1$, non-failure gain 2$.\n\nFeel free to get wild and do plots.\nIdentify the failure probability at which accepting the component doesn’t lead to any loss or any gain, so it doesn’t matter whether we discard or accept. (You can solve this as you prefer: analytically with an equation, visually with a plot, by trial & error on several cases, or whatnot.)\nConsider the special case with failure probability 0% and failure cost 10$. This means no new component will ever fail. To decide in such a case we do not need imaginary repetitions; but confirm that we arrive at the same logical conclusion whether we reason through imaginary repetitions or not.\nConsider this completely different problem:\n\nA patient is examined by a brand-new medical diagnostics AI system.\nThe AI first performs some clinical tests on the patient. The tests give an uncertain forecast of whether the patient has a particular disease or not.\nThen the AI decides whether the patient should be dismissed without treatment, or treated with a particular medicine.\nIf the patient is dismissed, then the life expectancy doesn’t increase or decrease if the disease is not present, but it decreases by 9 years if the disease is actually present. If the patient is treated, then the life expectancy decreases by 1 year if the disease is not present (owing to treatment side-effects), and also if the disease is present (because it cures the disease, giving 10 more years to the patient, minus 1 year for the side effects).\nFor this patient, the clinical tests indicate that there is a \\(10\\%\\) probability that the patient has the disease.\n\nShould the diagnostic AI dismiss or treat the patient? Find differences and similarities, even numerical, with the assembly-line problem.\n\n\n\n\n\nFrom the solution of the problem and from the exploring exercises, we gather some instructive points:\n\nIs it enough if we simply know that the component is less likely to fail than not? in other words, if we simply know that the probability of failure is less than \\(50\\%\\)?\nObviously not. We found that if the failure probability is \\(10\\%\\) then it’s best to discard; but if it’s \\(5\\%\\) then it’s best to accept. In both cases the component was less likely to fail than not, but the decisions were different. Moreover, we found that the probability affected the loss if one made the non-optimal decision. Therefore:\nKnowledge of exact probabilities is absolutely necessary for making the best decision\nIs it enough if we simply know that failure leads to a cost? that is, that its gain is less than the gain for non-failure?\nObviously not. The situation is similar to that with the probability. In the exercise we found that if the failure cost is \\(10\\$\\) then it’s best to discard; but if it’s \\(5\\$\\) then it’s best to accept. It’s also best to accept if the failure cost is \\(10\\$\\) but the non-failure gain is \\(2\\$\\). Therefore:\nKnowledge of the exact gains and losses is absolutely necessary for making the best decision\nIs this kind of decision situation only relevant to assembly lines and sales?\nBy all means not. We found a clinical situation that’s exactly analogous: there’s uncertainty, there are gains and losses (of time rather than money), and the best decision depends on both."
  },
  {
    "objectID": "framework.html#our-focus-decision-making-inference-and-data-science",
    "href": "framework.html#our-focus-decision-making-inference-and-data-science",
    "title": "2  Framework",
    "section": "2.2 Our focus: decision-making, inference, and data science",
    "text": "2.2 Our focus: decision-making, inference, and data science\nEvery data-driven engineering project is unique, with its unique difficulties and problems. But there are also problems common to all engineering projects.\nIn the scenarios we explored above, we found an extremely important problem-pattern. There is a decision or choice to make (and “not deciding” is not an option – or it’s just another kind choice). Making a particular decision will lead to some consequences, some leading to a desired goal, others leading to something undesirable. The decision is difficult because its consequences are not known with certainty, given the information and data available in the problem. We may lack information and data about past or present details, about future events and responses, and so on. This is what we call a problem of decision-making under uncertainty or under risk1, or simply a “decision problem” for short.1 We’ll avoid the word “risk” because it has several different technical meanings in the literature, some even contradictory.\nThis problem-pattern appears literally everywhere. But our explored scenarios also suggest that this problem-pattern has a sort of systematic solution method.\nIn this course we’re going to focus on decision problems and their systematic solution method. We’ll learn a framework and some abstract notions that allow us to frame and analyse this kind of problem, and we’ll learn a universal set of principles to solve it. This set of principles goes under the name of Decision Theory. \nBut what do decision-making under uncertainty and Decision Theory have to do with data and data science? The three are profoundly and tightly related on many different planes:\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nDecision theory in expert systems and artificial intelligence\n\n\n\nWe saw that probability values are essential in a decision problem. How do we find them? As you can imagine, data play an important part in their calculation. In our intro example, the failure probability must come from observations or experiments on similar electronic components.\nWe saw that also the values of gains and losses are essential. Data play an important part in their calculation as well.\nData science is based on the laws of Decision Theory. Here’s an analogy: a rocket engineer relies on fundamental physical laws (balance of momentum, energy, and so on) for making a rocket work. Failure to account for those laws leads at best to sub-optimal solutions, at worst to disasters. As we shall see, the same is true for a data scientist and the rules of decision theory.\nMachine-learning algorithms, in particular, are realizations or approximations of the rules of Decision Theory. This is clear, for instance, considering that the main task of a machine-learning classifier is to decide among possible output labels or classes.\nThe rules of Decision Theory are also the foundations upon which artificial-intelligence agents, which must make optimal inferences and decisions, are built.\n\nThese five planes will constitute the major parts of the present course.\n\n\n@@ TODO add examples: algorithm giving outputs is a decision agent.\nThere are other important aspects in engineering problems, besides the one of making decisions under uncertainty. For instance the discovery or the invention of new technologies and solutions. These aspects can barely be planned or decided; but their fruits, once available, should be handled and used optimally – thus leading to a decision problem.\nArtificial intelligence is proving to be a valuable aid in these more creative aspects too. This kind of use of AI is outside the scope of the present notes. Some aspects of this creativity-assisting use, however, do fall within the domain of the present notes. A pattern-searching algorithm, for example, can be optimized by means of the method we are going to study."
  },
  {
    "objectID": "framework.html#our-goal-optimality-not-success",
    "href": "framework.html#our-goal-optimality-not-success",
    "title": "2  Framework",
    "section": "2.3 Our goal: optimality, not “success”",
    "text": "2.3 Our goal: optimality, not “success”\nWhat should we demand from a systematic method for solving decision problems?\nBy definition, in a decision problem under uncertainty there is generally no method to determine the decision that surely leads to the desired consequence – if such a method existed, then the problem would not have any uncertainty! Therefore, if there is a method to deal with decision problems, its goal cannot be the determination of the successful decision. This also means that a priori we cannot blame an engineer for making an unsuccessful decision in a situation of uncertainty.\nImagine two persons, Henry and Tina, who must bet on “heads” or “tails” under the following conditions (but who otherwise don’t get any special thrill from betting):\n\nIf the bet is “heads” and the coin lands “heads”, the person wins a small amount of money; but if it lands “tails”, they lose a large amount of money.\nIf the bet is “tails” and the coin lands “tails”, the person wins a small amount of money; if it lands “heads”, they lose the same small amount of money.\n\nHenry chooses the first bet, on “heads”. Tina chooses the second bet, on “tails”. The coin comes down “heads”. So Henry wins the small amount of money, while Tina loses the same small amount. What would we say about their decisions?\nHenry’s decision was lucky, and yet irrational: he risked losing much more money than in the second bet, without any possibility of at least winning more. Tina’s decision was unlucky, and yet rational: the possibility and amount of winning was the same in the two bets, and she chose the bet with the least amount of loss. We expect that any person making Henry’s decision in similar, future bets will eventually lose more money than any person making Tina’s decision.\nThis example shows two points. First, “success” is generally not a good criterion to judge a decision under uncertainty; success can be the pure outcome of luck, not of smarts. Second, even if there is no method to determine which decision is successful, there is a method to determine which decision is rational or optimal, given the particular gains, losses, and uncertainties involved in the decision problem. We had a glimpse of this method in our introductory scenarios.\nLet us emphasize, however, that we are not giving up on “success”, or trading it for “optimality”. Indeed we’ll find that Decision Theory automatically leads to the successful decision in problems where uncertainty is not present or is irrelevant. It’s a win-win. It’s important to keep this point in mind:\n\n\n\n\n\n\n\n\n\n\n\nAiming to find the solutions that are successful can make us fail to find those that are optimal when the successful ones cannot be determined.\nAiming to find the solutions that are optimal makes us automatically find those that are successful when those can be determined.\n\n\n\nWe shall later witness this fact with our own eyes, and will take it up again in the discussion of some misleading techniques to evaluate machine-learning algorithms."
  },
  {
    "objectID": "framework.html#decision-theory",
    "href": "framework.html#decision-theory",
    "title": "2  Framework",
    "section": "2.4 Decision Theory",
    "text": "2.4 Decision Theory\nSo far we have mentioned that Decision Theory has the following features:\n\n it tells us what’s optimal and, when possible, what’s successful\n it takes into consideration decisions, consequences, costs and gains\n it is able to deal with uncertainties\n\nWhat other kinds of features should we demand from it, in order to be applied to as many kinds of decision problems as possible, and to be relevant for data science?\nIf we find an optimal decision in regards to some outcome, it may still happen that the decision can be realized in several ways that are equivalent in regard to the outcome, but inequivalent in regard to time or resources. In the assembly-line scenario, for example, the decision discard could be carried out by burning, recycling, and so on. We thus face a decision within a decision. In general, a decision problem may involve several decision sub-problems, in turn involving decision sub-sub-problems, and so on.\nIn data science, a common engineering goal is to design and build an automated or AI-based device capable of making an optimal decision in a specific kind of uncertain situations. Think for instance of an aeronautic engineer designing an autopilot system, or a software company designing an image classifier.\nDecision Theory turns out to meet these demands too, thanks to the following features:\n\n it is susceptible to recursive, sequential, and modular application\n it can be used not only for human decision-makers, but also for automated or AI devices\n\n\n\nDecision Theory has a long history, going back to Leibniz in the 1600s and partly even to Aristotle in the −300s, and appearing in its present form around 1920–1960. What’s remarkable about it is that it is not only a framework, but the framework we must use. A logico-mathematical theorem shows that any framework that does not break basic optimality and rationality criteria has to be equivalent to Decision Theory. In other words, any “alternative” framework may use different technical terminology and rewrite mathematical operations in a different way, but it boils down to the same notions and operations of Decision Theory. So if you wanted to invent and use another framework, then either (a) it would lead to some irrational or illogical consequences, or (b) it would lead to results identical to Decision Theory’s. Many frameworks that you are probably familiar with, such as optimization theory or Boolean logic, are just specific applications or particular cases of Decision Theory.\nThus we list one more important characteristic of Decision Theory:\n\n it is normative\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\n\nJudgment under uncertainty\nHeuristics and Biases\nThinking, Fast and Slow\n\n\n\nNormative contrasts with descriptive. The purpose of Decision Theory is not to describe, for example, how human decision-makers typically make decisions. Because human decision-makers typically make irrational, sub-optimal, or biased decisions. That’s exactly what we want to avoid and improve!"
  },
  {
    "objectID": "basic_decisions.html#graphical-representation-and-elements",
    "href": "basic_decisions.html#graphical-representation-and-elements",
    "title": "3  Basic decision problems",
    "section": "3.1 Graphical representation and elements",
    "text": "3.1 Graphical representation and elements\nA basic decision problem can be represented by a diagram like this:\n\n\nIt has one decision node, usually represented by a square , from which the available decisions depart as lines. Each decision leads to an uncertainty node, usually represented by a circle , from which the possible outcomes depart as lines. Each outcome leads to a particular utility value. The uncertainty of each outcome is quantified by a probability.\nA basic decision problem is analysed in terms of these elements:\n\n Agent, and background or prior information. The agent is the person or device that has to make the decision. An agent always possess (or has been programmed with) specific background information that is used and taken for granted in the decision-making process. This background information determines the probabilities and utilities of the outcomes, together with other available data and information. Since different agents typically have different background information, we shall somehow conflate agents and prior information.\n\n\n\nWe’ll use the neutral pronouns it/its when referring to an agent, since an agent could be a person or a machine.\n\n Decisions, also called courses of actions, available to the agent. They are assumed to be mutually exclusive and exhaustive; this can always be achieved by recombining them if necessary, as we’ll discuss later.\n Outcomes of the possible decisions. Every decision can have a different set of outcomes, or some outcomes can appear for several or all decisions (in this case they are reported multiple times in the decision diagram). Note that even if an outcome can happen for two or more different decisions, its probabilities can still be different depending on the decision.\n Probabilities for each of the outcomes. Their values typically depend on the background information, the decision, and the additional data.\n Utilities: the gains or losses associated with each of the possible outcomes. Their values also depend on the background information, the decision, and the additional data.\n Data and other additional information, sometimes called evidence. They differ from the background information in that they can change with every decision instance made by the same agent, while the background information stays the same. In the assembly-line scenario, for example, the test results could be different for every new electric component.\n\n\n\n\n\n\n\n\n\n\n\n\n\n Remember: What matters is to be able to identify these elements in a concrete problem, understanding their role. Their technical names don’t matter.\n\n\nNote that it is not always the case that the outcomes are unknown and the data are known. As we’ll discuss later, in some situations we reason in hypothetical or counterfactual ways, using hypothetical data and considering outcomes which have already occurred.\n\n\n\n\n\n\n Reading\n\n\n\n§ 1.1.4 in Artificial Intelligence\n\n\n\n\n\n\n\n\n Exercise\n\n\n\n\nIdentify the element above in the assembly-line decision problem of the introduction 1.\nSketch the diagram of the assembly-line decision problem.\n\n\n\nSome of the decision-problem elements listed above may themselves need to be analysed by a decision sub-problem. For instance, the utilities could depend on uncertain factors: thus we have a decision sub-problem to determine the optimal values to be used for the utilities of the main problem. This is an example of the modular character of decision theory.\nWe shall soon see how to mathematically represent these elements.\nThe elements above must be identified unambiguously in every decision problem. The analysis into these elements greatly helps in making the problem and its solution well-defined.\nAn advantage of decision theory is that its application forces us to make sense of an engineering problem. A useful procedure is to formulate the general problem in terms of the elements above, identifying them clearly. If the definition of any of the terms involves uncertainty of further decisions, then we analyse it in turn as a decision sub-problem, and so on.\n\nSuppose someone (probably a politician) says: “We must solve the energy crisis by reducing energy consumption or producing more energy”. From a decision-making point of view, this person has effectively said nothing whatsoever. By definition the “energy crisis” is the problem that energy production doesn’t meet demand. So this person has only said “we would like the problem to be solved”, without specifying any solution. A decision-theory approach to this problem requires us to specify which concrete courses of action should be taken for reducing consumption or increasing productions, and what their probable outcomes, costs, and gains would be.\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nSee MacKay’s options-vs-costs rational analysis in Sustainable Energy – without the hot air"
  },
  {
    "objectID": "basic_decisions.html#inference-utility-maximization",
    "href": "basic_decisions.html#inference-utility-maximization",
    "title": "3  Basic decision problems",
    "section": "3.2 Inference, utility, maximization",
    "text": "3.2 Inference, utility, maximization\nThe solution of a basic decision-making problem can be roughly divided into three main stages: inference, utility assessment, and expected-utility maximization.\n Inference is the stage where the probabilities of the possible outcomes are calculated. Its rules are given by the Probability Calculus. Inference is independent from decision: in some situations we may simply wish to assess whether some hypotheses, conjectures, or outcomes are more or less plausible than others, without making any decision. This kind of assessment can be very important in problems of communication and storage, and it is specially considered by Information Theory.\nThe calculation of probabilities can be the part that demands most thinking, time, and computational resources in a decision problem. It is also the part that typically makes most use of data – and where data can be most easily misused.\nRoughly half of this course will be devoted in understanding the laws of inference, their applications, uses, and misuses.\n\n Utility assesment is the stage where the gains or losses of the possible outcomes are calculated. Often this stage requires further inferences and further decision-making sub-problems. The theory underlying utility assessment is still much underdeveloped, compared to probability theory.\n\n Expected-utility maximization is the final stage where the probabilities and gains or costs of the possible outcomes are combined, in order to determine the optimal decision."
  },
  {
    "objectID": "inference.html#what-is-inference",
    "href": "inference.html#what-is-inference",
    "title": "4  Inference",
    "section": "4.1 What is inference?",
    "text": "4.1 What is inference?\nDrawing an inference could mean: to assess whether an unknown outcome or hypothesis is true or false, on the basis of some known data and information. This definition, however, is too limited in two respects, especially in problems of engineering and data science. We need to make it more general:\n\nFrom truth to probability\nOur starting point is decision-making under uncertainty, because in concrete engineering problems it is often impossible to determine the truth or falsity of a hypothesis with certainty.\nSo by drawing an inference we cannot mean to assess the plausibility or credibility or probability rather than the truth or falsity. Truth-determination is still included as a special case: fully certain corresponds to true, and impossible corresponds to false. We’ll discuss the “plausibility” idea in more detail later.\n\n\nHypothetical and counterfactual inferences\nIt is not always the case that we want to assess the truth or plausibility of something unknown on the basis of something known. An agent must often make hypothetical or counterfactual reasoning.\nOur assembly-line scenario gives a concrete example. The inference there is that the component will fail within a year if it is accepted and sold. But that hasn’t been decided yet; maybe it’s going to be rejected instead. This is an example of hypothetical reasoning: we are making an inference based partly on facts and partly on imagined information, whose truth is still unknown.\nIn other situations, for example when backtracking an error, we may need to assess the plausibility of an outcome which did not happen, based on a hypothesis which may or may not be true. This assessment may help us in ruling out that hypothesis. This is an example of counterfactual reasoning.\nBoth these kinds of reasoning are staples of scientific research. We will see that they continuously enter in the calculation of probabilities."
  },
  {
    "objectID": "inference.html#mathematical-representation-of-inferences-and-their-objects",
    "href": "inference.html#mathematical-representation-of-inferences-and-their-objects",
    "title": "4  Inference",
    "section": "4.3 Mathematical representation of inferences and their objects",
    "text": "4.3 Mathematical representation of inferences and their objects\nWe shall soon explore the rules that govern inference. But first we must address two questions:\n\nInference about what? and given what? So far we’ve spoken about “outcomes”, “data”, “hypotheses”, “background knowledge”, and similar concepts. They are not identical concepts; can we view them all as instances of a more unified notion?\nHow to mathematically represent this unified notion and those concepts? A mathematical representation is necessary for systematically approaching inference and decision problems of any complexity, and also for implementation in computer algorithms and artificial-intelligence agents."
  },
  {
    "objectID": "sentences.html#data-outcomes-agents-decisions",
    "href": "sentences.html#data-outcomes-agents-decisions",
    "title": "5  Sentences",
    "section": "5.1 Data, outcomes, agents, decisions",
    "text": "5.1 Data, outcomes, agents, decisions\nIs there a flexible and general way of representing data, outcomes, background information, hypotheses, assumptions, and even decisions?\nWhen speaking of “data”, what comes to mind to many people is basically numbers or collections of numbers. Maybe numbers, then, could be used to represent all the concepts above. This possibility turns out to be too restricting.\nI give you this number: “\\(8\\)”, saying that it is “data”. But what is it about? You, as an agent, can hardly call this number a piece of information, because you have no clue what to do with it. Instead, if I tell you: “The number of official planets in the solar system is 8”, then we can say that I’ve given you data. So “data” is not just numbers: a number is not “data” unless there’s an additional verbal, non-numeric context accompanying it, even if only implicitly. Sure, we could represent this meta-data information as numbers too; but this move would only shift the problem one level up: we would need an auxiliary verbal context explaining what the meta-data numbers are about.\nData can, moreover, be completely non-numeric. A clinician saying “The patient has fully recovered from the disease” (we imagine to know who’s the patient and what was the disease) is giving us a piece of information that we could further use, for instance, to make prognoses about other, similar patients. The clinician’s statement surely is “data”, but essentially non-numeric data. In some situations we can represent it as “1”, while “0” would represent “not recovered”. Yet the opposite convention could also be used, showing that these two numbers have intrinsically nothing to do with the clinician’s data.\nBut the examples above actually reveal the answer to our needs. In the examples we expressed the data by means of sentences. Clearly any piece of information, outcome, hypothesis, decision can be expressed by a sentence. We shall therefore use sentences, also called propositions or statements,1 to represent and communicate background information, data, outcomes, hypotheses, knowns, unknowns, and any other things on which or from which inferences can be drawn. In some cases we can of course summarize a sentence by a number, as a shorthand, when the full meaning of the sentence is understood.1 These three terms are not always equivalent in formal logic, but here we’ll use them as synonyms.\nBut what is a sentence, more exactly? The everyday meaning of this word will work for us, even though there is still a lot of research in logic an artificial intelligence on how to define and use sentences. We shall adopt this useful definition:\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nPropositions\n\n\n\n\n\n\n\n\n\n\n\n\nA “sentence” is a verbal message for which we can determine whether it is true or false, at least in principle and in such a way that all interested receivers of the message would agree.\n\n\nFor instance, in most engineering contexts the phrase “This valve will operate for at least two months” is a sentence; whereas the phrase “Apples are much tastier than pears” is not, because it’s a matter of personal taste – there’s no objective criterion to determine its truth or falsity. However, the phrase “Rita finds apples tastier than pears” could be a sentence (its truth is found by asking Rita).\n\n\nWe shall see later that a phrase may contain a lot of technical terms, and still not be a sentence.\nNote that a sentence can contain numbers, and even pictures and graphs: this possibility is not excluded from the definition above.\nThe use of sentences in our framework has important practical consequences:\n\nClarity, analysis, goal-orientation. A data engineer must acquire information and convey information. “Acquiring information” does not simply consist in making measurements or counting something: the engineer must understand what is being measured and why. If data is gathered from third parties, the engineer must ask what exactly the data mean and how they were acquired. In designing and engineering a solution, it is important to understand what information or outcomes the end user exactly wants. A data engineer will often ask “wait, what do you mean by that?”. This question is not just an unofficial parenthesis in the official data-transfer workflow between the engineer and someone else. It is an integral part of that workflow: it means that some data have not been completely transferred yet.\nArtificial Intelligence. Sentences are the central components of knowledge representation in AI agents.\n\n\n\n\n\n\n\n Reading\n\n\n\n§ 7.1 in Artificial Intelligence.\n\n\n\nNotation\nIn these notes we’ll denote sentences by sans-serif italic letters: \\(\\mathsfit{A},\\mathsfit{B},\\mathsfit{a},\\mathsfit{b},\\dotsc\\) For example, \\[\n\\mathsfit{A} \\coloneqq \\textsf{\\small`The power output is 100\\,W'}\n\\] means that the symbol \\(\\mathsfit{A}\\) stands for the sentence above. Often we shall simply write sentences in abbreviated form, when their full meaning is understood from the context. For instance, we could abbreviate the sentence above to “\\(O = 100\\,\\mathrm{W}\\)”, where \\(O\\) is a physical variable (not a sentence!) denoting the power output; or even just to “\\(100\\,\\mathrm{W}\\)”.\nWe’ll next see how more complex sentences are built from simpler ones. No matter whether complex or simple, any sentence can be represented by symbols like the ones above."
  },
  {
    "objectID": "sentences.html#combining-sentences",
    "href": "sentences.html#combining-sentences",
    "title": "5  Sentences",
    "section": "5.2 Combining sentences",
    "text": "5.2 Combining sentences\n\nBasic sentences\nIn analysing the information, data, outcomes, hypotheses that enter into an inference problem, it is convenient to find a collection of basic sentences2 out of which all other sentences of interest can be constructed. These basic sentences often represent elementary pieces of information in the problem.2 A more technical term is atomic sentences.\nConsider for instance the following statement, which could appear in our assembly-line scenario:\n\n“The electronic component is still whole after the shock test and the subsequent heating test. The voltage reported in the power test is either 90 mV or 110 mV.”\n\n\nIn this statement we can identify at least four basic sentences, which we denote by convenient symbols: \\[\\begin{aligned}\n\\mathsfit{s} &\\coloneqq \\textsf{\\small`The component is whole after the shock test'}\n\\\\\n\\mathsfit{h} &\\coloneqq \\textsf{\\small`The component is whole after the heating test'}\n\\\\\n\\mathsfit{v}_{90} &\\coloneqq \\textsf{\\small`The power-test voltage reading is 90\\,mV'}\n\\\\\n\\mathsfit{v}_{110} &\\coloneqq \\textsf{\\small`The power-test voltage reading is 110\\,mV'}\n\\end{aligned}\n\\]\nThe inference may actually require more basic sentences than just these. For instance, it might become necessary to consider basic sentences with other values for the reported voltage, such as \\[\\begin{aligned}\n\\mathsfit{v}_{110} &\\coloneqq \\textsf{\\small`The power-test voltage reading is 100\\,mV'}\n\\\\\n\\mathsfit{v}_{80} &\\coloneqq \\textsf{\\small`The power-test voltage reading is 80\\,mV'}\n\\end{aligned}\\] and so on.\nThere may also be other basic sentences coming from data or hypotheses that aren’t stated explicitly because they’re obvious. Yet they may have to be spelled out. An example in our scenario is the sentence \\[\n\\textsf{\\small`The electronic component cannot be broken after the shock test and whole after the subsequent heating test'}\n\\] which must necessarily be true for physical reasons.\n\n\nConnectives\nHow do we construct the statement above and other more complex statements out of basic sentences?\nWe consider one operation to change a sentence into another related to it, and two operations to combine two or more sentences together. These operations are called connectives, and you may have already encountered them in Boolean algebra. Our natural language offers many more operations to combine sentences, but these three connectives turn out to be all we need in virtually all engineering and data science problems:\n\n\nNot:  \\(\\lnot\\)\n\nfor example, \\[\n\\lnot \\mathsfit{s} = \\textsf{\\small`The component is broken after the shock test'}\n\\]\n\nAnd:  \\(\\land\\)\n\nfor example, \\[\n\\mathsfit{s} \\land \\mathsfit{h} = \\textsf{\\small`The component is whole after the shock and heating tests'}\n\\]\n\nOr:  \\(\\lor\\)\n\nfor example, \\[\n\\mathsfit{v}_{90} \\lor \\mathsfit{v}_{110} = \\textsf{\\small`The power-test voltage reading is 90\\,mV, or 110\\,mV, or both'}\n\\]\n\n\n\n\n\nThe connectives can be applied multiple times, to form increasingly complex sentences.\n  Note some important subtleties of the connectives:\n\nThere is no strict correspondence between the words “not”, “and”, “or” in natural language and the three connectives. For instance the and connective could correspond to the words “but” or “whereas”, or just to a comma “ , ”.\nNot doesn’t mean some kind of complementary quality, but only the negation. For instance,  \\(\\lnot\\textsf{\\small`The chair is black'}\\)  generally does not mean  \\(\\textsf{\\small`The chair is white'}\\) ,   although in some situations these two sentences could amount to the same thing.\nIt’s best to always declare explicitly what the not of a sentence concretely means. In our example we take \\[\n  \\lnot\\textsf{\\small`The component is whole'} \\equiv \\textsf{\\small`The component is broken'}\n  \\] But in other examples the negation of “being whole” could comprise several different conditions. A good guideline is to always state the not of a sentence in positive terms.\nOr does not exclude that both the sentences it connects can be true. So in our example  \\(\\mathsfit{v}_{90} \\lor \\mathsfit{v}_{110}\\)  does not exclude, a priori, that the reported voltage could be both 90 mV and 110 mV. (There is a connective for that: “exclusive-or”, but it can be constructed out of the three we already have.)\n\nFrom the last remark we see that the sentence \\[\n\\textsf{\\small`The power-test voltage reading is 90\\,mV or 110\\,mV'}\n\\] does not correspond to   \\(\\mathsfit{v}_{90} \\lor \\mathsfit{v}_{110}\\) .  It is implicitly clear that a voltage reading cannot yield two different values at the same time. Convince yourself that the correct way to write that sentence is this: \\[\n(\\mathsfit{v}_{90} \\lor \\mathsfit{v}_{110})\n\\land\n\\lnot(\\mathsfit{v}_{90} \\land \\mathsfit{v}_{110})\n\\]\nFinally, the full data statement of our present example can be written in symbols as follows: \\[\n\\mathsfit{s} \\land \\mathsfit{h} \\land\n(\\mathsfit{v}_{90} \\lor \\mathsfit{v}_{110})\n\\land\n\\lnot(\\mathsfit{v}_{90} \\land \\mathsfit{v}_{110})\n\\]"
  },
  {
    "objectID": "sentences.html#distinguishing-assumptions-and-outcomes",
    "href": "sentences.html#distinguishing-assumptions-and-outcomes",
    "title": "5  Sentences",
    "section": "5.3 Distinguishing assumptions and outcomes",
    "text": "5.3 Distinguishing assumptions and outcomes\nNow we know how to represent arbitrarily complex sentences and express them in symbols. Let’s introduce a way to clearly distinguish sentences that constitute the assumptions from those that constitute the outcome in a basic decision problem. In our assembly-line example, suppose that assumptions are the statement discussed in the previous section. The outcome to be inferred is expressed by this sentence: \\[\n\\mathsfit{f} \\coloneqq \\textsf{\\small`The component will fail within a year'}\n\\]\nTo distinguish assumptions and outcomes we can simply use a vertical bar,3 like  “\\(\\color[RGB]{68,119,170}\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}}\\)” :3 Notation in formal logic uses the symbols  ” \\(\\models\\) ”  or  “ \\(\\vdash\\) ”,  and writes assumptions on the left, outcomes on the right. We use the notation used in probability logic.\n\non its left side we write the sentence representing the outcome,\non its right side we write the sentences that make up our assumptions, and-ed together:\n\n\\[\n\\textit{\\small outcome} \\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}} \\textit{\\small assumptions}\n\\]\nSo in our example we write: \\[\nf \\nonscript\\:\\Big\\vert\\nonscript\\:\\mathopen{}\n\\mathsfit{s} \\land \\mathsfit{h} \\land\n(\\mathsfit{v}_{90} \\lor \\mathsfit{v}_{110})\n\\land\n\\lnot(\\mathsfit{v}_{90} \\land \\mathsfit{v}_{110})\n\\]\nThe collection of assumptions on the right side of the bar “\\(\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}}\\)” is called the conditional. The expression above is read\n“\\(\\mathsfit{c}_{2,90}\\)   given   \\(\\mathsfit{t}_{1,50} \\land \\mathsfit{t}_{2,50} \\land\\dotsb\\)”\nor\n“\\(\\mathsfit{c}_{2,90}\\)   conditional on   \\(\\mathsfit{t}_{1,50} \\land \\mathsfit{t}_{2,50} \\land\\dotsb\\)”.\n\n\nWe are now equipped with all the notions and symbolic notation to deal with our first concrete goal: drawing uncertain inferences."
  },
  {
    "objectID": "sentences.html#inferences-certain-and-uncertain",
    "href": "sentences.html#inferences-certain-and-uncertain",
    "title": "5  Sentences",
    "section": "5.4 Inferences: certain and uncertain",
    "text": "5.4 Inferences: certain and uncertain\nIn a decision problem we have a set of different outcomes that we want to assess given the same assumptions: \\[\\begin{aligned}\n\\textit{\\small outcome\\,1} &\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}} \\textit{\\small assumptions}\n\\\\\n\\textit{\\small outcome\\,2} &\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}} \\textit{\\small assumptions}\n\\\\\n\\textit{\\small outcome\\,3} &\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}} \\textit{\\small assumptions}\n\\\\\n\\dotso&\n\\end{aligned}\\]\nThe first goal in a decision problem is to assess these outcomes. What do we mean by “assess”? We cannot demand that the truth or falsity of the outcomes be determined with certainty."
  },
  {
    "objectID": "truth_inference.html#building-blocks",
    "href": "truth_inference.html#building-blocks",
    "title": "6  Truth inference",
    "section": "6.1 Building blocks",
    "text": "6.1 Building blocks\nConsider the following trivial problem. An inspector examines an electronic component out of a production line. The information available to the inspector is the following:\n\nThe component can either come from the production line in Oslo, or from the one in Rome.\nIf the component is defective, it cannot come from Oslo.\nThe component is found to be defective.\n\nThe question is: from which production line does the component come from?\nThe answer is obvious: from the Rome line. But how could we draw this obvious and sure inference? Which rules did we follow? Did we make any hidden assumptions, or use information that wasn’t explicitly mentioned?\nLogic is the huge field that formalizes and makes rigorous the rules that a rational person or an artificial intelligence should use in drawing sure inferences. We’ll get a glimpse of it here, as a trampoline for jumping towards the more general inferences that we need in data-driven engineering problems.\n\nAnalysis of the problem\nLet’s write down the basic sentences that constitute our data and the inferences we want to draw. We identify three basic sentences, which we can represent by these symbols:\n\n\\(o \\coloneqq \\textsf{\\small`The component comes from the Oslo line'}\\)\n\\(r \\coloneqq \\textsf{\\small`The component comes from the Rome line'}\\)\n\\(d \\coloneqq \\textsf{\\small`The component is defective'}\\)\n\nObviously the inspector possesses even more information which is implicitly understood. It’s clear, for instance, that the component cannot come from both Oslo and Rome. Let’s denote this information with\n\n\\(I \\coloneqq{}\\)(a long collection of sentences explaining all other implicitly understood information).\n\n\nWith the sentences above we can express more complex details and hypotheses appearing in the inspector’s problem, in particular:\n\n\\(o \\lor r = \\textsf{\\small`The component comes from either the Oslo line or the Rome line'}\\)\n\\(\\lnot(o \\land r) = \\textsf{\\small`The component cannot come from both the Oslo and the Rome lines'}\\)\n$ o $\n\n\n\nData, assumptions, desired conclusions\nThe inspector knows for certain the following facts:\n\n\\(o \\lor r\\), \\(\\textsf{\\small`The component comes from either the Oslo line or the Rome line'}\\)\n\\(\\lnot(o \\land r)\\), \\(\\textsf{\\small`The component cannot come from both the Oslo and the Rome lines'}\\)\n\\(d\\), \\(\\textsf{\\small`The component is defective'}\\)\n\\(I\\), all remaining implicit information\n\nWe and them all together: \\[\nd \\land (o \\lor r) \\land \\lnot (o \\land r) \\land I \\ .\n\\]\nThe inspector knows, moreover, this hypothetical consequence:\n\n\\(\\lnot o \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} d \\land (o \\lor r) \\land \\lnot (o \\land r) \\land I\\), if the component is defective, it cannot come from the Oslo production line."
  },
  {
    "objectID": "truth_inference.html#background-information-and-conditional",
    "href": "truth_inference.html#background-information-and-conditional",
    "title": "6  Truth inference",
    "section": "6.2 Background information and conditional",
    "text": "6.2 Background information and conditional"
  },
  {
    "objectID": "truth_inference.html#truth-inference-rules",
    "href": "truth_inference.html#truth-inference-rules",
    "title": "6  Truth inference",
    "section": "6.3 Truth-inference rules",
    "text": "6.3 Truth-inference rules\nDeduction systems in formal logic give us a set of rules for making correct inferences, that is, for correctly determining whether the conclusions of interest are true or false. These rules are represented in a wide variety of ways, as steps leading from one conclusion to another one. The picture here on the margin, for instance, shows how a proof of our inference would look like, using the so-called sequent calculus.\n\n\n\n\n\nThe bottom formula is our conclusion; the formulae above it represent steps in the proof. Each line denotes the application of an inference rule. The two formulae with no line above are our two assumptions.\n\n\n\n\nWe can compactly encode all inference rules in the following way. First, represent true by the number 1, and false by 0. Second, symbolically write that conclusion \\(C\\) is true, given assumptions \\(A\\), as follows: \\[\n\\mathrm{T}(C \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} A) = 1 \\ .\n\\] or with 0 if it’s false.\nThe rules of truth inference are then encoded by the following equations, which must always hold for any sentences \\(A,B,C\\), no matter whether they are basic or complex:\n\n\n\nRule for “not”:\n\n\\[\\mathrm{T}(\\lnot A \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} B)\n+ \\mathrm{T}(A \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} B)\n= 1 \\tag{6.1}\\]\n\nRule for “and”:\n\n\\[\n\\mathrm{T}(A \\land B \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} C)\n= \\mathrm{T}(A \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} B \\land C) \\cdot\n\\mathrm{T}(B \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} C)\n= \\mathrm{T}(B \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} A \\land C) \\cdot\n\\mathrm{T}(A \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} C)\n\\tag{6.2}\\]\n\nRule for “or”:\n\n\\[\\mathrm{T}(A \\lor B \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} C)\n= \\mathrm{T}(A \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} C) +\n\\mathrm{T}(B \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} C)\n- \\mathrm{T}(A \\land B \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} C)\n\\tag{6.3}\\]\n\nRule of self-consistency:\n\n\\[\\mathrm{T}(A \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} A \\land C)\n= 1\n\\tag{6.4}\\]\n\n\n\n\n\n\nLet’s see how the inference rule (?eq-example-rule), for example, is encoded in these equations. The rule starts with saying that \\(a \\land b\\) is true according to \\(D\\). This means that \\(\\mathrm{T}(a \\land b \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} D)=1\\). But, by rule (6.2), we must then have \\(\\mathrm{T}(b \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} a \\land D) \\cdot \\mathrm{T}(a \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} D) = 1\\). This can only happen if both \\(\\mathrm{T}(b \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} a \\land D)\\) and \\(\\mathrm{T}(a \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} D)\\) are equal to \\(1\\). So we can conclude that \\(\\mathrm{T}(a \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} D)=1\\), which is exactly the conclusion under the line in rule (?eq-example-rule).\n\n\n\n\n\n\n Exercise\n\n\n\nTry to prove our initial inference\n\\[\n\\frac{\n(b \\lor r) \\land \\lnot (b \\land r) \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} D\n\\qquad\n\\lnot r \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} D\n}{\nb\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} D\n}\n\\]\nusing the basic rules (6.1, 6.2, 6.3, 6.4). Remember that you can use each rule as many times as you like, and that there is not only one way of constructing a proof."
  },
  {
    "objectID": "truth_inference.html#logical-ai-agents-and-their-limitations",
    "href": "truth_inference.html#logical-ai-agents-and-their-limitations",
    "title": "6  Truth inference",
    "section": "6.4 Logical AI agents and their limitations",
    "text": "6.4 Logical AI agents and their limitations\nThe basic rules above are also the rules that a logical artificial-intelligent agent should follow.\n\n\n\n\n\n\n Reading\n\n\n\nCh. 7 in Artificial Intelligence\n\n\nMany – if not most – inference problems that a data engineer must face are, however, of the uncertain kind: it is not possible to surely infer the truth of some data, and the truth of some initial data may not be known either. In the next chapter we shall see how to generalize the logic rules to uncertain situations.\n\n\n\n\n\n\n For the extra curious\n\n\n\nOur cursory visit of formal logic only showed a microscopic part of this vast field. The study of logic rules continues still today, with many exciting developments and applications. Feel free take a look at Logic in Computer Science, Mathematical Logic for Computer Science, Natural Deduction Systems in Logic"
  },
  {
    "objectID": "probability_inference.html#when-truth-isnt-known-probability",
    "href": "probability_inference.html#when-truth-isnt-known-probability",
    "title": "7  Probability inference",
    "section": "7.1 When truth isn’t known: probability",
    "text": "7.1 When truth isn’t known: probability\nIn most real-life and engineering situations we don’t know the truth or falsity of sentences and hypotheses that interest us. But this doesn’t mean that nothing can be said or done in such situations.\nWhen we cross a busy city street we look left and right to check whether any cars are approaching. We typically don’t look up to check whether something is falling from the sky. Yet, couldn’t it be false that cars are approaching? and couldn’t it be true that some object is falling from the sky? Of course both events are possible. Then why do we look left and right, but not up?\nThe main reason1 is that we believe strongly that cars might be approaching, believe very weakly that some object might be falling from the sky. In other words, we consider the first occurrence to be very probable; the second, extremely improbable.1 We shall see later that one more factor enters the explanation.\nWe shall take the notion of probability as intuitively understood (just as we did with the notion of truth). Terms equivalent for “probability” are degree of belief, plausibility, credibility.\n\n\n\n\n\n\n\n\n\n\n In technical discourse, likelihood means something different and is not a synonym of “probability”, as we’ll explain later.\n\n\nProbabilities are quantified between 0 and 1, or equivalently between 0% and 100%. Assigning to a sentence a probability 1 is the same as saying that it is true; and a probability 0, that it is false. A probability of 0.5 represents a belief completely symmetric with respect to truth and falsity.\nIt is important to emphasize and agree on some facts about probabilities:\n\nProbabilities are assigned to sentences. Consider an engineer working on a problem of electric-power distribution in a specific geographical region. At a given moment the engineer may believe with 75% probability that the measured average power output in the next hour will be 100 MW. The 75% probability is assigned not to the quantity “100 MW”, but to the sentence \\[\n\\textsf{\\small`The measured average power output in the next hour will be 100\\,MW'}\n\\] This difference is extremely important. Consider the alternative sentence \\[\n\\textsf{\\small`The average power output in the next hour will be \\emph{set} to 100\\,MW'}\n\\] the quantity is the same, but the meaning is very different. The probability can therefore be very different (if the engineer is the person deciding the output, the probability is 100%). The probability depends not only on a number, but on what it’s being done with that number – measuring, setting, third-party reporting, and so on. Often we still write simply \\(\\textsf{\\small`\\(\\mathsf{O = 100\\,W}\\)'}\\) or even just \\(\\textsf{\\small`100\\,W'}\\), provided that the full sentence behind the shorthand is understood.\nProbabilities are agent- and context-dependent. A coin is tossed, comes down heads, and is quickly hidden from view. Alice sees that it landed heads-up. Bob instead doesn’t manage to see the outcome and has no clue. Alice considers the sentence \\(\\textsf{\\small`Coin came down heads'}\\) to be true, that is, to have 100% probability. Bob considers the same sentence to have 50% probability.\nNote how Alice and Bob assign two different probabilities to the same sentence; yet both assignments are completely rational. If Bob assigned 100% to \\(\\textsf{\\small`heads'}\\), we would suspect that he had seen the outcome after all; if he assigned 0% to \\(\\textsf{\\small`heads'}\\), we would consider that groundless and silly. We would be baffled if Alice assigned 50% to \\(\\textsf{\\small`heads'}\\), because she saw the outcome was actually heads; we would hypothesize that she feels unsure about what she saw.\nAn omniscient agent would know the truth or falsity of every sentence, and assign only probabilities 0 or 1. Some authors speak of “actual (but unknown) probabilities”; if there were “actual” probabilities, they would be all 0 or 1, and it would be pointless to speak about probabilities at all – every inference would be a truth inference.\nProbabilities are not frequencies. The fraction of defective mechanical components to total components produced per year in some factory is a quantity that can be physically measured and would be agreed upon by every agent. It is a frequency, not a degree of belief or probability. It is important to understand the difference between them, to avoid making sub-optimal decisions; we shall say more about this difference later. Frequencies can be unknown to some agents, probabilities cannot be unknown (but can be difficult to calculate). Be careful when you read authors speaking of an “unknown probability”; either they actually mean “unknown frequency”, or a probability that has to be calculated (it’s “unknown” in the same sense that the value of \\(1-0.7 \\cdot 0.2/(1-0.3)\\) is unknown to you right now).\nProbabilities are not physical properties. Whether a tossed coin lands heads up or tails up is fully determined by the initial conditions (position, orientation, momentum, rotational momentum) of the toss and the boundary conditions (air velocity and pressure) during the flight. The same is true for all macroscopic engineering phenomena (even quantum phenomena have never been proved to be non-deterministic, and there are deterministic and experimentally consistent mathematical representations of quantum theory). So we cannot measure a probability using some physical apparatus; and the mechanisms underlying any engineering problem boil down to physical laws, not to probabilities.\n\n\n\n\n\n\n\n Reading\n\n\n\nDynamical Bias in the Coin Toss \n\n\nThese facts are not just a matter of principle. They have important practical consequences. A data engineer who is not attentive to the source of the data (measured? set? reported, and so maybe less trustworthy?), or who does not carefully assess the context of a probability, or who mixes it up with something else, or who does not take advantage (when possible) of the physics involved in the engineering problem, will design a system with sub-optimal performance2 – or even cause deaths.2 This fact can be mathematically proven."
  },
  {
    "objectID": "probability_inference.html#no-new-building-blocks",
    "href": "probability_inference.html#no-new-building-blocks",
    "title": "7  Probability inference",
    "section": "7.2 No new building blocks",
    "text": "7.2 No new building blocks\nIn discussing truth-inference we introduced notations such as \\(\\mathrm{T}(a \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} b \\land D)\\), which stands for the truth-value 0 or 1 of sentence \\(a\\) in the context of data \\(D\\) and supposing (even if only hypothetically) sentence \\(b\\) to be true. We can simply extend this notation to probability-values, using a \\(\\mathrm{P}\\) instead of \\(\\mathrm{T}\\): \\[\\mathrm{P}(a \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} b \\land D) \\in [0,1]\\] represents the probability or degree of belief in sentence \\(a\\) in the context of data \\(D\\) and supposing also sentence \\(b\\) to be true. Keep in mind that both \\(a\\) and \\(b\\) could be complex sentences (for instance \\(a = (\\lnot c \\lor d) \\land e\\)). Note that truth-values are included as the special cases1 or 0: \\[\n\\mathrm{P}(a \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} b \\land D) = 0\\text{ or }1\n\\quad\\Longleftrightarrow\\quad\n\\mathrm{T}(a \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} b \\land D) = 0\\text{ or }1\n\\]"
  },
  {
    "objectID": "probability_inference.html#probability-inference-rules",
    "href": "probability_inference.html#probability-inference-rules",
    "title": "7  Probability inference",
    "section": "7.3 Probability-inference rules",
    "text": "7.3 Probability-inference rules\nExtending our truth-inference notation to probability-inference notation has been straightforward. But how do we draw inferences when probabilities are involved?\nConsider the inference about my umbrella in a more uncertain situation:\n\n\\[\n\\frac{\n\\mathrm{P}(\\textsf{\\small`My umbrella is either blue or red'}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}D)=1\\quad\n\\mathrm{P}(\\textsf{\\small`My umbrella is not red'} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} D)=0.5\n}{\n\\mathrm{P}(\\textsf{\\small`My umbrella is blue'} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} D) = \\mathord{?}\n}\n\\]\n\nor more compactly, using the symbols we introduced earlier, \\[\n\\frac{\n\\mathrm{P}\\bigl[(b \\lor r) \\land \\lnot (b \\land r)\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}D\\bigr]=1\\quad\n\\mathrm{P}(\\lnot r\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} D)=0.5\n}{\n\\mathrm{P}( b \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} D) = \\mathord{?}\n}\n\\] This says, above the line, that: according to our data \\(D\\) my umbrella is either blue or red (and can’t be both), with full certainty; and according to our data we have no preferential beliefs on whether my umbrella is not red. What should then be the probability of my umbrella being blue, according to our data?\nIntuitively that probability should be 50%: \\(\\mathrm{P}( b \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} D)=0.5\\). But which rules did we follow in arriving at this probability? More generally, which rules should we follow in assigning new probabilities from given ones?\nThe amazing result is that the rules for truth-inference, formulae (6.1, 6.3, 6.2, 6.4), extend also to probability-inference. The only difference is that they now hold for all values in the range \\([0,1]\\), rather than only values \\(0\\) and \\(1\\).\nThis important result was taken more or less for granted at least since Laplace in the 1700s. But was formally proven for the first time in the 1940s by R. T. Cox; the proof has been refined since then. What kind of proof is it? It shows that if we don’t follow the rules we arrive at illogical conclusions; we’ll show some examples later.\nHere are the fundamental rules of probability inference. In these rules, all probabilities can have values in the range \\(\\mathrm{P}() \\in [0,1]\\), and the symbols \\(a,b,D\\) represent sentences of any complexity:\n\n\n\n\n\n\n\n   THE FUNDAMENTAL LAWS OF INFERENCE   \n\n\n\n\n“Not” \\(\\boldsymbol{\\lnot}\\) rule\n\n\\[\\mathrm{P}(\\lnot a \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} D)\n+ \\mathrm{P}(a \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} D)\n= 1\\]\n\n\n“And” \\(\\boldsymbol{\\land}\\) rule\n\n\\[\n\\mathrm{P}(a \\land b \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} D)\n= \\mathrm{P}(a \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} b \\land D) \\cdot\n\\mathrm{P}(b \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} D)\n= \\mathrm{P}(b \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} a \\land D) \\cdot\n\\mathrm{P}(a \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} D)\n\\]\n\n\n“Or” \\(\\boldsymbol{\\lor}\\) rule\n\n\\[\\mathrm{P}(a \\lor b \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} D)\n= \\mathrm{P}(a \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} D) +\n\\mathrm{P}(b \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} D)\n- \\mathrm{P}(a \\land b \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} D)\n\\]\n\n\nSelf-consistency rule\n\n\\[\\mathrm{P}(a \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} a \\land D)\n= 1\n\\]\n\n\n\n\n\nIt is amazing that ALL inference is nothing else but a repeated application of these four rules – billions of times or more, in some inferences. All machine-learning algorithms are just applications or approximations of these rules. Methods that you may have heard about in statistics are just specific applications of these rules. Truth inferences are also special applications of these rules. Most of this course is, at bottom, just a study of how to apply these rules in particular kinds of problems.\n\n\n\n\n\n\n Reading\n\n\n\n\nProbability, Frequency and Reasonable Expectation\nCh. 2 of Bayesian Logical Data Analysis for the Physical Sciences\n§§ 1.0–1.2 of Data Analysis\nFeel free to skim through §§ 2.0–2.4 of Probability Theory"
  },
  {
    "objectID": "probability_inference.html#how-the-inference-rules-are-used",
    "href": "probability_inference.html#how-the-inference-rules-are-used",
    "title": "7  Probability inference",
    "section": "7.4 How the inference rules are used",
    "text": "7.4 How the inference rules are used\nThe fundamental rules represent, first of all, constraints of logical consistency among probabilities. If we have probabilities \\(\\mathrm{P}(a\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}D)=0.7\\), \\(\\mathrm{P}(b\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}a\\land D)=0.1\\), \\(\\mathrm{P}(a\\land b\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}D)=0.2\\), then there’s an inconsistency somewhere, because these values violate the and-rule:  \\(0.2 \\ne 0.1 \\cdot 0.7\\).  In this case we must find the inconsistency and solve it. Since probabilities are quantified by real numbers, however, it’s possible and acceptable to have slight discrepancies owing to numerical round-off errors.\nThe rules also imply more general constraints. For example we must always have \\[\\begin{gather*}\n\\mathrm{P}(a\\land b \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}D) \\le \\min\\bigl\\{\\mathrm{P}(a\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}D),\\  \\mathrm{P}(b\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}D)\\bigr\\}\n\\\\\n\\mathrm{P}(a\\lor b \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}D) \\ge \\max\\bigl\\{\\mathrm{P}(a\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}D),\\  \\mathrm{P}(b\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}D)\\bigr\\}\n\\end{gather*}\\]\n\n\n\n\n\n\n Exercise\n\n\n\nTry to prove the two constraints above\n\n\nThe main use of the rules in concrete applications is for calculating new probabilities from given ones. The calculated probabilities will be automatically consistent. For each equation shown in the rules we can calculate one probability given the remaining ones in the equation, with some special cases when values of \\(0\\) or \\(1\\) appear.\nFor example, if we have \\(\\mathrm{P}(a \\land b\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} D)=0.2\\) and \\(\\mathrm{P}(a\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}D)=0.7\\), from the and-rule we can find \\(\\mathrm{P}(b\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} a \\land D)\\): \\[\\begin{multline*}\n\\underbracket{\\color[RGB]{34,136,51}\\mathrm{P}(a \\land b \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} D)}_{0.2}\n= {\\color[RGB]{238,102,119}\\mathrm{P}(b \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} a \\land D)} \\cdot\n\\underbracket{\\color[RGB]{34,136,51}\\mathrm{P}(a \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} D)}_{0.7}\n\\\\[1em]\n\\Longrightarrow\\quad\n{\\color[RGB]{238,102,119}\\mathrm{P}(b\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} a \\land D)} =\n\\frac{\\color[RGB]{34,136,51}\n\\mathrm{P}(a\\land b \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} D)\n}{\\color[RGB]{34,136,51}\n\\mathrm{P}(a\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}D)\n} = \\frac{0.2}{0.7}\n\\approx 0.2857\n\\end{multline*}\\]\n\nLet us now solve the umbrella inference from the previous section. Starting from \\[\n\\mathrm{P}\\bigl[(b \\lor r) \\land \\lnot (b \\land r)\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}D\\bigr]=1 \\ ,\n\\quad\n\\mathrm{P}(\\lnot r\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} D)=0.5\n\\] we arrive at \\[\n\\mathrm{P}( b \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} D) = 0.5\n\\]\nby following from top to bottom the steps depicted here:\n\n\n\n@@ example medical diagnosis\n\nDerived rules\nThe rules above are in principle all we need to use. But from them it is possible to derive some additional shortcut rules that are automatically consistent with the fundamental ones.\nFirst, it is possible to show that all rules you may know from Boolean algebra are a consequence of the fundamental rules. For example, we can always make the following convenient replacements anywhere in a probability expression: \\[\n\\begin{gathered}\nA \\land A = A \\lor A = A\n\\qquad\n\\lnot\\lnot A = A\n\\\\[1ex]\nA\\land B = B \\land A\n\\qquad\nA \\lor B = B \\lor A\n\\\\[1ex]\n\\lnot (A \\land B) = \\lnot A \\lor \\lnot B\n\\qquad\n\\lnot (A \\lor B) = \\lnot A \\land \\lnot B\n\\\\[1ex]\nA \\land (B \\lor C) = (A \\land B) \\lor (A \\land C)\n\\\\[1ex]\nA \\lor (B \\land C) = (A \\lor B) \\land (A \\lor C)\n\\end{gathered}\n\\]\nTwo other derived rules are used extremely often, so we treat them separately."
  },
  {
    "objectID": "probability_inference.html#law-of-total-probability-or-extension-of-the-conversation",
    "href": "probability_inference.html#law-of-total-probability-or-extension-of-the-conversation",
    "title": "7  Probability inference",
    "section": "7.5 Law of total probability or “extension of the conversation”",
    "text": "7.5 Law of total probability or “extension of the conversation”"
  },
  {
    "objectID": "probability_inference.html#bayess-theorem",
    "href": "probability_inference.html#bayess-theorem",
    "title": "7  Probability inference",
    "section": "7.6 Bayes’s theorem",
    "text": "7.6 Bayes’s theorem\n\n\n\n\n\nBayes’s theorem guest-starring in The Big Bang Theory"
  },
  {
    "objectID": "probability_inference.html#consequences-of-not-following-the-rules",
    "href": "probability_inference.html#consequences-of-not-following-the-rules",
    "title": "7  Probability inference",
    "section": "7.7 consequences of not following the rules,",
    "text": "7.7 consequences of not following the rules,\n@@ §12.2.3 of AI\n\nExercise: Monty-Hall problem & variations\nExercise: clinical test & diagnosis"
  },
  {
    "objectID": "probability_inference.html#common-points-of-certain-and-uncertain-inference",
    "href": "probability_inference.html#common-points-of-certain-and-uncertain-inference",
    "title": "7  Probability inference",
    "section": "7.8 Common points of certain and uncertain inference",
    "text": "7.8 Common points of certain and uncertain inference\n\nNo premises? No conclusions!\n\n\n\n\n\n\n\n Differences in terminology\n\n\n\n\nSome texts speak of the probability of a “random3 variable”, or more precisely of the probability that a random variable takes on a particular value. As you notice, we have just expressed that idea by means of a sentence. The viewpoint and terminology of random variables is a special case of that of sentences. As already discussed, in concrete applications it is important to know how a variable “takes on” a value: for example it could be directly measured, indirectly reported, or purposely set. Thinking in terms of sentences, rather than of random variables, allows us to account for these important differences.\nSome texts speak of the probability of an “event”. For all purposes an “event” is just what’s expressed in a sentence.\n\n3 What does ”random” mean? Good luck finding an understandable and non-circular definition in texts that use that word. In these notes, if the word ”random” is ever used, it means ”unpredictable” or ”unsystematic”.It’s a question for sociology of science why some people keep on using less flexible points of view or terminologies. Probably they just memorize them as students and then a fossilization process sets in."
  },
  {
    "objectID": "data_information.html#kinds-of-data",
    "href": "data_information.html#kinds-of-data",
    "title": "8  Data and information",
    "section": "8.1 Kinds of data",
    "text": "8.1 Kinds of data\n\nBinary\n\n\nNominal\n\n\nOrdinal\n\n\nContinuous\n\nunbounded\nbounded\ncensored\n\n\n\nComplex data\n2D, 3D, images, graphs, etc.\n\n\n“Soft” data\n\norders of magnitude\nphysical bounds"
  },
  {
    "objectID": "data_information.html#data-transformations",
    "href": "data_information.html#data-transformations",
    "title": "8  Data and information",
    "section": "8.2 Data transformations",
    "text": "8.2 Data transformations\n\nlog\nprobit\nlogit"
  },
  {
    "objectID": "probability_distributions.html#the-difference-between-statistics-and-probability-theory",
    "href": "probability_distributions.html#the-difference-between-statistics-and-probability-theory",
    "title": "9  Allocation of uncertainty among possible data values: probability distributions",
    "section": "9.1 The difference between Statistics and Probability Theory",
    "text": "9.1 The difference between Statistics and Probability Theory\nStatistics is the study of collective properties of collections of data. It does not imply that there is any uncertainty.\nProbability theory is the quantification and propagation of uncertainty. It does not imply that we have collections of data."
  },
  {
    "objectID": "probability_distributions.html#whats-distributed",
    "href": "probability_distributions.html#whats-distributed",
    "title": "9  Allocation of uncertainty among possible data values: probability distributions",
    "section": "9.2 What’s “distributed”?",
    "text": "9.2 What’s “distributed”?\nDifference between distribution of probability and distribution of (a collection of) data."
  },
  {
    "objectID": "probability_distributions.html#distributions-of-probability",
    "href": "probability_distributions.html#distributions-of-probability",
    "title": "9  Allocation of uncertainty among possible data values: probability distributions",
    "section": "9.3 Distributions of probability",
    "text": "9.3 Distributions of probability\n\nRepresentations\n\nDensity function\nHistogram\nScatter plot\n\nBehaviour of representations under transformations of data."
  },
  {
    "objectID": "probability_distributions.html#summaries-of-distributions-of-probability",
    "href": "probability_distributions.html#summaries-of-distributions-of-probability",
    "title": "9  Allocation of uncertainty among possible data values: probability distributions",
    "section": "9.4 Summaries of distributions of probability",
    "text": "9.4 Summaries of distributions of probability\n\nLocation\nMedian, mean\n\n\nDispersion or range\nQuantiles & quartiles, interquartile range, median absolute deviation, standard deviation, half-range\n\n\nResolution\nDifferential entropy\n\n\nBehaviour of summaries under transformations of data and errors in data"
  },
  {
    "objectID": "probability_distributions.html#outliers-and-out-of-population-data",
    "href": "probability_distributions.html#outliers-and-out-of-population-data",
    "title": "9  Allocation of uncertainty among possible data values: probability distributions",
    "section": "9.5 Outliers and out-of-population data",
    "text": "9.5 Outliers and out-of-population data\n(Warnings against tail-cutting and similar nonsense-practices)"
  },
  {
    "objectID": "probability_distributions.html#marginal-and-conditional-distributions-of-probability",
    "href": "probability_distributions.html#marginal-and-conditional-distributions-of-probability",
    "title": "9  Allocation of uncertainty among possible data values: probability distributions",
    "section": "9.6 Marginal and conditional distributions of probability",
    "text": "9.6 Marginal and conditional distributions of probability"
  },
  {
    "objectID": "probability_distributions.html#collecting-and-sampling-data",
    "href": "probability_distributions.html#collecting-and-sampling-data",
    "title": "9  Allocation of uncertainty among possible data values: probability distributions",
    "section": "9.7 Collecting and sampling data",
    "text": "9.7 Collecting and sampling data\n\n“Representative” samples\nSize of minimal representative sample = (2^entropy)/precision\n\nExercise: data with 14 binary variates, 10000 samples\n\n\n\nUnavoidable sampling biases\nIn high dimensions, all datasets are outliers.\nData splits and cross-validation cannot correct sampling biases"
  },
  {
    "objectID": "probability_distributions.html#quirks-and-warnings-about-high-dimensional-data",
    "href": "probability_distributions.html#quirks-and-warnings-about-high-dimensional-data",
    "title": "9  Allocation of uncertainty among possible data values: probability distributions",
    "section": "9.8 Quirks and warnings about high-dimensional data",
    "text": "9.8 Quirks and warnings about high-dimensional data"
  },
  {
    "objectID": "making_decisions.html#decisions-possible-situations-and-consequences",
    "href": "making_decisions.html#decisions-possible-situations-and-consequences",
    "title": "10  Making decisions",
    "section": "10.1 Decisions, possible situations, and consequences",
    "text": "10.1 Decisions, possible situations, and consequences"
  },
  {
    "objectID": "making_decisions.html#gains-and-losses-utilities",
    "href": "making_decisions.html#gains-and-losses-utilities",
    "title": "10  Making decisions",
    "section": "10.2 Gains and losses: utilities",
    "text": "10.2 Gains and losses: utilities\n\nFactors that enter utility quantification\nUtilities can rarely be assigned a priori."
  },
  {
    "objectID": "making_decisions.html#making-decisions-under-uncertainty-maximization-of-expected-utility",
    "href": "making_decisions.html#making-decisions-under-uncertainty-maximization-of-expected-utility",
    "title": "10  Making decisions",
    "section": "10.3 Making decisions under uncertainty: maximization of expected utility",
    "text": "10.3 Making decisions under uncertainty: maximization of expected utility"
  },
  {
    "objectID": "inference.html#proposal-and-conditional",
    "href": "inference.html#proposal-and-conditional",
    "title": "4  Inference",
    "section": "4.2 Proposal and conditional",
    "text": "4.2 Proposal and conditional\n\n\nSome additional aspects of inference must be emphasized:\n\nKnown and unknown: by whom?\nWhat’s known to an agent can be unknown to another agent, and vice versa. This means that every inference is always relative to an agent’s background knowledge. This fact is especially important in data science and artificial intelligence.\nConsider an engineer testing the programming of a particular AI system. The engineer inputs specially-chosen data and checks the AI’s decisions about specially-chosen outcomes. In this scenario we have two ongoing inferences, drawn by two agents:\n\nThe AI is one agent. This agent knows the inputs, but doesn’t know the true outcomes. This agent’s inference is about the outcomes.\nThe engineer is the other agent. This agent knows inputs and true outcomes – both are “data” for this agent – but doesn’t know the AI’s final decisions. This agent’s inference is about the AI’s decisions.\n\n\n\nFuture and past\nAn inference may concern not only unknown future outcomes,1 but also unknown past outcomes. For instance, imagine that a particular device has stopped working, and an engineer needs to assess the plausibility that a mechanical failure happened, or an electronic failure happened. In this case the inference is about something unknown to the engineer (and maybe to anyone else), but that has already happened. Astrophysicists and archaeologists draw this kind of inferences all the time.1 an inference about the future is often called a forecast."
  }
]