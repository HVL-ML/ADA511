[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ADA511: Data science and data-driven engineering",
    "section": "",
    "text": "Science is built up with facts, as a house is with stones. But a collection of facts is no more a science than a heap of stones is a house.     (H. Poincaré)\n\nPreface\n**WARNING: THIS IS A WORKING DRAFT. TEXT WILL CHANGE A LOT. MANY PASSAGES ARE JUST TEMPORARY, INCOHERENT, AND DISJOINTED.\nTo be written."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "To be written: motivation and structure of this course."
  },
  {
    "objectID": "framework.html#decision-theory",
    "href": "framework.html#decision-theory",
    "title": "2  Framework",
    "section": "2.1 Decision Theory",
    "text": "2.1 Decision Theory\nThere is always a goal underlying any engineering problem. The problem itself is how to reach that goal. Typically there are several possible courses of actions available; the question is which one to choose. Choice of a specific action will lead to some consequences, which could be the goal we want to reach, but could also be something else, possibly undesirable. The decision upon a specific action is often difficult because its consequences are not known with certainty. This uncertainty comes in turn from a more general uncertainty about the whole context of the problem: unknown past or present details, unknown future events and responses, and so on.\n@@ example\nThis is what we call a decision problem.\nA specific course of action may in turn be realized in several ways that are equivalent in regard to the outcome, but inequivalent in regard to costs, time, resources. We thus face a decision within a decision. In general, a decision problem may involve several decision sub-problems, in turn involving sub-sub-problems, and so on.\nThe main engineering goal itself could be to design and build an automated device that chooses an optimal course of action in a specific kind of uncertain situation. Think for instance of an aeronautic engineer who is designing an autopilot system.\nTherefore, to analyse and tackle this kind of problems we would like to have a framework with the following features: it should take into consideration choices, consequences, costs and gains, and uncertainties; it should be susceptible to recursive application if needed; and it should be suited to being used not only for human engineers, but also for automated or artificial-intelligence devices.\nSuch a framework does exist: it is called Decision Theory.\nDecision theory has a long history, going back to Leibniz in the 1600s and partly even to Aristotle in the −300s, and appearing in its present form around 1920–1960. What’s remarkable about it is that it is not only a framework, but the framework we must use. A logico-mathematical theorem shows that any framework that does not break basic optimality and rationality criteria has to be equivalent to Decision Theory (in other words, it can use different technical terminology and rewrite mathematical operations in a different way, but it boils down to the same notions and operations of Decision Theory). So if you wanted to invent and use another framework, then either (a) it would lead to some irrational or illogical consequences, or (b) it would lead to results identical to Decision Theory’s. Many frameworks that you are probably familiar with, such as optimization theory, are just specific applications or particular cases of Decision Theory.\nDecision theory consists of two main theories: Probability Theory, which deals with data, information, uncertainty, inference; and Utility Theory, which deals with actions, consequences, gain and loss, decisions.\nWe shall get acquainted with Decision Theory step by step, introducing its main ideas and notions as they become necessary. Let us start with the first building blocks."
  },
  {
    "objectID": "framework.html#anatomy-of-a-decision-problem",
    "href": "framework.html#anatomy-of-a-decision-problem",
    "title": "2  Framework",
    "section": "2.2 Anatomy of a decision problem",
    "text": "2.2 Anatomy of a decision problem\nAn extremely important – and surprisingly often neglected – first step in every engineering problem is to define exactly what the problem is. This means, in particular, to specify unambiguously the goals, the available data and information, the available courses of action, the hypotheses of interest.\nDecision theory analyses any problem in terms of minimal decision problems, each of which consists of these elements:\n\nAgent: the person or device that has to make an optimal decision.\nAssumptions: data and background information that are known, or temporarily imagined to be known, to the agent.\nConclusions: hypotheses, conjectures, or actual facts that the agent wishes to assess; often they are related to the unknown consequences of the possible actions.\nProbabilities of the conclusions given the assumptions.\nCourses of actions: the choices available to the agent.\nUtilities: the gains and losses involved in choosing a course of action.\n\nTogether with the “assumptions” and “conclusions” it is also useful to keep in mind two more and somewhat distinct elements:\n\nKnowns: the data and information that are actually known to the agent.\nUnknowns: hypotheses, conjectures, and situations whose truth or falsity are actually unknown to the agent.\n\n\n\n\n\n\n\n\n\n\n\n\n\nRemember: What matters is to be able to identify these elements in a concrete engineering problem, understanding their role. Their technical names don’t matter.\n\n\nThe basic idea is that the agent will estimate effects of the conclusions given the assumptions, and from this estimate choose the optimal course of action.\nSome of these elements may themselves be open to be analysed by a decision sub-problem. For instance, it may be unknown what the utilities are: thus we have a decision sub-problem to determine the optimal values we should choose for them. This is an example of the modularity of decision theory.\nThe elements above must be unambiguously identified in any decision problem. The analysis of the problem into these elements greatly helps in making the problem and its solution well-defined. Suppose someone (probably a politician) says: “We must solve the energy crisis by reducing energy consumption or producing more energy”. This person has effectively said nothing whatsoever. By definition the “energy crisis” is the problem that energy production doesn’t meet demand. So this person has only said “we would like the problem to be solved”, without giving any solution. A decision-theory approach to this problem requires us to specify which concrete courses of action should be taken for reducing consumption or increasing productions, and what their costs, gains, and probable consequences would be.\n\n\n\n\n\n\n\n\n For the curious\n\n\n\nSee MacKay’s rational options-costs analysis in Sustainable Energy – without the hot air\n\n\nAn advantage of decision theory is that its application forces us to make sense of an engineering problem. A useful procedure is to formulate the general problem in terms of the elements above, identifying them clearly. If the definition of any of the terms involves uncertainty of further decisions, then we analyse it in turn as a decision sub-problem, and so on.\n\nThere are important aspects of engineering that cannot be captured by decision theory or, likely, by any logico-mathematical framework: the discovery or the invention of new technologies and solutions, for instance. These aspects cannot be planned or decided; but their fruits, once available, can be handled and used optimally by means of decision theory.\nArtificial intelligence is proving to be a valuable aid in these aspects too. This kind of use of AI is outside the scope of the present notes. Some aspects of this creativity-assisting use, however, do fall within the domain of decision theory and of the present notes. A pattern-searching algorithm, for example, can be optimized by means of decision theory, if we have an idea of the kind of patterns that could be useful."
  },
  {
    "objectID": "first_building_blocks.html#agents",
    "href": "first_building_blocks.html#agents",
    "title": "3  First building blocks",
    "section": "3.1 Agents",
    "text": "3.1 Agents\nThe agent is the person or device that has to make a choice between different courses of action. An agent has a specific set of data and background information available, a specific set of choices, and can incur specific gains or losses dependent on the consequences of the available choices.\n\n\nWe’ll use the neutral pronouns it/its when referring to an agent, since an agent could be a person or a machine.\nIt is important to identify the agent or agents involved in a problem, because each one will generally have different data, or different available actions, or different gains and losses. A person buying an insurance policy from an insurance company is an example of decision problem with two agents, the person and the company, that have roughly the same data and a common course of action (buy-sell) that is optimal for both. The optimality comes from the fact that the two agents have very different gains and losses for their various courses of action.\n\n\n\n\n\n\n Reading\n\n\n\n§ 1.1.4 in Artificial Intelligence\n\n\n\nNotation\nWhen necessary, agents are typically denoted by capital letters: \\(A, B,\\dotsc\\). But we’ll rarely need symbols for them."
  },
  {
    "objectID": "first_building_blocks.html#assumptions-conclusions-knowns-unknowns-sentences",
    "href": "first_building_blocks.html#assumptions-conclusions-knowns-unknowns-sentences",
    "title": "3  First building blocks",
    "section": "3.2 Assumptions & conclusions, knowns & unknowns: Sentences",
    "text": "3.2 Assumptions & conclusions, knowns & unknowns: Sentences\n\nAssumptions\nThe “assumptions” often include all or part of the data and information that the agent knows. There is a difference between “assumptions” and “knowns”, however. In approaching a decision problem, and especially when considering decision sub-problems of a larger problem, the agent must often reason hypothetically or counterfactually.\nConsider for instance an aeronautics problem where it must be decided whether to replace the fuel currently employed by a particular aircraft model, with a newly produced fuel type. To assess the consequence of employing the new fuel, the engineer must momentarily imagine that the new fuel is actually used and then assess thermodynamic, environmental, and economic consequences of this imagined situation. This is an example of hypothetical reasoning. Hypothetical reasoning is sometimes assisted by performing experiments in restricted and controlled conditions, in which the new fuel is really used. Such supporting experiments, however, may not be viable, and in any case they are not full reflections of the hypothetical situation.\nThe engineer may also have data from another aircraft model with which the new fuel was ultimately not used, and may try to assess what the consequences of the new fuel would have been if it had been replaced in that model, because such assessment may be easier post-facto. This is an example of counterfactual reasoning.\nIn either instance, the engineer sets up a decision problem in which the assumptions consist of a combination of real data and of a situation that in one case is only imagined, and in the other case never happened. Both kinds of reasoning are staples of scientific research.\n\n\nConclusions\nThe “conclusions” often include all or part of the hypotheses or conjectures whose truths are unknown to the agent, and the agent would like to assess. Conclusions may also include known data, however. Similarly to “assumptions” and “knowns”, there’s a difference between “conclusions” and “unknowns”.\nConsider a data engineer testing the responses of a new machine-learning algorithm, in controlled conditions; this is an example of decision problem about a decision agent. The algorithm being evaluated assesses the truth of a particular situation; this truth is unknown to the algorithm itself, but known to the engineer. More generally, we shall see that an agent may have to assess the truth of a known situation, assuming an unknown one, as a temporary step in a more general inference.\n\n\nSentences\nIs there a flexible and general way of representing assumptions, conclusions, knowns, unknowns, data, information, hypotheses; and, later, also consequences and actions?\nWhen speaking of “data”, what comes to mind to many people is basically numbers or collections of numbers. So numbers could perhaps be used to representing assumptions etc. This option turns out to be too limiting, however.\nI give you this number: \\(8\\), saying that it is “data”. But what is it about? As a decision agent, you can hardly call this number a piece of information, because you have no clue what to do with it. Instead, if I tell you: “The number of official planets in the solar system is 8”, then we can say that I’ve given you data. So “data” is not just numbers: a number is not “data” unless there’s some verbal, non-numeric context accompanying it – even if this context is only implicitly understood. Note that representing this meta-data information as numbers only shifts the problem one level up: we would need auxiliary verbal context explaining what the meta-data numbers are about.\nData can, moreover, also be completely non-numeric. A clinician saying “The patient has fully recovered from the disease” (we imagine to know who’s the patient and what was the disease) is giving us a piece of information that we could further use, for instance, to make prognoses about other, similar patients. The clinician’s statement surely is “data”, but essentially non-numeric data. In some situations we can represent it as “1”, while “0” would represent “not recovered”; yet the opposite convention could also be used, showing that these numbers have really nothing to do with the clinician’s data.\nBut the examples above actually contain the answer to our needs. In them we expressed the data by means of sentences. Clearly any piece of information, or hypothesis, conclusion, consequence, action can be expressed by a sentence. We shall therefore use sentences, also called propositions or statements1, to represent and communicate assumptions, conclusions, knowns, unknowns, data, information, hypotheses, consequences, and actions. In some cases we can of course summarize a sentence by a number, as a shorthand, when the full meaning of the sentence is understood.1 These three terms are not always equivalent in Formal Logic, but here we’ll use them as synonyms.\nBut what is a sentence? The everyday meaning of this word will work for us, even though there is still a lot of research in logic an artificial intelligence on how to define and use sentences. We shall adopt this useful definition:\n\n\n\n\n\n\n\n\n For the curious Propositions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\na “sentence” is a verbal message for which we can determine, at least in principle, whether it is true or false, in a way that all interested receivers of the message would agree.\n\n\nFor instance, in most engineering contexts the phrase “This valve will operate for at least two months” is a sentence; whereas the phrase “Apples are much tastier than pears” is not, because it’s a matter of personal taste. However, the phrase “Rita finds apples tastier than pears” could be a sentence.\nNote that a sentence can contain numbers, and even pictures and graphs: this possibility is not excluded from the definition above.\nThe use of sentences in our framework has important practical consequences:\n\nClarity, analysis, goal-orientation. A data engineer must acquire information and convey information. Acquiring information is not simply making some measurement or counting something: the engineer must understand what is being measured and why. If data is gathered from third parties, the engineer must ask what exactly the data mean and how they were acquired. In designing and engineering a solution, it is important to understand what information or outcomes the end user exactly wants. A data engineer will often ask “wait, what do you mean by that?”; this question is not just an unofficial parenthesis in the official data-transfer workflow between the engineer and someone else. It is an integral part of that workflow; it means that the data has not been completely transferred yet.\nArtificial Intelligence. Sentences are the central components of knowledge representation and inference in artificial-intelligence agents.\n\n\n\n\n\n\n\n Reading\n\n\n\n§ 7.1 in Artificial Intelligence\n\n\n\nNotation\nWe’ll denote sentences by sans-serif italic letters: \\(\\mathsfit{A},\\mathsfit{B},\\mathsfit{a},\\mathsfit{b},\\dotsc\\) For example, \\[\n\\mathsfit{O} \\coloneqq \\textsf{\\small`The power output is 100 W'}\n\\] means that the symbol \\(\\mathsfit{O}\\) stands for the sentence above. Often we shall simply write sentences in abbreviated form, when their full meaning is understood from the context; for example “\\(O = 100\\,\\mathrm{W}\\)” or even just “\\(100\\,\\mathrm{W}\\)” for the sentence above.\nWe’ll next see how more complex sentences are built from simpler ones. No matter whether complex or simple, any sentence can be represented by symbols like the ones above."
  },
  {
    "objectID": "first_building_blocks.html#combining-sentences",
    "href": "first_building_blocks.html#combining-sentences",
    "title": "3  First building blocks",
    "section": "3.3 Combining sentences",
    "text": "3.3 Combining sentences\n\nBasic sentences\nIn analysing the assumptions and conclusions of a decision problem it is convenient to find a collection of basic sentences2 out of which all other sentences of interest can be constructed. Often these basic sentences represent a elementary pieces of information in the problem.2 A more technical term is “atomic”\nConsider for instance the following statement in a High-Performance Computing engineering problem:\n\n“All three CPUs report a temperature of 50 °C. CPUs 1 and 2 are consuming 100 W, whereas CPU 3 is consuming either 110 W or 90 W.”\n\nFor the sake of this example, let’s say that the statement above represents data, that is, it’s the description of a factual situation. But keep in mind that in a different problem – say, one where the unknown temperatures and consumptions of the three CPU need to be assessed – the same statement could represent a hypothesis, that is, one possible state of affairs among other possible ones.\nIn the statement above we can identify at least seven basic sentences, which we denote by convenient symbols: \\[\\begin{align*}\n\\mathsfit{t}_{1,50} &\\coloneqq \\textsf{\\small`CPU\\,1 reports a temperature of 50\\,°C'}\n\\\\\n\\mathsfit{t}_{2,50} &\\coloneqq \\textsf{\\small`CPU\\,2 reports a temperature of 50\\,°C'}\n\\\\\n\\mathsfit{t}_{3,50} &\\coloneqq \\textsf{\\small`CPU\\,3 reports a temperature of 50\\,°C'}\n\\\\\n\\mathsfit{c}_{1,100} &\\coloneqq \\textsf{\\small`CPU\\,1 is consuming 100\\,W'}\n\\\\\n\\mathsfit{c}_{2,100} &\\coloneqq \\textsf{\\small`CPU\\,2 is consuming 100\\,W'}\n\\\\\n\\mathsfit{c}_{3,90} &\\coloneqq \\textsf{\\small`CPU\\,3 is consuming 90\\,W'}\n\\\\\n\\mathsfit{c}_{3,110} &\\coloneqq \\textsf{\\small`CPU\\,3 is consuming 110\\,W'}\n\\end{align*}\\]\nThe decision problem may actually require more basic sentences than just these. For instance, it might become necessary to consider basic sentences with other values for the temperature and of the power consumption, such as \\[\\begin{aligned}\n\\mathsfit{t}_{1,55} &\\coloneqq \\textsf{\\small`CPU\\,1 reports a temperature of 55\\,°C'} \\ ,\n\\\\\n\\mathsfit{t}_{1,60} &\\coloneqq \\textsf{\\small`CPU\\,1 reports a temperature of 60\\,°C'} \\ ,\n\\end{aligned}\\] and so on, and similarly for the others CPUs and for the power consumption. Moreover, the phrase “All three CPUs…” suggests that the basic sentence \\[\n\\mathsfit{n} \\coloneqq \\textsf{\\small`There are three CPUs'}\n\\] might be part of the data as well. Finally, there are obvious data that we don’t even think about but may have to be spelled out explicitly. In our problem an example is the sentence \\[\n\\textsf{\\small`CPU\\,3 cannot be consuming both 90\\,W and 110\\,W'} \\ .\n\\]\n\n\nConnectives\nHow do we construct the initial data sentence and other complex sentences out of the basic ones?\nWe consider one way or operation to change a sentence into another related to it, and two ways or operations to combine two or more sentences together. These operations are called connectives. Our natural language offer many more operations to combine sentences, but these three turn out to be all we need in virtually all engineering problems. The three connectives and their symbols are:\n\nNot:  \\(\\lnot\\)\n\nfor example, \\[\n\\lnot \\mathsfit{t}_{1,55} = \\textsf{\\small`CPU\\,1 reports a temperature different from 55\\,°C'}\n\\]\n\nAnd:  \\(\\land\\)\n\nfor example, \\[\n\\mathsfit{t}_{2,55} \\land \\mathsfit{c}_{2,100} = \\textsf{\\small`CPU\\,2 reports a temperature of 55\\,°C and is consuming 100\\,W'}\n\\]\n\nOr:  \\(\\lor\\)\n\nfor example, \\[\n\\mathsfit{c}_{3,90} \\lor \\mathsfit{c}_{2,110} = \\textsf{\\small`CPU\\,3 is consuming 90\\,W, or 110\\,W, or both'}\n\\]\n\n\nNote some important subtleties of the connectives:\n\n\nThere is not a strict correspondence between the words “not”, “and”, “or” in natural language and the three connectives. For instance the and connective could correspond to the words “but” or “whereas”, or just to a comma “ , ”.\n“Not” doesn’t mean some kind of complementary quality, but only the negation. For instance, \\(\\lnot\\textsf{\\small`The chair is black'}\\) does not mean \\(\\textsf{\\small`The chair is white'}\\).\n“Or” does not exclude that both sentences can be true. So in our example  \\(\\mathsfit{c}_{3,90} \\lor \\mathsfit{c}_{2,110}\\)  does not exclude, a priori, that CPU 3 can be consuming both 90 W and 100 W. (There is a connective for that: “exclusive-or”, but it can be constructed out of the three we already have.)\n\n\nFrom the last remark we see that the sentence \\[\n\\textsf{\\small`CPU\\,3 is consuming either 110\\,W or 90\\,W'}\n\\] does not correspond to   \\(\\mathsfit{c}_{3,90} \\lor \\mathsfit{c}_{3,110}\\). The situation assumes implicitly that a CPU cannot have two different power consumption rates at the same time. Convince yourself that the correct way to write that sentence is this: \\[\n(\\mathsfit{c}_{3,90} \\lor \\mathsfit{c}_{3,110})\n\\land\n\\lnot(\\mathsfit{c}_{3,90} \\land \\mathsfit{c}_{3,110})\n\\]\nFinally, the full initial statement can be written in symbols as follows: \\[\n\\mathsfit{t}_{1,50} \\land \\mathsfit{t}_{2,50} \\land \\mathsfit{t}_{3,50}\n\\land \\mathsfit{c}_{1,100} \\land \\mathsfit{c}_{2,100}\n\\land (\\mathsfit{c}_{3,90} \\lor \\mathsfit{c}_{3,110})\n\\land\n\\lnot(\\mathsfit{c}_{3,90} \\land \\mathsfit{c}_{3,110})\n\\]"
  },
  {
    "objectID": "first_building_blocks.html#distinguishing-assumptions-and-conclusions",
    "href": "first_building_blocks.html#distinguishing-assumptions-and-conclusions",
    "title": "3  First building blocks",
    "section": "3.4 Distinguishing assumptions and conclusions",
    "text": "3.4 Distinguishing assumptions and conclusions\nNow we know to represent arbitrarily complex sentences and express them in symbols. Let’s introduce a way to clearly distinguish those that constitute the assumptions from those that constitute the conclusions in a specific decision problem.\nWe can simply use the symbol  “\\(\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}}\\)”,  a vertical bar3:3 Notation in formal logic uses the symbols  \\(\\models\\)  or  \\(\\vdash\\),  and writes assumptions on the left, conclusions on the right. We use the notation used in probability logic.\n\non its left side those that make up our desired conclusion,\non its right side we write the sentences that make up our assumptions, and-ed together:\n\n\\[\n\\textit{\\small conclusion} \\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}} \\textit{\\small assumptions}\n\\]\nSo, in symbols, our inference is this: \\[\nb \\nonscript\\:\\Big\\vert\\nonscript\\:\\mathopen{} (b\\lor r)\\, \\land\\, \\lnot r\\, \\land\\, \\lnot(b \\land r)\\, \\land\\, D\n\\] Note how we and-ed all assumptions together. The collection of assumptions on the right side of the bar “\\(\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}\\)” is called the conditional. The expression above is read “\\(b\\) given \\((b\\lor r)\\)” etc., or “\\(b\\) conditional on \\((b\\lor r)\\)” etc.\nAll data in the conditional (that is, on the right side of “\\(\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}\\)”) are considered (at least temporarily) to be true. Our goal is to determine whether the sentence on the left of “\\(\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}\\)” is true or false.\n“Basic” in the sense that we will not analyse these sentences into further sub-sentences. In the trivial example above we identify three such sentences: \\(\\textsf{\\small`Rita has an umbrella'}\\), \\(\\textsf{\\small`The umbrella is blue'}\\), and \\(\\textsf{\\small`The umbrella is red'}\\). Let’s represent them by symbols: \\[\n\\begin{aligned}\nb &\\coloneqq \\textsf{\\small`Rita's umbrella is blue'}\n\\\\\nr &\\coloneqq \\textsf{\\small`Rita's umbrella is red'}\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\nConnectives\nWe didn’t consider \\(\\textsf{\\small`Rita's umbrella is either blue or red'}\\), \\(\\textsf{\\small`The umbrella is not blue'}\\), and \\(\\textsf{\\small`The umbrella is not red'}\\) as basic sentences. These sentences can in fact be expressed in terms of the basic sentences \\(b\\) and \\(r\\). We consider one way or operation to change a sentence into another related to it, and two ways or operations to combine two or more sentences together. These operations are called connectives. Our natural language offers many more operations to combine sentences, but these three turn out to be all we need in virtually all engineering problems. The three connectives are:\n\nNot: \\(\\lnot\\)\n\nfor example, \\[\n\\lnot b = \\textsf{\\small`Rita's umbrella is not blue'}\n\\]\n\nAnd: \\(\\land\\)\n\nfor example, \\[\nb \\land r = \\textsf{\\small`Rita's umbrella is blue, and it is red'}\n\\]\n\nOr: \\(\\lor\\)\n\nfor example, \\[\nb \\lor r = \\textsf{\\small`Rita's umbrella is blue, or red, or both'}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n Note some subtleties of the connectives:\n\n“Not” doesn’t mean some kind of complementary quality, but only the negation. For instance, \\(\\lnot\\textsf{\\small`Rita's umbrella is black'}\\) does not mean \\(\\textsf{\\small`Rita's umbrella is white'}\\).\n\\(b \\lor r\\) does not exclude, a priori, that the umbrella cannot be both blue and black (there is a connective for that: “exclusive-or”, but it can be constructed out of the three we already have.)\nIt is important to distinguish between logical impossibility and physical impossibility. It is physically impossible that an umbrella be fully white and fully black, but formal logic does allow us to consider the sentence “the umbrella is fully white and fully black”, without setting it true or false a priori. We express its physical impossibility by assigning to it the value false in any inference we want to make. This generality of formal logic is a feature: it allows us to entertain and study hypotheses even if we still don’t know whether they are physically impossible. Scientific research could not be done without this feature.\n\n\n\nWhat about the “if” in “if the umbrella…”? Shouldn’t that be a connective? This “if” is treated in a special way; we shall now see how it works.\n\n\n\nThere is an old and still ongoing debate in logic and probability on how to deal with “if”-statements. Here we take the seemingly most common approach, which is simple and effective in applied problems.\nObviously we possess even more information, which is implicitly understood. For example, it’s understood that the umbrella is all of one colour. It must also be understood what an umbrella is, what “red” and “blue” mean, and so on. Usually this kind of information is extremely obvious or irrelevant, so we don’t think about it very much and don’t mention it. In real engineering problems, however, it may happen that we have to stop and examine this implicit, hidden knowledge, maybe to check whether it contains contradictory information or contradictory goals. It’s a good idea to represent this implicitly-understood information by a symbol, for example \\(I\\) (for “extra information”; we could have used \\(K\\) for “knowledge”).\n\n\nSeparating assumptions and conclusions\nNow we have the sentences that represent all our data and the information we are interested in, and even symbols to represent them in a compact way. It’s important to keep the assumptions we make well separated from the conclusions we want to draw. Let’s introduce a symbol to do just that.\nWe can simply use a vertical bar: on its right side we write the sentences that make up our assumptions, and on its left side those that make up our desired conclusions.4 So, in symbols, our inference is this: \\[\nb \\nonscript\\:\\Big\\vert\\nonscript\\:\\mathopen{} (b\\lor r)\\, \\land\\, \\lnot r\\, \\land\\, \\lnot(b \\land r)\\, \\land\\, D\n\\] Note how we and-ed all assumptions together. The collection of assumptions on the right side of the bar “\\(\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}\\)” is called the conditional. The expression above is read “\\(b\\) given \\((b\\lor r)\\)” etc., or “\\(b\\) conditional on \\((b\\lor r)\\)” etc.4 Current notation in logic uses the symbols \\(\\models\\) or \\(\\vdash\\), and writes assumptions on the left, conclusions on the right. We use a different notation for an easier transition to probability logic.\nAll data in the conditional (that is, on the right side of “\\(\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}\\)”) are considered (at least temporarily) to be true. Our goal is to determine whether the sentence on the left of “\\(\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}\\)” is true or false."
  },
  {
    "objectID": "first_building_blocks.html#well-posed-and-ill-posed-sentences",
    "href": "first_building_blocks.html#well-posed-and-ill-posed-sentences",
    "title": "3  First building blocks",
    "section": "3.5 well-posed and ill-posed sentences",
    "text": "3.5 well-posed and ill-posed sentences\nWe face problems when the sentences that should convey information and data are not clear. Suppose that an electric-car model consumes 150 Wh/km and has a range of 200 km; a second car model consumes 250 Wh/km and has a range of 600 km. Someone says “I think the second model is better; what do you think?”. It isn’t clear how we should answer; what does “better” mean? If it refers to consumption, then the first car model is “better”. If it refers to range, then the second model is “better”. If it refers to a combination of these two characteristics, or to something else, then we simply can’t answer. Here we have a problem with querying and giving data, because the sentence underlying such query is not clear.\nWe say that such sentences are not well-posed, or that they are ill-posed.\nThis may seem an obvious discussion to you. Yet you’d be surprised by how often unclear sentences appear in scientific papers about data engineering! Not seldom we find discussions and disagreements that actually come from unclear underlying sentences, that two parties interpret in different ways.\nAs a data engineer, you’ll often have the upper hand if you are on the lookout for ill-posed sentences. Whenever you face an important question, or you’re given an important piece of information, or you must provide an important piece of information, always take a little time to examine whether the question or information is actually well-posed.\n@@ [TODO] Exercise: give actual paper to analyse_"
  },
  {
    "objectID": "first_building_blocks.html#reading-list",
    "href": "first_building_blocks.html#reading-list",
    "title": "3  First building blocks",
    "section": "Reading list",
    "text": "Reading list"
  },
  {
    "objectID": "inference.html#what-is-inference",
    "href": "inference.html#what-is-inference",
    "title": "4  Inference",
    "section": "4.1 What is inference?",
    "text": "4.1 What is inference?\nThe first core problem in all data-driven engineering applications – and in daily life too – is to draw inferences, that is, acquire information. We may wish to acquire information out of simple curiosity, or for some specific engineering reason or goal, as we’ll discuss later. Examples:\n\nWe’d like to know whether it’ll rain today, so we can decide whether to get an umbrella or rain clothes.\n\n\n\nA clinician would like to know which disease affects a patient, so as to decide for the optimal treatment.\nThe X-player of this game of Xs & Os:  needs to know where put the next X in order to win.\nThe computer of a self-driving car needs to know whether a particular patch of colours in the visual field is a person, so as to slow down the car and stop.\nIn order to launch a rocket to the Moon, a rocket engineer needs to know, within two significant digits, how much is the velocity \\(\\sqrt{2\\,G\\,M/r\\,}\\), where \\(G=6.67 \\cdot 10^{-11}\\,\\mathrm{m^3\\,s^{-2}\\,kg^{-1}}\\), and \\(M = 5.97 \\cdot 10^{24}\\,\\mathrm{kg}\\) and \\(r = 6.37 \\cdot 10^{6}\\,\\mathrm{m}\\) are the mass and radius of the Earth.\nWe’d like to know whether the rolled die will show , so we can win a bet.\nAn aircraft’s autopilot system needs to predict how much the aircraft’s roll will change by increasing the right wing’s angle of attack by 0.1 rad.\nAn archaeologist would like to know whether the fossil bone just dug out belonged to a Tyrannosaurus rex.\nAn automated system in an assembly line needs to predict whether an electric component of a widget will fail within the next two years.\n\nNote how each of these inferences boils down to determining whether some sentences are true or false. In example 1. we want to know whether the sentence \\(\\textsf{\\small`It rains today'}\\) is true or not. In example 2. the clinician wants to know which of the sentences \\(\\textsf{\\small`The patient has pneumonia'}\\), \\(\\textsf{\\small`The patient has asthma'}\\), \\(\\textsf{\\small`The patient has bronchitis'}\\), and so on, are true (several can be true at the same time). In example 5. the rocket engineer wants to know which among the sentences \\(\\textsf{\\small`The velocity is 0.010\\,m/s'}\\), \\(\\textsf{\\small`The velocity is 0.011\\,m/s'}\\), …, \\(\\textsf{\\small`The velocity is 130\\,m/s'}\\), and so on, is true. The sentences that underlie an inference can be extremely many and complex, and yet we must have an idea of what they are (otherwise, do we really know what our inference is about?).\n\n\n\n\n\n\nExercise\n\n\n\nTry to identify which sentences underlie the other example inferences above."
  },
  {
    "objectID": "inference.html#certain-and-uncertain-inference",
    "href": "inference.html#certain-and-uncertain-inference",
    "title": "4  Inference",
    "section": "4.2 Certain and uncertain inference",
    "text": "4.2 Certain and uncertain inference\nThe example inferences above present very different levels of difficulty.\nInferences 3. and 5. are special because they can actually be drawn exactly, that is, we really find out which of their underlying sentences are true and false. In example 3. it is trivial that putting the next X in the mid-right slot makes the X-player win. In example 5. a couple of mathematical operations show that the sentence \\(\\textsf{\\small`The velocity is 11\\,km/s'}\\) is true. When we can obtain the data we want from the data we have by using “only”1 logic and mathematical operations, our inference is certain, also called a “deduction”; in these notes we shall call it a truth inference. But every deduction can be basically drawn by repeatedly applying the rules of logic.1 “Only” in quotation marks because the logical analysis and operations leading to the answer can still be computationally very expensive.\nThe other example inferences cannot be drawn exactly, in the sense that we cannot know for sure whether all their underlying sentences are true or false. But this doesn’t mean that we cannot say anything whatsoever. In example 6. we consider the sentence \\(\\textsf{\\small`The die shows six pips'}\\) to be more likely false than true. In example 2. the clinician might be quite sure about the disease, after observing the symptoms. On the other hand, in example 1. we might really have no clue whether \\(\\textsf{\\small`It rains today'}\\) will turn out to be true or false. These inferences are uncertain. Certain inferences can be considered as a limit case of uncertain ones, in which the uncertainty vanishes or is extremely small.\nTo draw certain inferences, we follow the rules of Logic. What rules do we follow to draw uncertain inferences?"
  },
  {
    "objectID": "truth_inference.html#building-blocks",
    "href": "truth_inference.html#building-blocks",
    "title": "5  Truth inference",
    "section": "5.1 Building blocks",
    "text": "5.1 Building blocks\nConsider the following trivial problem. An inspector examines an electronic component out of a production line. The information available to the inspector is the following:\n\nThe component can either come from the production line in Oslo, or from the one in Rome.\nIf the component is defective, it cannot come from Oslo.\nThe component is found to be defective.\n\nThe question is: from which production line does the component come from?\nThe answer is obvious: from the Rome line. But how could we draw this obvious and sure inference? Which rules did we follow? Did we make any hidden assumptions, or use information that wasn’t explicitly mentioned?\nLogic is the huge field that formalizes and makes rigorous the rules that a rational person or an artificial intelligence should use in drawing sure inferences. We’ll get a glimpse of it here, as a trampoline for jumping towards the more general inferences that we need in data-driven engineering problems.\n\nAnalysis of the problem\nLet’s write down the basic sentences that constitute our data and the inferences we want to draw. We identify three basic sentences, which we can represent by these symbols:\n\n\\(o \\coloneqq \\textsf{\\small`The component comes from the Oslo line'}\\)\n\\(r \\coloneqq \\textsf{\\small`The component comes from the Rome line'}\\)\n\\(d \\coloneqq \\textsf{\\small`The component is defective'}\\)\n\nObviously the inspector possesses even more information which is implicitly understood. It’s clear, for instance, that the component cannot come from both Oslo and Rome. Let’s denote this information with\n\n\\(I \\coloneqq{}\\)(a long collection of sentences explaining all other implicitly understood information).\n\n\nWith the sentences above we can express more complex details and hypotheses appearing in the inspector’s problem, in particular:\n\n\\(o \\lor r = \\textsf{\\small`The component comes from either the Oslo line or the Rome line'}\\)\n\\(\\lnot(o \\land r) = \\textsf{\\small`The component cannot come from both the Oslo and the Rome lines'}\\)\n$ o $\n\n\n\nData, assumptions, desired conclusions\nThe inspector knows for certain the following facts:\n\n\\(o \\lor r\\), \\(\\textsf{\\small`The component comes from either the Oslo line or the Rome line'}\\)\n\\(\\lnot(o \\land r)\\), \\(\\textsf{\\small`The component cannot come from both the Oslo and the Rome lines'}\\)\n\\(d\\), \\(\\textsf{\\small`The component is defective'}\\)\n\\(I\\), all remaining implicit information\n\nWe and them all together: \\[\nd \\land (o \\lor r) \\land \\lnot (o \\land r) \\land I \\ .\n\\]\nThe inspector knows, moreover, this hypothetical consequence:\n\n\\(\\lnot o \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} d \\land (o \\lor r) \\land \\lnot (o \\land r) \\land I\\), if the component is defective, it cannot come from the Oslo production line."
  },
  {
    "objectID": "truth_inference.html#background-information-and-conditional",
    "href": "truth_inference.html#background-information-and-conditional",
    "title": "5  Truth inference",
    "section": "5.2 Background information and conditional",
    "text": "5.2 Background information and conditional"
  },
  {
    "objectID": "truth_inference.html#truth-inference-rules",
    "href": "truth_inference.html#truth-inference-rules",
    "title": "5  Truth inference",
    "section": "5.3 Truth-inference rules",
    "text": "5.3 Truth-inference rules\nDeduction systems in formal logic give us a set of rules for making correct inferences, that is, for correctly determining whether the conclusions of interest are true or false. These rules are represented in a wide variety of ways, as steps leading from one conclusion to another one. The picture here on the margin, for instance, shows how a proof of our inference would look like, using the so-called sequent calculus.\n\n\n\n\n\nThe bottom formula is our conclusion; the formulae above it represent steps in the proof. Each line denotes the application of an inference rule. The two formulae with no line above are our two assumptions.\n\n\n\n\nWe can compactly encode all inference rules in the following way. First, represent true by the number 1, and false by 0. Second, symbolically write that conclusion \\(C\\) is true, given assumptions \\(A\\), as follows: \\[\n\\mathrm{T}(C \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} A) = 1 \\ .\n\\] or with 0 if it’s false.\nThe rules of truth inference are then encoded by the following equations, which must always hold for any sentences \\(A,B,C\\), no matter whether they are basic or complex:\n\n\n\nRule for “not”:\n\n\\[\\mathrm{T}(\\lnot A \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} B)\n+ \\mathrm{T}(A \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} B)\n= 1 \\tag{5.1}\\]\n\nRule for “and”:\n\n\\[\n\\mathrm{T}(A \\land B \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} C)\n= \\mathrm{T}(A \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} B \\land C) \\cdot\n\\mathrm{T}(B \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} C)\n= \\mathrm{T}(B \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} A \\land C) \\cdot\n\\mathrm{T}(A \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} C)\n\\tag{5.2}\\]\n\nRule for “or”:\n\n\\[\\mathrm{T}(A \\lor B \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} C)\n= \\mathrm{T}(A \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} C) +\n\\mathrm{T}(B \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} C)\n- \\mathrm{T}(A \\land B \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} C)\n\\tag{5.3}\\]\n\nRule of self-consistency:\n\n\\[\\mathrm{T}(A \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} A \\land C)\n= 1\n\\tag{5.4}\\]\n\n\n\n\n\n\nLet’s see how the inference rule (?eq-example-rule), for example, is encoded in these equations. The rule starts with saying that \\(a \\land b\\) is true according to \\(D\\). This means that \\(\\mathrm{T}(a \\land b \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} D)=1\\). But, by rule (5.2), we must then have \\(\\mathrm{T}(b \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} a \\land D) \\cdot \\mathrm{T}(a \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} D) = 1\\). This can only happen if both \\(\\mathrm{T}(b \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} a \\land D)\\) and \\(\\mathrm{T}(a \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} D)\\) are equal to \\(1\\). So we can conclude that \\(\\mathrm{T}(a \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} D)=1\\), which is exactly the conclusion under the line in rule (?eq-example-rule).\n\n\n\n\n\n\n Exercise\n\n\n\nTry to prove our initial inference\n\\[\n\\frac{\n(b \\lor r) \\land \\lnot (b \\land r) \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} D\n\\qquad\n\\lnot r \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} D\n}{\nb\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} D\n}\n\\]\nusing the basic rules (5.1, 5.2, 5.3, 5.4). Remember that you can use each rule as many times as you like, and that there is not only one way of constructing a proof."
  },
  {
    "objectID": "truth_inference.html#logical-ai-agents-and-their-limitations",
    "href": "truth_inference.html#logical-ai-agents-and-their-limitations",
    "title": "5  Truth inference",
    "section": "5.4 Logical AI agents and their limitations",
    "text": "5.4 Logical AI agents and their limitations\nThe basic rules above are also the rules that a logical artificial-intelligent agent should follow.\n\n\n\n\n\n\n Reading\n\n\n\nCh. 7 in Artificial Intelligence\n\n\nMany – if not most – inference problems that a data engineer must face are, however, of the uncertain kind: it is not possible to surely infer the truth of some data, and the truth of some initial data may not be known either. In the next chapter we shall see how to generalize the logic rules to uncertain situations.\n\n\n\n\n\n\n For the extra curious\n\n\n\nOur cursory visit of formal logic only showed a microscopic part of this vast field. The study of logic rules continues still today, with many exciting developments and applications. Feel free take a look at Logic in Computer Science, Mathematical Logic for Computer Science, Natural Deduction Systems in Logic"
  },
  {
    "objectID": "probability_inference.html#when-truth-isnt-known-probability",
    "href": "probability_inference.html#when-truth-isnt-known-probability",
    "title": "6  Probability inference",
    "section": "6.1 When truth isn’t known: probability",
    "text": "6.1 When truth isn’t known: probability\nIn most real-life and engineering situations we don’t know the truth or falsity of sentences and hypotheses that interest us. But this doesn’t mean that nothing can be said or done in such situations.\nWhen we cross a busy city street we look left and right to check whether any cars are approaching. We typically don’t look up to check whether something is falling from the sky. Yet, couldn’t it be false that cars are approaching? and couldn’t it be true that some object is falling from the sky? Of course both events are possible. Then why do we look left and right, but not up?\nThe main reason1 is that we believe strongly that cars might be approaching, believe very weakly that some object might be falling from the sky. In other words, we consider the first occurrence to be very probable; the second, extremely improbable.1 We shall see later that one more factor enters the explanation.\nWe shall take the notion of probability as intuitively understood (just as we did with the notion of truth). Terms equivalent for “probability” are degree of belief, plausibility, credibility.\n\n\n\n\n\n\n\n\n\n\n In technical discourse, likelihood means something different and is not a synonym of “probability”, as we’ll explain later.\n\n\nProbabilities are quantified between 0 and 1, or equivalently between 0% and 100%. Assigning to a sentence a probability 1 is the same as saying that it is true; and a probability 0, that it is false. A probability of 0.5 represents a belief completely symmetric with respect to truth and falsity.\nIt is important to emphasize and agree on some facts about probabilities:\n\nProbabilities are assigned to sentences. Consider an engineer working on a problem of electric-power distribution in a specific geographical region. At a given moment the engineer may believe with 75% probability that the measured average power output in the next hour will be 100 MW. The 75% probability is assigned not to the quantity “100 MW”, but to the sentence \\[\n\\textsf{\\small`The measured average power output in the next hour will be 100\\,MW'}\n\\] This difference is extremely important. Consider the alternative sentence \\[\n\\textsf{\\small`The average power output in the next hour will be \\emph{set} to 100\\,MW'}\n\\] the quantity is the same, but the meaning is very different. The probability can therefore be very different (if the engineer is the person deciding the output, the probability is 100%). The probability depends not only on a number, but on what it’s being done with that number – measuring, setting, third-party reporting, and so on. Often we still write simply \\(\\textsf{\\small`\\(\\mathsf{O = 100\\,W}\\)'}\\) or even just \\(\\textsf{\\small`100\\,W'}\\), provided that the full sentence behind the shorthand is understood.\nProbabilities are agent- and context-dependent. A coin is tossed, comes down heads, and is quickly hidden from view. Alice sees that it landed heads-up. Bob instead doesn’t manage to see the outcome and has no clue. Alice considers the sentence \\(\\textsf{\\small`Coin came down heads'}\\) to be true, that is, to have 100% probability. Bob considers the same sentence to have 50% probability.\nNote how Alice and Bob assign two different probabilities to the same sentence; yet both assignments are completely rational. If Bob assigned 100% to \\(\\textsf{\\small`heads'}\\), we would suspect that he had seen the outcome after all; if he assigned 0% to \\(\\textsf{\\small`heads'}\\), we would consider that groundless and silly. We would be baffled if Alice assigned 50% to \\(\\textsf{\\small`heads'}\\), because she saw the outcome was actually heads; we would hypothesize that she feels unsure about what she saw.\nAn omniscient agent would know the truth or falsity of every sentence, and assign only probabilities 0 or 1. Some authors speak of “actual (but unknown) probabilities”; if there were “actual” probabilities, they would be all 0 or 1, and it would be pointless to speak about probabilities at all – every inference would be a truth inference.\nProbabilities are not frequencies. The fraction of defective mechanical components to total components produced per year in some factory is a quantity that can be physically measured and would be agreed upon by every agent. It is a frequency, not a degree of belief or probability. It is important to understand the difference between them, to avoid making sub-optimal decisions; we shall say more about this difference later. Frequencies can be unknown to some agents, probabilities cannot be unknown (but can be difficult to calculate). Be careful when you read authors speaking of an “unknown probability”; either they actually mean “unknown frequency”, or a probability that has to be calculated (it’s “unknown” in the same sense that the value of \\(1-0.7 \\cdot 0.2/(1-0.3)\\) is unknown to you right now).\nProbabilities are not physical properties. Whether a tossed coin lands heads up or tails up is fully determined by the initial conditions (position, orientation, momentum, rotational momentum) of the toss and the boundary conditions (air velocity and pressure) during the flight. The same is true for all macroscopic engineering phenomena (even quantum phenomena have never been proved to be non-deterministic, and there are deterministic and experimentally consistent mathematical representations of quantum theory). So we cannot measure a probability using some physical apparatus; and the mechanisms underlying any engineering problem boil down to physical laws, not to probabilities.\n\n\n\n\n\n\n\n Reading\n\n\n\nDynamical Bias in the Coin Toss \n\n\nThese facts are not just a matter of principle. They have important practical consequences. A data engineer who is not attentive to the source of the data (measured? set? reported, and so maybe less trustworthy?), or who does not carefully assess the context of a probability, or who mixes it up with something else, or who does not take advantage (when possible) of the physics involved in the engineering problem, will design a system with sub-optimal performance2 – or even cause deaths.2 This fact can be mathematically proven."
  },
  {
    "objectID": "probability_inference.html#no-new-building-blocks",
    "href": "probability_inference.html#no-new-building-blocks",
    "title": "6  Probability inference",
    "section": "6.2 No new building blocks",
    "text": "6.2 No new building blocks\nIn discussing truth-inference we introduced notations such as \\(\\mathrm{T}(a \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} b \\land D)\\), which stands for the truth-value 0 or 1 of sentence \\(a\\) in the context of data \\(D\\) and supposing (even if only hypothetically) sentence \\(b\\) to be true. We can simply extend this notation to probability-values, using a \\(\\mathrm{P}\\) instead of \\(\\mathrm{T}\\): \\[\\mathrm{P}(a \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} b \\land D) \\in [0,1]\\] represents the probability or degree of belief in sentence \\(a\\) in the context of data \\(D\\) and supposing also sentence \\(b\\) to be true. Keep in mind that both \\(a\\) and \\(b\\) could be complex sentences (for instance \\(a = (\\lnot c \\lor d) \\land e\\)). Note that truth-values are included as the special cases1 or 0: \\[\n\\mathrm{P}(a \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} b \\land D) = 0\\text{ or }1\n\\quad\\Longleftrightarrow\\quad\n\\mathrm{T}(a \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} b \\land D) = 0\\text{ or }1\n\\]"
  },
  {
    "objectID": "probability_inference.html#probability-inference-rules",
    "href": "probability_inference.html#probability-inference-rules",
    "title": "6  Probability inference",
    "section": "6.3 Probability-inference rules",
    "text": "6.3 Probability-inference rules\nExtending our truth-inference notation to probability-inference notation has been straightforward. But how do we draw inferences when probabilities are involved?\nConsider the inference about my umbrella in a more uncertain situation:\n\n\\[\n\\frac{\n\\mathrm{P}(\\textsf{\\small`My umbrella is either blue or red'}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}D)=1\\quad\n\\mathrm{P}(\\textsf{\\small`My umbrella is not red'} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} D)=0.5\n}{\n\\mathrm{P}(\\textsf{\\small`My umbrella is blue'} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} D) = \\mathord{?}\n}\n\\]\n\nor more compactly, using the symbols we introduced earlier, \\[\n\\frac{\n\\mathrm{P}\\bigl[(b \\lor r) \\land \\lnot (b \\land r)\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}D\\bigr]=1\\quad\n\\mathrm{P}(\\lnot r\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} D)=0.5\n}{\n\\mathrm{P}( b \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} D) = \\mathord{?}\n}\n\\] This says, above the line, that: according to our data \\(D\\) my umbrella is either blue or red (and can’t be both), with full certainty; and according to our data we have no preferential beliefs on whether my umbrella is not red. What should then be the probability of my umbrella being blue, according to our data?\nIntuitively that probability should be 50%: \\(\\mathrm{P}( b \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} D)=0.5\\). But which rules did we follow in arriving at this probability? More generally, which rules should we follow in assigning new probabilities from given ones?\nThe amazing result is that the rules for truth-inference, formulae (5.1, 5.3, 5.2, 5.4), extend also to probability-inference. The only difference is that they now hold for all values in the range \\([0,1]\\), rather than only values \\(0\\) and \\(1\\).\nThis important result was taken more or less for granted at least since Laplace in the 1700s. But was formally proven for the first time in the 1940s by R. T. Cox; the proof has been refined since then. What kind of proof is it? It shows that if we don’t follow the rules we arrive at illogical conclusions; we’ll show some examples later.\nHere are the fundamental rules of probability inference. In these rules, all probabilities can have values in the range \\(\\mathrm{P}() \\in [0,1]\\), and the symbols \\(a,b,D\\) represent sentences of any complexity:\n\n\n\n\n\n\n\n   THE FUNDAMENTAL LAWS OF INFERENCE   \n\n\n\n\n“Not” \\(\\boldsymbol{\\lnot}\\) rule\n\n\\[\\mathrm{P}(\\lnot a \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} D)\n+ \\mathrm{P}(a \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} D)\n= 1\\]\n\n\n“And” \\(\\boldsymbol{\\land}\\) rule\n\n\\[\n\\mathrm{P}(a \\land b \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} D)\n= \\mathrm{P}(a \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} b \\land D) \\cdot\n\\mathrm{P}(b \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} D)\n= \\mathrm{P}(b \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} a \\land D) \\cdot\n\\mathrm{P}(a \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} D)\n\\]\n\n\n“Or” \\(\\boldsymbol{\\lor}\\) rule\n\n\\[\\mathrm{P}(a \\lor b \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} D)\n= \\mathrm{P}(a \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} D) +\n\\mathrm{P}(b \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} D)\n- \\mathrm{P}(a \\land b \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} D)\n\\]\n\n\nSelf-consistency rule\n\n\\[\\mathrm{P}(a \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} a \\land D)\n= 1\n\\]\n\n\n\n\n\nIt is amazing that ALL inference is nothing else but a repeated application of these four rules – billions of times or more, in some inferences. All machine-learning algorithms are just applications or approximations of these rules. Methods that you may have heard about in statistics are just specific applications of these rules. Truth inferences are also special applications of these rules. Most of this course is, at bottom, just a study of how to apply these rules in particular kinds of problems.\n\n\n\n\n\n\n Reading\n\n\n\n\nProbability, Frequency and Reasonable Expectation\nCh. 2 of Bayesian Logical Data Analysis for the Physical Sciences\n§§ 1.0–1.2 of Data Analysis\nFeel free to skim through §§ 2.0–2.4 of Probability Theory"
  },
  {
    "objectID": "probability_inference.html#how-the-inference-rules-are-used",
    "href": "probability_inference.html#how-the-inference-rules-are-used",
    "title": "6  Probability inference",
    "section": "6.4 How the inference rules are used",
    "text": "6.4 How the inference rules are used\nThe fundamental rules represent, first of all, constraints of logical consistency among probabilities. If we have probabilities \\(\\mathrm{P}(a\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}D)=0.7\\), \\(\\mathrm{P}(b\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}a\\land D)=0.1\\), \\(\\mathrm{P}(a\\land b\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}D)=0.2\\), then there’s an inconsistency somewhere, because these values violate the and-rule:  \\(0.2 \\ne 0.1 \\cdot 0.7\\).  In this case we must find the inconsistency and solve it. Since probabilities are quantified by real numbers, however, it’s possible and acceptable to have slight discrepancies owing to numerical round-off errors.\nThe rules also imply more general constraints. For example we must always have \\[\\begin{gather*}\n\\mathrm{P}(a\\land b \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}D) \\le \\min\\bigl\\{\\mathrm{P}(a\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}D),\\  \\mathrm{P}(b\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}D)\\bigr\\}\n\\\\\n\\mathrm{P}(a\\lor b \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}D) \\ge \\max\\bigl\\{\\mathrm{P}(a\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}D),\\  \\mathrm{P}(b\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}D)\\bigr\\}\n\\end{gather*}\\]\n\n\n\n\n\n\n Exercise\n\n\n\nTry to prove the two constraints above\n\n\nThe main use of the rules in concrete applications is for calculating new probabilities from given ones. The calculated probabilities will be automatically consistent. For each equation shown in the rules we can calculate one probability given the remaining ones in the equation, with some special cases when values of \\(0\\) or \\(1\\) appear.\nFor example, if we have \\(\\mathrm{P}(a \\land b\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} D)=0.2\\) and \\(\\mathrm{P}(a\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}D)=0.7\\), from the and-rule we can find \\(\\mathrm{P}(b\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} a \\land D)\\): \\[\\begin{multline*}\n\\underbracket{\\color[RGB]{34,136,51}\\mathrm{P}(a \\land b \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} D)}_{0.2}\n= {\\color[RGB]{238,102,119}\\mathrm{P}(b \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} a \\land D)} \\cdot\n\\underbracket{\\color[RGB]{34,136,51}\\mathrm{P}(a \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} D)}_{0.7}\n\\\\[1em]\n\\Longrightarrow\\quad\n{\\color[RGB]{238,102,119}\\mathrm{P}(b\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} a \\land D)} =\n\\frac{\\color[RGB]{34,136,51}\n\\mathrm{P}(a\\land b \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} D)\n}{\\color[RGB]{34,136,51}\n\\mathrm{P}(a\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}D)\n} = \\frac{0.2}{0.7}\n\\approx 0.2857\n\\end{multline*}\\]\n\nLet us now solve the umbrella inference from the previous section. Starting from \\[\n\\mathrm{P}\\bigl[(b \\lor r) \\land \\lnot (b \\land r)\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}D\\bigr]=1 \\ ,\n\\quad\n\\mathrm{P}(\\lnot r\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} D)=0.5\n\\] we arrive at \\[\n\\mathrm{P}( b \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} D) = 0.5\n\\]\nby following from top to bottom the steps depicted here:\n\n\n\n@@ example medical diagnosis\n\nDerived rules\nThe rules above are in principle all we need to use. But from them it is possible to derive some additional shortcut rules that are automatically consistent with the fundamental ones.\nFirst, it is possible to show that all rules you may know from Boolean algebra are a consequence of the fundamental rules. For example, we can always make the following convenient replacements anywhere in a probability expression: \\[\n\\begin{gathered}\nA \\land A = A \\lor A = A\n\\qquad\n\\lnot\\lnot A = A\n\\\\[1ex]\nA\\land B = B \\land A\n\\qquad\nA \\lor B = B \\lor A\n\\\\[1ex]\n\\lnot (A \\land B) = \\lnot A \\lor \\lnot B\n\\qquad\n\\lnot (A \\lor B) = \\lnot A \\land \\lnot B\n\\\\[1ex]\nA \\land (B \\lor C) = (A \\land B) \\lor (A \\land C)\n\\\\[1ex]\nA \\lor (B \\land C) = (A \\lor B) \\land (A \\lor C)\n\\end{gathered}\n\\]\nTwo other derived rules are used extremely often, so we treat them separately."
  },
  {
    "objectID": "probability_inference.html#law-of-total-probability-or-extension-of-the-conversation",
    "href": "probability_inference.html#law-of-total-probability-or-extension-of-the-conversation",
    "title": "6  Probability inference",
    "section": "6.5 Law of total probability or “extension of the conversation”",
    "text": "6.5 Law of total probability or “extension of the conversation”"
  },
  {
    "objectID": "probability_inference.html#bayess-theorem",
    "href": "probability_inference.html#bayess-theorem",
    "title": "6  Probability inference",
    "section": "6.6 Bayes’s theorem",
    "text": "6.6 Bayes’s theorem\n\n\n\n\n\nBayes’s theorem guest-starring in The Big Bang Theory"
  },
  {
    "objectID": "probability_inference.html#consequences-of-not-following-the-rules",
    "href": "probability_inference.html#consequences-of-not-following-the-rules",
    "title": "6  Probability inference",
    "section": "6.7 consequences of not following the rules,",
    "text": "6.7 consequences of not following the rules,\n@@ §12.2.3 of AI\n\nExercise: Monty-Hall problem & variations\nExercise: clinical test & diagnosis"
  },
  {
    "objectID": "probability_inference.html#common-points-of-certain-and-uncertain-inference",
    "href": "probability_inference.html#common-points-of-certain-and-uncertain-inference",
    "title": "6  Probability inference",
    "section": "6.8 Common points of certain and uncertain inference",
    "text": "6.8 Common points of certain and uncertain inference\n\nNo premises? No conclusions!\n\n\n\n\n\n\n\n Differences in terminology\n\n\n\n\nSome texts speak of the probability of a “random3 variable”, or more precisely of the probability that a random variable takes on a particular value. As you notice, we have just expressed that idea by means of a sentence. The viewpoint and terminology of random variables is a special case of that of sentences. As already discussed, in concrete applications it is important to know how a variable “takes on” a value: for example it could be directly measured, indirectly reported, or purposely set. Thinking in terms of sentences, rather than of random variables, allows us to account for these important differences.\nSome texts speak of the probability of an “event”. For all purposes an “event” is just what’s expressed in a sentence.\n\n3 What does ”random” mean? Good luck finding an understandable and non-circular definition in texts that use that word. In these notes, if the word ”random” is ever used, it means ”unpredictable” or ”unsystematic”.It’s a question for sociology of science why some people keep on using less flexible points of view or terminologies. Probably they just memorize them as students and then a fossilization process sets in."
  },
  {
    "objectID": "data_information.html#kinds-of-data",
    "href": "data_information.html#kinds-of-data",
    "title": "7  Data and information",
    "section": "7.1 Kinds of data",
    "text": "7.1 Kinds of data\n\nBinary\n\n\nNominal\n\n\nOrdinal\n\n\nContinuous\n\nunbounded\nbounded\ncensored\n\n\n\nComplex data\n2D, 3D, images, graphs, etc.\n\n\n“Soft” data\n\norders of magnitude\nphysical bounds"
  },
  {
    "objectID": "data_information.html#data-transformations",
    "href": "data_information.html#data-transformations",
    "title": "7  Data and information",
    "section": "7.2 Data transformations",
    "text": "7.2 Data transformations\n\nlog\nprobit\nlogit"
  },
  {
    "objectID": "probability_distributions.html#the-difference-between-statistics-and-probability-theory",
    "href": "probability_distributions.html#the-difference-between-statistics-and-probability-theory",
    "title": "8  Allocation of uncertainty among possible data values: probability distributions",
    "section": "8.1 The difference between Statistics and Probability Theory",
    "text": "8.1 The difference between Statistics and Probability Theory\nStatistics is the study of collective properties of collections of data. It does not imply that there is any uncertainty.\nProbability theory is the quantification and propagation of uncertainty. It does not imply that we have collections of data."
  },
  {
    "objectID": "probability_distributions.html#whats-distributed",
    "href": "probability_distributions.html#whats-distributed",
    "title": "8  Allocation of uncertainty among possible data values: probability distributions",
    "section": "8.2 What’s “distributed”?",
    "text": "8.2 What’s “distributed”?\nDifference between distribution of probability and distribution of (a collection of) data."
  },
  {
    "objectID": "probability_distributions.html#distributions-of-probability",
    "href": "probability_distributions.html#distributions-of-probability",
    "title": "8  Allocation of uncertainty among possible data values: probability distributions",
    "section": "8.3 Distributions of probability",
    "text": "8.3 Distributions of probability\n\nRepresentations\n\nDensity function\nHistogram\nScatter plot\n\nBehaviour of representations under transformations of data."
  },
  {
    "objectID": "probability_distributions.html#summaries-of-distributions-of-probability",
    "href": "probability_distributions.html#summaries-of-distributions-of-probability",
    "title": "8  Allocation of uncertainty among possible data values: probability distributions",
    "section": "8.4 Summaries of distributions of probability",
    "text": "8.4 Summaries of distributions of probability\n\nLocation\nMedian, mean\n\n\nDispersion or range\nQuantiles & quartiles, interquartile range, median absolute deviation, standard deviation, half-range\n\n\nResolution\nDifferential entropy\n\n\nBehaviour of summaries under transformations of data and errors in data"
  },
  {
    "objectID": "probability_distributions.html#outliers-and-out-of-population-data",
    "href": "probability_distributions.html#outliers-and-out-of-population-data",
    "title": "8  Allocation of uncertainty among possible data values: probability distributions",
    "section": "8.5 Outliers and out-of-population data",
    "text": "8.5 Outliers and out-of-population data\n(Warnings against tail-cutting and similar nonsense-practices)"
  },
  {
    "objectID": "probability_distributions.html#marginal-and-conditional-distributions-of-probability",
    "href": "probability_distributions.html#marginal-and-conditional-distributions-of-probability",
    "title": "8  Allocation of uncertainty among possible data values: probability distributions",
    "section": "8.6 Marginal and conditional distributions of probability",
    "text": "8.6 Marginal and conditional distributions of probability"
  },
  {
    "objectID": "probability_distributions.html#collecting-and-sampling-data",
    "href": "probability_distributions.html#collecting-and-sampling-data",
    "title": "8  Allocation of uncertainty among possible data values: probability distributions",
    "section": "8.7 Collecting and sampling data",
    "text": "8.7 Collecting and sampling data\n\n“Representative” samples\nSize of minimal representative sample = (2^entropy)/precision\n\nExercise: data with 14 binary variates, 10000 samples\n\n\n\nUnavoidable sampling biases\nIn high dimensions, all datasets are outliers.\nData splits and cross-validation cannot correct sampling biases"
  },
  {
    "objectID": "probability_distributions.html#quirks-and-warnings-about-high-dimensional-data",
    "href": "probability_distributions.html#quirks-and-warnings-about-high-dimensional-data",
    "title": "8  Allocation of uncertainty among possible data values: probability distributions",
    "section": "8.8 Quirks and warnings about high-dimensional data",
    "text": "8.8 Quirks and warnings about high-dimensional data"
  },
  {
    "objectID": "making_decisions.html#decisions-possible-situations-and-consequences",
    "href": "making_decisions.html#decisions-possible-situations-and-consequences",
    "title": "9  Making decisions",
    "section": "9.1 Decisions, possible situations, and consequences",
    "text": "9.1 Decisions, possible situations, and consequences"
  },
  {
    "objectID": "making_decisions.html#gains-and-losses-utilities",
    "href": "making_decisions.html#gains-and-losses-utilities",
    "title": "9  Making decisions",
    "section": "9.2 Gains and losses: utilities",
    "text": "9.2 Gains and losses: utilities\n\nFactors that enter utility quantification\nUtilities can rarely be assigned a priori."
  },
  {
    "objectID": "making_decisions.html#making-decisions-under-uncertainty-maximization-of-expected-utility",
    "href": "making_decisions.html#making-decisions-under-uncertainty-maximization-of-expected-utility",
    "title": "9  Making decisions",
    "section": "9.3 Making decisions under uncertainty: maximization of expected utility",
    "text": "9.3 Making decisions under uncertainty: maximization of expected utility"
  }
]