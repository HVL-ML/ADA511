[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ADA511 0.1 Data science and data-driven engineering",
    "section": "",
    "text": "Dear student and aspiring data engineer\nThe goal of this course is not to help you learn how to tune the parameters of the latest kind of deep network, or how to efficiently handle large amounts of data, or how to do cross-validation in the fastest way, or what is the latest improvement in random-forest algorithms.\nThe goal of this course is to help you learn the principles to build the machine-learning algorithms and AI devices of the future. And, as a side effect, you’ll also learn how to concretely improve present-day algorithms, and also how to determine if any of them has already reached its maximal theoretical performance.\nHow can such a goal be achieved?\nThere is a small set of rules and one method that are mathematically guaranteed to output the optimal solution of any inference, prediction, classification, regression, and decision problem. You can think of this as the “unbeatable universal machine”. Or, from an AI point of view, you can think of these rules and method as the “laws of robotics” that should govern any ideal AI designed to draw inferences and make decisions.\nApproximations evolve toward the maximally optimal ideal method. The approximations used at any given time in history exploit the computational technologies then available. Deep networks, for instance, would have been a useless approximation 50 years ago, before the introduction of Graphical Processing Units.\nEvery new technological advance (think of possibly forthcoming quantum computers) opens up possibilities for new approximations that get us closer to the ideal optimum. To see and realize these possibilities, or to judge whether they have already been realized, a data scientist needs at the very least:\n-   to know the foundation of the maximally optimal method\n-   to think outside the box\nWithout the first requirement, how do you know what is the target to approximate towards, and how far you are from it? You risk:\n making an approximation that leads to worse results than before;\n evaluating the approximation in the wrong way, so you don’t even realize it’s worse than before;\n trying to improve an approximation that has already attained the theoretical optimum. Think about an engine that has already the maximal efficiency dictated by thermodynamics; and an engineer, ignorant of thermodynamics, who wastes effort in trying to improve it further.\nWithout the second requirement, you risk missing to take full advantage of the new technological possibilities. Consider the evolution of transportation: if you keep thinking in terms of how to improve a horse-carriage wooden wheels, you’ll never conceive a combustion engine; if you keep thinking in terms of how to improve combustion fuel, you’ll never conceive an electric motor. Existing approximations may of course be good starting points; but you need to clearly understand how they approximate the ideal optimum – so we’re back to the first requirement.\nIf you want to make advances in machine learning and AI, you must know how the ideal universal algorithm looks like, and you must not limit yourself to thinking of “training sets”, “cross-validation”, “supervised learning”, “overfitting”, “models”, and similar notions. In this course you’ll see for yourself that such notions are anchored to the present-day box of approximations.\nAnd we want to think outside that box.\nBut don’t worry: this course does not only want to prepare you for the future. With the knowledge and insights acquired, you will be able to propose and implement concrete improvements to present-day methods as well, or calculate whether they can’t be improved further."
  },
  {
    "objectID": "index.html#your-role-in-the-course-bugs-features",
    "href": "index.html#your-role-in-the-course-bugs-features",
    "title": "ADA511 0.1 Data science and data-driven engineering",
    "section": "Your role in the course Bugs & features",
    "text": "Your role in the course Bugs & features\nThis course is still in an experimental, “alpha” version. So you will not only learn something from it (hopefully), but also test it together with us, and help improving it for future students. Thank you for this in advance!\nFor this reason it’s good to clarify some goals and guidelines of this course: \n\n  Light maths requirements\n\nWe believe that the fundamental rules and methods can be understood and also used (at least in not too complex applications) without complex mathematics. Indeed the basic laws of inference and decision-making involve only the four basic operations \\(+ - \\times /\\). So this course only requires maths at a beginning first-year undergraduate level.\n\n\n\n  Informal style\n\nThe course notes are written in an informal style; for example they are not developed along “definitions”, “lemmata”, “theorems”. This does not mean that they are inexact. We will warn you about parts that are oversimplified or that only cover special contexts.\n\n\n\n  Names don’t constitute knowledge\n\n\n\n\nIn these course notes you’ll often stumble upon terms in blue bold and definitions in blue Italics. This typographic emphasis does not mean that those terms and definitions should be memorized: rather, it means that there are important ideas around there which you must try to understand and use. In fact we don’t care which terminology you adopt. Instead of the term statistical population, feel free to use the term pink apple if you like, as long you explain the terms you use by means of a discussion and examples.2 What’s important is that you know, can recognize, and can correctly use the ideas behind technical terms.\nMemorizing terms, definitions, and where to use them, is how large language models (like chatGPT) operate. If your study is just memorization of terms, you’ll have difficulties finding jobs in the future, because there will be algorithms that can do that better and at a cheaper cost than you.\n2 Some standard technical terms are no better. The common term random variable, for instance, often denotes something that is actually not “random” and not variable. Go figure. Using the term green banana would be less misleading!\n\n  Diverse textbooks\n\nThis course does not have only one textbook: it refers to and merges together parts from several books and articles. As you read these works, you will notice that they adopt quite different terminologies, employ different symbolic notations, give different definitions for similar ideas, and sometimes even contradict each other.\nThese differences and contradictions are a feature, not a bug!\nYou might think that this makes studying more difficult; but it actually helps you to really understand an idea and acquire real knowledge, because it forces you to go beyond words, symbols, and specific points of view and examples. This point connects with the previous point, “names don’t constitute knowledge”. The present course notes will help you build comprehension bridges across those books.\n\n\n\n  Artificial intelligence\n\nIn order to grasp and use the fundamental laws of inference and decision-making, we shall use notions that are also at the foundations of Artificial Intelligence (and less common in present-day machine learning). So you’ll also get a light introduction to AI for free. Indeed, a textbook that we’ll draw frequently from is Russell & Norvig’s Artificial Intelligence: A Modern Approach (we’ll avoid part V on machine learning, however, because it’s poorly explained and written).\n\n\n\n\n  Concrete examples\n\nSome students find it easier to grasp an idea by starting from an abstract description and then examining concrete examples; some find it easier the other way around. We try to make both happy by alternating between the two approaches. Ideas and notions are always accompanied by examples that we try to keep simple yet realistic, drawing from scenarios ranging from glass forensics to hotel booking.\n\n\n\n  Code\n\nWe shall perform inferences on concrete datasets, also comparing different methodologies. Most of these can be performed with any specific programming language, so you can use your favourite one – remember that we want to try to think outside the box of present-day technologies, and that includes present-day programming languages. Most examples in class and in exercises will be given in R and Python, but are easily translated into other languages.\n\n\n\n  Extra material\n\nThe course has strong connections with many other disciplines, such as formal logic, proof theory, psychology, philosophy, physics, and environmental sciences. We have tried to provide a lot of extra reading material in “For the extra curious” side boxes, for those who want to deepen their understanding of topics covered or just connected to the present course. Maybe you’ll stumble into a new passion or even into your life call?\n\n\n\n\n\n\n\n\n For the extra curious"
  },
  {
    "objectID": "index.html#course-structure",
    "href": "index.html#course-structure",
    "title": "ADA511 0.1 Data science and data-driven engineering",
    "section": "Course structure",
    "text": "Course structure\nThe course structure reflects the way in which the ideal universal decision-making machine works. It can be roughly divided into three or four parts, illustrated as follows (this is just a caricature, don’t take this diagram too literally):\n\n\n\n\nflowchart TB\n  P{{probability}} --o Z[/max expected utility\\]\n  subgraph probability [ ]\n  S([sentences]) --&gt; P\n  end\n  subgraph data [ ]\n  A[quantity] --o S\n%%  D[value] --o S\n  E[population] --o S\n  F[...] -.-o S\n  end\n  G[(problem)] --x A & E %% & D\n  G -.-x F\n  U{{utility}} --o Z\n  subgraph utility [ ]\n  C[decisions] --o S\n  S --&gt; U\n  T([gains]) --&gt; U\n  Q([costs]) --&gt; U\n  end\n  G --x C & T & Q\n  subgraph exp [ ]\n  Z --&gt; W(optimal solution)\n  end\n  %%\n  style G fill:#e67\n  style A fill:#cb4,stroke-width:0pt\n%%  style D fill:#cb4,stroke-width:0pt\n  style E fill:#cb4,stroke-width:0pt\n  style F fill:#cb4,stroke-width:0pt\n  style S fill:#283,color:#fff\n  style P fill:#283,stroke-width:3pt,color:#fff\n  style C fill:#6ce,stroke-width:0pt\n  style T fill:#6ce,stroke-width:0pt\n  style Q fill:#6ce,stroke-width:0pt\n  style U fill:#6ce,stroke-width:3pt\n  style Z fill:#47a,color:#fff,stroke-width:0pt,color:#fff\n  style W fill:#47a,color:#fff,stroke-width:0pt,color:#fff\n  style probability fill:#fff,stroke:#283,stroke-width:1px\n  style data fill:#fff,stroke:#cb4,stroke-width:1px\n  style utility fill:#fff,stroke:#6ce,stroke-width:1px\n  style exp fill:#fff,stroke:#47a,stroke-width:1px\n\n\n\n\n\n\nData parts develop the language in which a problem can be fed into the decision-making machine. Here you will also learn about important pitfalls in handling data.\nInference parts develop the “inference engine” of the machine. Here you will learn ideas at the foundation of AI; and you will also meet probability, but from a point of view that may be quite novel to you – and much more fun.\n\nThese two parts will alternate so that their development proceeds almost in parallel.\n\nThe utility part develops the “decision engine” of the machine. Here you will meet several ideas that will probably be quite new to you – but also very simple and intuitive.\nThe final solution part simply shows how the inference and utility engines combine together to yield the optimal solution to the problem. This part is simple, short, intuitive; it will be a breeze.\n\n\n\nAs soon as the inference and data parts are complete, you will be able to apply the machine to real inference problems, and also learn how the solution is approximated in some popular machine-learning algorithms.\nThese applications will immediately extend to decision problems as the short utility and solution part are covered. Again you will also see how this solution is approximated in other machine-learning algorithms, classification and regression ones."
  },
  {
    "objectID": "preface.html#mechanics-and-engineers",
    "href": "preface.html#mechanics-and-engineers",
    "title": "Preface",
    "section": "Mechanics and engineers",
    "text": "Mechanics and engineers\nWhat is the difference between a car mechanic and an automotive engineer?\nBoth have knowledge about cars, but their knowledge domains are different and focus on different goals.\nA car mechanic can keep your car in top-notch condition; can do different kinds of easy and difficult repairs if problems arise with it; knows whether a particular brand of valve can be used as a replacement for another brand; can recommend the optimal kind of tyres to use in a given season for different brands of cars. But a car mechanic would face difficulties in calculating the theoretical maximal efficiency of an engine; or predicting the temperature increase caused by a new kind of fuel; or exploiting the phase transition of a new kind of foam to design a safer airbag system; or calculating the optimal surface curvature for a spoiler. A car mechanic typically possesses a large amount of case-specific knowledge, and doesn’t need to know in depth the principles of electromechanics and thermochemistry, or the laws of balance of momentum, energy, entropy.\n\n\n\nVice versa, an automotive engineer can assess how to use the electromechanical properties of a new material in order to design a more efficient and environmentally friendly engine; can calculate how a new material-surface handling would affect air drag and speed; and ultimately can research how to exploit new physical phenomena to build completely new means of transportation. Yet, an automotive engineer could be completely incapable of changing a pipe in your car, or tell you whether it can use a particular brand of lubricant oil. An automotive engineer typically possesses knowledge about the principles of electromagnetism, mechanics, or thermochemistry; is acquainted with relevant physical laws; and doesn’t need to have in-depth case-specific kinds of knowledge.\n\n\n      \nNote that the differences just sketched do not imply a judgement of value. Both professions, kinds of knowledge, and goals are necessary, interesting, and couldn’t exist without each other. Choice between them is a subjective matter of personal passions and aspirations.\nIn fact there isn’t a clear divide between these two kinds of knowledge, but rather a continuum between two vague extremities. A car mechanic can have knowledge and insight about new technologies, and an automotive engineer can know how to fix a carburettor. The two sketches above are meant to expose and emphasize the existence of such a continuum of knowledge and of goals."
  },
  {
    "objectID": "preface.html#data-mechanics-and-data-engineers",
    "href": "preface.html#data-mechanics-and-data-engineers",
    "title": "Preface",
    "section": "Data mechanics and data engineers",
    "text": "Data mechanics and data engineers\nA continuum with two similar extremities can also be drawn in data science.\nSome data scientists have in-depth knowledge on, for instance, how to optimally store and read large amounts data; what kind of machine-learning algorithm to use in a given task; how to fine-tune an algorithm’s parameters, and the currently best software for this purpose. Their particular knowledge is fundamental for the working of today’s technological infrastructure.\nAt the same time, these data scientists typically face difficulties, for instance, in:\n\ncalculating the theoretical maximal accuracy or performance achievable – by any possible algorithm – in a given inference problem\nexplaining how the fundamental rules of inference and decision-making are implemented in a particular machine-learning algorithm\nidentifying which sub-optimal approximations to the fundamental rules are made by popular machine-learning algorithms\nexploiting new technologies to build new algorithms that do calculations closer to the exact theoretical ones, thereby achieving a performance closer to the theoretical optimum\n\nAnd it is also possible that they are not aware of, and maybe would be surprised by, some basic facts of data science. For instance:\n\nthere is an optimal, universal inference & decision algorithm, of which all machine-learning algorithms (from support vector machines and deep networks to random forests and large language models), are an approximation\nthere are only five or six fundamental laws upon which any inference, prediction, classification, regression, decision task is (or ought to be) based upon\nsplittings of data into “training set”, “validation set”, and similar sets, are not part of the exact application of the laws of inference and decision-making; such splittings arise as coarse approximations of the exact method.\ncross-validation and related techniques are not part of the exact method either; they also arise as approximations\noverfitting, underfitting and related notions are not problems that appear in the exact method (which takes care of them automatically); they also arise from approximations\nit is possible to calculate, within probable bounds, the maximal accuracy (or other performance metric) achievable by any classification or regression algorithm for a given application\nsome evaluation metrics, such as precision or the area under the curve of the receiver operating characteristic (AUC), have intrinsic flaws and may attribute higher values to worse-performing algorithms\n\n…because this is a kind of general and principled knowledge that these data scientists don’t need in their jobs. Their knowledge is more case-specific.\nDrawing a parallel with the car example, a data scientist with this kind of case-specific knowledge is like a “data mechanic”.\nA “data engineer”, on the other hand, is the kind of data scientist who has no difficulties with the knowledge and skills implicit in the bullet points above; but at the same time might not know what software to use for tuning parameters of a particular class of deep networks, or the best format to store particular kinds of data.\nJust like in the case of the automotive industry, the difference just sketched does not imply any judgement of value. Both kinds of knowledge and goals are important and can’t exist without each other."
  },
  {
    "objectID": "preface.html#goals-of-this-course",
    "href": "preface.html#goals-of-this-course",
    "title": "Preface",
    "section": "Goals of this course",
    "text": "Goals of this course\nThere is a plethora of academic courses, in all kinds of format, that target knowledge and goals for the “data mechanic”. Those courses are usually inadequate to cover the knowledge and goals for the “data engineer”. Some courses, misleadingly, even present approximations and recipes that are only valid for particular situations as if they were universal rules or methods instead.\nCourses that target the “data engineer” seem to be more rare. One possible reason is that this kind of knowledge is actually hidden in courses on probability, statistics, and risk analysis, presented with a language which makes only opaque and confusing connections with fields in data science and their goals; or, worse, with a language which emphasizes connections that are actually superficial and misleading.\nWe believe that it is important to teach and keep alive the less “mechanic” and more “engineer” side of data science:\n\nContinuous advances in computational technology – think of quantum computers – will offer completely novel and superior ways to approximate the exact method of inference and decision. Only the data scientist who knows the exact method and theory, and understands how present-day algorithms approximate it, will be able to exploit new technologies.\nEven without looking at the future, several present-day machine-learning algorithms could already be greatly optimized by any data engineer who is acquainted with the basic principles underlying data science.\nThe foundations of data science are the bridge to the sibling discipline of Artificial Intelligence.\n\nThe present course aspires to give an introduction to the “data engineer” side, rather than “data mechanic” one, of data science, but using a point of view more familiar to data scientists than to, say, statisticians.\nMore details about its aims, structure, and features are already given in the Dear student introduction."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Accept or discard?",
    "section": "",
    "text": "\\[\n\\DeclarePairedDelimiters{\\set}{\\{}{\\}}\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\\]\n\nLet’s start with a question that could arise in a particular engineering problem:\n\nA particular kind of electronic component is produced on an assembly line. At the end of the line, there is an automated inspection device that works as follows with every newly produced component coming out of the line.\nThe inspection device first makes some tests on the new component. The tests give an uncertain forecast of whether that component will fail within its first year of use, or after.\nThen the device decides whether the component is accepted and packaged for sale, or discarded and thrown away.\nWhen a new electronic component is sold, the manufacturer has a net gain of \\(1\\$\\), and that’s the net gain if the component works for at least a year. If the component fails within a year of use, however, the manufacturer incurs a net loss of \\(11\\$\\) (12$ loss, minus the 1$ gained at first), owing to warranty refunds and damage costs to be paid to the buyer. When a new electronic component is discarded, the manufacturer has \\(0\\$\\) net gain.\nFor a new electronic component just come out of the assembly line, the tests of the automated inspection device indicate that there is a \\(10\\%\\) probability that the component will fail within its first year of use.\n\n\n\n\nShould the inspection device accept the new component? or discard it?\n\nTry to give and motivate an answer:\n\n\n\n\n\n\n Very first exercise!\n\n\n\n\nShould the inspection device accept or discard the new component?\n\nIt doesn’t matter if you don’t get the correct answer; not even if you don’t manage to get an answer at all. The purpose here is for you to do some introspection about your own reasoning.\nThen examine and discuss the following points:\n\nWhich numerical elements in the problem seem to affect the answer?\nCan these numerical elements be clearly separated? How would you separate them?\nHow would the answer change, if these numerical elements were changed? Feel free to change them, also in extreme ways, and see how the answer would change.\nCould we solve the problem if we didn’t have the probabilities? Why?\nCould we solve the problem if we didn’t know the various gains and losses? Why?\nCan this problem be somehow abstracted, and then transformed into another one with completely different details? For instance, consider translating along these lines:\n\ninspection device → computer pilot of self-driving car\ntests → camera image\nfail within a year → pedestrian in front of car\naccept/discard → keep on going/ break"
  },
  {
    "objectID": "framework.html#what-does-the-intro-problem-tell-us",
    "href": "framework.html#what-does-the-intro-problem-tell-us",
    "title": "2  Framework",
    "section": "2.1 What does the intro problem tell us?",
    "text": "2.1 What does the intro problem tell us?\nLet’s approach the “accept or discard?” problem of the previous chapter 1 in an intuitive way.\n\n\nWe’re jumping the gun here, because we haven’t learned the method to solve this problem yet!\nFirst, what happens if we accept the component?\nWe must try to make sense of the \\(10\\%\\) probability that the component fails within a year. For the moment let’s use an intuitive imagination trick to make sense of this. Imagine that this situation is repeated 100 times. In 10 of these repetitions the accepted electronic component is sold and fails within a year after selling. In the remaining 90 repetitions, the component is sold and works fine for at least a year. Later on we’ll approach this in a more rigorous way, where the idea of “imaginary repetitions” is not needed.\nIn each of the 10 imaginary repetitions in which the component fails early, the manufacturer loses \\(\\color[RGB]{238,102,119}11\\$\\). That’s a total loss of \\({\\color[RGB]{204,187,68}10} \\cdot {\\color[RGB]{238,102,119}11\\$} = {\\color[RGB]{238,102,119}110\\$}\\). In each of the 90 imaginary repetitions in which the component doesn’t fail early, the manufacturer gains \\(\\color[RGB]{34,136,51}1\\$\\). That’s a total gain of \\({\\color[RGB]{204,187,68}90} \\cdot {\\color[RGB]{34,136,51}1\\$} = {\\color[RGB]{34,136,51}90\\$}\\). So over all 100 imaginary repetitions the manufacturer gains\n\\[\n{\\color[RGB]{204,187,68}10}\\cdot ({\\color[RGB]{238,102,119}-11\\$}) + {\\color[RGB]{204,187,68}90}\\cdot {\\color[RGB]{34,136,51}1\\$} = {\\color[RGB]{238,102,119}-20\\$}\n\\]\nthat is, the manufacturer has not gained, but lost \\(20\\$\\)! That’s an average of \\(0.2\\$\\) lost per repetition.\n\n\nNow let’s examine the second choice: what happens if we discard the component instead?\nIn this case we don’t need to invoke imaginary repetitions, but even if we do, it’s clear that the manufacturer doesn’t gain or lose anything – that is, the “gain” is \\(0\\$\\) – in each and all of the repetitions.\nThe conclusion is that if in a situation like this we accept the component, then we’ll lose \\(0.2\\$\\) on average; whereas if we discard it, then on average we’ll lose (or gain) \\(0\\$\\) on average.\nObviously the best, or “least worst”, decision to make is to discard the component.\n\n\n\n\n\n\n Exercises\n\n\n\n\nNow that we have an idea of the general reasoning, check what happens with different values of the probability of failure and of the failure cost: is it still best to discard? For instance, try with\n\nfailure probability 10% and failure cost 5$;\nfailure probability 5% and failure cost 11$;\nfailure probability 10%, failure cost 11$, non-failure gain 2$.\n\nFeel free to get wild and do plots.\nIdentify the failure probability at which accepting the component doesn’t lead to any loss or any gain, so it doesn’t matter whether we discard or accept. (You can solve this as you prefer: analytically with an equation, visually with a plot, by trial & error on several cases, or whatnot.)\nConsider the special case with failure probability 0% and failure cost 10$. This means no new component will ever fail. To decide in such a case we do not need imaginary repetitions; but confirm that we arrive at the same logical conclusion whether we reason through imaginary repetitions or not.\nConsider this completely different problem:\n\nA patient is examined by a brand-new medical diagnostics AI system.\nThe AI first performs some clinical tests on the patient. The tests give an uncertain forecast of whether the patient has a particular disease or not.\nThen the AI decides whether the patient should be dismissed without treatment, or treated with a particular medicine.\nIf the patient is dismissed, then the life expectancy doesn’t increase or decrease if the disease is not present, but it decreases by 10 years if the disease is actually present. If the patient is treated, then the life expectancy decreases by 1 year if the disease is not present (owing to treatment side-effects), but also if the disease is present (because it cures the disease, so the life expectancy doesn’t decrease by 10 years; but it still decreases by 1 year owing to the side effects).\nFor this patient, the clinical tests indicate that there is a \\(10\\%\\) probability that the patient has the disease.\n\nShould the diagnostic AI dismiss or treat the patient? Find differences and similarities, even numerical, with the assembly-line problem.\n\n\n\n\n\nFrom the solution of the problem and from the exploring exercises, we gather some instructive points:\n\nIs it enough if we simply know that the component is less likely to fail than not? in other words, if we simply know that the probability of failure is less than 50% but don’t know its precise value?\nObviously not. We found that if the failure probability is \\(10\\%\\) then it’s best to discard; but if it’s \\(5\\%\\) then it’s best to accept. In both cases the probability of failure was less than 50%, but the decisions were different. Moreover, we found that the probability affected amount of loss if one made the non-optimal decision. Therefore:\nKnowledge of precise probabilities is absolutely necessary for making the best decision\nIs it enough if we simply know that failure leads to a loss, and non-failure leads to a gain, without knowing the precise amounts of loss and gain?\nObviously not. In the exercise we found that if the failure cost is \\(11\\$\\) then it’s best to discard; but if it’s \\(5\\$\\) then it’s best to accept. It’s also best to accept if the failure cost is \\(11\\$\\) but the non-failure gain is \\(2\\$\\). Therefore:\nKnowledge of the precise gains and losses is absolutely necessary for making the best decision\nIs this kind of decision situation only relevant to assembly lines and sales?\nBy all means not. We found a clinical situation that’s exactly analogous: there’s uncertainty, there are gains and losses (of lifetime rather than money), and the best decision depends on both."
  },
  {
    "objectID": "framework.html#our-focus-decision-making-inference-and-data-science",
    "href": "framework.html#our-focus-decision-making-inference-and-data-science",
    "title": "2  Framework",
    "section": "2.2 Our focus: decision-making, inference, and data science",
    "text": "2.2 Our focus: decision-making, inference, and data science\nEvery data-driven engineering project is unique, with its unique difficulties and problems. But there are also problems common to all engineering projects.\nIn the scenarios we explored above, we found an extremely important problem-pattern. There is a decision or choice to make (and “not deciding” is not an option, or it’s just another kind of choice). Making a particular decision will lead to some consequences, some leading to something desirable, others leading to something undesirable. The decision is difficult because its consequences are not known with certainty, given the information and data available in the problem. We may lack information and data about past or present details, about future events and responses, and so on. This is what we call a problem of decision-making under uncertainty or under risk1, or simply a “decision problem” for short.1 We’ll avoid the word “risk” because it has several different technical meanings in the literature, some even contradictory.\nThis problem-pattern appears literally everywhere. But our explored scenarios also suggest that this problem-pattern has a sort of systematic solution method.\nIn this course we’re going to focus on decision problems and their systematic solution method. We’ll learn a framework and some abstract notions that allow us to frame and analyse this kind of problem, and we’ll learn a universal set of principles to solve it. This set of principles goes under the name of Decision Theory. \nBut what do decision-making under uncertainty and Decision Theory have to do with data and data science? The three are profoundly and tightly related on many different planes:\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nDecision theory in expert systems and artificial intelligence\n\n\n\nData science is based on the laws of Decision Theory. These laws are similar to what the laws of physics are to a rocket engineer. Failure to account for these fundamental laws leads at best to sub-optimal solutions, at worst to disasters.\nMachine-learning algorithms, in particular, are realizations or approximations of the rules of Decision Theory. This is clear, for instance, considering that a machine-learning classifier is actually choosing among possible output labels or classes.\nThe rules of Decision Theory are also the foundations upon which artificial-intelligence agents, which must make optimal inferences and decisions, are built.\nWe saw that probability values are essential to a decision problem. How do we find them? Data play an important part in their calculation. In our intro example, the failure probability must come from observations or experiments on similar electronic components.\nWe saw that also the values of gains and losses are essential. Data play an important part in their calculation as well.\n\nThese five planes will constitute the major parts and motivations of the present course.\n\n\nThere are other important aspects in engineering problems, besides the one of making decisions under uncertainty. For instance the discovery or the invention of new technologies and solutions. Aspects such as these two can barely be planned or decided. Their drive and direction, however, rest on a strive for improvement and optimization – and the fundamental laws of decision theory tell us what’s optimal and what’s not.\nArtificial intelligence is proving to be a valuable aid in these more creative aspects too. This kind of use of AI is outside the scope of the present notes. Some aspects of this creativity-assisting use, however, do fall within the domain of the present notes. A pattern-searching algorithm, for example, can be optimized by means of the method we are going to study."
  },
  {
    "objectID": "framework.html#sec-optimality",
    "href": "framework.html#sec-optimality",
    "title": "2  Framework",
    "section": "2.3 Our goal: optimality, not “success”",
    "text": "2.3 Our goal: optimality, not “success”\nWhat should we demand from a systematic method for solving decision problems?\nBy definition, in a decision problem under uncertainty there is generally no method to determine the decision that surely leads to the desired consequence – if such a method existed, then the problem would not have any uncertainty! Therefore, if there is a method to deal with decision problems, its goal cannot be the determination of the successful decision. This also means that a priori we cannot blame an engineer for making an unsuccessful decision in a situation of uncertainty. Then what should be the goal of such a method?\nImagine two persons, Henry and Tina, who must choose between a “heads-bet” or a “tails-bet” before a coin is tossed; the bets work as follows:\n\n“heads-bet”: if the coin lands heads, the person wins a small amount of money; but if it lands tails, they lose a large amount of money\n“tails-bet”: if the coin lands tails, the person wins a small amount of money; if it lands heads, they lose the same small amount of money\n\n\n\n\n\nflowchart LR\n  C[choose a bet!] ---|heads-bet| H([toss coin])\n  C ---|tails-bet| T([toss coin])\n  H ---|heads| HH{{+ $}}\n  H ---|tails| HT{{- $$$}}\n  T ---|heads| TH{{- $}}\n  T ---|tails| TT{{+ $}}\n  %%\n  linkStyle 0 stroke:#cb4,color:#cb4\n  linkStyle 1 stroke:#6ce,color:#6ce\n  linkStyle 2 stroke:#283,color:#283\n  linkStyle 3 stroke:#e67,color:#e67\n  linkStyle 4 stroke:#e67,color:#e67\n  linkStyle 5 stroke:#283,color:#283\n  style HH fill:#283,color:#fff,stroke-width:0px\n  style HT fill:#e67,color:#fff,stroke-width:0px\n  style TT fill:#283,color:#fff,stroke-width:0px\n  style TH fill:#e67,color:#fff,stroke-width:0px\n\n\n\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nWhich bet would you choose? why?\n\n\nHenry chooses the heads-bet. Tina chooses the tails-bet. The coin comes down heads. So Henry wins the small amount of money, while Tina loses the same small amount.\nWhat would we say about their decisions?\nHenry’s decision was lucky, and yet irrational: he risked losing much more money than he could win. Tina’s decision was unlucky, and yet rational: she wasn’t risking to lose more than she could win. Said otherwise, the two bets had the same winning prospects, but the heads-bet had more risk of loss than the tails-bet.\nWe expect that any person making Henry’s decision in similar, future bets will eventually lose more money than any person making Tina’s decision.\nThis example shows two points:\n\n“success” is generally not a good criterion to judge a decision under uncertainty; success can be the pure outcome of luck, not of smarts\neven if there is no method to determine which decision is successful, there is a method to determine which decision is rational or optimal, given the particular gains, losses, and uncertainties involved in the decision problem\n\nWe had a glimpse of this method in our introductory scenarios.\nLet us emphasize, however, that we are not giving up on “success”, or trading it for “optimality”. Indeed we’ll find that Decision Theory automatically leads to the successful decision in problems where uncertainty is not present or is irrelevant. It’s a win-win. It’s important to keep this point in mind:\n\n\n\n\n\n\n\n\n\n\n\nAiming to find the solutions that are successful can make us fail to find those that are optimal when the successful ones cannot be determined.\nAiming to find the solutions that are optimal makes us automatically find those that are successful when those can be determined.\n\n\n\nWe shall later witness this fact with our own eyes, and will take it up again in the discussion of some misleading techniques to evaluate machine-learning algorithms."
  },
  {
    "objectID": "framework.html#sec-decision-theory",
    "href": "framework.html#sec-decision-theory",
    "title": "2  Framework",
    "section": "2.4 Decision Theory",
    "text": "2.4 Decision Theory\nSo far we have mentioned that Decision Theory has the following features:\n\n it tells us what’s optimal and, when possible, what’s successful\n it takes into consideration decisions, consequences, costs and gains\n it is able to deal with uncertainties\n\nWhat other kinds of features should we demand from it, in order to be applied to as many kinds of decision problems as possible, and to be relevant for data science?\n\nIf we find an optimal decision in regards to some outcome, it may still happen that the decision can be realized in several ways that are equivalent in regard to the outcome, but inequivalent in regard to time or resources. In the assembly-line scenario, for example, the decision discard could be carried out by burning, recycling, and so on. We thus face a decision within a decision. In general, a decision problem may involve several decision sub-problems, in turn involving decision sub-sub-problems, and so on.\nIn data science, a common engineering goal is to design and build an automated or AI-based device capable of making an optimal decision in a specific kind of uncertain situations. Think for instance of an aeronautic engineer designing an autopilot system, or a software company designing an image classifier.\n\nDecision Theory turns out to meet these two demands too, thanks to the following features:\n\n it is susceptible to recursive, sequential, and modular application\n it can be used not only for human decision-makers, but also for automated or AI devices\n\n\n\nDecision Theory has a long history, going back to Leibniz in the 1600s and partly even to Aristotle in the −300s, and appearing in its present form around 1920–1960. What’s remarkable about it is that it is not only a framework, but the framework we must use. A logico-mathematical theorem shows that any framework that does not break basic optimality and rationality criteria has to be equivalent to Decision Theory. In other words, any “alternative” framework may use different technical terminology and rewrite mathematical operations in a different way, but it boils down to the same notions and operations of Decision Theory. So if you wanted to invent and use another framework, then either (a) it would lead to some irrational or illogical consequences, or (b) it would lead to results identical to Decision Theory’s. Many frameworks that you are probably familiar with, such as optimization theory or Boolean logic, are just specific applications or particular cases of Decision Theory.\nThus we list one more important characteristic of Decision Theory:\n\n it is normative\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\n\nJudgment under uncertainty\nHeuristics and Biases\nThinking, Fast and Slow\n\n\n\nNormative contrasts with descriptive. The purpose of Decision Theory is not to describe, for example, how human decision-makers typically make decisions. Because human decision-makers typically make irrational, sub-optimal, or biased decisions. That’s exactly what we want to avoid and improve!\n\n\n\n\n\n\n Study reading\n\n\n\nWho says that Decision Theory should be normative? – this is a respectable scientific question. If you found yourself wondering and doubting about this, then congratulations: that’s how a scientist should think!\nLater on we’ll examine material and arguments about this statement. If you like, feel free to already skim through the following works, as a starting point:\n\nCh. 15, especially § 15.1 and § “Bibliographical and Historical Notes” of Artificial Intelligence\nNormative Theories of Rational Choice: Expected Utility\nDecision Theory\nDecision Analysis"
  },
  {
    "objectID": "basic_decisions.html#graphical-representation-and-elements",
    "href": "basic_decisions.html#graphical-representation-and-elements",
    "title": "3  Basic decision problems",
    "section": "3.1 Graphical representation and elements",
    "text": "3.1 Graphical representation and elements\nA basic decision problem can be represented by a diagram like this:\n\nIt has one decision node, usually represented by a square , from which the available decisions depart as lines. Each decision leads to an inference node,1 usually represented by a circle , from which the possible outcomes depart as lines. Each outcome leads to a particular gain or loss, depending on the decision. The uncertainty of each outcome is quantified by a probability.1 also called chance node or uncertainty node\nA basic decision problem is analysed in terms of the following elements:\n\n\n\n\n\n\n\n\n\n\n\n\n Remember: What matters is to be able to identify these elements in a concrete problem, understanding their role. Their technical names don’t matter.\n\n\n\n Agent, and background or prior information. The agent is the person or device that has to make the decision. An agent possesses (or has been programmed with) specific background information that is used and taken for granted in the decision-making process. This background information determines the probabilities, gains, and losses of the outcomes, together with other available data and information. Different agents typically have different background information.\n\n\n\nAgent means “conductor”, “mover”, and similar (from Latin ago = to move or drive and similar meanings).\nWe’ll use the neutral pronouns it/its when referring to an agent, since an agent could be a person or a machine.\n\n Data and other additional information, sometimes called evidence. They differ from the background information in that they can change with every decision instance made by the same agent, while the background information stays the same. In the assembly-line scenario, for example, the test results could be different for every new electric component.\n Decisions available to the agent. They are assumed to be mutually exclusive and exhaustive; this can always be achieved by recombining them if necessary, as we’ll discuss later.\n\n\n\nDecisions are called courses of action in some literature.\n\n Outcomes of the possible decisions. Every decision can have a different set of outcomes, or some outcomes can appear for several or all decisions (in this case they are reported multiple times in the decision diagram). Note that even if an outcome can happen for two or more different decisions, its probabilities can still be different depending on the decision.\n\n\n\nMany other terms instead of outcome are used in the literature, for instance state or event.\n\n Probabilities for each of the outcomes and for each decision. Their values typically depend also on the background information and the additional data.\n Utilities: the gains or losses associated with each possible outcome and each decision. We shall mainly use the term utility, instead of “gain”, “loss”, and similar, for several reasons:\n\ngain and losses may involve not money, but time, or energy, or health, or emotional value, or other kinds of commodities and things that are important to us; or even a combination of them. The term “utility” is useful as a neutral term that doesn’t mean “money”, but depends on the context\nwe can just use one term instead of two: for example, when the utility is positive it’s a “gain”; when it’s negative it’s a “loss”\n\nThe particular numerical values of the utilities are always context-dependent: they may depend on the background information, the decisions, the outcomes, and the additional data.\nWe shall sometimes use the generic currency sign  ¤  to denote utilities, to make clear that gains and losses do not necessarily involve money, and not to reference any country in particular.\n\n\n\nThe relation between the elements above can be depicted as follows – but note that this is just for an intuitive illustration: \n\n\n\n\n\n\n Don’t over-interpret the decision diagram\n\n\n\n\nThe diagram above doesn’t have any temporal meaning, that is, it doesn’t mean that the decisions happen before the outcomes, or vice versa.\nIn some situations the outcome can be realized after the decision is made; for instance, someone bets on heads or tails, and then a coin is tossed.\nIn other situations, the outcome can be realized before the decision is made; for instance, sometimes a coin is tossed and covered, then one is asked to bet on what the outcome was. Another example is some research decision made by a archaeologist, the unknown being some detail about a dinosaur from millions of years ago.\nIn yet other situations the outcome may have a complex nature, and it may be realized partly before the decision is made, and partly after; for instance, someone can bet on the outcome of two coin tosses; one coin is tossed before the decision is made, and the other after.\nThe diagram above is not something that an agent must use in making decisions. It is not part of the theory. It’s just a very convenient way to visualize and operate with the mathematics underlying the theory.\nIt not always the case that the outcomes are unknown and the data are known. As we’ll discuss later, in some situations we reason in hypothetical or counterfactual ways, using hypothetical data and considering outcomes which have already occurred. In such situations we can still use diagrams like the one above, because the help us doing the calculation, although the actual outcome is already known.\n\n\n\n\n\n\n\n\n\n Study reading\n\n\n\n\n§ 1.1.4 in Artificial Intelligence\nSkim through Ch. 15 of Artificial Intelligence. No need to read thoroughly: just quickly glimpse whether there are ideas and notions that look familiar (a little like when you’re in a large crowd and look quickly around to see if there are any familiar faces)\n\n\n\n\n\n\n\n\n\n Exercise\n\n\n\n\nIdentify the elements above in the assembly-line decision problem of the introduction 1.\nSketch the decision diagram for the assembly-line decision problem.\n\n\n\nSome of the decision-problem elements listed above may need to be in turn analysed by a decision sub-problem. For instance, the utilities could depend on uncertain factors: thus we have a decision sub-problem to determine the optimal values to be used for the utilities of the main problem. This is an example of the modular character of decision theory.\nWe shall soon see how to mathematically represent these elements.\nThe elements above must be identified unambiguously in every decision problem. The analysis into these elements greatly helps in making the problem and its solution well-defined.\nAn advantage of decision theory is that its application forces us to make sense of an engineering problem. A useful procedure is to formulate the general problem in terms of the elements above, identifying them clearly. If the definition of any of the terms involves uncertainty of further decisions, then we analyse it in turn as a decision sub-problem, and so on.\n\nSuppose someone (probably a politician) says: “We must solve the energy crisis by reducing energy consumption or producing more energy”. From a decision-making point of view, this person has effectively said nothing whatsoever. By definition the “energy crisis” is the problem that energy production doesn’t meet demand. So this person has only said “we would like the problem to be solved”, without specifying any solution. A decision-theory approach to this problem requires us to specify which concrete courses of action should be taken for reducing consumption or increasing productions, and what their probable outcomes, costs, and gains would be.\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nSee MacKay’s options-vs-costs rational analysis in Sustainable Energy – without the hot air"
  },
  {
    "objectID": "basic_decisions.html#sec-decision-matrices",
    "href": "basic_decisions.html#sec-decision-matrices",
    "title": "3  Basic decision problems",
    "section": "3.2 Setting up a basic decision problem",
    "text": "3.2 Setting up a basic decision problem\nA basic decision problem can be set up along the following steps (which we illustrate afterwards with a couple of examples):\n\n\n\n\n\n\nSetup of a basic decision problem\n\n\n\n\nList all available decisions\nFor each decision, list its possible outcomes\nPool together all outcomes of all decisions, counting the common ones only once\nPrepare two tables: in each, display the decisions as rows, and the pooled outcomes as columns (or you can do the opposite: decisions as columns and outcomes as rows)\nIn one table, report the probabilities for all decision-outcome pairs. If an outcome is not available for that decision, give it a \\(0\\%\\) probability\nIn the other table, report the utilities for all decision-outcome pairs. If an outcome is not available for that decision, give it a \\(0\\) utility\n\n\n\n\nExample: the assembly-line problem\nLet’s apply the steps above in the assembly-line example of ch.  1:\n1. List all available decisions\nEasy: they are “accept the electronic component” and “discard it”.\n\n\n2. For each decision, list its possible outcomes\nIn general you will notice that some outcomes may be common to all decisions, while other outcomes can happen for some decisions only, or even for just one decision.\nIn the present example, the accept decision has two possible outcomes: “the component works with no faults for at least a year” and “the component fails within a year”.\nThe discard cannot have those outcomes, because the component is discarded. It has indeed only one outcome: “component discarded”.\n\n\n3. Pool together all outcomes of all decisions, counting the common ones only once\nIn total we have three pooled outcomes:\n\nno faults (from the accept decision)\nfails (from the accept decision)\ndiscarded (from the discard decision)\n\n\n\n4. Prepare two tables: in each, display the decisions as rows, and the pooled outcomes as columns (or you can do the opposite: decisions as columns and outcomes as rows)\nIn the present example each table looks like this:\n\n\n\n\n\n\n\n\n\n\n\nno faults for a year\nfails within a year\ndiscarded\n\n\n\n\naccept\n\n\n\n\n\ndiscard\n\n\n\n\n\n\n\n\n5. In one table, report the probabilities for all decision-outcome pairs. If an outcome is not available for that decision, give it a \\(0\\%\\) probability\n\nProbability table\n\n\n\n\n\n\n\n\n\nno faults for a year\nfails within a year\ndiscarded\n\n\n\n\naccept\n\\(90\\%\\)\n\\(10\\%\\)\n\\(0\\%\\)\n\n\ndiscard\n\\(0\\%\\)\n\\(0\\%\\)\n\\(100\\%\\)\n\n\n\nNote how the outcomes that do not exist for a particular decision have been given a \\(0\\%\\) probability (in grey). This is just a way of saying “this outcome can’t happen, if this decision is made”.\n\n\n6. In the other table, report the utilities for all decision-outcome pairs. If an outcome is not available for that decision, give it a \\(0\\) utility\n\nUtility table\n\n\n\n\n\n\n\n\n\nno faults for a year\nfails within a year\ndiscarded\n\n\n\n\naccept\n\\(+1\\$\\)\n\\(-11\\$\\)\n\\(0\\$\\)\n\n\ndiscard\n\\(0\\$\\)\n\\(0\\$\\)\n\\(0\\$\\)\n\n\n\nNote how the outcomes that do not exist for a particular decision have been given a \\(0\\$\\) utility (in grey). We shall see later that it actually doesn’t matter which utilities we give to these impossible outcomes.\n\n\n\n\n\n\n\n\n Exercises\n\n\n\nApply the steps above to the following basic decision problems (you only need to set them up with their probability & utility tables, but feel free to solve them as well, if you like):\n\nThe “heads-bet” vs “tails-bet” example of §  2.3. Assume that the “small amount” of money is \\(10\\$\\), the “large amount” is \\(1000\\$\\), and the two outcomes’ probabilities are \\(50\\%\\) each.\nPeter must reach a particular destination, and is undecided between two alternatives: go by car, or ride a bus, or go on foot. If he goes by car, he could arrive without problems, with a probability of \\(80\\%\\) and a utility of \\(10\\,¤\\), or he could get stuck in a traffic jam and arrive late, with a probability of \\(20\\%\\) and a utility of \\(-10\\,¤\\). If he rides a bus, he could arrive without problems, with a probability of \\(95\\%\\) and a utility of \\(15\\,¤\\), or arrive in time but travelling in a fully-packed bus, with a probability of \\(5\\%\\) and a utility of \\(-10\\,¤\\). If he goes on foot, he could arrive without problems, with a probability of \\(20\\%\\) and a utility of \\(20\\,¤\\), or he could get soaked from rain, with a probability of \\(80\\%\\) and a utility of \\(-5\\,¤\\).\n(We are using the symbol  “\\(¤\\)”  because Peter’s utilities are a combination of money savings, time of arrival, and comfort.)"
  },
  {
    "objectID": "basic_decisions.html#sec-make-decision",
    "href": "basic_decisions.html#sec-make-decision",
    "title": "3  Basic decision problems",
    "section": "3.3 How to make a basic decision?",
    "text": "3.3 How to make a basic decision?\nUp to now we have seen what are the elements of a basic decision problem, and how to arrange them in a diagram and with tables. But how do we determine what’s the optimal decision?\nDecision Theory says that the optimal decision is determined by the “principle of maximal expected utility”.\nWe shall study this principle more in detail toward the end of the course, although you already know its basic idea, because you intuitively used this very principle in solving all decision problems we met so far, starting from the assembly-line one.\nHowever, let’s quickly describe already now the basic procedure for this principle:\n\n\n\n\n\n\nPrinciple of maximal expected utility\n\n\n\n\nFor each decision, multiply the probability and the utility of each of its outcomes, and then sum up these products. This way you obtain the expected utility of the decision.\nChoose the decision that has the largest expected utility; if several decisions are maximal, choose any of them unsystematically.\n\n\n\nThis procedure can also be described in terms of the probability and utility tables introduced in the previous section:\n\nMultiply element-by-element the probability table and the utility table, obtaining a new table with the same number of rows and columns\nSum up the elements of each row of the new table (this sum is the expected utility); remember that every row corresponds to a decision\nChoose the decision corresponding to the largest of the sums above; if there are several maximal ones, choose among them unsystematically\n\n\nExample: the assembly-line problem\nMultiplying the Probability table and the Utility table above, element-by-element, we obtain the following table, where we also indicate the sum of each row:\n\n\nProbability × Utility table\n\n\n\n\n\n\n\n\n\n\nno faults for a year\nfails within a year\ndiscarded\nsum\n\n\n\n\naccept\n\\(+0.9\\$\\)\n\\(-1.1\\$\\)\n\\(0\\$\\)\n\\(\\boldsymbol{-0.2\\$}\\)\n\n\ndiscard\n\\(0\\$\\)\n\\(0\\$\\)\n\\(0\\$\\)\n\\(\\boldsymbol{0\\$}\\)\n\n\n\n\nand, as we already knew, discarding the electronic component is the decision with the maximal expected utility.\n\n\n\n\n\n\n Exercise\n\n\n\nFeel free to sketch some code (in your preferred programming language) that chooses the optimal decision according to the principle above. The code should take two inputs: the table or matrix of probabilities, and the table or matrix of utilities; and should give one output: the row-number of the optimal decision."
  },
  {
    "objectID": "basic_decisions.html#plan-for-the-next-chapters",
    "href": "basic_decisions.html#plan-for-the-next-chapters",
    "title": "3  Basic decision problems",
    "section": "3.4 Plan for the next chapters",
    "text": "3.4 Plan for the next chapters\nThe expected-utility maximization above is intuitive and simple, and is the last stage in a basic decision problem.\nBut there are two stages which occur before, and which are the most difficult:\n\n Inference\n\nis the stage where the probabilities of the possible outcomes are calculated. Its rules are given by the Probability Calculus. Inference is independent from decision: in some situations we may simply wish to assess whether some hypotheses, conjectures, or outcomes are more or less plausible than others, without making any decision. This kind of assessment can be very important in problems of communication and storage, and it is specially considered by Information Theory.\n\n\nThe calculation of probabilities can be the part that demands most thinking, time, and computational resources in a decision problem. It is also the part that typically makes most use of data – and where data can be most easily misused.\nRoughly half of this course will be devoted in understanding the laws of inference, their applications, uses, and misuses.\n\n\n Utility assesment\n\nis the stage where the gains or losses of the possible outcomes are calculated. Often this stage requires further inferences and further decision-making sub-problems. The theory underlying utility assessment is still much underdeveloped, compared to probability theory.\n\n\n\n\n\nWe shall now explore each of these two stages. We take up inference first because it is the most demanding and probably the one that can be optimized the most by new technologies."
  },
  {
    "objectID": "connection-1-ML.html#a-max-success-classifier-vs-an-optimal-classifier",
    "href": "connection-1-ML.html#a-max-success-classifier-vs-an-optimal-classifier",
    "title": "4  First connection with machine learning",
    "section": "4.1 A “max-success” classifier vs an optimal classifier",
    "text": "4.1 A “max-success” classifier vs an optimal classifier\n\n\n\n\n\n\n\n\n\n\nYou find the code for this chapter and exercises also in this JupyterLab notebook for R and (courtesy of Viktor Karl Gravdal!) this JupyterLab notebook for python.\n\n\nWe shall compare the results obtained in some numerical simulations by using\n\na Machine-Learning Classifier trained to do most successful guesses\na prototype “Optimal Predictor Machine” trained to make the optimal decision\n\nFor the moment we treat both as “black boxes”, that is, we don’t study yet how they’re calculating their outputs (although you may already have a good guess at how the Optimal Predictor Machine works).\nTheir operation is implemented in this R script that we now load:\n\nsource('code/mlc_vs_opm.R')\n\nThis script simply defines the function hitsvsgain():\nhitsvsgain(ntrials, chooseAtrueA, chooseAtrueB, chooseBtrueB, chooseBtrueA, probsA)\nhaving six arguments:\n\nntrials: how many simulations of guesses to make\nchooseAtrueA: utility gained by guessing A when the successful guess is indeed A\nchooseAtrueB: utility gained by guessing A when the successful guess is B instead\nchooseBtrueB: utility gained by guessing B when the successful guess is indeed B\nchooseBtrueA: utility gained by guessing B when the successful guess is A instead\nprobsA: a tuple of probabilities (between 0 and 1) to be used in the simulations (recycling it if necessary), for the successful guess being A; the corresponding probabilities for B are therefore 1-probsA. If this argument is omitted it defaults to 0.5 (not very interesting)"
  },
  {
    "objectID": "connection-1-ML.html#example-1-electronic-component",
    "href": "connection-1-ML.html#example-1-electronic-component",
    "title": "4  First connection with machine learning",
    "section": "4.2 Example 1: electronic component",
    "text": "4.2 Example 1: electronic component\nLet’s apply our two classifiers to the Accept or discard? problem of §  1. We call A the alternative in which the element won’t fail before one year, and should therefore be accepted if this alternative were known at the time of the decision. We call B the alternative in which the element will fail within a year, and should therefore be discarded if this alternative were known at the time of the decision. Remember that the crucial point here is that the classifiers don’t have this information at the moment of making the decision.\nWe simulate this decision for 100 000 components (“trials”), assuming that the probabilities of failure can be 0.05, 0.20, 0.80, 0.95. The values of the arguments should be clear:\n\nhitsvsgain(ntrials=100000, chooseAtrueA=+1, chooseAtrueB=-11, chooseBtrueB=0, chooseBtrueA=0, probsA=c(0.05, 0.20, 0.80, 0.95))\n\n\nTrials: 100000\nMachine-Learning Classifier: successes 87421 ( 87.4 %) | total gain -25903\nOptimal Predictor Machine:   successes 72548 ( 72.5 %) | total gain 9594\n\n\nNote how the machine-learning classifier is the one that makes most successful guesses (around 88%), and yet it leads to a net loss! If the utility were in kroner, this classifier would cause the company producing the components a net loss of more than 20 000 kr.\nThe optimal predictor machine, on the other hand, makes fewer successful guesses overall (around 72%), and yet it leads to a net gain! It would earn the company a net gain of around 10 000 kr.\n\n\n\n\n\n\n Exercise\n\n\n\nHow is this possible? Try to understand what’s happening; feel free to research this by modifying the hitsvsgain() function, so that it prints additional outputs."
  },
  {
    "objectID": "connection-1-ML.html#example-2-find-aladdin-image-recognition",
    "href": "connection-1-ML.html#example-2-find-aladdin-image-recognition",
    "title": "4  First connection with machine learning",
    "section": "4.3 Example 2: find Aladdin! (image recognition)",
    "text": "4.3 Example 2: find Aladdin! (image recognition)\nA typical use of machine-learning classifiers is for image recognition: for instance, the classifier guesses whether a particular subject is present in the image or not.\nIntuitively one may think that “guessing successfully” should be the best goal here. But exceptions to this may be more common than one thinks. Consider the following scenario:\n\nBianca has a computer folder with 10 000 photos. Some of these include her beloved cat Aladdin, who sadly passed away recently. She would like to select all photos that include Aladdin and save them in a separate “Aladdin” folder. Doing this by hand would take too long, if at all possible; so Bianca wants to employ a machine-learning classifier.\nFor Bianca it’s important that no photo with Aladdin goes missing, so she would be very sad if any photo with him weren’t correctly recognized; on the other hand she doesn’t mind if some photos without him end up in the “Aladdin” folder – she can delete them herself afterwards.\n\nLet’s apply and compare our two classifiers to this image-recognition problem, using again the hitsvsgain() function. We call A the case where Aladdin is present in a photo, and B where he isn’t. To reflect Bianca’s preferences, let’s use these “emotional utilities”:\n\nchooseAisA = +2: Aladdin is correctly recognized\nchooseBisA = -2: Aladdin is not recognized and photo goes missing\nchooseBisB = +1: absence of Aladding is correctly recognized\nchooseAisB = -1: photo without Aladding end up in “Aladding” folder\n\nand let’s say that the photos may have probabilities 0.3, 0.4, 0.6, 0.7 of including Aladding:\n\nhitsvsgain(ntrials=10000, chooseAtrueA=+2, chooseAtrueB=-1, chooseBtrueB=1, chooseBtrueA=-2, probsA=c(0.3, 0.4, 0.6, 0.7))\n\n\nTrials: 10000\nMachine-Learning Classifier: successes 6466 ( 64.7 %) | total gain 4455\nOptimal Predictor Machine:   successes 5841 ( 58.4 %) | total gain 5155\n\n\nAgain we see that the machine-learning classifier makes more successful guesses than the optimal predictor machine, but the latter yields a higher “emotional utility”.\nYou may sensibly object that this result could depend on the peculiar utilities or probabilities chosen for this example. The next exercise helps answering your objection.\n\n\n\n\n\n\n Exercise\n\n\n\n\nIs there any case in which the optimal predictor machine yields a strictly lower utility than the machine-learning classifier?\n\nTry using different utilities, for instance using ±5 instead of ±2, or whatever other values you please.\nTry using different probabilities as well.\n\nAs in the previous exercise, try to understand what’s happening. Consider this question: how many photos including Aladdin did each classifier miss?\nModify the hitsvsgain() function to output this result.\nDo the comparison using the following utilities: chooseAtrueA=+1, chooseAtrueB=-1, chooseBtrueB=1, chooseBtrueA=-1. What’s the result? what does this tell you about the relationship between the machine-learning classifier and the optimal predictor machine?"
  },
  {
    "objectID": "inference.html#sec-inference-scenarios",
    "href": "inference.html#sec-inference-scenarios",
    "title": "5  What is an inference?",
    "section": "5.1 The wide scope and characteristics of inferences",
    "text": "5.1 The wide scope and characteristics of inferences\nLet’s see a couple more informal examples of inference problems. For some of them an underlying decision-making problem is also alluded to:\n\nLooking at the weather we try to assess if it’ll rain today, to decide whether to take an umbrella.\nConsidering a patient’s symptoms, test results, and medical history, a clinician tries to assess which disease affects a patient, so as to decide on the optimal treatment.\nLooking at the present game position  the X-player, which moves next, wonders whether placing the next X on the mid-right position leads to a win.\nThe computer of a self-driving car needs to assess, from the current set of camera frames, whether a particular patch of colours in the frames is a person, so as to slow down the car and stop.\nGiven that \\(G=6.67 \\cdot 10^{-11}\\,\\mathrm{m^3\\,s^{-2}\\,kg^{-1}}\\), \\(M = 5.97 \\cdot 10^{24}\\,\\mathrm{kg}\\) (mass of the Earth), and \\(r = 6.37 \\cdot 10^{6}\\,\\mathrm{m}\\) (radius of the Earth), a rocket engineer needs to know how much is \\(\\sqrt{2\\,G\\,M/r\\,}\\).\nWe’d like to know whether the rolled die is going to show .\nAn aircraft’s autopilot system needs to assess how much the aircraft’s roll will change if the right wing’s angle of attack is increased by \\(0.1\\,\\mathrm{rad}\\).\nBy looking at the dimensions, shape, texture of a newly dug-out fossil bone, an archaeologist wonders whether it belonged to a Tyrannosaurus rex.\nA voltage test on a newly produced electronic component yields a reading of \\(100\\,\\mathrm{mV}\\). The electronic component turns out to be defective. An engineer wants to assess whether the voltage-test reading could have been \\(100\\,\\mathrm{mV}\\), if the component had not been defective.\nSame as above, but the engineer wants to assess whether the voltage-test reading could have been \\(80\\,\\mathrm{mV}\\), if the component had not been defective.\n\n\n\n\nFrom measurements of the Sun’s energy output and of concentrations of various substances in the Earth’s atmosphere over the past 500 000 years, and of the emission rates of various substances in the years 1900–2022, climatologists and geophysicists try to assess the rate of mean-temperature increase in the years 2023–2100.\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nCh. 10 in A Survival Guide to the Misinformation Age.\n\n\n\n\n\n\n\n\n\n\n Exercises\n\n\n\n\nFor each example above, pinpoint what has to be inferred, and also the agent interested in the inference.\nPoint out which of the examples above explicitly give data or information that should be used for the inference.\nFor the examples that do not give explicit data or information, speculate what information could be implicitly assumed. For those that do give explicit data, speculate which other additional information could be implicitly assumed.\nCan any of the inferences above be done perfectly, that is, without any uncertainty, based the data given explicitly or implicitly?\nFind the examples that explicitly involve a decision. In which of them does the decision affect the results of the inference? In which it does not?\nAre any of the inferences “one-time only” – that is, their object or the data on which they are based have never happened before and will never happen again?\nAre any of the inferences based on data and information that come chronologically after the object of the inference?\nAre any of the inferences about something that is actually already known to the agent that’s making the inference?\nAre any of the inferences about something that actually did not happen?\nDo any of the inferences use “data” or “information” that are actually known (within the scenario itself) to be fictive, that is, not real?\n\n\n\nFrom the examples and from your answers to the exercise we observe some very important characteristics of inferences:\n\nSome inferences can be made exactly, that is, without uncertainty: it is possible to say whether the object of the inference is true or false. Other inferences, instead, involve an uncertainty.\nAll inferences are based on some data and information, which may be explicitly expressed or only implicitly understood.\nAn inference can be about something past, but based on present or future data and information: inferences can show all sorts of temporal relations.\nAn inference can be essentially unrepeatable, because it’s about something unrepeatable or based on unrepeatable data and information.\nThe data and information on which an inference is based can actually be unknown; that is, they can be only momentarily contemplated as real. Such an inference is said to be based on hypothetical reasoning.\nThe object of an inference can actually be something already known to be false or not real: the inference tries to assess it in the case that some data or information had been different. Such an inference is said to be based on counterfactual reasoning."
  },
  {
    "objectID": "inference.html#sec-inference-origin",
    "href": "inference.html#sec-inference-origin",
    "title": "5  What is an inference?",
    "section": "5.2 Where are inferences drawn from?",
    "text": "5.2 Where are inferences drawn from?\nThis question is far from trivial. In fact it has connections with the earth-shaking development and theorems in the foundations of mathematics of the 1900s.\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nMathematics: The Loss of Certainty.\n\n\nThe proper answer to this question will take up the next sections. But a central point can be emphasized now:\n\n\n\n\n\n\n\n\n\n\n\nInferences can only be drawn from other inferences.\n\n\n\nIn order to draw an inference – calculate a probability – we usually go up a chain: we must first draw other inferences, and for drawing those we must draw yet other inferences, and so on.\nAt some point we must stop at inferences that we take for granted without further proof. These typically concern direct experiences and observations. For instance, you see a tree in front of you, so you can take “there’s a tree here” as a true fact. Yet, notice that the situation is not so clear-cut: how do you know that you aren’t hallucinating, for example, and there’s actually no tree there? That is taken for granted. If you analyse the possibility of hallucination, you realize that you are taking other things for granted, and so on. Probably most philosophical research in the history of humanity has been about grappling with this runaway process – which is also a continuous source of sci-fi films. In logic and mathematical logic, this corresponds to the fact that to prove some theorem, we must always start from some axioms. There are “inferences” – tautologies – that can be drawn without requiring others; but they are all trivial, such as “this component failed early, or it didn’t”. They are of little use in a real problem, although they have a deep theoretical importance.\n\n\n\n\n\nSci-fi films like The Matrix ultimately draw on the fact that we must take some inferences for granted without further proof.\n\n\n\n\nIn concrete applications we start from many inferences upon which everyone, luckily, agrees. But sometimes we must also use starting inferences that are more dubious or not agreed upon by anyone. In this case the final inference has a somewhat contingent character, and we accept it (as well as the solution of any underlying decision problem) as the best available for the moment. This is partly the origin of the term “model”."
  },
  {
    "objectID": "inference.html#sec-basic-elements-inference",
    "href": "inference.html#sec-basic-elements-inference",
    "title": "5  What is an inference?",
    "section": "5.3 Basic elements of an inference",
    "text": "5.3 Basic elements of an inference\nLet us start to introduce some mathematical notation and more precise terminology for inferences.\nEvery inference has an “object” – what is to be assessed – as well as data, information, hypotheses, or hypothetical scenarios on which it is based. We call proposal the object of the inference, and conditional what the inference is based upon. We separate them with a vertical bar  “ \\(\\pmb{\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}}}\\) ”, which can be pronounced “given” or “conditional on”. Finally, we put parentheses around this and a “\\(\\mathrm{P}\\)” in front:\n\n\nProposal is Johnson’s (1924) terminology; Keynes (1921) uses “conclusion”; modern textbooks do not seem to use any specialized term. Conditional is modern terminology; other terms used: “evidence”, “premise”, “supposal”. The vertical bar, originally a solidus, was introduced by Keynes (1921).\n\\[\n\\mathrm{P}( \\underbracket[1px]{\\color[RGB]{34,136,51}\\boldsymbol{\\cdots}}_{\\textit{proposal}}\n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\underbracket[1px]{\\color[RGB]{68,119,170}\\boldsymbol{\\cdots}}_{\\textit{conditional}}\n) = {\\color[RGB]{238,102,119}\\boldsymbol{\\cdots}\\%}\n\\]\nthis means “the probability that [proposal], supposing [conditional], is . . . %”. Or also: “supposing [conditional], we can infer [proposal] with . . . % probability”.\nWe have seen that to calculate the probability for an inference, we must use the probabilities of other inferences, which in turn are calculated by using the probabilities of other inferences, and so on, until we arrive at probabilities that are taken for granted. A basic inference process could therefore be schematized like this:\n\n\n\n\n\n\n\n\nThe next important task ahead of us is to introduce a flexible and enough general mathematical representation for the objects and the bases of an inference. Then we shall finally study the rules for drawing correct inferences."
  },
  {
    "objectID": "sentences.html#sec-central-comps",
    "href": "sentences.html#sec-central-comps",
    "title": "6  Sentences",
    "section": "6.1 The central components of knowledge representation",
    "text": "6.1 The central components of knowledge representation\nWhen speaking of “data”, what comes to mind to many people is basically numbers or collections of numbers. Maybe numbers, then, could be used to represent all the variety of items exemplified above. This option, however, turns out to be too restrictive.\nI give you this number: “\\(8\\)”, saying that it is “data”. But what is it about? You, as an agent, can hardly call this number a piece of information, because you have no clue what to do with it.\nInstead, if I tell you: “The number of official planets in the solar system is 8”, then we can say that I’ve given you data. You can do different things with this piece of information (for instance: if you had decided to send one probe to each official planet, now you know you have to build eight probes).\nSo, “data” is not just numbers: a number is not “data” unless there’s an additional verbal and non-numeric context accompanying it – even if only implicitly. Sure, we could represent this meta-data information as numbers too; but this move would only shift the problem one level up: we would need an auxiliary verbal context explaining what the meta-data numbers are about.\nData can, moreover, be completely non-numeric. A clinician saying “The patient has fully recovered from the disease” (we imagine to know who’s the patient and what was the disease) is giving us a piece of information that we could further use, for instance, to make prognoses about other, similar patients. The clinician’s statement surely is “data”, but essentially non-numeric data. Sure, in some situations we could represent it as “1”, while “0” would represent “not recovered”; but the opposite convention could also be used, or the numbers “0.3” and “174” could be used. These numbers have intrinsically nothing to do with the clinician’s “recovery” data.\n\n\nBut the examples above actually reveal the answer to our needs! In the examples we expressed the data by means of sentences. Clearly any measurement result, decision outcome, hypothesis, not-real event, assumption, data, and any piece of information can be expressed by a sentence.\nWe shall therefore use sentences, also called propositions or statements,1 to represent and communicate all the kinds of “items” that can be the proposal or conditional of an inference. In some cases we can of course summarize a sentence by a number, as a shorthand, when the full meaning of the sentence is understood.\n\n1 These three terms are not always equivalent in formal logic, but here we’ll use them as synonyms.\nSentences are the central components of knowledge representation in AI agents. For example they appear at the heart of automated control programs and fault-management systems in NASA spacecrafts.\n\n\n (From the SMART paper)\n\n\n\n\n\n\n Study reading\n\n\n\n\n§ 7.1 in Artificial Intelligence.\nTake a quick look at these:\n\nSMART: A propositional logic-based trade analysis and risk assessment tool for a complex mission\naround p. 22 in No More Band-Aids: Integrating FM into the Onboard Execution Architecture \npart IV in Model-based programming of intelligent embedded systems and robotic space explorers"
  },
  {
    "objectID": "sentences.html#identifying-and-working-with-sentences",
    "href": "sentences.html#identifying-and-working-with-sentences",
    "title": "6  Sentences",
    "section": "6.2 Identifying and working with sentences",
    "text": "6.2 Identifying and working with sentences\nBut what is a sentence, more exactly? The everyday meaning of this word will work for us, even though there are more precise definitions – and still a lot of research in logic an artificial intelligence on how to define and use sentences. We shall adopt this useful definition:\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nPropositions\n\n\nA sentence is a verbal message for which an agent can determine, at least in principle, whether it is true or false.\nLet’s make this definition clearer with some remarks:\n\n\n A sentence doesn’t have to contain only words. It can contain pictures, sounds, and other non-verbal items. For example, the following:\n“This:  is an animated picture of Saitama.”\nis a sentence, even if it contains animated graphics, because we can say that it is true. Likewise, the following:\n“This link leads to a song by Pink Floyd.”\nis also a sentence, even if it contains links and audio, because we can say that it is false (that’s a song by Monty Python).\n A meaningful phrase may not be a sentence. For instance, a phrase like “Apples are much tastier than pears” may not be a sentence, because it’s a matter of personal taste whether it’s true or false; moreover, an agent’s opinion about apples and pears might change from time to time.\nThe phrase “Jenny right now finds apples tastier than pears”, on the other hand, could be a sentence; its truth being found by asking Jenny at that moment.\nIn an engineering context, the phrase “This valve will operate for at least two months” is a sentence, even if its truth is unknown at the moment: one has to wait two months, and then its truth will be unambiguously known.\n\n\n\n An expression involving technical terms may not be a sentence (and not meaningful either). For instance, in a data-science context the phrase “This neural-network algorithm has better performance than that random-forest one” is not a sentence unless we have objectively specified what “better” means (higher accuracy? higher true-positive rate? faster?), for example by adopting a particular comparison metric.\nSome expressions involving technical terms may appear to be sentences at first; but a deeper analysis then reveals that they are not. A famous example is the sentence “The two events (at different spatial locations) are simultaneous”. Einstein showed that there’s no physical way to determine whether such an expression is true or false. Its truth turns out to be a matter of convention (also in Newtonian mechanics). The Theory of Relativity was born from this observation.\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nOn the electrodynamics of moving bodies.\n\n\n\n\n\n\n\n\n\n\n\n\n Be particularly careful when reading scientific and engineering papers with a lot of technical terms and phrases: technical jargon often makes it especially difficult to understand whether something true or at least meaningful is being said, or not!\n\n\n\n A sentence can be expressed in different ways by different phrases and in different languages. For instance, “The temperature is 248.15 K”, “Temperaturen ligger på minus 25 grader”, and “25 °C is the value of the temperature” all represent the same sentence.\n\n\nThere are many advantages in working with sentences (rather than just numbers), and in keeping in mind that inference is about sentences:\nFirst, it leads to clarity in engineering problems and makes them more goal-oriented. A data engineer must acquire information and convey information. “Acquiring information” does not simply consist in making measurements or counting something: the engineer must understand what is being measured and why. If data is gathered from third parties, the engineer must ask what exactly the data mean and how they were acquired. In designing and engineering a solution, it is important to understand what information or outcomes the end user exactly wants. The “what”, “why”, “how” are expressed by sentences. A data engineer will often ask “wait, what do you mean by that?”. This question is not just an unofficial parenthesis in the official data-transfer workflow between the engineer and someone else. It is an integral part of that workflow: it means that some information has not been completely transferred yet.\nSecond, it is extremely important in AI and machine-learning design. A (human) engineer may proceed informally when drawing inferences, without worrying about “sentences” unless a need for disambiguation arises. A data engineer who’s designing or programming an algorithm that will do inferences automatically, must instead be unambiguous and cover beforehand all possible cases that the algorithm will face.\n\n\nWe therefore agree that the proposal and the conditional of an inference have to be sentences. This means that the proposal of the inference must be something that can be true or false. Many inferences, especially when they concern numerical measurements, involve several sentences. For example, an inference about the result of rolling a die actually consists of the probabilities for six separate proposals:\n\\[\n\\begin{aligned}\n&\\textsf{\\small`The result of the roll is 1'}\n\\\\\n&\\textsf{\\small`The result of the roll is 2'}\n\\\\\n&\\dotso\n\\\\\n&\\textsf{\\small`The result of the roll is 6'}\n\\end{aligned}\n\\]\nLater on we shall see how to work with more complex inferences of this kind. In real applications it can be useful, on some occasions, to pause and reduce an inference to its basic set of true/false sentences; this analysis may reveal contradictions in our inference problem. A simple way to do this is to reduce the complex inference into a set of yes/no questions.\nThis kind of analysis is also important in information-theoretic situations: the information content provided by an inference, when measured in Shannons, is related to the minimal amount of yes/no questions that the inference answers.\n\n\n\n\n\n\n Exercise\n\n\n\nRewrite each inference scenario of §  5.1 in a formal way, as one or more inferences\n\\[\n\\textit{[proposal]}\\ \\pmb{\\nonscript\\:\\Big\\vert\\nonscript\\:\\mathopen{}}\\ \\textit{[conditional]}\n\\]\nwhere proposal and conditional are well-defined sentences.\nIn ambiguous cases, use your judgement and motivate your choices."
  },
  {
    "objectID": "sentences.html#sec-sentence-notation",
    "href": "sentences.html#sec-sentence-notation",
    "title": "6  Sentences",
    "section": "6.3 Notation and abbreviations",
    "text": "6.3 Notation and abbreviations\nWriting full sentences would take up a lot of space. Even an expression such as “The speed is 10 m/s” is not a sentence, strictly speaking, because it leaves unspecified the speed of what, when it was measured and in which frame of reference, what we mean by “speed”, how the unit “m/s” is defined, and so on.\nTypically we leave the full content of a sentence to be understood from the context, and we denote the sentence by a simple expression such as\n\\[\n\\textsf{\\small The speed is 10\\,m/s}\n\\]\nor even more compactly introducing physical symbols:\n\\[\nv \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}10\\,\\mathrm{m/s}\n\\]\nwhere \\(v\\) is a physical variable denoting the speed; or sometimes even writing simply\n\\[\n10\\,\\mathrm{m/s}\n\\]\nIn some problems it’s useful to introduce symbols to denote sentences. As mentioned before, in these notes we’ll use sans-serif italic letters: \\(\\mathsfit{A},\\mathsfit{B},\\mathsfit{a},\\mathsfit{b},\\dotsc\\), possibly with sub- or super-scripts. For instance, the sentence “The speed is 10 m/s” could be denoted by the symbol \\(\\mathsfit{S}_{10}\\). We abbreviate such a definition like this:\n\\[\n\\mathsfit{S}_{10} \\coloneqq\\textsf{\\small`The speed is 10\\,m/s'}\n\\]\nwhich means “the symbol \\(\\mathsfit{S}_{10}\\) is defined to be the sentence \\(\\textsf{\\small`The speed is 10\\,m/s'}\\)”.\n\n\n\n\n\n\n We must be wary of how much we shorten sentences\n\n\n\nConsider these three:\n\\[\n\\begin{aligned}\n&\\textsf{\\small`The speed is measured to be 10\\,m/s'}\n\\\\\n&\\textsf{\\small`The speed is set to 10\\,m/s'}\n\\\\\n&\\textsf{\\small`The speed is reported, by a third party, to be 10\\,m/s'}\n\\end{aligned}\n\\]\nThe quantity “10 m/s” is the same in all three sentences, but their meanings are very different. They represent different kinds of data. These differences greatly affect any inference about or from these data. For instance, in the third case an engineer may not take the indirectly-reported speed “10 m/s” at face value, unlike the first case. In a scenario where all three sentences can occur, it would be ambiguous to simply write “\\(v = 10\\,\\mathrm{m/s}\\)”: would the equal-sign mean “measured”, “set”, or “indirectly reported”?\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nHow would you denote the three sentences above, to make their differences clear?\n\n\n\n\n\n\n\n\n\n\nGet familiar with abbreviations of sentences\n\n\n\nTo summarize, a sentence like\n\\[\n\\textsf{\\small`The temperature $T$ has value $x$'}\n\\]\ncould be abbreviated in these different ways:\n\nA symbol for the sentence (note the sans-serif font):\n\\[\n\\mathsfit{S}\n\\]\nSome key word appearing in the sentence:\n\\[\n\\textsf{\\small temperature}\n\\]\nAn equality:\n\\[\nT\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\]\nThe quantity appearing in the sentence:\n\\[\nT\n\\]\nThe value appearing in the sentence:\n\\[\nx\n\\]\n\nGet familiar with these kinds of abbreviations because they’re all very common. Some texts may even jump from one abbreviation to another in the same page or paragraph!"
  },
  {
    "objectID": "sentences.html#sec-connecting-sentences",
    "href": "sentences.html#sec-connecting-sentences",
    "title": "6  Sentences",
    "section": "6.4 Connecting sentences",
    "text": "6.4 Connecting sentences\n\nAtomic sentences\nIn analysing the measurement results, decision outcomes, hypotheses, assumptions, data and information that enter into an inference problem, it is convenient to find a collection of basic sentences or, using a more technical term, atomic sentences out of which all other sentences of interest can be constructed. These atomic sentences often represent elementary pieces of information in the problem.\nConsider for instance the following composite sentence, which could appear in our assembly-line scenario:\n\n“The electronic component is still whole after the shock test and the subsequent heating test. The voltage reported in the final power test is either 90 mV or 110 mV.”\n\nIn this statement we can identify at least four atomic sentences, which we denote by these symbols:\n\\[\\begin{aligned}\n\\mathsfit{s} &\\coloneqq\\textsf{\\small`The component is whole after the shock test'}\n\\\\\n\\mathsfit{h} &\\coloneqq\\textsf{\\small`The component is whole after the heating test'}\n\\\\\n\\mathsfit{v}_{90} &\\coloneqq\\textsf{\\small`The power-test voltage reading is 90\\,mV'}\n\\\\\n\\mathsfit{v}_{110} &\\coloneqq\\textsf{\\small`The power-test voltage reading is 110\\,mV'}\n\\end{aligned}\n\\]\nThe inference may actually require additional atomic sentences. For instance, it might become necessary to consider atomic sentences with other values for the reported voltage, such as\n\\[\\begin{aligned}\n\\mathsfit{v}_{110} &\\coloneqq\\textsf{\\small`The power-test voltage reading is 100\\,mV'}\n\\\\\n\\mathsfit{v}_{80} &\\coloneqq\\textsf{\\small`The power-test voltage reading is 80\\,mV'}\n\\end{aligned}\\]\nand so on.\n\n\nConnectives\nHow do we construct composite sentences, like the one above, out of atomic sentences?\nWe consider three ways: one operation to change a sentence into another related to it, and two operations to combine two or more sentences together. These operations are called connectives; you may have encountered them already in Boolean algebra. Our natural language offers many more operations to combine sentences, but these three connectives turn out to be all we need in virtually all engineering and data-science problems:\n\n\n\n\n\n\n\n\n\n\n\n\nNot (symbol  \\(\\lnot\\) )\n\nexample:\n\n\n\\[\\begin{aligned}\n\\mathsfit{s} &\\coloneqq\\textsf{\\small`The component is whole after the shock test'}\n\\\\[1ex]\n\\lnot \\mathsfit{s} &= \\textsf{\\small`The component is broken after the shock test'}\n\\end{aligned}\\]\n\nAnd (symbols  \\(\\land\\)  also  \\(\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\) )\n\nexample:\n\n\n\\[\n\\begin{aligned}\n\\mathsfit{s} &\\coloneqq\\textsf{\\small`The component is whole after the shock test'}\n\\\\\n\\mathsfit{h} &\\coloneqq\\textsf{\\small`The component is whole after the heating test'}\n\\\\[1ex]\n\\mathsfit{s} \\land \\mathsfit{h} &= \\textsf{\\small`The component is whole after the shock and heating tests'}\n\\\\\n\\mathsfit{s} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{h} &= \\textsf{\\small`The component is whole after the shock and heating tests'}\n\\end{aligned}\n\\]\n\nOr (symbol  \\(\\lor\\) )\n\nexample:\n\n\n\\[\\begin{aligned}\n\\mathsfit{v}_{90} &\\coloneqq\\textsf{\\small`The power-test voltage reading is 90\\,mV'}\n\\\\\n\\mathsfit{v}_{110} &\\coloneqq\\textsf{\\small`The power-test voltage reading is 110\\,mV'}\n\\\\[1ex]\n\\mathsfit{v}_{90} \\lor \\mathsfit{v}_{110} &= \\textsf{\\small`The power-test voltage reading is 90\\,mV, or 110\\,mV, or both'}\n\\end{aligned}\\]\n\n\n\nThese connectives can be applied multiple times, to form increasingly composite sentences.\nThe and connective appears very frequently in probability formulae. Using its standard symbol “\\(\\land\\)” would consume a lot of horizontal space. For this reason a comma “\\(\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\)” is often used as an alternative symbol. So the expressions \\(\\mathsfit{s} \\land \\mathsfit{h}\\) and \\(\\mathsfit{s} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{h}\\) are completely equivalent.\n\n\n\n\n\n\n Important subtleties of the connectives:\n\n\n\n\nThere is no strict correspondence between the words “not”, “and”, “or” in natural language and the three connectives. For instance the and connective could correspond to the words “but” or “whereas”, or just to a comma “ , ”.\nNot means not some kind of complementary quality, but the denial. For instance,  \\(\\lnot\\textsf{\\small`The chair is black'}\\)  generally does not mean  \\(\\textsf{\\small`The chair is white'}\\) ,   (although in some situations these two sentences could amount to the same thing).\nIt’s always best to declare explicitly what the not of a sentence concretely means. In our example we take\n\\[\n  \\lnot\\textsf{\\small`The component is whole'} \\coloneqq\\textsf{\\small`The component is broken'}\n  \\]\nBut in other examples the negation of “being whole” could comprise several different conditions. A good guideline is to always state the not of a sentence in positive terms.\nOr does not exclude that both the sentences it connects can be true. So in our example  \\(\\mathsfit{v}_{90} \\lor \\mathsfit{v}_{110}\\)  does not exclude, a priori, that the reported voltage could be both 90 mV and 110 mV. (There is a connective for that: “exclusive-or”, but it can be constructed out of the three we already have.)\n\n\n\nFrom the last remark we see that the sentence\n\\[\n\\textsf{\\small`The power-test voltage reading is 90\\,mV or 110\\,mV'}\n\\]\ndoes not correspond to   \\(\\mathsfit{v}_{90} \\lor \\mathsfit{v}_{110}\\) .  It is implicitly understood that a voltage reading cannot yield two different values at the same time. Convince yourself that the correct way to write that sentence is this:\n\\[\n(\\mathsfit{v}_{90} \\lor \\mathsfit{v}_{110})\n\\land\n\\lnot(\\mathsfit{v}_{90} \\land \\mathsfit{v}_{110})\n\\]\nFinally, the full composite sentence of the present example can be written in symbols as follows:\n\n“The electronic component is still whole after the shock test and the subsequent heating test. The voltage reported in the final power test is either 90 mV or 110 mV.”\n\n\\[\n\\textcolor[RGB]{102,204,238}{\\mathsfit{s}} \\land \\textcolor[RGB]{34,136,51}{\\mathsfit{h}} \\land\n(\\textcolor[RGB]{238,102,119}{\\mathsfit{v}_{90}} \\lor \\textcolor[RGB]{170,51,119}{\\mathsfit{v}_{110}})\n\\land\n\\lnot\n(\\textcolor[RGB]{238,102,119}{\\mathsfit{v}_{90}} \\land \\textcolor[RGB]{170,51,119}{\\mathsfit{v}_{110}})\n\\]\n\n\n\n\n\n\n\n\n Study reading\n\n\n\nJust take a quick look at § 7.4.1 in Artificial Intelligence and note the similarities with what we’ve just learned. In these notes we follow a faster approach leading directly to probability logic."
  },
  {
    "objectID": "sentences.html#if-then",
    "href": "sentences.html#if-then",
    "title": "6  Sentences",
    "section": "6.5 “If… then…”",
    "text": "6.5 “If… then…”\nSentences expressing data and information in natural language also appear connected with if… then…. For instance: “If the voltage reading is 200 mV, then the component is defective”. This kind of expression actually indicates that the following inference\n\\[\n\\textsf{\\small`The component is defective'} \\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}} \\textsf{\\small`The voltage reading is 200\\,mV'}\n\\]\nis true.\nThis kind of information is very important because it often is the starting point from which to arrive at the final inferences we’re interested in. We shall discuss it more in detail in the next sections.\n\n\n\n\n\n\n Careful\n\n\n\nThere is a connective in logic, called “material conditional”, which is also often translated as “if… then…”. But it is not the same as the inference relation discussed above. “If… then…” in natural language usually denotes an inference rather than a material conditional.\nResearch is still ongoing on these topics. If you are curious and in for a headache, look over The logic of conditionals.\n\n\n\n\n\nWe are now equipped with all the notions and symbolic notation to deal with our next task: learning the rules for drawing correct inferences.\n@@ TODO: add connections to impossibility of large language models to learn maths (Gödel & Co.)."
  },
  {
    "objectID": "truth_inference.html#sec-trivial-inference",
    "href": "truth_inference.html#sec-trivial-inference",
    "title": "7  Truth inference",
    "section": "7.1 A trivial inference",
    "text": "7.1 A trivial inference\nConsider again the assembly-line scenario of §  1, and suppose that an inspector has the following information about an electric component:\n\nThis electric component had an early failure (within a year of use). If an electric component fails early, then at production it didn’t pass either the heating test or the shock test. This component passed the shock test.\n\nThe inspector wants to assess whether the component did not pass the heating test.\nFrom the data and information given, the conclusion is that the component for sure did not pass the heating test. This conclusion is certain and somewhat trivial. But how did we obtain it? Which rules did we follow to arrive at it from the given data?\nFormal logic, with its deduction systems, is the huge field that formalizes and makes rigorous the rules that a rational person or an artificial intelligence should use in drawing sure inferences like the one above. We’ll now get a glimpse of it, as a trampoline for jumping towards more general and uncertain inferences."
  },
  {
    "objectID": "truth_inference.html#analysis-and-representation-of-the-problem",
    "href": "truth_inference.html#analysis-and-representation-of-the-problem",
    "title": "7  Truth inference",
    "section": "7.2 Analysis and representation of the problem",
    "text": "7.2 Analysis and representation of the problem\nFirst let’s analyse our simple problem and represent it with more compact symbols.\n\nAtomic sentences\nWe can introduce the following atomic sentences and symbols:\n\\[\n\\begin{aligned}\n\\mathsfit{h}&\\coloneqq\\textsf{\\small`The component passed the heating test'}\n\\\\\n\\mathsfit{s}&\\coloneqq\\textsf{\\small`The component passed the shock test'}\n\\\\\n\\mathsfit{f}&\\coloneqq\\textsf{\\small`The component had an early failure'}\n\\\\\n\\mathsfit{I}&\\coloneqq\\textsf{\\small (all other implicit background information)}\n\\end{aligned}\n\\]\n\n\nProposal\nThe proposal is \\(\\lnot\\mathsfit{h}\\), but in the present case we could also have chosen \\(\\mathsfit{h}\\).\n\n\nConditional\nThe bases for the inference are two known facts in the present case: \\(\\mathsfit{s}\\) and \\(\\mathsfit{f}\\). There may also be other obvious facts implicitly assumed in the inference, which we denote by \\(\\mathsfit{I}\\).\n\n\nStarting inferences\nLet us emphasize again that any inference is drawn from other inferences, which are either taken for granted, or drawn in turn from others. In the present case we are told that if an electric component fails early, then at production it didn’t pass either the heating test or the shock test. We write this as\n\\[\n\\lnot\\mathsfit{h}\\lor \\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I}\n\\]\nand we shall take this to be true (that is, to have probability \\(100\\%\\)).\nBut our scenario actually has at least one more, hidden, inference. We said that the component failed early, and that it did pass the shock test. This means, in particular, that it must be possible for the component to pass the shock test, even if it fails early. This means that\n\\[\n\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I}\n\\]\ncannot be false.\n\n\nTarget inference\nThe inference that the inspector wants to draw can be compactly written:\n\n\\[\n\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{s}\\land \\mathsfit{f}\\land \\mathsfit{I}\n\\]"
  },
  {
    "objectID": "truth_inference.html#truth-inference-rules",
    "href": "truth_inference.html#truth-inference-rules",
    "title": "7  Truth inference",
    "section": "7.3 Truth-inference rules",
    "text": "7.3 Truth-inference rules\n\nDeduction systems; a specific choice\nFormal logic gives us a set of rules for correctly drawing sure inferences, when such inferences are possible. These rules can be formulated in different ways, leading to a wide variety of deduction systems (each one with a wide variety of possible notations). The picture on the margin, for instance, shows how a proof of how our inference would look like, using the so-called sequent calculus, which consists of a dozen or so inference rules.\n\n\n\n\n\nThe bottom formula is the target inference. Each line denotes the application of an inference rule, from one or more inferences above the line, to one below the line. The two formulae with no line above are our starting inference, and a tautology.\n\n\n\n\nWe choose to compactly encode all truth-inference rules in the following way.\nFirst, represent true by the number \\(\\mathbf{1}\\), and false by \\(\\mathbf{0}\\).\nSecond, symbolically write that a proposal \\(\\mathsfit{Y}\\) is true, given a conditional \\(\\mathsfit{X}\\), as follows:\n\\[\n\\mathrm{T}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}) = 1\n\\]\nor “\\(=0\\)” if it’s false.\nThe rules of truth-inference are then encoded by the following equations, which must always hold for any atomic or composite sentences \\(\\mathsfit{X},\\mathsfit{Y},\\mathsfit{Z}\\):\n\n\n\n\n\n\n\n\n\n\n\n\n\nRule for “not”:\n\n\\[\\mathrm{T}(\\lnot \\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n+ \\mathrm{T}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= 1 \\tag{7.1}\\]\n\nRule for “and”:\n\n\\[\n\\mathrm{T}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= \\mathrm{T}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}\\land \\mathsfit{Z}) \\cdot\n\\mathrm{T}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= \\mathrm{T}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}\\land \\mathsfit{Z}) \\cdot\n\\mathrm{T}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\tag{7.2}\\]\n\nRule for “or”:\n\n\\[\\mathrm{T}(\\mathsfit{X}\\lor \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= \\mathrm{T}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) +\n\\mathrm{T}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n- \\mathrm{T}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\tag{7.3}\\]\n\nRule for truth:\n\n\\[\\mathrm{T}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}\\land \\mathsfit{Z})\n= 1\n\\tag{7.4}\\]\n\n\n\nHow to use the rules: Each equality can be rewritten in different ways according to the usual rules of algebra. Then the resulting left side can be replaced by the right side, and vice versa. The numerical values of starting inferences can be replaced in the corresponding expressions.\n\n\n\nLet’s see two examples:\n\nfrom one rule for “and” we can obtain the equality\n\\[\n{\\color[RGB]{102,204,238}\\mathrm{T}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}\\land \\mathsfit{Z})}\n=\\color[RGB]{204,187,68}\\frac{\\mathrm{T}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})}{\\mathrm{T}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})}\n\\]\nprovided that \\(\\mathrm{T}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) \\ne 0\\). Then wherever we see the left side, we can replace it with the fraction on the right side, and vice versa.\nfrom the rule for “or” we can obtain the equality\n\\[\n{\\color[RGB]{102,204,238}\n\\mathrm{T}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) - \\mathrm{T}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})}\n=\\color[RGB]{204,187,68}\n\\mathrm{T}(\\mathsfit{X}\\lor \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) - \\mathrm{T}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\]\nAgain wherever we see the left side, we can replace it with the sum on the right side, and vice versa.\n\n\n\nTarget inference in our scenario\nLet’s see how these rules allow us to arrive at our target inference,\n\\[\n\\color[RGB]{238,102,119}\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{s}\\land \\mathsfit{f}\\land \\mathsfit{I})\n\\]\nstarting from the given ones\n\\[\n\\color[RGB]{34,136,51}\\mathrm{T}(\\lnot\\mathsfit{h}\\lor \\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I}) = 1\n\\ ,\n\\qquad\n\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I}) \\ne 0\n\\]\nOne possibility is to work backwards from the target inference:\n\n\\[\n\\begin{aligned}\n&\\color[RGB]{238,102,119}\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{s}\\land \\mathsfit{f}\\land \\mathsfit{I})&&\n\\\\[1ex]\n&\\qquad=\\frac{\\mathrm{T}(\\lnot\\mathsfit{h}\\land \\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n{\\color[RGB]{34,136,51}\\underbracket[1pt]{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}_{\\ne 0}}\n&&\\text{\\small ∧-rule and starting inference}\n\\\\[1ex]\n&\\qquad=\\frac{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\lnot\\mathsfit{h}\\land \\mathsfit{f}\\land \\mathsfit{I})\\cdot\n\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})\n}\n{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n&&\\text{\\small ∧-rule}\n\\\\\n&\\qquad=\\frac{\\bigl[1-\\mathrm{T}(\\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\lnot\\mathsfit{h}\\land \\mathsfit{f}\\land \\mathsfit{I})\\bigr]\\cdot\n\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})\n}\n{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n&&\\text{\\small ¬-rule}\n\\\\\n&\\qquad=\\frac{\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})-\n\\mathrm{T}(\\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\lnot\\mathsfit{h}\\land \\mathsfit{f}\\land \\mathsfit{I})\\cdot\n\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})\n}\n{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n&&\\text{\\small algebra}\n\\\\\n&\\qquad=\\frac{\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})-\n\\mathrm{T}(\\lnot\\mathsfit{s}\\land \\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})\n}\n{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n&&\\text{\\small  ∧-rule}\n\\\\\n&\\qquad=\\frac{{\\color[RGB]{34,136,51}\\mathrm{T}(\\lnot\\mathsfit{h}\\land \\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}-\n\\mathrm{T}(\\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})\n}\n{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n&&\\text{\\small  ∨-rule}\n\\\\\n&\\qquad=\\frac{{\\color[RGB]{34,136,51}1} -\n\\mathrm{T}(\\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})\n}\n{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n&&\\text{\\small starting inference}\n\\\\\n&\\qquad=\\frac{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n&&\\text{\\small ¬-rule}\n\\\\[1ex]\n&\\qquad=\\color[RGB]{238,102,119}1\n&&\\text{\\small algebra}\n\\end{aligned}\n\\]\n\nTherefore \\(\\color[RGB]{238,102,119}\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{s}\\land \\mathsfit{f}\\land \\mathsfit{I}) = 1\\). We find that, indeed, the electronic component must for sure have failed the heating test!\n\n\n\n\n\n\n Exercise\n\n\n\nRetrace the proof above step by step. At each step, how was its particular rule (indicated on the right) used?\n\n\n\n\nThe way in which the rules can be applied to arrive at the target inference is not unique. In fact, in some concrete applications it can require a lot of work to find how to connect target inference with starting ones via the rules. The result, however, will always be the same:\n\n\n\n\n\n\n\n\n\n\n\nThe rules of truth-inference are self-consistent: even if applied in different sequences of steps, they always lead to the same final result.\n\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nProve the target inference \\(\\color[RGB]{238,102,119}\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{s}\\land \\mathsfit{f}\\land \\mathsfit{I}) = 1\\) using the rules of truth-inference, but beginning from the starting inference \\(\\color[RGB]{34,136,51}\\mathrm{T}(\\lnot\\mathsfit{h}\\land \\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})=1\\).\n\n\n\n\n[Optional] Equivalence with truth-tables\nIf you have studied Boolean algebra, you may be familiar with truth-tables; for instance the one for “and” displayed on the side. The truth-inference rules (7.1)–(7.4) contain the truth-tables that you already know as special cases.\n\n\n\n\n\n\\(\\mathsfit{X}\\)\n\\(\\mathsfit{Y}\\)\n\\(\\mathsfit{X}\\land \\mathsfit{Y}\\)\n\n\n\n\n1\n1\n1\n\n\n1\n0\n0\n\n\n0\n1\n0\n\n\n0\n0\n0\n\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nUse the truth-inference rules for “or” and “and” to build the truth-table for “or”. Check if it matches the one you already knew.\n\n\nThe truth-inference rules (7.1)–(7.4) are more complicated than truth-tables, but have two important advantages First, they allow us to work with conditionals, and to move sentences between proposals and conditionals. Second, they provide a smoother transition to the rules for probability-inference."
  },
  {
    "objectID": "truth_inference.html#logical-ai-agents-and-their-limitations",
    "href": "truth_inference.html#logical-ai-agents-and-their-limitations",
    "title": "7  Truth inference",
    "section": "7.4 Logical AI agents and their limitations",
    "text": "7.4 Logical AI agents and their limitations\nThe truth-inference discussed in this section are also the rules that a logical AI agent should follow. For example, the automated control and fault-management programs in NASA spacecrafts, mentioned in §  6.1, are programmed according to these rules.\n\n\n\n\n\n\n Study reading\n\n\n\nLook over Ch. 7 in Artificial Intelligence.\n\n\nMany – if not most – inference problems that human and AI agents must face are, however, of the uncertain kind: it is not possible to surely infer the truth of some outcome, and the truth of some initial data or initial inferences may not be known either. We shall now see how to generalize the truth-inference rules to uncertain situations.\n\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nOur cursory visit of formal logic only showed a microscopic part of this vast field. The study of truth-inference rules continues still today, with many exciting developments and applications. Feel free take a look at\n\nLogic in Computer Science\nMathematical Logic for Computer Science\nNatural Deduction Systems in Logic"
  },
  {
    "objectID": "probability_inference.html#sec-probability-def",
    "href": "probability_inference.html#sec-probability-def",
    "title": "8  Probability inference",
    "section": "8.1 When truth isn’t known: probability",
    "text": "8.1 When truth isn’t known: probability\nWhen we cross a busy city street we look left and right to check whether any cars are approaching. We typically don’t look up to check whether something is falling from the sky. Yet, couldn’t it be false that cars are approaching at that moment? and couldn’t it be true that some object is falling from the sky? Of course both events are possible. Then why do we look left and right, but not up?\nThe main reason is that we believe strongly that cars might be approaching, and believe very weakly that some object might be falling from the sky. In other words, we consider the first occurrence to be very probable, and the second extremely improbable.\nWe shall take the notion of probability as intuitively understood (just as we did with the notion of truth). Terms equivalent to “probability” are degree of belief, plausibility, credibility1.1 credibility literally means “believability” (from Latin credo = to believe).\n\n\n\n\n\n\n Beware of likelihood as a synonym for probability\n\n\n\nIn everyday language, “likelihood” is synonym with “probability”. In technical writings about probability or statistics, however, “likelihood” means something different and is not a synonym of “probability”, as we explain below (§  8.8.1).\n\n\nProbabilities are quantified between \\(0\\) and \\(1\\), or equivalently between \\(0\\%\\) and \\(100\\%\\). Assigning to a sentence a probability 1 is the same as saying that it is true; and a probability 0, that it is false. A probability of \\(0.5\\) represents a belief completely symmetric with respect to truth and falsity.\nLet’s emphasize and agree on some important facts about probabilities:\n\n Probabilities are assigned to sentences. We already discussed this point in §  6.3, but let’s reiterate it. Consider an engineer working on a problem of electric-power distribution in a specific geographical region. At a given moment the engineer may believe with \\(75\\%\\) probability that the measured average power output in the next hour will be 100 MW. The \\(75\\%\\) probability is assigned not to the quantity “100 MW”, but to the sentence\n\\[\n\\textsf{\\small`The measured average power output in the next hour will be 100\\,MW'}\n\\]\nThis difference is extremely important. Consider the alternative sentence\n\\[\n\\textsf{\\small`The average power output in the next hour will be \\emph{set} to 100\\,MW'}\n\\]\nthe numerical quantity is the same, but the meaning is very different. The probability can therefore be very different (if the engineer is the person deciding how to set that output, then the probability is \\(100\\%\\), because has decided and knows what the output is). The probability depends not only on a number, but on what it’s being done with that number – measuring, setting, third-party reporting, and so on. Often we write simply “\\(O \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}10\\,\\mathrm{W}\\)” provided that the full sentence behind this kind of shorthand is understood.\n Probabilities are agent- and knowledge-dependent. A coin is tossed, comes down heads, and is quickly hidden from view. Alice sees that it landed heads-up. Bob instead doesn’t manage to see the outcome and has no clue. Alice considers the sentence \\(\\textsf{\\small`Coin came down heads'}\\) to be true, that is, to have \\(100\\%\\) probability. Bob considers the same sentence to have \\(50\\%\\) probability.\nNote how Alice and Bob assign two different probabilities to the same sentence; yet both assignments are completely rational. If Bob assigned \\(100\\%\\) to \\(\\textsf{\\small`heads'}\\), we would suspect that he had seen the outcome after all; if he assigned \\(0\\%\\) to \\(\\textsf{\\small`heads'}\\), we would consider that unreasonable (he didn’t see the outcome, so why exclude \\(\\textsf{\\small`heads'}\\)?). At the same time we would be baffled if Alice assigned only \\(50\\%\\) to \\(\\textsf{\\small`heads'}\\), because she actually saw that the outcome was heads; maybe we would hypothesize that she feels unsure about what she saw.\nAn omniscient agent would know the truth or falsity of every sentence, and assign only probabilities 0 or 1. Some authors speak of “actual (but unknown) probabilities”. But if there were “actual” probabilities, they would be all 0 or 1, and it would be pointless to speak about probabilities at all – every inference would be a truth-inference.\n Probabilities are not frequencies. Consider the fraction of defective mechanical components to total components produced per year in some factory. This quantity can be physically measured and, once measured, would be agreed upon by every agent. It is a frequency, not a degree of belief or probability.\nIt is important to understand the difference between probability and frequency: mixing them up may lead to sub-optimal decisions. Later we shall say more about the difference and the precise relations between probability and frequency.\nFrequencies can be unknown to some agents. Probabilities cannot be “unknown”: they can only be difficult to calculate. Be careful when you read authors speaking of an “unknown probability”: they actually mean either “unknown frequency”, or a probability that has to be calculated (it’s “unknown” in the same sense that the value of \\(1-0.7 \\cdot 0.2/(1-0.3)\\) is “unknown” to you right now).\n Probabilities are not physical properties. Whether a tossed coin lands heads up or tails up is fully determined by the initial conditions (position, orientation, momentum, rotational momentum) of the toss and the boundary conditions (air velocity and pressure) during the flight. The same is true for all macroscopic engineering phenomena (even quantum phenomena have never been proved to be non-deterministic, and there are deterministic and experimentally consistent mathematical representations of quantum theory). So we cannot measure a probability using some physical apparatus; and the mechanisms underlying any engineering problem boil down to physical laws, not to probabilities.\n\n\n\n\n\n\n\n Study reading\n\n\n\nDynamical Bias in the Coin Toss. \n\n\n\n\nThese points listed above are not just a matter of principle. They have important practical consequences. A data scientist who is not attentive to the source of the data (measured? set? reported, and so maybe less trustworthy?), or who does not carefully assess the context of a probability, or who mixes a probability with a frequency, or who does not take advantage (when possible) of the physics involved in the a problem – such data scientist will design systems with sub-optimal performance2 – or even cause deaths.2 This fact can be mathematically proven."
  },
  {
    "objectID": "probability_inference.html#sec-uncertain-inference",
    "href": "probability_inference.html#sec-uncertain-inference",
    "title": "8  Probability inference",
    "section": "8.2 An unsure inference",
    "text": "8.2 An unsure inference\nConsider now the following variation of the trivial inference problem of §  7.1.\n\nThis electric component had an early failure. If an electric component fails early, then at production it either didn’t pass the heating test or didn’t pass the shock test. The probability that it didn’t pass both tests is 10%. There’s no reason to believe that the component passed the heating test, more than it passed the shock test.\n\nThe inspector wants to assess, also in this case, whether the component did not pass the heating test.\nFrom the data and information given, what would you say is the probability that the component didn’t pass the heating test?\n\n\n\n\n\n\n Exercises\n\n\n\n\nTry to argue why a conclusion cannot be drawn with certainty in this case. One way to argue this is to present two different scenarios that fit the given data but have opposite conclusions.\nTry to reason intuitively and assess the probability that the component didn’t pass the heating test. Should it be larger or smaller than 50%? Why?"
  },
  {
    "objectID": "probability_inference.html#probability-notation",
    "href": "probability_inference.html#probability-notation",
    "title": "8  Probability inference",
    "section": "8.3 Probability notation",
    "text": "8.3 Probability notation\nFor this inference problem we can’t find a true or false final value. The truth-inference rules (7.1)–(7.4) therefore cannot help us here. In fact even the “\\(\\mathrm{T}(\\dotso \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\dotso)\\)” notation is unsuitable, because it only admits the values \\(1\\) (true) and \\(0\\) (false).\nLet us first generalize this notation in a straightforward way:\nFirst, let’s represent the probability or degree of belief of a sentence by a number in the range \\([0,1]\\), that is, between \\(\\mathbf{1}\\) (certainty or true) and \\(\\mathbf{0}\\) (impossibility or false). The value \\(0.5\\) represents that the belief in the truth of the sentence is as strong as that in its falsity.\nSecond, let’s symbolically write that the probability of a proposal \\(\\mathsfit{Y}\\), given a conditional \\(\\mathsfit{X}\\), is some number \\(p\\), as follows:\n\\[\n\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}) = p\n\\]\nNote that this notation includes the notation for truth-values as a special case:\n\\[\n\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}) = 0\\text{ or }1\n\\quad\\Longleftrightarrow\\quad\n\\mathrm{T}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}) = 0\\text{ or }1\n\\]"
  },
  {
    "objectID": "probability_inference.html#sec-fundamental",
    "href": "probability_inference.html#sec-fundamental",
    "title": "8  Probability inference",
    "section": "8.4 Inference rules",
    "text": "8.4 Inference rules\nExtending our truth-inference notation to probability-inference notation has been straightforward. But which rules should we use for drawing inferences when probabilities are involved?\nThe amazing result is that the rules for truth-inference, formulae (7.1)–(7.4), extend also to probability-inference. The only difference is that they now hold for all values in the range \\([0,1]\\), rather than for \\(0\\) and \\(1\\) only.\nThis important result was taken more or less for granted at least since Laplace in the 1700s. But was formally proven for the first time in the 1946 by R. T. Cox. The proof has been refined since then. What kind of proof is it? It shows that if we don’t follow the rules we are doomed to arrive at illogical conclusions; we’ll show some examples later.\n\n\nFinally, here are the fundamental rules of all inference. They are encoded by the following equations, which must always hold for any atomic or composite sentences \\(\\mathsfit{X},\\mathsfit{Y},\\mathsfit{Z}\\):\n\n\n\n\n\n\n\n   THE FUNDAMENTAL LAWS OF INFERENCE   \n\n\n\n\n\n\\(\\boldsymbol{\\lnot}\\) “Not” rule\n\n\\[\\mathrm{P}(\\lnot \\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n+ \\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= 1\\]\n\n\n\\(\\boldsymbol{\\land}\\) “And” rule\n\n\\[\n\\mathrm{P}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= \\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}\\land \\mathsfit{Z}) \\cdot\n\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= \\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}\\land \\mathsfit{Z}) \\cdot\n\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\]\n\n\n\\(\\boldsymbol{\\lor}\\) “Or” rule\n\n\\[\\mathrm{P}(\\mathsfit{X}\\lor \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= \\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) +\n\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n- \\mathrm{P}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\]\n\n\nTruth rule\n\n\\[\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}\\land \\mathsfit{Z})\n= 1\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to use the rules: Each equality can be rewritten in different ways according to the usual rules of algebra. Then the resulting left side can be replaced by the right side, and vice versa. The numerical values of starting inferences can be replaced in the corresponding expressions.\n\n\n\nIt is amazing that ALL inference is nothing else but a repeated application of these four rules – billions of times or more in some cases. All machine-learning algorithms are just applications or approximations of these rules. Methods that you may have heard about in statistics are just specific applications of these rules. Truth inferences are also special applications of these rules. Most of this course is, at bottom, just a study of how to apply these rules in particular kinds of problems.\n\n\n\n\n\n\n Study reading\n\n\n\n\nSkim through Probability, Frequency and Reasonable Expectation. Try to get the ideas behind the reasoning, even if you can’t follow the mathematical details.\nCh. 2 of Bayesian Logical Data Analysis for the Physical Sciences\nCh. 1 of Probability\n§§ 1.0–1.2 of Data Analysis\nSkim through Chs–1–2 of Probability Theory\n\n\n\n \nThe fundamental inference rules are used in the same way as their truth-inference special case: Each equality can be rewritten in different ways according to the usual rules of algebra. Then left and right side of the equality thus obtained can replace each other in a proof."
  },
  {
    "objectID": "probability_inference.html#solution-of-the-uncertain-inference-example",
    "href": "probability_inference.html#solution-of-the-uncertain-inference-example",
    "title": "8  Probability inference",
    "section": "8.5 Solution of the uncertain-inference example",
    "text": "8.5 Solution of the uncertain-inference example\nArmed with the fundamental rules of inference, let’s solve our earlier inference problem. As usual we first analyse it, find what are its proposal and conditional, and which starting inferences are given in the problem.\n\nAtomic sentences\n\\[\n\\begin{aligned}\n\\mathsfit{h}&\\coloneqq\\textsf{\\small`The component passed the heating test'}\n\\\\\n\\mathsfit{s}&\\coloneqq\\textsf{\\small`The component passed the shock test'}\n\\\\\n\\mathsfit{f}&\\coloneqq\\textsf{\\small`The component had an early failure'}\n\\\\\n\\mathsfit{J}&\\coloneqq\\textsf{\\small (all other implicit background information)}\n\\end{aligned}\n\\]\nThe background information in this example is different from the previous, truth-inference one, so we use the different symbol \\(\\mathsfit{J}\\) for it.\n\n\nProposal, conditional, and target inference\nThe proposal is \\(\\lnot\\mathsfit{h}\\), just like in the truth-inference example.\nThe conditional is different now. We know that the component failed early, but we don’t know whether it passed the shock test. Hence the conditional is \\(\\mathsfit{f}\\land \\mathsfit{J}\\).\nThe target inference is therefore\n\\[\n\\color[RGB]{238,102,119}\\mathrm{P}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\n\\]\n\n\nStarting inferences\nWe are told that if an electric component fails early, then at production it didn’t pass either the heating test or the shock test. Let’s write this as\n\\[\n\\color[RGB]{34,136,51}\\mathrm{P}(\\lnot\\mathsfit{h}\\lor \\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J}) = 1\n\\]\nWe are also told that there is a \\(10\\%\\) probability that both tests fail\n\\[\n\\color[RGB]{34,136,51}\\mathrm{P}(\\lnot\\mathsfit{h}\\land \\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J}) = 0.1\n\\]\nFinally the problem says that there’s no reason to believe that the component didn’t pass the heating test, more than it didn’t pass the shock test. This can be written as follows:\n\\[\n\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J}) = \\mathrm{P}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\n\\]\nNote this interesting situation: we are not given the numerical values of these two probabilities, we are only told that they are equal. This is an example of application of the principle of indifference, which we’ll discuss more in detail later.\n\n\nFinal inference\nAlso in this case there is no unique way of applying the rules to reach our target inference, but all ways lead to the same result. Let’s try to proceed backwards:\n\n\\[\n\\begin{aligned}\n&\\color[RGB]{238,102,119}\\mathrm{P}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})&&\n\\\\[1ex]\n&\\qquad= {\\color[RGB]{34,136,51}\\mathrm{P}(\\lnot\\mathsfit{s}\\lor \\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})}\n+ {\\color[RGB]{34,136,51}\\mathrm{P}(\\lnot\\mathsfit{s}\\land \\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})}\n- \\mathrm{P}(\\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\n&&\\text{\\small ∨-rule}\n\\\\[1ex]\n&\\qquad= {\\color[RGB]{34,136,51}1}\n+ {\\color[RGB]{34,136,51}0.1}\n- \\mathrm{P}(\\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\n&&\\text{\\small starting inferences}\n\\\\[1ex]\n&\\qquad= 0.1 + \\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\n&&\\text{\\small ¬-rule}\n\\\\[1ex]\n&\\qquad= 0.1 + \\mathrm{P}(\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\n&&\\text{\\small starting inference}\n\\\\[1ex]\n&\\qquad= 0.1 + 1 -\\color[RGB]{238,102,119}\\mathrm{P}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\n&&\\text{\\small ¬-rule}\n\\end{aligned}\n\\]\n\nThe target probability appears on the left and right side with opposite signs. We can solve for it:\n\\[\n\\begin{aligned}\n2\\,{\\color[RGB]{238,102,119}\\mathrm{P}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})} &= 0.1 + 1\n\\\\[1ex]\n{\\color[RGB]{238,102,119}\\mathrm{P}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})} &= 0.55\n\\end{aligned}\n\\]\nSo the probability that the component didn’t pass the heating test is \\(55\\%\\).\n\n\n\n\n\n\n Exercises\n\n\n\n\nTry to find an intuitive explanation of why the probability is 55%, slightly larger than 50%. If your intuition says this probability is wrong, then\n\nCheck the proof of the inference for mistakes, or try to find a proof with a different path.\nExamine your intuition critically and educate it.\n\nCheck how the target probability \\(\\mathrm{P}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\\) changes if we change the value of the probability \\(\\mathrm{P}(\\lnot\\mathsfit{s}\\land \\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\\) from \\(0.1\\).\n\nWhat result do we obtain if \\(\\mathrm{P}(\\lnot\\mathsfit{s}\\land \\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})=0\\)? Can it be intuitively explained?\nWhat if \\(\\mathrm{P}(\\lnot\\mathsfit{s}\\land \\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})=1\\)? Does the result make sense?"
  },
  {
    "objectID": "probability_inference.html#how-the-inference-rules-are-used",
    "href": "probability_inference.html#how-the-inference-rules-are-used",
    "title": "8  Probability inference",
    "section": "8.6 How the inference rules are used",
    "text": "8.6 How the inference rules are used\nIn the solution above you noticed that the equations of the fundamental rules are not only used to obtain some of the probabilities appearing in them from the remaining probabilities.\nThe rules represent, first of all, constraints of logical consistency3 among probabilities. For instance, if we have probabilities  \\(\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{X}\\land \\mathsfit{Z})=0.1\\),  \\(\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z})=0.7\\),  and \\(\\mathrm{P}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z})=0.2\\),  then there’s an inconsistency somewhere, because these values violate the and-rule:  \\(0.2 \\ne 0.1 \\cdot 0.7\\).  In this case we must find the inconsistency and solve it. However, since probabilities are quantified by real numbers, it’s possible and acceptable to have slight discrepancies within numerical round-off errors.3 The technical term is coherence.\nThe rules also imply more general constraints. For example we must always have\n\\[\n\\begin{gathered}\n\\mathrm{P}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}) \\le \\min\\set[\\big]{\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}),\\  \\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z})}\n\\\\\n\\mathrm{P}(\\mathsfit{X}\\lor \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}) \\ge \\max \\set[\\big]{\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}),\\  \\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z})}\n\\end{gathered}\n\\]\n\n\n\n\n\n\n Exercise\n\n\n\nTry to prove the two constraints above."
  },
  {
    "objectID": "probability_inference.html#consequences-of-not-following-the-rules",
    "href": "probability_inference.html#consequences-of-not-following-the-rules",
    "title": "8  Probability inference",
    "section": "8.7 Consequences of not following the rules",
    "text": "8.7 Consequences of not following the rules\nThe fundamental rules of inference guarantee that the agent’s uncertain reasoning is self-consistent, and that it follows logic when there’s no uncertainty. Breaking the rules means that the resulting inference has some logical or irrational inconsistencies.\nThere are many examples of inconsistencies that appear when the rules are broken. Imagine for instance an agent that gives an 80% probability that it rains (above 1 mm) in the next hour; and it also gives a 90% probability that it rains (above 1 mm) and the average wind is above 3⋅m/s in the next hour. This is clearly unreasonable, because the raining scenario alone would be true with wind above 3 m/s and also below 3⋅m/s – so it should be more probable than the scenario where the wind is above 3 m/s. In fact those two probabilities break the and-rule.\n\n\n\n\n\n\n Exercise\n\n\n\nProve that the two probabilities in the example above break the and-rule. (Hint: you must use the fact that probabilities are numbers between 0 and 1, and that multiplying a number by something between 0 and 1 can only yield a smaller number.)\n\n\n\n\n\n\n\n\n Study reading\n\n\n\n\n§ 12.2.3 in Artificial Intelligence\nAs you continue your studies, go through chapters 4–8 of Rational Choice in an Uncertain World, just to get the main messages and an overview of curious psychological phenomena."
  },
  {
    "objectID": "probability_inference.html#remarks-on-terminology-and-notation",
    "href": "probability_inference.html#remarks-on-terminology-and-notation",
    "title": "8  Probability inference",
    "section": "8.8 Remarks on terminology and notation",
    "text": "8.8 Remarks on terminology and notation\n\nLikelihood\nIn everyday language, “likely” is often a synonym of “probable”; and “likelihood”, of “probability”. But in technical writings about probability, inference, and decision-making, “likelihood” has a very different meaning. Beware of this important difference in definition:\n\\(\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{X})\\) is:\n\nthe probability of \\(\\mathsfit{Y}\\) given \\(\\mathsfit{X}\\) (or conditional on \\(\\mathsfit{X}\\)),\nthe likelihood of \\(\\mathsfit{X}\\) in view of \\(\\mathsfit{Y}\\).\n\nLet’s express this also in a different way:\n\nthe probability of \\(\\mathsfit{Y}\\) given \\(\\mathsfit{X}\\), is \\(\\mathrm{P}({\\color[RGB]{68,119,170}\\mathsfit{Y}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{X})\\).\nthe likelihood of \\(\\mathsfit{Y}\\) in view of \\(\\mathsfit{X}\\), is \\(\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}{\\color[RGB]{68,119,170}\\mathsfit{Y}})\\).\n\n\n\n\n\n\n\n\n\n\n\nA priori there is no relation between the probability and the likelihood of a sentence \\(\\mathsfit{Y}\\): this sentence could have very high probability and very low likelihood, and vice versa.\n\n\nIn these notes we’ll avoid the possibly confusing term “likelihood”. All we need to express can be phrased in terms of probability.\n\n\nOmitting background information\nIn the analyses of the inference examples of §  7.1 and §  8.2 we defined sentences (\\(\\mathsfit{I}\\) and \\(\\mathsfit{J}\\)) expressing all background information, and always included these sentences in the conditionals of the inferences – because those inferences obviously depended on that background information.\nIn many concrete inference problems the background information usually stays there in the conditional from beginning to end, while the other sentences jump around between conditional and proposal as we apply the rules of inference. For this reason the background information is often omitted from the notation, being implicitly understood. For instance, if the background information is denoted \\(\\mathsfit{I}\\), one writes\n\n“\\(\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{X})\\)”  instead of  \\(\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{X}\\land \\mathsfit{I})\\)\n“\\(\\mathrm{P}(\\mathsfit{Y})\\)”  instead of  \\(\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\\)\n\nThis is what’s happening when you see in books probabilities “\\(P(x)\\)” without conditional.\nSuch practice may be convenient, but be wary of it, especially in particular situations:\n\nIn some inference problems we suddenly realize that we must distinguish between cases that depend on hypotheses, say \\(\\mathsfit{H}_1\\) and \\(\\mathsfit{H}_2\\), that were buried in the background information \\(\\mathsfit{I}\\). If the background information \\(\\mathsfit{I}\\) is explicitly reported in the notation, this is no problem: we can rewrite it as\n\\[ \\mathsfit{I}= (\\mathsfit{H}_1 \\lor \\mathsfit{H}_2) \\land \\mathsfit{I}'\\]\nand proceed, for example using the rule of extension of the conversation. If the background information was not explicitly written, this may lead to confusion and mistakes. For instance there may suddenly appear two instances of \\(\\mathrm{P}(\\mathsfit{X})\\) with different values, just because one of them is invisibly conditional on \\(\\mathsfit{I}\\), the other on \\(\\mathsfit{I}'\\).\nIn some inference problems we are considering several different instances of background information – for example because more than one agent is involved. It’s then extremely important to write the background information explicitly, lest we mix up the different agents’s degrees of belief.\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nA once-famous paper published in the quantum-theory literature arrived at completely wrong results simply by omitting background information, mixing up probabilities having different conditionals.\n\n\nThis kind of confusion from poor notation happens more often than one thinks, and even appears in scientific literature.\n\n\n“Random variables”\nSome texts speak of the probability of a “random variable”, or more precisely of the probability “that a random variable takes on a particular value”. As you notice, we have just expressed that idea by means of a sentence. The viewpoint and terminology of random variables is therefore a special case of that based on sentences, which we use here.\nThe dialect of “random variables” does not offer any advantages in concepts, notation, terminology, or calculations, and it has several shortcomings:\n\n\n\n\n\nJames Clerk Maxwell is one of the main founders of statistical mechanics and kinetic theory (and electromagnetism). Yet he never used the word “random” in his technical writings. Maxwell is known for being very clear and meticulous with explanations and terminology.\n\n\n\nAs discussed in §  8.1, in concrete applications it is important to know how a quantity “takes on” a value: for example it could be directly measured, indirectly reported, or purposely set to that specific value. Thinking and working in terms of sentences, rather than of random variables, allows us to account for these important differences.\nWe want a general AI agent to be able to deal with uncertainty and probability even in situations that do not involve mathematical sets.\nVery often the object (proposal) of a probability is not a “variable”: it is actually a constant value that is simply unknown (simple example: we are uncertain about the mass of a particular block of concrete, so we speak of the probability of some mass value; this doesn’t mean that the mass of the block is changing).\nWhat does “random” (or “chance”) mean? Good luck finding an understandable and non-circular definition in texts that use that word; strangely enough, they never define it. In these notes, if the word “random” is ever used, it stands for “unpredictable” or “unsystematic”.\n\nIt’s a question for sociology of science why some people keep on using less flexible points of view or terminologies. Probably they just memorize them as students and then a fossilization process sets in.\n\n\nFinally, some texts speak of the probability of an “event”. For all purposes an “event” is just what’s expressed in a sentence."
  },
  {
    "objectID": "derived_rules.html#sec-truth-stable",
    "href": "derived_rules.html#sec-truth-stable",
    "title": "9  Shortcut rules",
    "section": "9.1 Falsity and truth cannot be altered by additional knowledge",
    "text": "9.1 Falsity and truth cannot be altered by additional knowledge\nSuppose that sentence \\(\\mathsfit{X}\\) is judged to be completely impossible conditional on sentence \\(\\mathsfit{Z}\\):\n\\[\n\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) = 0\n\\]\nIt can then be proved, from the fundamental rules, that \\(\\mathsfit{X}\\) is also completely impossible if we add information to \\(\\mathsfit{Z}\\). That is, for any sentence \\(\\mathsfit{Y}\\) we’ll also have\n\\[\n\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}\\land \\mathsfit{Z}) = 0\n\\]\n\n\n\n\n\n\n Exercise\n\n\n\nTry to prove this. (Hint: try using the and-rule one or more times.)\n\n\n What if we use \\(\\lnot\\mathsfit{X}\\) for \\(\\mathsfit{Y}\\), that is, what if we acquire knowledge that \\(\\mathsfit{X}\\) is actually true? Then it can be proved that all probability calculations break down. The problem is that \\(\\lnot\\mathsfit{X}\\) and \\(\\mathsfit{Z}\\) turn out to be mutually contradictory, so all inferences are starting from contradictory premises. Just as in formal logic, from contradictory premises we can obtain any conclusion whatsoever.\nNote that this problem does not arise, however, if \\(\\mathsfit{X}\\) is only extremely improbable conditional on \\(\\mathsfit{Z}\\) – say with a probability of \\(10^{-100}\\) – rather than flat-out impossible. In practical applications we often approximate extremely small probabilities by \\(0\\), or extremely large ones by \\(1\\). If the probability calculations break down, we must then step back and correct the approximation.\n\n\nBy using the not-rule it is possible to prove that full certainty about a sentence behaves in a similar manner. If sentence \\(\\mathsfit{X}\\) is judged to be completely certain conditional on sentence \\(\\mathsfit{Z}\\):\n\\[\n\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) = 1\n\\]\nthen, from the fundamental rules, \\(\\mathsfit{X}\\) is also completely certain if we add information to \\(\\mathsfit{Z}\\). That is, for any sentence \\(\\mathsfit{Y}\\) we’ll also have\n\\[\n\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}\\land \\mathsfit{Z}) = 1\n\\]"
  },
  {
    "objectID": "derived_rules.html#sec-boolean",
    "href": "derived_rules.html#sec-boolean",
    "title": "9  Shortcut rules",
    "section": "9.2 Boolean algebra",
    "text": "9.2 Boolean algebra\nFirst, it is possible to show that all rules you may know from Boolean algebra are a consequence of the fundamental rules of §  8.4. So we can always make the following convenient replacements anywhere in a probability expression:\n\n\n\n\n\n\nDerived rules: Boolean algebra\n\n\n\n\\[\n\\begin{gathered}\n\\lnot\\lnot \\mathsfit{X}= \\mathsfit{X}\n\\\\[1ex]\n\\mathsfit{X}\\land \\mathsfit{X}= \\mathsfit{X}\n\\\\[1ex]\n\\mathsfit{X}\\lor \\mathsfit{X}= \\mathsfit{X}\n\\\\[1ex]\n\\mathsfit{X}\\land \\mathsfit{Y}= \\mathsfit{Y}\\land \\mathsfit{X}\n\\\\[1ex]\n\\mathsfit{X}\\lor \\mathsfit{Y}= \\mathsfit{Y}\\lor \\mathsfit{X}\n\\\\[1ex]\n\\mathsfit{X}\\land (\\mathsfit{Y}\\lor \\mathsfit{Z}) = (\\mathsfit{X}\\land \\mathsfit{Y}) \\lor (\\mathsfit{X}\\land \\mathsfit{Z})\n\\\\[1ex]\n\\mathsfit{X}\\lor (\\mathsfit{Y}\\land \\mathsfit{Z}) = (\\mathsfit{X}\\lor \\mathsfit{Y}) \\land (\\mathsfit{X}\\lor \\mathsfit{Z})\n\\\\[1ex]\n\\lnot (\\mathsfit{X}\\land \\mathsfit{Y}) = \\lnot \\mathsfit{X}\\lor \\lnot \\mathsfit{Y}\n\\\\[1ex]\n\\lnot (\\mathsfit{X}\\lor \\mathsfit{Y}) = \\lnot \\mathsfit{X}\\land \\lnot \\mathsfit{Y}\n\\end{gathered}\n\\]\n\n\nFor example, a partial proof of the rule \\(\\mathsfit{X}\\land \\mathsfit{X}= \\mathsfit{X}\\), called “and-idempotence”, from the four fundamental rules goes as follows:\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\mathsfit{X}\\land \\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})&&\n\\\\[1ex]\n&\\qquad= \\mathrm{P}(\\mathsfit{X}| \\mathsfit{X}\\land \\mathsfit{Z}) \\cdot \\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})&&\n&&\\text{\\small ∧-rule}\n\\\\[1ex]\n&\\qquad= 1 \\cdot \\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})&&\n&&\\text{\\small truth-rule}\n\\\\[1ex]\n&\\qquad= \\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\end{aligned}\n\\]\nand with a similar procedure it can be shown that \\(\\mathsfit{X}\\land \\mathsfit{X}\\) can be replaced with \\(\\mathsfit{X}\\) no matter where it appears. The above proof shows that the and-idempotence rule is tightly connected with the truth-rule of inference."
  },
  {
    "objectID": "derived_rules.html#sec-idempotent",
    "href": "derived_rules.html#sec-idempotent",
    "title": "9  Shortcut rules",
    "section": "9.3 Subtle importance of apparently trivial rules",
    "text": "9.3 Subtle importance of apparently trivial rules\nSome of the fundamental or derived rules may seem obvious or unimportant, but actually are of extreme importance in data science. For instance the and-idempotence rule effectively asserts that whenever we draw inferences, redundant information or data is automatically counted only once.\nThis amazing feature saves us from a lot of headaches. Imagine that an AI decision agent at the assembly line has been given background information equivalent to saying that if an electronic component passes the heating test (\\(\\mathsfit{h}\\)), then its probability of early failure (\\(\\mathsfit{f}\\)) is only 10%:\n\\[\\mathrm{P}(\\mathsfit{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{h}\\land \\mathsfit{Z}) = 0.1\\]\nA new voltage test has also been devised, and if a component passes this test (\\(\\mathsfit{v}\\)) then its probability of early failure is also only 10%:\n\\[\\mathrm{P}(\\mathsfit{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{v}\\land \\mathsfit{Z}) = 0.1\\]\nHowever, it is discovered that the voltage test works in exactly the same way as the heating test – they’re basically the same test: \\(\\mathsfit{v}=\\mathsfit{h}\\). This means that if an element passes the heating test, then it has automatically passed the voltage test as well:\n\\[\\mathrm{P}(\\mathsfit{v}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{h}\\land \\mathsfit{Z}) = 1\\]\nor equivalently, \\(\\mathsfit{v}\\land \\mathsfit{h}= \\mathsfit{h}\\land \\mathsfit{h}= \\mathsfit{h}\\).\nNow suppose that inadvertently we give our AI decision agent the redundant information that the element has passed the heating test and (therefore) the voltage test. What will the agent say about the probability of early failure, given this duplicate information? Let’s calculate:\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\mathsfit{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{v}\\land \\mathsfit{h}\\land \\mathsfit{Z})&&\n\\\\[1ex]\n&\\qquad= \\frac{\\mathrm{P}(\\mathsfit{f}\\land \\mathsfit{v}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{h}\\land \\mathsfit{Z})}{\n\\mathrm{P}(\\mathsfit{v}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{h}\\land \\mathsfit{Z})\n}\n&&\\text{\\small ∧-rule}\n\\\\[1ex]\n&\\qquad= \\frac{\\mathrm{P}(\\mathsfit{f}\\land \\mathsfit{v}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{h}\\land \\mathsfit{Z})}{1}\n=\\mathrm{P}(\\mathsfit{f}\\land \\mathsfit{v}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{h}\\land \\mathsfit{Z})\n&&\\text{\\small initial probability}\n\\\\[1ex]\n&\\qquad= \\mathrm{P}(\\mathsfit{v}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{h}\\land \\mathsfit{Z}) \\cdot\n\\mathrm{P}(\\mathsfit{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{h}\\land \\mathsfit{Z})\n&&\\text{\\small ∧-rule}\n\\\\[1ex]\n&\\qquad= 1 \\cdot\n\\mathrm{P}(\\mathsfit{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{h}\\land \\mathsfit{Z})\n&&\\text{\\small truth cannot be altered}\n\\\\[1ex]\n&\\qquad= 0.1\n&&\\text{\\small initial probability}\n\\end{aligned}\n\\]\nLuckily the agent correctly detected the redundancy of the sentence \\(\\mathsfit{v}\\) (“the element passed the voltage test”) and automatically discarded it, thank to the truth-rule, or equivalently the and-idempotence rule.\n This feature is of paramount importance in machine learning and data-driven engineering: the “features” that we give as an input to a machine-learning classifier could contain redundancies that we don’t recognize owing to the complexity of the data space. But if the classifier makes inferences according to the four fundamental rules, it will automatically discard any redundant features."
  },
  {
    "objectID": "derived_rules.html#sec-extension-conversation",
    "href": "derived_rules.html#sec-extension-conversation",
    "title": "9  Shortcut rules",
    "section": "9.4 Law of total probability or “extension of the conversation”",
    "text": "9.4 Law of total probability or “extension of the conversation”\nSuppose we have a set of \\(n\\) sentences \\(\\set{\\mathsfit{Y}_1, \\mathsfit{Y}_2, \\dotsc, \\mathsfit{Y}_n}\\) having these two properties:\n\nThey are mutually exclusive, meaning that the “and” of any two of them is false, given some background knowledge \\(\\mathsfit{Z}\\):\n\\[\n  \\mathrm{P}(\\mathsfit{Y}_1\\land\\mathsfit{Y}_2\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}) = 0\\ , \\quad\n  \\mathrm{P}(\\mathsfit{Y}_1\\land\\mathsfit{Y}_3\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}) = 0\\ , \\quad\n\\dotsc \\ , \\quad\n  \\mathrm{P}(\\mathsfit{Y}_{n-1}\\land\\mathsfit{Y}_n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}) = 0\n  \\]\nThey are exhaustive, meaning that the “or” of all of them is true, given the background knowledge \\(\\mathsfit{Z}\\):\n\\[\n  \\mathrm{P}(\\mathsfit{Y}_1\\lor \\mathsfit{Y}_2 \\lor \\dotsb \\lor \\mathsfit{Y}_n \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}) = 1\n  \\]\n\nThen the probability of a sentence \\(\\mathsfit{X}\\), conditional on \\(\\mathsfit{Z}\\), is equal to a combination of probabilities conditional on \\(\\mathsfit{Y}_1,\\mathsfit{Y}_2,\\dotsc\\):\n\n\n\n\n\n\nDerived rule: extension of the conversation\n\n\n\n\n\\[\n\\begin{aligned}\n\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) =\n\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}_1 \\land \\mathsfit{Z})\\cdot \\mathrm{P}(\\mathsfit{Y}_1 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) +\n\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}_2 \\land \\mathsfit{Z})\\cdot \\mathrm{P}(\\mathsfit{Y}_2 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) +{}&\n\\\\[2ex]\n\\dotsb + \\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}_n \\land \\mathsfit{Z})\\cdot \\mathrm{P}(\\mathsfit{Y}_n \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})&\n\\end{aligned}\n\\]\n\n\n\nThis rule is useful when it is difficult to assess the probability of a sentence conditional on the background information, but it is easier to assess the probabilities of that sentence conditional on several auxiliary sentences – often representing hypotheses that exclude one another, and of which we know at least one is true. The name extension of the conversation for this derived rule comes from the fact that we are able to call the additional sentences into play.\nThis situation occurs very often in concrete applications, especially in problems where the probabilities of several competing hypotheses have to be assessed."
  },
  {
    "objectID": "derived_rules.html#sec-bayes-theorem",
    "href": "derived_rules.html#sec-bayes-theorem",
    "title": "9  Shortcut rules",
    "section": "9.5 Bayes’s theorem",
    "text": "9.5 Bayes’s theorem\nThe probably most famous – or infamous – rule derived from the laws of inference is Bayes’s theorem. It allows us to relate the probability where a sentence \\(\\mathsfit{Y}\\) appear in the proposal and another \\(\\mathsfit{X}\\) in the conditional, with the probability where they are exchanged:\n\n\n\n\n\nBayes’s theorem guest-starring in The Big Bang Theory\n\n\n\n\n\n\n\n\nDerived rule: Bayes’s theorem\n\n\n\n\n\\[\n\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}\\land \\mathsfit{Z}) =\n\\frac{\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}\\land \\mathsfit{Z})\\cdot \\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})}{\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})}\n\\]\n\n\n\nObviously this rule can only be used if \\(\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) &gt; 0\\), that is, if the sentence \\(\\mathsfit{X}\\) is not false conditional on \\(\\mathsfit{Z}\\).\n\n\n\n\n\n\n Exercise\n\n\n\nProve Bayes’s theorem from the fundamental rules of inference.\n\n\nBayes’s theorem is extremely useful when we want to assess the probability of a sentence, typically a hypothesis, given some conditional, typically data; and we can easily assess the probability of the data conditional on the hypothesis. Note, however, that the sentences \\(\\mathsfit{Y}\\) and \\(\\mathsfit{X}\\) in the theorem can be about anything whatsoever: \\(\\mathsfit{Y}\\) does not always need to be a “hypothesis”, and \\(\\mathsfit{X}\\) “data”.\n\n\n\n\n\n\n Study reading\n\n\n\n§ 8.8 of Rational Choice in an Uncertain World"
  },
  {
    "objectID": "derived_rules.html#sec-bayes-extension",
    "href": "derived_rules.html#sec-bayes-extension",
    "title": "9  Shortcut rules",
    "section": "9.6 Bayes’s theorem & extension of the conversation",
    "text": "9.6 Bayes’s theorem & extension of the conversation\nBayes’s theorem is often with several sentences \\(\\set{\\mathsfit{Y}_1, \\mathsfit{Y}_2, \\dotsc, \\mathsfit{Y}_n}\\) that are mutually exclusive and exhaustive. Typically these represent competing hypotheses. In this case the probability of the sentence \\(\\mathsfit{X}\\) in the denominator can be expressed using the rule of extension of the conversation:\n\n\n\n\n\n\n\nDerived rule: Bayes’s theorem with extension of the conversation\n\n\n\n\n\\[\n\\mathrm{P}(\\mathsfit{Y}_1 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}\\land \\mathsfit{Z}) =\n\\frac{\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}_1 \\land \\mathsfit{Z})\\cdot \\mathrm{P}(\\mathsfit{Y}_1 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})}{\n\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}_1 \\land \\mathsfit{Z})\\cdot \\mathrm{P}(\\mathsfit{Y}_1 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) +\n\\dotsb + \\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}_n \\land \\mathsfit{Z})\\cdot \\mathrm{P}(\\mathsfit{Y}_n \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n}\n\\]\n\nand similarly for \\(\\mathsfit{Y}_2\\) and so on.\n\n\n\nWe will use this form of Bayes’s theorem very frequently."
  },
  {
    "objectID": "derived_rules.html#the-many-facets-of-bayess-theorem",
    "href": "derived_rules.html#the-many-facets-of-bayess-theorem",
    "title": "9  Shortcut rules",
    "section": "9.7 The many facets of Bayes’s theorem",
    "text": "9.7 The many facets of Bayes’s theorem\nBayes’s theorem is a very general result of the fundamental rules of inference, valid for any sentences \\(\\mathsfit{X},\\mathsfit{Y},\\mathsfit{Z}\\). This generality leads to many uses and interpretations.\nThe theorem is often proclaimed to be the rule according to which we “update our beliefs”. The meaning of this proclamation is the following. Let’s say that at some point \\(\\mathsfit{Z}\\) represents all your knowledge. Your degree of belief about some sentence \\(\\mathsfit{Y}\\) is then (at least in theory) the value of \\(\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\\). At some later point, let’s say that you get to know – maybe thanks to an observation you made – that the sentence \\(\\mathsfit{X}\\) is true. Your whole knowledge at that point is represented no longer by \\(\\mathsfit{Z}\\), but by \\(\\mathsfit{X}\\land \\mathsfit{Z}\\). Your degree of belief about \\(\\mathsfit{Y}\\) is then given by the value of \\(\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}\\land\\mathsfit{Z})\\). Bayes’s theorem allows you to find your degree of belief about \\(\\mathsfit{Y}\\) conditional on your new state of knowledge, from the one conditional on your old state of knowledge.\nThis chronological element, however, comes only from this particular way of using Bayes’s theorem. The theorem can more generally be used to connect any two states of knowledge \\(\\mathsfit{Z}\\) and \\(\\mathsfit{X}\\land\\mathsfit{Z}\\), no matter their temporal order, even if they happen simultaneously, and even if they belong to two different agents.\n\n\n\n\n\n\n Exercise\n\n\n\nUsing Bayes’s theorem and the fundamental laws of inference, prove that if \\(\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})=1\\), that is, if you already know that \\(\\mathsfit{X}\\) is true in your current state of knowledge \\(\\mathsfit{Z}\\), then\n\\[\n\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}\\land \\mathsfit{Z}) = \\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\]\nthat is, your degree of belief about \\(\\mathsfit{Y}\\) doesn’t change.\nIs this result reasonable?\n\n\n\n\n\n\n\n\n Study reading\n\n\n\n\n§§ 4.1–4.3 in Medical Decision Making give one more point of view on Bayes’s theorem.\nCh. 3 of Probability\nA graphical explanation of how Bayes’s theorem works mathematically (using a specific interpretation of the theorem):"
  },
  {
    "objectID": "monty.html#sec-monty-motivation",
    "href": "monty.html#sec-monty-motivation",
    "title": "10  Monty Hall and related inference problems",
    "section": "10.1 Motivation: calculation vs intuition",
    "text": "10.1 Motivation: calculation vs intuition\nThe “Monty Hall problem”, inspired by the TV show Let’s make a deal! hosted by Monty Hall, was proposed in the Parade magazine in 1990 (the numbers of the doors are changed here):\n\nSuppose you are on a game show and given a choice of three doors. Behind one is a car; behind the others are goats. You pick door No. 1, and the host, who knows what is behind them [and wouldn’t open the door with the car], opens No. 2, which has a goat. He then asks if you want to pick No. 3. Should you switch?\n\n\n\n\nThe web is full of insightful intuitive solutions and of informal probability discussions about this inference problem. Our purpose here is different: we want to solve it mechanically, just by applying the fundamental rules of inference (§  8.4) and the shortcut rules (§  9) derived from them. This purpose has two main reasons:\n\nWe want to be able to implement or encode the procedure algorithmically in an AI agent.\nWe generally cannot ground inferences on intuition. Intuition is shaky ground, and hopeless in data-science inference problems involving millions of data with thousands of numbers. To solve such complex problems we need to use a more mechanical procedure mathematically guaranteed to be self-consistent. That’s the probability calculus. Intuition is useful either for arriving at a method which we can eventually prove, by mathematical and logical means, to be correct; or for approximately explaining a method that we already know, again by mathematical and logical means, to be correct.\n\n\n\n\n\n\n\n Misleading intuition in high dimensions\n\n\n\nAs an example of our intuition can be completely astray in problems involving many data dimensions, consider the following fact.\nTake a one-dimensional Gaussian distribution of probability. You have probably heard that the probability that a data point is within a distance of three standard deviations from the peak is approximately \\(99.73\\%\\). If we take a two-dimensional (symmetric) Gaussian distribution, the probability that a data point (two real numbers) is within three standard deviations from the peak is \\(98.89\\%\\) – slightly less that the one-dimensional case. For a three-dimensional Gaussian, the probability that a data point (three real numbers) is within three standard deviations from the peak is \\(97.07\\%\\) – slightly smaller yet.\nNow try to answer this question: for a 100-dimensional Gaussian, what is the probability that a data point is within three standard deviations from the peak? The answer is \\(\\boldsymbol{(1.83 \\cdot 10^{-32})\\%}\\). This probability is so small that you would never observe a data point within three standard deviations from the peak, even if you checked one data point every second for the same duration as the present age of the universe – which is “only” around \\(4\\cdot 10^{17}\\) seconds.\n\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nFor further examples of our intuition leads us astray in high dimensions see\n\nCounterintuitive Properties of High Dimensional Space\nExercise 2.20 (and its solution) in Information Theory, Inference, and Learning Algorithms\n\n\n\nIt is instructive, however, if you also check what your intuition told you about the problem:\n\n\n\n\n\n\n Exercise\n\n\n\nExamine what your intuition tells you the answer should be, without spending too much time thinking, just as if you were on the game show. Examine which kind of heuristics your intuition uses. If you already know the solution to this puzzle, try to remember what your intuition told you the first time you faced it. Keep your observations in mind for later on."
  },
  {
    "objectID": "monty.html#sec-monty-agent",
    "href": "monty.html#sec-monty-agent",
    "title": "10  Monty Hall and related inference problems",
    "section": "10.2 Which agent? whose knowledge?",
    "text": "10.2 Which agent? whose knowledge?\nA sentence can be assigned different probabilities by different agents having different background information, although in some cases different background information can still lead to numerically equal probabilities.\nIn the present case, who’s the agent solving the inference problem? And what background information does it have?\nFrom the problem statement it sounds like you are the agent; but we can imagine that you have programmed an AI agent having your same background information, and ready to make the decision for you.\nWe must agree on which background information \\(\\mathsfit{K}\\) to give to this agent. Let’s define \\(\\mathsfit{K}\\) as the knowledge you have right before picking door 1. We make this choice so that we can add your door pick as additional information."
  },
  {
    "objectID": "monty.html#sec-monty-sentences",
    "href": "monty.html#sec-monty-sentences",
    "title": "10  Monty Hall and related inference problems",
    "section": "10.3 Define the atomic sentences relevant to the problem",
    "text": "10.3 Define the atomic sentences relevant to the problem\nThe following sentences seem sufficient:\n\\[\n\\begin{aligned}\n\\mathsfit{K}&\\coloneqq\\text{\\small[the background knowledge discussed in the previous section]}\n\\\\[1ex]\n\\mathsfit{\\small car1} &\\coloneqq\\textsf{\\small`The car is behind door 1'}\n\\\\\n\\mathsfit{\\small you1} &\\coloneqq\\textsf{\\small`You initially pick door 1'}\n\\\\\n\\mathsfit{\\small host2} &\\coloneqq\\textsf{\\small`The host opens door 2'}\n\\\\\n&\\text{\\small and similarly for the other door numbers}\n\\end{aligned}\n\\]\nWe could have used other symbols for the sentences, for instance “\\(C_1\\)” instead of “\\(\\mathsfit{\\small car1}\\)”; the specific symbol choice doesn’t matter. We could also have stated the sentences slightly differently, for instance “You choose door 1 at the beginning of the game”; what’s important is that we understand and agree on the meaning of the atomic sentences above."
  },
  {
    "objectID": "monty.html#sec-monty-goal",
    "href": "monty.html#sec-monty-goal",
    "title": "10  Monty Hall and related inference problems",
    "section": "10.4 Specify the desired inference",
    "text": "10.4 Specify the desired inference\nWe want the probability of the sentences \\(\\mathsfit{\\small car1}\\), \\(\\mathsfit{\\small car2}\\), \\(\\mathsfit{\\small car3}\\), given the knowledge that you picked door 1 (\\(\\mathsfit{\\small you1}\\)), that the host opened door 2 (\\(\\mathsfit{\\small host2}\\)), and the remaining background knowledge (\\(\\mathsfit{K}\\)). So in symbols we want the values of the following probabilities:\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\\n&\\mathrm{P}(\\mathsfit{\\small car2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\\n&\\mathrm{P}(\\mathsfit{\\small car3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\end{aligned}\n\\]\nYou may object: “but we already know that there’s no car behind door 2, the one opened by the host; so that probability is 0%”. That’s correct, but how did you arrive at that probability value? Remember our goal: to solve this inference mechanically. That intuitive probability therefore must either appear as an initial probability, or be derived via the inference rules. No intuitive shortcuts."
  },
  {
    "objectID": "monty.html#sec-monty-prior",
    "href": "monty.html#sec-monty-prior",
    "title": "10  Monty Hall and related inference problems",
    "section": "10.5 Specify all initial probabilities",
    "text": "10.5 Specify all initial probabilities\nAs discussed in §  5.2, any inference – logical or uncertain – can only be derived from other inferences, or taken for granted as a starting point (“initial probability”, or “axiom” in logic). The only inferences that don’t need any initial probabilities are tautologies. We must write down explicitly the initial probabilities implicit in the present inference problem:\n\nThe car is for sure behind one of the three doors, and cannot be behind more than one door:\n\\[\n\\begin{gathered}\n\\mathrm{P}(\\mathsfit{\\small car1} \\lor \\mathsfit{\\small car2} \\lor \\mathsfit{\\small car3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) = 1\n\\\\[1ex]\n\\mathrm{P}(\\mathsfit{\\small car1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small car2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) =\n\\mathrm{P}(\\mathsfit{\\small car1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small car3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) =\n\\mathrm{P}(\\mathsfit{\\small car2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small car3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) = 0\n\\end{gathered}\n\\]\nRemember from the shortcut rule for the stability of truth and falsity (§  9.1) that the \\(1\\) and \\(0\\) probabilities above do not change if we and additional information to \\(\\mathsfit{K}\\).\nThe host cannot open the door you picked or the door with the car. This translates in several initial probabilities; here are some:\n\\[\\begin{gathered}\n\\mathrm{P}(\\mathsfit{\\small host2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) = 0\n\\\\[1ex]\n\\mathrm{P}(\\mathsfit{\\small host1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) =\n\\mathrm{P}(\\mathsfit{\\small host3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) = 0\n  \\end{gathered}\n  \\]\nThe host must open one door, and cannot open more than one door:\n\\[\n\\begin{gathered}\n\\mathrm{P}(\\mathsfit{\\small host1} \\lor \\mathsfit{\\small host2} \\lor \\mathsfit{\\small host3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) = 1\n\\\\[1ex]\n\\mathrm{P}(\\mathsfit{\\small host1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small host2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) =\n\\mathrm{P}(\\mathsfit{\\small host1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small host3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) =\n\\mathrm{P}(\\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small host3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) = 0\n\\end{gathered}\n\\]\n\n\n\nThe probabilities above are all quite clear from the description of the inference problem. But implicit in that description are some more probabilities that will be needed in our inference. The values of these probabilities can be more open to debate, because the problem, as stated, provides ambiguous information. Later you shall explore possible alternative values for these probabilities.\n\nIt is equally probable that the car is behind any of the three doors, and your initial pick doesn’t change this uncertainty:\n\\[\\begin{aligned}\n\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) &= \\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) = 1/3\n\\\\\n\\mathrm{P}(\\mathsfit{\\small car2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) &= \\mathrm{P}(\\mathsfit{\\small car2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) = 1/3\n\\\\\n\\mathrm{P}(\\mathsfit{\\small car3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) &= \\mathrm{P}(\\mathsfit{\\small car3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) = 1/3\n  \\end{aligned}\n  \\]\nIf the host can choose between two doors (because the car is behind the door you picked initially), we are equally uncertain about the choice:\n\\[\n\\mathrm{P}(\\mathsfit{\\small host2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) =\n\\mathrm{P}(\\mathsfit{\\small host3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) = 1/2\n  \\]\n\nThis probability could be analysed into further hypotheses. Maybe the host, out of laziness, could more probably open the door that’s closer. But from the problem it isn’t fully clear which one is closer. The host could also more probably open the door that’s further from the one you choose. The host could have a predetermined scheme on which door to open. The hypotheses are endless; we can imagine some making \\(\\mathsfit{\\small host2}\\) more probable, and some making \\(\\mathsfit{\\small host3}\\) more probable, conditional on \\(\\mathsfit{\\small you1} \\land \\mathsfit{\\small car1} \\land \\mathsfit{K}\\). The probability of 50% seems like a good compromise. Later we shall examine the effects of changing this probability.\n\nSome peculiar probabilities\nWe defined the background knowledge \\(\\mathsfit{K}\\) as the one you have right before choosing door 1. We did this so that the sentence \\(\\mathsfit{\\small you1}\\), expressing your door pick, can be added as additional information: \\(\\mathsfit{\\small you1}\\land \\mathsfit{K}\\).\nThen it is legitimate to ask: what is the probability that you pick door 1, given only the background information \\(\\mathsfit{K}\\):\n\\[\\mathrm{P}(\\mathsfit{\\small you1}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\\ ?\\]\nTo answer this question we would need to specify \\(\\mathsfit{K}\\) more in detail. For instance it is possible that you planned to pick door 1 already the day before. In this case we would have \\(\\mathrm{P}(\\mathsfit{\\small you1}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) = 1\\) or very nearly so. Or you could pick door 1 right on the spot, with no clear conscious thought process behind your choice. In this case we would have \\(\\mathrm{P}(\\mathsfit{\\small you1}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) = 1/3\\) or a similar value.\nLuckily in the present problem these probabilities are not needed or, if they are used, their numerical values turn out not to matter (they “cancel out”).\n\n\n\n\n\n\n Silly literature\n\n\n\nSome texts on probability say that if you have decided something and therefore know it in advance for certain, then the probability of that something is undefined “because it is not random”. Obviously this is nonsense. If you already know something, then the probability of that something is well-defined and its value is \\(100%\\) – or something short of this value, if you want to make allowance for the occurrence of unplanned events."
  },
  {
    "objectID": "monty.html#sec-monty-solution",
    "href": "monty.html#sec-monty-solution",
    "title": "10  Monty Hall and related inference problems",
    "section": "10.6 Solution",
    "text": "10.6 Solution\nLet’s try first to calculate \\(\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\\), that is, the probability that the car is behind the door you picked.\nSeeing that we have several initial probabilities of the “\\(\\mathrm{P}(\\mathsfit{\\small host} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small car} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\\)” form, we can use Bayes’s theorem together with the “extension of the conversation” (§  9.6) to swap the positions of “\\(\\mathsfit{\\small car}\\)” and “\\(\\mathsfit{\\small host}\\)” sentences between supposal and conditional. In the present case the exhaustive and mutually exclusive sentences are \\(\\mathsfit{\\small car1}\\), \\(\\mathsfit{\\small car2}\\), \\(\\mathsfit{\\small car3}\\):\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\[1ex]\n&\\qquad=\\frac{\n{\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{\\small host2}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})} \\cdot\n{\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})}\n}{\n\\enspace\\left[\\,\\begin{gathered}\n{\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{\\small host2}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})} \\cdot\n{\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})} +{}\\\\\n{\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{\\small host2}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})} \\cdot\n{\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{\\small car2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})} +{}\\\\\n\\mathrm{P}(\\mathsfit{\\small host2}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) \\cdot\n{\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{\\small car3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})}\n\\end{gathered}\\,\\right]\\enspace\n}\n\\\\[1ex]\n&\\qquad=\\dotso\n\\end{aligned}\n\\]\nAll probabilities in green are initial probabilities discussed in the previous steps. Let’s substitute their values:\n\\[\n\\begin{aligned}\n&\\qquad=\\frac{\n{\\color[RGB]{34,136,51}1/2} \\cdot\n{\\color[RGB]{34,136,51}1/3}\n}{\n\\enspace\\left[\\,\\begin{gathered}\n{\\color[RGB]{34,136,51}1/2} \\cdot\n{\\color[RGB]{34,136,51}1/3} +{}\\\\\n{\\color[RGB]{34,136,51}0} \\cdot\n{\\color[RGB]{34,136,51}1/3} +{}\\\\\n\\mathrm{P}(\\mathsfit{\\small host2}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) \\cdot\n{\\color[RGB]{34,136,51}1/3}\n\\end{gathered}\\,\\right]\\enspace\n}\n\\\\[1ex]\n&\\qquad=\\frac{ 1/6\n}{\n1/6 +\n\\mathrm{P}(\\mathsfit{\\small host2}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) \\cdot\n1/3\n}\n\\\\[1ex]\n&\\qquad=\\dotso\n\\end{aligned}\n\\]\nAll that’s left is to find \\(\\mathrm{P}(\\mathsfit{\\small host2}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\\). It’s intuitively clear that this probability is 100%, because the host is forced to choose door 2 if you picked door 1 and the car is behind door 3. But our purpose is to make a full derivation starting from the initial probabilities only. We can find this probability by applying the or-rule and the and-rule to the probabilities that the host opens at least one door and cannot open more than one:\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\mathsfit{\\small host2}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\[1ex]\n&\\qquad=\n\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{\\small host2} \\lor \\mathsfit{\\small host1} \\lor \\mathsfit{\\small host3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\\n&\\qquad\\quad{}-\n\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{\\small host1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\\n&\\qquad\\quad{}-\n\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{\\small host3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\\n&\\qquad\\quad{}+\n\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{\\small host1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small host2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\\n&\\qquad\\quad{}+\n\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{\\small host1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small host3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\\n&\\qquad\\quad{}+\n\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small host3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\\n&\\qquad\\quad{}-\n\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{\\small host1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small host3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\[1ex]\n&\\qquad= 1 - 0 - 0 + 0 + 0 + 0 - 0 = 1\n\\end{aligned}\n\\]\nas expected.\nFinally, using this probability in our previous calculation we find\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\[1ex]\n&\\qquad=\\frac{ 1/6\n}{\n1/6 +\n\\mathrm{P}(\\mathsfit{\\small host2}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) \\cdot\n1/3\n}\n\\\\[1ex]\n&\\qquad=\\frac{ 1/6\n}{\n1/6 +\n1 \\cdot\n1/3\n}\n= \\frac{1/6}{3/6} = \\boldsymbol{\\frac{1}{3}}\n\\end{aligned}\n\\]\nthat is, there’s a 1/3 probability that the car is behind the door we picked!\n\n\nWhat about door 3, that is, the probability \\(\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\\)? Also in this case we can use Bayes’s theorem with the extension of the conversation. The calculation is immediate, because we have already calculated all the relevant pieces:\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\mathsfit{\\small car3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\[1ex]\n&\\qquad=\\frac{\n\\mathrm{P}(\\mathsfit{\\small host2}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) \\cdot\n\\mathrm{P}(\\mathsfit{\\small car3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n}{\n\\enspace\\left[\\,\\begin{gathered}\n\\mathrm{P}(\\mathsfit{\\small host2}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) \\cdot\n\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) +{}\\\\\n\\mathrm{P}(\\mathsfit{\\small host2}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) \\cdot\n\\mathrm{P}(\\mathsfit{\\small car2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) +{}\\\\\n\\mathrm{P}(\\mathsfit{\\small host2}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) \\cdot\n\\mathrm{P}(\\mathsfit{\\small car3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\end{gathered}\\,\\right]\\enspace\n}\n\\\\\n&\\qquad=\\frac{\n1 \\cdot\n1/3\n}{\n\\enspace\\left[\\,\\begin{gathered}\n1/2 \\cdot\n1/3 +{}\\\\\n0 \\cdot\n1/3 +{}\\\\\n1\n\\cdot\n1/3\n\\end{gathered}\\,\\right]\\enspace\n}\n\\\\[1ex]\n&\\qquad=\\frac{1/3}{1/2} = \\boldsymbol{\\frac{2}{3}}\n\\end{aligned}\n\\]\nthat is, there’s a 2/3 probability that the car is behind door 3. If we’d like to win the car we should then switch doors.\n\n\n\n\n\n\n Exercise\n\n\n\nPerform a similar calculation to find \\(\\mathrm{P}(\\mathsfit{\\small car2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\\)"
  },
  {
    "objectID": "monty.html#sec-monty-remarks",
    "href": "monty.html#sec-monty-remarks",
    "title": "10  Monty Hall and related inference problems",
    "section": "10.7 Remarks on the use of Bayes’s theorem",
    "text": "10.7 Remarks on the use of Bayes’s theorem\nThe previous calculations may have been somewhat boring; but, again, the purpose was to see with our own eyes that the final result comes from the application of the four fundamental laws of inference to the initial probabilities – and from nothing else.\nYou notice that at several points our calculations could have taken a different path. For instance, in order to find \\(\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\\) we applied Bayes’s theorem to swap the sentences \\(\\mathsfit{\\small car1}\\) and \\(\\mathsfit{\\small host2}\\) in their supposal and conditional positions. Couldn’t we have swapped \\(\\mathsfit{\\small car1}\\) and \\(\\mathsfit{\\small host2}\\land \\mathsfit{\\small you1}\\) instead? That is, couldn’t we have made a calculation starting with\n\\[\n\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n=\\frac{\n\\mathrm{P}(\\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) \\cdot\n\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n}{\\dotso} \\enspace ?\n\\]\nafter all this is also a legitimate application of Bayes’s theorem.\nThe answer is: yes, we could have; and the final result would have been the same. The self-consistency of the probability calculus guarantees that there are no “wrong steps”, as long as every step is an application of one of the four fundamental rules (or of their shortcuts). The worst that can happen is that we take a longer route – but to exactly the same result. In fact it’s possible that there’s a shorter calculation route to arrive at the probabilities that we found in the previous section. But it doesn’t matter, because it would lead to the same result."
  },
  {
    "objectID": "monty.html#sec-monty-sensitivity",
    "href": "monty.html#sec-monty-sensitivity",
    "title": "10  Monty Hall and related inference problems",
    "section": "10.8 Sensitivity analysis",
    "text": "10.8 Sensitivity analysis\nIn §  10.5 we briefly discussed possible interpretations or variations of the Monty Hall problem, for which the probability that the host chooses among the available doors 2 and 3 (if the car is behind the door you picked) is different from 50%.\nWhen we are curious to know how an initial probability value can affect the final probabilities, we can leave its value as a variable, and check how the final probabilities change as we change this variable. This procedure is often called sensitivity analysis. Try to do a sensitivity analysis for the Monty Hall problem:\n\n\n\n\n\n\n Exercise\n\n\n\nInstead of assuming\n\\[\\mathrm{P}(\\mathsfit{\\small host2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) =\n\\mathrm{P}(\\mathsfit{\\small host3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) = 1/2\\]\nassign a generic variable value \\(p\\)\n\\[\\mathrm{P}(\\mathsfit{\\small host2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) = p\n\\qquad\n\\mathrm{P}(\\mathsfit{\\small host3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) = 1-p\\]\nwhere \\(p\\) could be any value between \\(0\\) and \\(1\\).\n\nCalculate \\(\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\\) as done in the previous sections, but keeping \\(p\\) as a generic variable. You therefore well find a probability \\(\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\\) that depend numerically on \\(p\\); it could be considered as a function of \\(p\\)\nPlot how the value of \\(\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\\) depends on \\(p\\), as the latter ranges from \\(0\\) to \\(1\\).\nFor which range of values of \\(p\\) is it convenient to switch door, that is, \\(\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) &lt; 1/2\\) ?\nImagine and describe alternative scenarios or background information that would lead to specific values of \\(p\\) different from \\(0.5\\)."
  },
  {
    "objectID": "monty.html#sec-monty-variations",
    "href": "monty.html#sec-monty-variations",
    "title": "10  Monty Hall and related inference problems",
    "section": "10.9 Variations and further exercises",
    "text": "10.9 Variations and further exercises\n\n\n\n\n\n\n Exercise: other variations\n\n\n\n\nIn §  10.2 we decided that the agent in this inference was you, with the knowledge \\(\\mathsfit{K}\\) right before you picked door 1. Try to change the agent: do you arrive at different probabilities?\n\nConsider a person in the audience, right before you picked door 1, as the agent, and re-solve the problem, adjusting all initial probabilities as needed.\nConsider the host as the agent, right before you picked door 1, and re-solve the problem, adjusting all initial probabilities as needed. Note that the host knows for certain where the car is, so you need to provide this additional, secret information. Consider the cases where the car is behind door 1 and behind door 3.\n\n\n\n\n\nSuppose a friend of yours backstage gave you partial information about the location of the car (you cheater!), which makes you believe that the car should be closer to door 1, and assign the probabilities\n\\[\\begin{aligned}\n\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}') &= \\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}') = 1/3 + q\n\\\\\n\\mathrm{P}(\\mathsfit{\\small car2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}') &= \\mathrm{P}(\\mathsfit{\\small car2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}') = 1/3\n\\\\\n\\mathrm{P}(\\mathsfit{\\small car3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}') &= \\mathrm{P}(\\mathsfit{\\small car3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}') = 1/3 - q\n  \\end{aligned}\n  \\]\nwith \\(0 \\le q \\le 1/3\\) (this is different background information, so we denote \\(\\mathsfit{K}'\\)). Re-solve the problem keeping the variable \\(q\\), and find if there’s any value for \\(q\\) for which it’s best to keep door 1.\n\n\n\n\n\n\n\n\n\n\n\n Exercise: making decisions\n\n\n\nIn this chapter we only solved the inference problem for the Monty Hall scenario: we calculated the probabilities of various outcomes. No decision has been made yet.\n\nAssign utilities to winning the car or winning the goat from the point of view of an agent who values the car more. The available decisions are, of course, “keep door 1” vs “switch to door 3”. Then solve the decision-making problem according to the procedure of §  3.3. What’s the optimal decision?\nNow assign utilities from the point of view of an agent who values the goat more than the car. Then solve the decision-making problem according to the usual procedure. What’s the optimal decision?\n\n\n\n\n\n\n\n\n\n\n\n Exercise: the Sleeping Beauty problem\n\n\n\nTake a look at the inference problem presented in this video:\n\nand try to solve it, using not intuition but the mechanical procedure and steps as in the Monty Hall solution above.\nNote that the video asks “What do you believe is the probability that the coin came up heads?”. Since probability and degree of belief are the same thing, that is like asking “What do you believe is your belief that the coin came up heads?” which is a redundant or quirky question. Instead, simply answer the question “What is your degree of belief (that is, probability) that the coin came up heads?”."
  },
  {
    "objectID": "connection-2-ML.html#sec-1stconn-inference",
    "href": "connection-2-ML.html#sec-1stconn-inference",
    "title": "11  Second connection with machine learning",
    "section": "11.1 “Learning” and “output” from the point of view of inference & decision",
    "text": "11.1 “Learning” and “output” from the point of view of inference & decision\nThe remarks above reveal similarities with what an agent does when drawing an inference: it uses known pieces of information, expressed by sentences \\({\\color[RGB]{34,136,51}\\mathsfit{D}_1}, {\\color[RGB]{34,136,51}\\mathsfit{D}_2}, {\\color[RGB]{34,136,51}\\dots}, {\\color[RGB]{34,136,51}\\mathsfit{D}_N}\\), together with some background or built-in information \\(\\color[RGB]{204,187,68}\\mathsfit{I}\\), in order to calculate the probability of a piece of information of a similar kind, expressed by a sentence \\(\\color[RGB]{238,102,119}\\mathsfit{D}_{N+1}\\):\n\\[\n\\mathrm{P}(\\color[RGB]{238,102,119}\\mathsfit{D}_{N+1}\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\\mathsfit{D}_{N} \\land \\dotsb \\land \\mathsfit{D}_2 \\land \\mathsfit{D}_1 \\color[RGB]{0,0,0}\\land {\\color[RGB]{204,187,68}\\mathsfit{I}})\n\\]\nWe can thus consider a first tentative correspondence:\n\\[\n\\mathrm{P}(\\underbracket[0ex]{\\color[RGB]{238,102,119}\\mathsfit{D}_{N+1}}_{\\mathclap{\\color[RGB]{238,102,119}\\text{outcome?}}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\\underbracket[0ex]{\\mathsfit{D}_N \\land \\dotsb \\land \\mathsfit{D}_2 \\land \\mathsfit{D}_1}_{\\mathclap{\\color[RGB]{34,136,51}\\text{training data?}}}\n\\color[RGB]{0,0,0}\\land \\underbracket[0ex]{\\color[RGB]{204,187,68}\\mathsfit{I}}_{\\mathrlap{\\color[RGB]{204,187,68}\\uparrow\\ \\text{architecture?}}})\n\\]\nThis correspondence seems convincing with regard to architecture and training data: in both cases we’re speaking about the use of pre-existing or built-in information, combined with additional one.\nBut it is less convincing with regarding to the outcome, because an agent gives the probabilities for several possible “outputs”, it doesn’t just yield one. This indicates that there must be also be some decision involved among the possible outcomes.\nWe’ll return to this tentative connection later."
  },
  {
    "objectID": "connection-2-ML.html#sec-1stconn-outputs",
    "href": "connection-2-ML.html#sec-1stconn-outputs",
    "title": "11  Second connection with machine learning",
    "section": "11.2 Why different outputs?",
    "text": "11.2 Why different outputs?\nIn the past chapters we have seen, over and over, what was claimed in the introduction to the present lecture notes: that an inference & decision problem has only one optimal solution. Once we specify the utilities and the initial probabilities of the problem, the fundamental rules of inference and the principle of maximal expected utility leads to one unique answer (unless, of course, there are several optimal ones with equal expected utilities).\nDifferent machine-learning algorithms, trained with the same training data, often give different answers or outputs to the same problem. Where do these differences come from? From the point of view of decision theory there are three possibilities, which don’t exclude one another:\n\nThe initial probabilities given to the algorithms are different. Since the training data are the same, this means that the background information built into one machine-learning algorithm is different from those built into another.\nIt is therefore important to understand what are the built-in background information and initial probabilities of different machine-learning algorithms. The built-in assumptions of an algorithm must match those of the real problem as closely as possible, in order to avoid sub-optimal or even disastrously wrong answers and outputs.\nThe utilities built into one machine-learning algorithm are different from those built into another.\nIt is therefore also important to understand what are the built-in utilities of different machine-learning algorithms. The built-in utilities must also match those of the real problem as closely as possible.\nThe calculations made by the algorithms are approximate, and different algorithms make different kinds of approximations. This means that the algorithms don’t arrive at the unique answer determined by decision theory, but to some other answers which may be approximately close to the correct one and to one another – or not.\nIt is therefore important to understand what are the calculation approximations made by different machine-learning algorithms. Some approximations may be too crude for some real problems, and may again lead to sub-optimal or even disastrously wrong answers and outputs."
  },
  {
    "objectID": "connection-2-ML.html#sec-1stconn-preprocess",
    "href": "connection-2-ML.html#sec-1stconn-preprocess",
    "title": "11  Second connection with machine learning",
    "section": "11.3 Data pre-processing and the data-processing inequality",
    "text": "11.3 Data pre-processing and the data-processing inequality\n“Data pre-processing” is a collective name given to very different operations on data before they are used in some algorithm to solve a decision or inference problem. Some of these operations are often said to be “essential” or “crucial” for the solution of these problems. This statement in not completely true, and needs qualification.\nWe can divide pre-processing procedures in roughly three categories:\n\nInconsistency checks\n\nProcedures in this category make sure that the data are what they were intended to be. For instance, if data should consist of the power outputs of several engines, but one datapoint is the physical weight of an engine, then that “datapoint” is actually no data at all for the present problem. It’s something included by mistake and should be removed. Similarly if, say, data about distances should be expressed in metres, but some turn out to be expressed in kilometres (this case overlaps with formatting procedures described below). Such procedures are necessary and useful, but they are just consistency checks and do not change the information contained in the proper data.\n\n\n\n\n\n\n\n\n\n\n\n\nIn later chapters we shall say more about some often erroneous procedures, like “tail trimming”, that actually remove proper data, lead to sub-optimal or completely erroneous solutions.\n\n\n\nFormatting\n\nThese procedures make sure that data are in the correct format to be inputted into the algorithm. They may also include rescaling of numerical values for avoiding numerical overflow or underflow errors during computation. Such procedures are often necessary and useful, but they just change the way data are encoded, possibly including the zero and unit of measurement; they do not actually change the information contained in the data.\n\n“Mutilation” or information-alteration\n\nProcedures of this kind alter the content of proper data. For instance, such a procedure may replace, in a data set of temperatures, a datapoint having value 20 °C with one having value 25 °C; this is not just a simple rescaling. These procedures include “de-noising”, “de-biasing”, “de-trending”, “filtering”, “dimensionality reduction” and similar procedures (often having noble-sounding names). We must state, clearly and strongly, that within Decision Theory and Probability Theory, such information-altering pre-processing is not necessary , and is in fact detrimental; this is why we call it “mutilation” here.\n\n\nIt is important that you understand that such data pre-processing is not something that one has to do in data science in general – quite the opposite, in principle you should not do it, because it is a destructive procedure. Such pre-processing is done in order to correct deficiencies of the algorithms currently in use, as discussed below.\nIf we build an “optimal predictor machine” that fully operates according to the four rules of inference (§  8.4) and of maximization of expected utility, then the data fed into this machine should not be pre-processed with any information-altering procedures. The reason is that the four fundamental rules automatically take care of factors such as noise, bias, systematic errors, redundancy in the optimal way. We briefly discussed in §  9.3 and saw a simple example of how redundancy is accounted for by the four rules.\nIf we have information about noise or other factors affecting the data, then we should include this information in the background information provided to the “optimal predictor machine”, rather than altering the data given to it. The reason, in intuitive terms, is that the machine does the adjustments while fully exploring the data themselves, so it can more deeply “see” how to make optimal adjustments given the “inner structure” of the data. In the pre-processing phase – as the prefix “pre-” indicates – we don’t have the full picture about the data, so any adjustment risks eliminating actually useful information and is always sub-optimal.\nMore formally, this is the content of the data-processing inequality from information theory:\n\n\n\n\n\n\nData-processing inequality\n\n\n\n\n“No clever manipulation of the data can improve the inferences that can be made from the data”\n(Elements of Information Theory § 2.8)\nor, from the complementary point of view:\n“Data processing can only destroy information”\n(Information Theory, Inference, and Learning Algorithms exercise 8.9)\n\n\n\n\n\n\n\n\n\n Study reading\n\n\n\n\nSkim through § 2.8 of Elements of Information Theory\nTake a look Exercise 8.9 and its solution in Information Theory, Inference, and Learning Algorithms\n\n\n\n\n\nThere are two main, partially connected reasons why one performs “mutilation” pre-processing of data:\n\nThe algorithm used is non-optimal: it’s only using approximations of the four fundamental rules, and therefore cannot remove noise, bias, redundancies, and similar in an optimal way – or at all. In this case, pre-processing is an approximate way of correcting this deficiency of the non-optimal algorithm.\nFull optimal processing is computationally too expensive. In this case we try to simplify the optimal calculation by doing in advance and in a cruder, faster way some of the “cleaning” that the full calculation would otherwise spend time doing in an optimal way."
  },
  {
    "objectID": "quantities_types.html#motivation-for-the-data-i-part",
    "href": "quantities_types.html#motivation-for-the-data-i-part",
    "title": "12  Quantities and data types",
    "section": "Motivation for the “Data I” part",
    "text": "Motivation for the “Data I” part\nIn the “Inference I” part we surveyed the four fundamental rules of inference, which determine how the degrees of belief of an agent should propagate and be mutually consistent, and we explored some of their consequences and applications. The rules can be used with any sentences whatsoever, so their application can be developed in detail in a wide variety of directions, with applications ranging from robotics to psychology. Each of these possible developments would require a full university course by itself.\nWe shall now restrict our attention to applications typical of engineering, data science, and machine learning, such as classification, forecast, prognosis, and hypothesis testing, in situations that involve quantifiable and measurable phenomena. For this purpose we focus on sentences of particular kinds, which can express such quantification and measurement. In a sense, we develop a specialized “language” for this kind of situations.\nStill, since we’re dealing with sentences, the probability calculus and inference rules apply without changes of any kind."
  },
  {
    "objectID": "quantities_types.html#quantities",
    "href": "quantities_types.html#quantities",
    "title": "12  Quantities and data types",
    "section": "12.1 Quantities",
    "text": "12.1 Quantities\n\nQuantities, values, domains\nMost decisions and inferences in engineering and data science involve things or properties of things that we can measure. We represent them by mathematical objects – most often, collections of numbers – with particular mathematical properties and operations.\nThe mathematical properties reflect the kind of activities that we can do with these things. For instance, colours are represented by particular tuples of numbers, and these tuples can be multiplied by some numeric weights and added, to obtain another tuple. This mathematical operation represents the fact that colours can be obtained by mixing other colours in different proportions. Physics and engineering are founded on this approach.\n\n\n25% #FF0000 + 75% #0000FF = #4000C0\nIt’s difficult to find a general term to denote any instance of such “things” and their mathematical representation. Yet it’s convenient if we find one, so we can discuss the general theory without getting bogged down in individual cases. To this purpose we’ll borrow the term quantity from physics and engineering.\n\n\n\n\n\n\n\n\n\n\nThe definition of “quantity” we are using here is similar to the one having the maximum specific level as defined in § 1.1 of the International vocabulary of metrology by the Joint Committee for Guides in Metrology.\nUsing the word “quantity” this way is just a convention between us. Other texts and scientists may use other words – for example “variable”, “event”, “state”. When you read a text or listen to a scientist, try to grasp the general idea behind the words.\nAs a general term we prefer the word “quantity” to a word like “variable”, because the latter word may give the idea of something changing in time – which may very well not be the case (think of the mass of a block of concrete). Same goes with a word like “state” for the opposite reason.\n\n\n\n\nWe distinguish between a quantity and its value. For instance, a quantity could be “The temperature at the point with coordinates  60.3775029, 5.3869233, 643,  at time 1895-10-04T10:03:14Z”; and its value could be \\(24\\,\\mathrm{°C}\\).\nThis distinction is necessary in inference and decision problems, because we may not know the value of a particular quantity. We then consider every possible value that quantity could have, and we can assign a probability to each. The set of possible values is called the domain of the quantity. In our temperature example, the domain is the set of all possible temperatures from \\(0\\,\\mathrm{K}\\) and above.\nAnother example:\n\nquantity: the image taken by a particular camera at a particular time, represented by a specific collection of numbers (say 128 × 128 × 3$ integers between \\(0\\) and \\(255\\))\nvalues: one possible value is this:  (corresponding to a grid of 128 × 128 × 3 specific numbers), another possible value is this: , and there are many other possible values\ndomain: the collection of \\(256^{3\\times128\\times128} \\approx 10^{118 370}\\) possible images (corresponding to the collection of possible grids of numeric values)\n\nOther examples of quantities and domains:\n\nThe distance between two objects in the Solar System at a specific time. The domain could be, say, all values from \\(0\\,\\mathrm{m}\\) to \\(6\\cdot10^{12}\\,\\mathrm{m}\\) (Pluto’s average orbital distance).\nThe number of total views of some online video (at a specific time), with a domain, say, from 0 to 20 billions.\nThe force on an object (at a specific time and place). The domain could be, say, 3D vectors with components in \\([-100\\,\\mathrm{N},\\,+100\\,\\mathrm{N}]\\).\nThe degree of satisfaction in a customer survey, with five possible values Not at all satisfied, Slightly satisfied, Moderately satisfied, Very satisfied, Extremely satisfied.\nThe graph representing a particular social network. Individuals are represented by nodes, and different kinds of relationships by directed or undirected links between nodes, possibly with numbers indicating their strength. The domain consists of all possible graphs with, say, \\(0\\) to \\(10000\\) nodes and all possible combinations of links and weights between the nodes.\nThe relationship between the input voltage and output current of an electric component. The domain could be all possible continuous curves from \\([0\\,\\mathrm{V}, 10\\,\\mathrm{V}]\\) to \\([0\\,\\mathrm{A}, 1\\,\\mathrm{A}]\\).\nA 1-minute audio track recorded by a device with a sampling frequency of 48 kHz (that is, 48 000 audio samples per second). The domain could be all possible sequences of 2 880 000 numbers in \\([0,1]\\).\nThe subject of an image, with domain of three possible values cat, dog, something else.\nThe roll, pitch, yaw of a rocket (at a specific time and place), with domain \\((-180°,+180°]\\times(-90°,+90°]\\times(-180°,+180°]\\).\n\n\n\nThe vague term “data” typically means the values of a collection of quantities.\nIn these notes we agree that a quantity has one, and only one, value.\n\n\n\n\n\n\n Quantity vs variate or variable\n\n\n\nWe can consider something that changes with time, or with space, or from individual to individual, or from unit to unit. This “something” is then not a quantity, according to our present terminology, but a collection of quantities: one for each time, or space, or individual. Later we shall call this collection a variate, especially when it refers to individuals or unit; or a variable.\nFor instance, your height at this exact moment is a quantity, but your height throughout your life is a variable, and the height (at this moment) across all Norwegian people is a variate.\nThese are just terminological conventions adopted in these notes. As mentioned before, different scientists often adopt different terms. What matters is not the terms, but that you have a clear understanding of the difference between the two notions that we here call “quantity” and “variate”.\n\n\n\n\nNotation\nWe shall denote quantities by italic letters, such as \\(X\\), or \\(U\\), or \\(A\\). The sentences that appear in decision-making and inferences are therefore often of the kind “the quantity \\(X\\) was observed to have value \\(x\\)”, where “\\(x\\)” stands for a specific value, for instance . This kind of sentences are often abbreviated in the form “\\(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\\)”.\n\n\n\n\n\n\n\n\n\n\n Keep in mind our discussion from §  6.3: we must make clear what that “\\(\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\)” means; it could mean “observed”, “set”, “reported”, and so on.\n Note the subtle difference between \\(X\\), in italics, and \\(\\mathsfit{X}\\), in so-called sans-serif typographic family. The first denotes a quantity, the second denotes a sentence. Usually we don’t have to worry too much about these symbol differences, because the meaning of the symbol is clear from the context. But just in case, you know the convention."
  },
  {
    "objectID": "quantities_types.html#sec-basic-types",
    "href": "quantities_types.html#sec-basic-types",
    "title": "12  Quantities and data types",
    "section": "12.2 Basic types of quantities",
    "text": "12.2 Basic types of quantities\nAs the examples above show, quantities and data come in all sorts and with different degrees of complexity. There is no clear-cut divide between different sorts of quantities. The same quantity can moreover be viewed and represented in many different ways, depending on the specific context, problem, purpose, and background information.\nIt is possible, however, to roughly differentiate between a handful of basic types of quantities, from which more complex types are built. Here is one kind of differentiation that is useful for inference problems about quantities:\n\nNominal\nA nominal or categorical quantity has a domain with a discrete and usually finite number of values. The values are not related by any mathematical property, and do not have any specific order.\nThis means that it does not make sense to say, for instance, that some value is “twice” or “1.5 times” another, or “larger” or “later” than another one. Nor does it make sense to “add” two quantities. In particular, there is no notion of cumulative probability, quantile, median, average, or standard deviation for a nominal quantity.\nExamples: the possible breeds of a dog, or the characters of a film.\nIt is of course possible to represent the values of a nominal quantity with numbers; say 1 for Dachshund, 2 for Labrador, 3 for Dalmatian, and so on. But that doesn’t mean that\nDalmatian\\({}-{}\\)Labrador\\({}={}\\)Labrador\\({}-{}\\)Dachshund\n(just because \\(3-2=2-1\\)) or similar nonsense.\n\n\nOrdinal\nAn ordinal quantity has a domain with a discrete and usually finite number of values. The values are not related by any mathematical property, but they do have a specific order.\nThis means that it does not make sense to say that some value is “twice” or “1.5 times” another, and we cannot add or subtract two values. But it does make sense to say, for any two values, which one has higher rank, for example “stronger”, or “later”, “larger”, and similar. Owing to the ordering property, it does make sense to speak of cumulative probability, quantile, and median of an ordinal quantity; but there is no notion of average or standard deviation for an ordinal quantity.\nExample: a pain-intensity scale. A patient can say whether some pain is more severe than another, but it isn’t clear what a pain “twice as severe” as another would mean (although there’s a lot of research on more precise quantification of pain). Another example: the “strength of friendship” in a social network. We can say that we have a “stronger friendship” with a person than with another; but it doesn’t make sense to say that we are “four times stronger friends” with a person than with another.\nIt is possible to represent the values of an ordinal quantity with numbers which reflect the order of the values. But it’s important to keep in mind that differences or averages of such numbers do not make sense. For this reason the use of numbers can be misleading at times; a less misleading possibility is to represent ordered values by alphabet letters, for example.\n\n\nBinary\nA binary or dichotomous quantity has only two possible values. It can be seen as a special case of a nominal or ordinal quantity, but the fact of having only two values lends it some special properties in inference problems. This is why we list it separately.\nObviously it doesn’t make much sense to speak of the difference or average of the two values; and their ranking is trivial even if it makes sense.\nThere’s an abundance of examples of binary quantities: yes/no answers, presence/absence of something, and so on.\n\n\nInterval\nAn interval quantity has a domain that can be discrete or continuous, finite or infinite. The values do admit some mathematical operations, at least convex combination and subtraction. They also admit an ordering.\nThis means that we can say, at the very least, whether the interval or “distance” between a pair of values is the same, or larger, or smaller than the interval between another pair. For this reason we can also say whether a value is larger than another. We can also take weighted sums of values, called convex combinations (keep in mind that simple addition of values may be meaningless for some quantities).\nOwing to these mathematical properties, it does make sense to speak of the cumulative probability, quantile, median, and also average and standard deviation for an interval quantity.\nThe number of electronic components produced in a year by an assembly line is an example of a discrete interval quantity. The power output of a nuclear plant at a given time is an example of a continuous interval quantity.\nIt is also possible to speak of ratio quantities, which are a special case of interval quantities, but we won’t have use of this distinction in the present notes.\n\n\nHow to decide the basic type of a quantity?\nTo attribute a basic type to a quantity we must ultimately check how that quantity is defined, obtained, and used. In some cases the values of the quantity may give some clue; for example, if we see values “\\(2.74\\)”, “\\(8.23\\)”, “\\(3.01\\)”, then the quantity is probably of the interval kind. But if we see values “\\(1\\)”, “\\(2\\)”, “\\(3\\)”, it’s unclear whether the quantity is interval, ordinal, nominal, or maybe of yet some other kind.\nThe type of a quantity also depends on its use in the specific problem. A quantity of a more complex type can be treated as a simpler type if needed. For instance, the response time of some device is in principle an interval quantity (measured, say, in seconds, as precisely as we want); but in a specific situation we could simply label its values as slow, medium, fast, thus turning it into an ordinal quantity.\n@@ TODO: add examples for image spaces\n\n\n\n\n\n\n Exercises\n\n\n\n\nFor each example at the beginning of the present section, assess whether that quantity can be considered as being of a basic type, and which type.\nFor each basic type discussed above, find two more concrete examples of that type of quantity\n\n\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nOn the theory of scales of measurement"
  },
  {
    "objectID": "quantities_types.html#other-attributes-of-basic-types",
    "href": "quantities_types.html#other-attributes-of-basic-types",
    "title": "12  Quantities and data types",
    "section": "12.3 Other attributes of basic types",
    "text": "12.3 Other attributes of basic types\nIt is useful to consider other basic aspects of quantities that are somewhat transversal to “type”. These aspects are also important when drawing inferences.\n\nDiscrete vs continuous\nNominal and ordinal quantities have discrete domains. The domain of an interval quantity can be discrete or continuous. In practice all domains are discrete, since we cannot observe, measure, report, or store values with infinite precision. In a modern computer, for example, a real number can “only” take on \\(2^{64} \\approx 20 000 000 000 000 000 000\\) possible values. In many situations the available precision is so high that we can consider the quantity as continuous for all practical purposes and use the mathematics of continuous sets – derivation, integration, and so on – to our advantage.\n\n\nBounded vs unbounded\nOrdinal and interval quantities may have domains with no minimum value, or no maximum value, or neither. Typical terms for these situations are lower- or upper-bounded, or left- or right-bounded, and unbounded; or similar terms.\nWhether to treat a quantity domain as bounded or unbounded depends on the quantity, the specific problem, and the computational resources. For example, the number of times a link on a webpage has been clicked can in principle be (right-)unbounded. Another example is the distance between two objects: we can consider it unbounded, but in concrete problems might be bounded, say, by the size of a laboratory, or by Earth’s circumference, or the Solar System’s extension, and so on.\n\n\n\n\n\n\n Exercises\n\n\n\n\nIf you had to set a maximum number of times a web link can be clicked, what number would you choose? Try to find a reasonable number, considering factors such as how fast a person can repeatedly click on a link, how long can a website (or the Earth?) last, and how many people can live in such an extent of time.\nWhat about the age of a person? What bound would you set, if you had to treat it as a bounded quantity?\n\n\n\n\n\nFinite vs infinite\nThe domain of a discrete quantity can consist of a finite or – at least in theory – an infinite number of possible values (the domain of a continuous quantity always has an infinite number of values). A domain can be infinite and yet bounded: consider the numbers in the range \\([0,1]\\).\nWhether to treat a domain as finite or infinite depends on the quantity, the specific problem, and the computational resources. For example, the intensity of a base colour in a pixel of a particular image might really take on 256 discrete steps between \\(0\\) and \\(1\\): \\(0, 0.0039215686, 0.0078431373, \\dotsc, 1\\). But in some situations we can treat this domain as practically infinite, with any possible value between \\(0\\) and \\(1\\).\n\n\nRounded\nA continuous interval quantity may be rounded, owing to the way it’s measured. In this case the quantity could be considered discrete rather than continuous. Rounding can impact the way we do inferences about such a quantity.\n\n\n The Iris dataset from its original paper\nThe famous Iris dataset, for instance, consists of several lengths – continuous interval quantities – of parts of flowers. All values are rounded to the millimetre, even if in reality the lengths could have intermediate values, of course. The age of a person is another frequent example of an in-principle continuous quantity which is rounded, say to the year or the month.\nIn some situations it’s important to be aware of rounding, because it can lead to quantities with different unrounded values to have identical rounded ones.\n\n\nCensored\nThe measurement procedure of a quantity may have an artificial lower or upper bound. A clinical thermometer, for instance, could have a maximum reading of \\(45\\,\\mathrm{°C}\\). If we measure with it the temperature of a \\(50\\,\\mathrm{°C}\\)-hot body, we’ll read “\\(45\\,\\mathrm{°C}\\)”, not the real temperature.\nA quantity with this characteristic is called censored, more specifically left-censored or right-censored when there’s only one artificial bound. The bound is called the censoring value.\nA censoring value denotes an actual value that could also be greater or less. This is important when we draw inferences about this kind of quantities.\n\n\n\n\n\n\n\n\n\n Exercises\n\n\n\nExplore datasets in a database such as the UC Irvine Machine Learning Repository:\n\nRead the description of the quantities listed in the dataset (sometimes in a readme file included with the dataset download)\nAnalyse the values of some of the quantities in the dataset: check if they can be considered continuous, discrete, or rounded; bounded or unbounded; uncensored or censored; and so on."
  },
  {
    "objectID": "quantities_types.html#sec-true-quantities",
    "href": "quantities_types.html#sec-true-quantities",
    "title": "12  Quantities and data types",
    "section": "12.4 “True” vs “measured” values",
    "text": "12.4 “True” vs “measured” values\nA difference is often drawn, especially in physics and engineering, between the “true” value of a quantity and the value “measured” or “observed” with a particular measuring instrument. What’s the difference? and how is the “true” value defined?\nThere are deep philosophical questions and choices underlying this distinction, and it would take a whole university course to do them justice.\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nThe Logic of Modern Physics\n\n\nIntuitively we define the “true” value as the value that would be measured with an instrument that is perfectly calibrated and as precise as theoretically possible. If we make a distinction between such value and the currently measured value then we’re implying that the current measurement is made with a less precise instrument, and the true and measured values could be different.\nIn some circumstances this distinction is unimportant: an agent can use the “measured” value without worries, and consider it as the “true” one. Typically this is the case when the possible discrepancy between measured and true value is enough small to have no consequences. In other circumstances the discrepancy is important: slightly different values lead to quite different consequences. Then it is necessary for the agent to try to infer – using the probability calculus – the true value, using the measured one as “data” or “evidence”. Said otherwise, the agent doesn’t use the measured value directly, but only as an intermediate step to guess the true value. The latter, in turn, can be used for further inferences.\nFrom the point of view of inference and decision-making, this distinction doesn’t lead to anything methodologically new: it just means that an agent has to do a chain of inferences instead of just one, using the four rules of inference as usual. This situation often requires the definition of two distinct quantities, the “true” and the “observed”, which can have slightly different domains. For instance we could have a voltage \\(V_\\text{obs}\\) measured with rounding to \\(1\\,\\mathrm{V}\\) and therefore with discrete domain \\(\\set{10\\,\\mathrm{V}, 11\\,\\mathrm{V}, 12\\,\\mathrm{V}, \\dotsc}\\); while needing the “true” voltage \\(V_\\text{true}\\) with a precision of at least \\(0.01\\,\\mathrm{V}\\), so this latter quantity could have a continuous domain.\nIn solving data-science and engineering problems it’s important to make clear whether a particular quantity value can be considered “true” and used as-is, or only “observed” with insufficient precision and used as data to infer the true value."
  },
  {
    "objectID": "quantities_types_multi.html#sec-data-multiv",
    "href": "quantities_types_multi.html#sec-data-multiv",
    "title": "13  Joint quantities and complex data types",
    "section": "13.1 Joint quantities",
    "text": "13.1 Joint quantities\nSome sets of basic quantities are just that: simple collections of quantities, in the sense that they do not have new properties or allow for new kinds of operations. We shall call these joint quantities when we need to distinguish them from quantities of a basic kind; but usually they are also simply called quantities.\nThe values of a joint quantity are just tuples of values of its basic component quantities. Their domain is the Cartesian product of the domains of the basic quantities.\nConsider for instance the age, sex1, and nationality of a particular individual. They can be represented as an interval-continuous quantity \\(A\\), a binary one \\(S\\), and a nominal one \\(N\\). We can join them together to form the joint quantity  “(age, sex, nationality)”  which can be denoted by  \\((A,S,N)\\).  One value of this joint quantity is, for example, \\((25\\,\\mathrm{y}, {\\small\\verb;F;}, {\\small\\verb;Norwegian;})\\). The domain could be1 We define sex by the presence of at least one Y chromosome or not. It is different from gender, which involves how a person identifies.\n\\[\n[0,+\\infty)\\times\n\\set{{\\small\\verb;F;}, {\\small\\verb;M;}} \\times\n\\set{{\\small\\verb;Afghan;}, {\\small\\verb;Albanian;}, \\dotsc, {\\small\\verb;Zimbabwean;}}\n\\]\n\nDiscreteness, boundedness, continuity\nA joint quantity may not be simply characterized as “discrete”, or “bounded”, or “infinite”, and so on. Usually we must specify these characteristics for each of its basic component quantities instead. Sometimes a joint quantity is called, for instance, “continuous” if all its basic components are continuous; but other conventions are also used.\n\n\n\n\n\n\n Exercises\n\n\n\nConsider again the examples of §  12.1.1. Do you find any examples of joint quantities?"
  },
  {
    "objectID": "quantities_types_multi.html#sec-data-complex",
    "href": "quantities_types_multi.html#sec-data-complex",
    "title": "13  Joint quantities and complex data types",
    "section": "13.2 Complex quantities",
    "text": "13.2 Complex quantities\nSome complex quantities can be represented as sets of quantities of basic types. These sets, however, are “more than the sum of their parts”: they possess new physical and mathematical properties and operations that do not apply or do not make sense for the single components.\nFamiliar examples are vectorial quantities from physics and engineering, such as location, velocity, force, torque. Another example are images, when represented as grids of basic quantities.\nConsider for example a 4 × 4 monochrome image, represented as a grid of 16 binary quantities \\(0\\) or \\(1\\). Three possible values could be these:\n    \nrepresented by the numeric matrices   \\(\\begin{psmallmatrix}1&0&0&0\\\\0&0&0&0\\\\0&0&0&0\\\\0&0&0&0\\end{psmallmatrix}\\), \\(\\begin{psmallmatrix}0&1&0&0\\\\0&0&0&0\\\\0&0&0&0\\\\0&0&0&0\\end{psmallmatrix}\\), \\(\\begin{psmallmatrix}0&0&0&0\\\\0&0&0&0\\\\0&0&0&0\\\\0&0&0&1\\end{psmallmatrix}\\).\nFrom the point of view of the individual binary quantities, these three “values” are equally different from one another: where one of them has grid value \\(1\\), the others have \\(0\\). But properly considered as images, we can say that the first and the second are somewhat more “similar” or “closer” to each other than the first and the third. This similarity can be represented and quantified by a metric over the domain of all such images. This metric involves all basic binary quantities at once; it is not a property of each of them individually.\nMore generally, complex quantities have additional, peculiar properties, represented by mathematical structures, which distinguish them from simpler joint quantities; although there is not a clear separation between the two.\nThese properties and structures are very important for inference problems, and usually make them computationally very hard. The importance of machine-learning methods lies to a great extent in the fact that they allow us to do approximate inference on these kinds of complex data. The peculiar structures of these data, however, are often also the cause of striking failures of some machine-learning methods, for example the reason why they may classify incorrectly, or correctly but for the wrong reasons."
  },
  {
    "objectID": "probability_distributions.html#motivation-for-the-inference-ii-part",
    "href": "probability_distributions.html#motivation-for-the-inference-ii-part",
    "title": "14  Probability distributions",
    "section": "Motivation for the “Inference II” part",
    "text": "Motivation for the “Inference II” part\nIn the “Data I” part we developed a “language”, that is, particular kinds of sentences, to approach inferences and probability calculations typical of data-science and engineering problems.\nIn the present part we focus on probability calculations that often occur with this kind of sequences, and on useful visual representation of such probabilities. Still, at bottom we’re just using the four fundamental rules of inference over and over again – nothing more than that."
  },
  {
    "objectID": "probability_distributions.html#sec-distribute-prob",
    "href": "probability_distributions.html#sec-distribute-prob",
    "title": "14  Probability distributions",
    "section": "14.1 Distribution of probabilities among values",
    "text": "14.1 Distribution of probabilities among values\nWhen an agent is uncertain about the value of a quantity, its uncertainty is expressed and quantified by assigning a degree of belief, conditional on the agent’s knowledge, to all the possible cases regarding the true value.\nFor a temperature measurement, for instance, the cases could be “The temperature is measured to have value 271 K”, “The temperature is measured to have value 272 K”, and so on up to 275 K. These cases are expressed by mutually exclusive and exhaustive sentences. Denoting the temperature with \\(T\\), these sentences can be abbreviated as\n\\[\n{\\color[RGB]{34,136,51}T = 271\\,\\mathrm{K}} \\ , \\quad\n{\\color[RGB]{34,136,51}T = 272\\,\\mathrm{K} \\ ,} \\quad\n{\\color[RGB]{34,136,51}T = 273\\,\\mathrm{K} \\ ,} \\quad\n{\\color[RGB]{34,136,51}T = 274\\,\\mathrm{K} \\ ,} \\quad\n{\\color[RGB]{34,136,51}T = 275\\,\\mathrm{K}} \\ .\n\\]\nThe agent’s belief about the quantity is then expressed by the probabilities about these sentences, conditional on the agent’s state of knowledge \\({\\color[RGB]{204,187,68}\\mathsfit{I}}\\): \n\\[\\begin{aligned}\n\\mathrm{P}({\\color[RGB]{34,136,51}T \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}271\\,\\mathrm{K}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{204,187,68}\\mathsfit{I}}) &= {\\color[RGB]{170,51,119}0.04} \\\\[1ex]\n\\mathrm{P}({\\color[RGB]{34,136,51}T \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}272\\,\\mathrm{K}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{204,187,68}\\mathsfit{I}}) &= {\\color[RGB]{170,51,119}0.10} \\\\[1ex]\n\\mathrm{P}({\\color[RGB]{34,136,51}T \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}273\\,\\mathrm{K}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{204,187,68}\\mathsfit{I}}) &= {\\color[RGB]{170,51,119}0.18} \\\\[1ex]\n\\mathrm{P}({\\color[RGB]{34,136,51}T \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}274\\,\\mathrm{K}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{204,187,68}\\mathsfit{I}}) &= {\\color[RGB]{170,51,119}0.28} \\\\[1ex]\n\\mathrm{P}({\\color[RGB]{34,136,51}T \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}275\\,\\mathrm{K}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{204,187,68}\\mathsfit{I}}) &= {\\color[RGB]{170,51,119}0.40}\n\\end{aligned}\n\\]\nthat sum up to one:\n\\[\n\\begin{aligned}\n&\\mathrm{P}({\\color[RGB]{34,136,51}T \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}271\\,\\mathrm{K}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{204,187,68}\\mathsfit{I}}) +\n\\dotsb +\n\\mathrm{P}({\\color[RGB]{34,136,51}T \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}275\\,\\mathrm{K}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{204,187,68}\\mathsfit{I}}) \\\\\n&\\quad{}=\n{\\color[RGB]{170,51,119}0.04}+{\\color[RGB]{170,51,119}0.10}+{\\color[RGB]{170,51,119}0.18}+{\\color[RGB]{170,51,119}0.28}+{\\color[RGB]{170,51,119}0.40} \\\\\n&\\quad{}=\n{\\color[RGB]{170,51,119}1}\n\\end{aligned}\\]\nThis collection of probabilities is called a probability distribution.\n\n\n\n\n\n\n What’s “distributed”?\n\n\n\nThe probability is distributed among the possible values, not the quantity, as illustrated in the side picture. The quantity cannot be “distributed”: it has one, definite value, which is however unknown to us.\n\n\n\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nConsider three sentences \\(\\mathsfit{X}_1, \\mathsfit{X}_2, \\mathsfit{X}_3\\) that are mutually exclusive and exhaustive on conditional \\(\\mathsfit{I}\\), that is:\n\\[\n\\begin{gathered}\n\\mathrm{P}(\\mathsfit{X}_1 \\land \\mathsfit{X}_2 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) =\n\\mathrm{P}(\\mathsfit{X}_1 \\land \\mathsfit{X}_3 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) =\n\\mathrm{P}(\\mathsfit{X}_2 \\land \\mathsfit{X}_3 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = 0\n\\\\\n\\mathrm{P}(\\mathsfit{X}_1 \\lor \\mathsfit{X}_2 \\lor \\mathsfit{X}_3 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = 1\n\\end{gathered}\n\\]\nProve, using the fundamental rules of inferences and any derived rules from §  8, that we must then have\n\\[\n\\mathrm{P}(\\mathsfit{X}_1 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) + \\mathrm{P}(\\mathsfit{X}_2 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) + \\mathrm{P}(\\mathsfit{X}_3 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = 1\n\\]\n\n\nLet’s see how probability distributions can be represented and visualized for the basic types of quantities discussed in §  12.\nWe start with probability distributions over discrete domains."
  },
  {
    "objectID": "probability_distributions.html#sec-discr-prob-distr",
    "href": "probability_distributions.html#sec-discr-prob-distr",
    "title": "14  Probability distributions",
    "section": "14.2 Discrete probability distributions",
    "text": "14.2 Discrete probability distributions\n\nTables and functions\nA probability distribution over a discrete domain can obviously be displayed as a table of values and their probabilities. For instance\n\n\n\n\n\n\n\n\n\n\n\nvalue\n271 K\n272 K\n273 K\n274 K\n275 K\n\n\n\n\nprobability\n0.04\n0.10\n0.18\n0.28\n0.40\n\n\n\nIn the case of ordinal or interval quantities it is sometimes possible to express the probability as a function of the value. For instance, the probability distribution above could be summarized by this function of the value \\({\\color[RGB]{34,136,51}t}\\):\n\\[\n\\mathrm{P}({\\color[RGB]{34,136,51}T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}t} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{204,187,68}\\mathsfit{I}}) =\n{\\color[RGB]{170,51,119}\\frac{({\\color[RGB]{34,136,51}t}/\\textrm{\\small K} - 269)^2}{90}}\n\\quad\\text{\\small (rounded to two decimals)}\n\\]\n\n\nA graphical representation is often helpful to detect features, peculiarities, and even inconsistencies in one or more probability distributions.\n\n\nHistograms and area-based representations\nA probability distribution for a nominal, ordinal, and discrete-interval quantity can be neatly represented by a histogram.\n\n\n\n\n\nHistogram for the probability distribution over possible component failures\n\n\nThe possible values are placed on a line. For an ordinal or interval quantity, the sequence of values on the line should correspond to their natural order. For a nominal quantity the order is irrelevant.\nA rectangle is then drawn above each value. Typically the rectangles are contiguous. The bases of the rectangles are all equal, and the areas of the rectangles are proportional to the probabilities. Since the bases are equal, this implies that the heights of the rectangles are also proportional to the probabilities.\nSuch kind of drawing can of course be horizontal, vertical, upside-down, and so on, depending on convenience.\nSince the probabilities must sum to one, the total area of the rectangles represents the unit of area. So in principle there is no need of writing probability values on some vertical axis, or grid, or similar visual device, because the probability value can be visually read as the ratio of a rectangle area to the total area. An axis or grid can nevertheless be helpful. Alternatively the probabilities can be reported above or below each rectangle.\nNominal quantities do not have any specific order, so their values do not need to be ordered on a line. Other area-based representations, such as pie charts, can also be used for these quantities.\n\n\nLine-based representations\nHistograms give faithful representations of discrete probability distributions. Their graphical bulkiness, however, can be a disadvantage in some situations; for instance when we want to have a clearer idea of how the probability distribution varies across values for ordinal or interval quantities; or when we want to compare several probability distributions over the same values.\nIn these cases we can use standard line plots, or variations thereof. Compare the following example.\nA technician wonders which component of a laptop failed first (only one can fail at a time), with seven possible alternatives (this is a nominal quantity): \\(\\set{{\\small\\verb;hard-drive;}, {\\small\\verb;motherboard;}, {\\small\\verb;CPU;}, {\\small\\verb;keyboard;}, {\\small\\verb;screen;}, {\\small\\verb;graphics-card;}, {\\small\\verb;PCI;}}\\). Before examining the laptop, the technician’s belief about which component failed first is distributed among the seven alternatives as in the blue histogram with solid borders. After a first inspection of the laptop, the technician’s belief has a new distribution, as in the red histogram with dashed borders:\n\nIt requires some concentration to tell the two probability distributions apart, and for example understand where their peaks are. Let us represent them by two line plots instead: solid blue with circles for the pre-inspection belief distribution, and dashed red with squares for the post-inspection one:\n\nthis line plot displays more cleanly the differences between the two distributions. We see that at first the technician most strongly believed the \\({\\small\\verb;keyboard;}\\) to be the faulty candidate, followed by the \\({\\small\\verb;PCI;}\\). After the preliminary inspection, the technician most strongly believes the \\({\\small\\verb;PCI;}\\) to be the faulty candidate, followed by the \\({\\small\\verb;graphics card;}\\)."
  },
  {
    "objectID": "probability_distributions.html#sec-prob-densities",
    "href": "probability_distributions.html#sec-prob-densities",
    "title": "14  Probability distributions",
    "section": "14.3 Probability densities",
    "text": "14.3 Probability densities\nDistributions of probability over continuous domains present several counter-intuitive aspects, which essentially arise because we are dealing with uncountable infinities – while often still using linguistic expressions that make at most sense for countable infinities. Here we follow a practical and realistic approach for working with such distributions.\nConsider a quantity \\(X\\) with a continuous domain. When we say that such a quantity has some value \\(x\\) we really mean that it has a value somewhere in the range   \\(x -\\epsilon/2\\)  to  \\(x+\\epsilon/2\\),   where the width \\(\\epsilon\\) is usually extremely small. For example, for double-precision values stored in a computer, the width1 must be at least \\(\\epsilon \\approx 2\\cdot 10^{-16}\\) :1 more precisely the relative width\n## R code\n&gt; 1.234567890123456 == 1.234567890123455\n[1] FALSE\n\n&gt; 1.2345678901234567 == 1.2345678901234566\n[1] TRUE\nand a value 1.3 really represents a range between 1.29999999999999982236431605997495353221893310546875 and 1.300000000000000266453525910037569701671600341796875, this range coming from the internal binary representation of 1.3. Often the width \\(\\epsilon\\) is much larger than the computer’s precision, and comes from the precision with which the value is experimentally measured.\nProbabilities are therefore assigned to such small ranges, not to single values. Since these ranges are very small, they are also very numerous. The total probability assigned to all of them must still amount to \\(1\\); therefore each small range receives an extremely small amount of probability. A standard Gaussian distribution for a real quantity, for instance, assigns a probability of approximately \\(8\\cdot 10^{-17}\\), or \\(0.00000000000000008\\), to a range of width \\(2\\cdot 10^{-16}\\) around the value \\(0\\). All other ranges are assigned even smaller probabilities.\nIn would be impractical to work with such small probabilities. We use probability densities instead. As implied by the term “density”, a probability density is the amount of probability \\(P\\) assigned to a standard range of width \\(\\epsilon\\), divided by that width. For example, if the probability assigned to a range of width  \\(\\epsilon=2\\cdot10^{-16}\\)  around \\(0\\) is  \\(P=7.97885\\cdot10^{-17}\\),  then the probability density around \\(0\\) is\n\\[\n\\frac{P}{\\epsilon} =\n\\frac{7.97885\\cdot10^{-17}}{2\\cdot10^{-16}} = 0.398942\n\\]\nwhich is a simpler number to work with.\nProbability densities are convenient because they usually do not depend on the range width \\(\\epsilon\\), if it’s small enough. Owing to physics reasons, we don’t expect a situation where \\(X\\) is between \\(0.9999999999999999\\) and \\(1.0000000000000001\\) to be very different from one where \\(X\\) is between \\(1.0000000000000001\\) and \\(1.0000000000000003\\). The probabilities assigned to these two small ranges of width \\(\\epsilon=2\\cdot 10^{-16}\\) each will therefore be approximately equal, let’s say \\(P\\) each. Now if we use a small range of width \\(\\epsilon\\) around \\(X=1\\), the probability is \\(P\\), and the probability density is \\(P/\\epsilon\\). If we consider a range of double width \\(2\\,\\epsilon\\) around \\(X=1\\), then the probability is \\(P+P\\) instead, but the probability density is still\n\\[\\frac{P+P}{2\\,\\epsilon} =\n\\frac{1.59577\\cdot10^{-16}}{4\\cdot10^{-16}}\n= 0.398942 \\ .\n\\]\nAs you see, even if we consider a range with double the width as before, the probability density is still the same.\n\n\nIn these notes we’ll denote probability densities with a lowercase \\(\\mathrm{p}\\), with the following notation:\n\\[\n\\underbracket[0pt]{\\mathrm{p}}_{\\mathrlap{\\color[RGB]{119,119,119}\\!\\uparrow\\ \\textit{lowercase}}}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) \\coloneqq\n\\frac{\n\\overbracket[0pt]{\\mathrm{P}}^{\\mathrlap{\\color[RGB]{119,119,119}\\!\\downarrow\\ \\textit{uppercase}}}(\\textsf{\\small`\\(X\\) has value between \\(x-\\epsilon/2\\) and \\(x+\\epsilon/2\\)'} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}{\\epsilon}\n\\]\nThis definition works even if we don’t specify the exact value of \\(\\epsilon\\), as long as it’s small enough.\n\n\n\n\n\n\n Probability densities are not probabilities\n\n\n\nIf \\(X\\) is a continuous quantity, the expression “\\(\\mathrm{p}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}2.5 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})=0.3\\)” does not mean “There is a \\(0.3\\) probability that \\(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}2.5\\)”. The probability that \\(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}2.5\\) exactly is, if anything, zero.\nThat expression means “There is a  \\(0.3\\cdot \\epsilon\\)   probability that \\(X\\) is between \\(2.5-\\epsilon/2\\) and \\(2.5+\\epsilon/2\\), for any \\(\\epsilon\\) small enough”.\nIn fact, probability densities can be larger than 1, because they are obtained by dividing by a number, the range width, that is in principle arbitrary. This fact shows that they cannot be probabilities.\nIt is important not to mix up probability and probability densities: we shall see later that densities have very different properties, for example with respect to maxima and averages.\n\n\nA helpful practice (though followed by few texts) is to always write a probability density as\n\\[\\mathrm{p}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\,\\mathrm{d}X\\]\nwhere “\\(\\mathrm{d}X\\)” stands for the width of a small range around \\(x\\). This notation is also helpful with integrals. Unfortunately it becomes a little cumbersome when we are dealing with more than one quantity.\n\nPhysical dimensions and units\nIn the International System of Units (SI)2, “Degree of belief” is considered to be a dimensionless quantity, or more precisely a quantity of dimension “1”. This is why we don’t write units such as “metres” (\\(\\mathrm{m}\\)), “kilograms” (\\(\\mathrm{kg}\\)) or similar together with a probability value.2 see also the material at the International Bureau of Weights and Measures (BIPM)\nA probability density, however, is defined as the ratio of a probability amount and an interval \\(\\epsilon\\) of some quantity. This latter quantity might well have physical dimensions, say “metres” \\(\\mathrm{m}\\). Then the ratio, which is the probability density, has dimensions \\(1/\\mathrm{m}\\). So probability densities in general have physical dimensions.\nAs another example, suppose that an agent with background knowledge \\(\\mathsfit{I}\\) assigns a degree of belief \\(0.00012\\) to an interval of temperature of width \\(0.0001\\,\\mathrm{°C}\\), around the temperature \\(T = 20\\,\\mathrm{°C}\\). Then the probability density at \\(20\\,\\mathrm{°C}\\) is equal to\n\\[\n\\mathrm{p}(T \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}20\\mathrm{°C} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) =\n\\frac{0.00012}{0.0001\\,\\mathrm{°C}} = 1.2\\,\\mathrm{°C^{-1}}\n\\]\nIt is an error to report probability densities without their correct physical units. In fact, keeping track of these units is often useful for consistency checks and finding errors in calculations, just like in other engineering or physics calculations.\nOn the other hand, if we write probability densities as previously suggested, in this case as “\\(\\mathrm{p}(T \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}20\\mathrm{°C} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\,\\mathrm{d}T\\)”, then the density written this way is a pure probability: the units “\\(\\mathrm{°C^{-1}}\\)” disappear because multiplied by \\(\\mathrm{d}T\\), which has the inverse units \\(\\mathrm{°C}\\)."
  },
  {
    "objectID": "probability_distributions.html#sec-represent-dens",
    "href": "probability_distributions.html#sec-represent-dens",
    "title": "14  Probability distributions",
    "section": "14.4 Representation of probability densities",
    "text": "14.4 Representation of probability densities\n\nLine-based representations\nThe histogram and the line representations become indistinguishable for a probability density.\nIf we represent the probability \\(P\\) assigned to a small range of width \\(\\epsilon\\) as the area of a rectangle, and the width of the rectangle is equal to \\(\\epsilon\\), then the height \\(P/\\epsilon\\) of the rectangle is numerically equal to the probability density. The difference from histograms for discrete quantities lies in the values reported on the vertical axis: for discrete quantities the values are probabilities (the areas of the rectangles), but for continuous quantities they are probability densities (the heights of the rectangles). This is also evident from the fact that the values reported on the vertical axis can be larger than 1, as in the plots on the side.\nThe rectangles, however, are so thin (usually thinner than a pixel on a screen) that they appear just as vertical lines, and together they look just like a curve delimiting a coloured area. If we don’t colour the area underneath we just have a line-based, or rather curve-based, representation of the probability density.\n\n\n   As the width \\(\\epsilon\\) of the small ranges is decreased, a histogram based on these widths become indistinguishable from a line plot\nIt can be good to keep in mind that the curve representing the probability density is not quite a function – in fact it’s best to call it a “density” or a “density function”. There are important reasons for keeping this distinction, which have also consequences for probability calculations, but we shall not delve into them in this course.\n\n\nScatter plots\nLine plots of a probability density are very informative, but they can also be slightly deceiving. Try the following experiment.\nConsider a continuous quantity \\(X\\) with the following probability density:\n\nWe want to represent the amount of probability in any small range, say between \\(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}0\\) and \\(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}0.1\\), by drawing in that range a number of short thin lines, the number being proportional to the probability. So a range with 10 lines has twice the probability of a range with 5 lines. The probability density around a value is therefore roughly represented by the density of the lines around that value.\nSuppose that we have 50 lines available to distribute this way. Where should we place them?\n\n\n\n\n\n\n\n Exercise\n\n\n\n\nWhich of these plots shows the correct placement of the 50 lines? (NB: the position of the correct answer is determined by a pseudorandom-number generator.)\n\n\n\n\n\n\n(A)\n\n\n\n\n\n\n\n(B)\n\n\n\n\n\n\n\n\n\n(C)\n\n\n\n\n\n\n\n(D)\n\n\n\n\n\n\n\n\n\nIn a scatter plot, the probability density is (approximately) represented by density of lines, or points, or similar objects, as in the examples above (only one above, though, correctly matches the density represented by the curve).\nAs the experiment and exercise above may have demonstrated, line plots sometimes give us slightly misleading ideas of how the probability is distributed across the domain; for example, peaks at some values make us overestimate the probability density around those values. Scatter plots often give a less misleading representation of the probability density.\nScatter plots are also useful for representing probability densities in more than one dimension – sometimes even in infinite dimensions! They can moreover be easier to produce computationally than line plots.\n\n\n\n\n\n\n\n\n Study reading\n\n\n\n\n§§ 5.3.0–5.3.1 of Risk Assessment and Decision Analysis with Bayesian Networks"
  },
  {
    "objectID": "probability_distributions.html#sec-combined-probs",
    "href": "probability_distributions.html#sec-combined-probs",
    "title": "14  Probability distributions",
    "section": "14.5 Combined probabilities",
    "text": "14.5 Combined probabilities\nA probability distribution is defined over a set of mutually exclusive and exhaustive sentences. In some inference problems, however, we do not need the probability of those sentences, but of some other sentence that can be obtained from them by an or operation. The probability of this sentence can then be obtained by a sum, according to the or-rule of inference. We can call this a combined probability. Let’s explain this procedure with an example.\nBack to our initial assembly-line scenario from §  1, the inference problem was to predict whether a specific component would fail within a year or not. Consider the time when the component will fail (if sold), and represent it by the quantity \\(T\\) with the following 24 different values, where “\\(\\mathrm{mo}\\)” stands for “months”:\n\\[\\begin{aligned}\n&\\textsf{\\small`The component will fail during it 1st month of use'}\\\\\n&\\textsf{\\small`The component will fail during it 2nd month of use'}\\\\\n&\\dotsc \\\\\n&\\textsf{\\small`The component will fail during it 23rd month of use'}\\\\\n&\\textsf{\\small`The component will fail during it 24th month of use or after'}\n\\end{aligned}\\]\nwhich we can shorten to  \\(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}1 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}2 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsc \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}24\\); note the slightly different meaning of the last value.\n\n\n\n\n\n\n Exercise\n\n\n\nWhat is the basic type of the quantity \\(T\\)? Which other characteristics does it have? for instance discrete? unbounded? rounded? uncensored?\n\n\nSuppose that the inspection device – our agent – has internally calculated a probability distribution for \\(T\\), conditional on its internal programming and the results of the tests on the component, collectively denoted \\(\\mathsfit{I}\\). The probabilities, compactly written, are\n\\[\n\\mathrm{P}(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}1 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}), \\quad\n\\mathrm{P}(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}2 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}), \\quad\n\\dotsc, \\quad\n\\mathrm{P}(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}24 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\n\\]\nTheir values are stored in the file failure_probability.csv and plotted in the histogram on the side.\n\n\n\nWhat’s important for the agent’s decision about rejecting or accepting the component, is not the exact time when it will fail, but only whether it will fail within the first year or not. That is, the agent needs the probability of the sentence \\(\\textsf{\\small`The component will fail within a year of use'}\\). But this sentence is just the or of the first 12 sentences expressing the values of \\(T\\):\n\\[\n\\begin{aligned}\n&\\textsf{\\small`The component will fail within a year of use'}\n\\\\&\\qquad{}\\equiv\n\\textsf{\\small`The component will fail during it 1st month of use'}\n\\lor{}\n\\\\&\\qquad\\qquad\n\\textsf{\\small`The component will fail during it 2nd month of use'}\n\\lor \\dotsb\n\\\\&\\qquad\\qquad\n\\dotsb \\lor\n\\textsf{\\small`The component will fail during it 12th month of use'}\n\\\\[1ex]&\\qquad{}\\equiv\n(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}1) \\lor (T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}2) \\lor \\dotsb \\lor (T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}12)\n\\end{aligned}\n\\]\nThe probability needed by the agent is therefore\n\\[\n\\mathrm{P}(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}1 \\lor T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}2 \\lor \\dotsb \\lor T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}12\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\]\nwhich can be calculated using the or-rule, considering that the sentences involved are mutually exclusive:\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\textsf{\\small`The component will fail within a year of use'} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\\\[1ex]&\\qquad{}=\n\\mathrm{P}(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}1 \\lor T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}2 \\lor \\dotsb \\lor T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}12\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\\\[1ex]&\\qquad{}=\n\\mathrm{P}(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}1 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}) + \\mathrm{P}(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}2 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}) + \\dotsb + \\mathrm{P}(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}12 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\n\\\\[1ex]&\\qquad{}= \\sum_{t=1}^{12} \\mathrm{P}(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}t \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\end{aligned}\n\\]\n\n\n\n\n\n\nSum notation\n\n\n\nWe shall often use the \\(\\sum\\)-notation for sums, as in the example above. A notation like “\\(\\displaystyle\\sum_{i=5}^{20}\\)” means: write multiple copies of what’s written on its right side, and in each copy replace the symbol “\\(i\\)” with values from \\(5\\) to \\(20\\), in turn; then sum up these copies. The symbol “\\(i\\)” is called the index of the sum. Sometimes the initial and final values, \\(5\\) and \\(20\\) in the example, are omitted if they are understood from the context, and the sum is written simply “\\(\\displaystyle\\sum_{i}\\)”.\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nUsing your favourite programming language:\n\nLoad the file failure_probability.csv containing the probabilities.\nInspect this file, find the headers of its columns and so on.\nCalculate the probability that the component will fail within a year of use.\nCalculate the probability that the component will fail “within two months of use, or after a year of use”."
  },
  {
    "objectID": "joint_probability.html#joint-probability-distributions",
    "href": "joint_probability.html#joint-probability-distributions",
    "title": "15  Joint probability distributions",
    "section": "15.1 Joint probability distributions",
    "text": "15.1 Joint probability distributions\nA joint quantity is just a collection or set of quantities of basic types. Saying that a joint quantity has a particular value means that each basic component quantity has a particular value in its specific domain. This is expressed by an and of sentences.\nConsider for instance the joint quantity \\(X\\) consisting of the age \\(\\color[RGB]{102,204,238}A\\) and sex \\(\\color[RGB]{34,136,51}S\\) of a specific person. The fact that \\(X\\) has a particular value is expressed by a composite sentence such as\n\\[\n\\textsf{\\small`The person's age is 25 years and the person's sex is female'}\n\\]\nwhich we can compactly write with an and:\n\\[\n{\\color[RGB]{102,204,238}A\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}25\\,\\mathrm{y}} \\land {\\color[RGB]{34,136,51}S\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathrm{f}}\n\\]\nAll the possible composite sentences of this kind are mutually exclusive and exhaustive.\nAn agent’s uncertainty about \\(X\\)’s true value is therefore represented by a probability distribution over all and-ed sentences of this kind, representing all possible joint values:\n\\[\n\\mathrm{P}\\bigl({\\color[RGB]{102,204,238}A \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}25\\,\\mathrm{y}} \\land {\\color[RGB]{34,136,51}S\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathrm{f}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}\\bigr) \\ , \\qquad\n\\mathrm{P}\\bigl({\\color[RGB]{102,204,238}A \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}31\\,\\mathrm{y}} \\land {\\color[RGB]{34,136,51}S\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathrm{m}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}\\bigr) \\ , \\qquad\n\\dotsc\n\\]\nwhere \\(\\mathsfit{I}\\) is the agent’s state of knowledge, and the probabilities sum up to one. We call each of these probabilities a joint probability, and their collection a joint probability distribution. Usually these probabilities are written in much abbreviated form, and a comma “\\(\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\)” is used instead of “\\(\\land\\)” (§  6.4); for instance you can commonly find the following notation:\n\\[\n\\mathrm{P}(A\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}25 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}S\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathrm{f} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\]\nor even just\n\\[\n\\mathrm{P}(25, \\mathrm{f} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\]"
  },
  {
    "objectID": "joint_probability.html#sec-repr-joint-prob",
    "href": "joint_probability.html#sec-repr-joint-prob",
    "title": "15  Joint probability distributions",
    "section": "15.2 Representation of joint probability distributions",
    "text": "15.2 Representation of joint probability distributions\nThere is a wide variety of ways of representing joint probability distributions, and new ways are invented (and rediscovered) all the time. In some cases, especially when the quantity has more than three component quantities, it can become impossible to graphically represent the probability distribution in a faithful way. Therefore one often tries to represent only some aspects or features of interest from the full distribution. Whenever you see a plot of a joint probability distribution, you should carefully read what the plot shows and how it was made. Here we only illustrate some examples and ideas for representations.\n\nTables\nWhen a joint quantity consists of two, discrete and finite component quantities, the joint probabilities can be reported as a table, sometimes called a contingency table1.1 this term is most often used for joint distributions of frequencies rather than probability\nExample: Consider the next patient that will arrive at a particular hospital. There’s the possibility of arrival by \\({\\small\\verb;ambulance;}\\), \\({\\small\\verb;helicopter;}\\), or \\({\\small\\verb;other;}\\) transportation means; and the possibility that the patient will need \\({\\small\\verb;urgent;}\\) \\({\\small\\verb;non-urgent;}\\) care. These can be seen as two quantities \\(T\\) (nominal) and \\(U\\) (binary). When these two quantities are taken together; their joint probability distribution is as follows, conditional on the hospital’s data \\(\\mathsfit{I}_{\\text{H}}\\):\n\n\nTable 15.1: Joint probability distribution for transportation and urgency\n\n\n\n\n\n\n\n\n\n\\(\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}u \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}t\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}_{\\text{H}})\\)\n\ntransportation at arrival \\(T\\)\n\n\n\n\nambulance\nhelicopter\nother\n\n\nurgency \\(U\\)\nurgent\n0.11\n0.04\n0.03\n\n\nnon-urgent\n0.17\n0.01\n0.64\n\n\n\n\nWe see for instance that the most probable possibility is that the next patient will arrive by transportation means other than ambulance and helicopter, and won’t require urgent care:\n\\[\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;other;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}_{\\text{H}}) =\n0.64\\]\nIt is also possible to replace the numerical probability values with graphical representations; for example as shades of a colour, or squares with different areas.\n\n\n\n\n\n\n Exercise – never forget the agent!\n\n\n\nWho could be the agent whose degrees of belief are represented in the table above? What could be the background information leading to such beliefs?\n\n\n\n\nScatter plots and similar\nProbability distributions for nominal, ordinal, or discrete-interval quantities can be represented by histograms or line plots, as we saw in §  14.2. Histograms could be generalized to quantities consisting of two joint discrete quantities: a probability could be represented by a cuboid or rectangular prism (a sort of small tower with rectangular section), or cylinder, or similar. This representation, even if it can look flamboyant, is often inconvenient because some of the three-dimensional objects can be hidden from view, as in the generic example on the side.\n\n\n Example of generalized histogram (from Mathematica)\nAlternatively, one can replace the numerical values of the probabilities in the tabular representation of the previous section with some graphical encoding.\nAn example is some colour scheme, say white for probability \\(0\\), black for probability \\(1\\), and grey levels for intermediate probabilities; or some other colour scheme. This is sometimes called a “density histogram”; see the generic example on the side. This representation can be useful for qualitative or semi-quantitative assessments, for example seeing which joint values have highest probabilities.\n\n\n Example of density histogram (from Mathematica)\nAnother example, similar to the scatter plot (§  14.4.2), is to encode the probability values with a proportional number of points or other shapes, as illustrated here for the probabilities of table 15.1:\n\n\n\nFigure 15.1: Scatter plot for the urgency-transportation joint probability distribution\n\n\nthe points do not need to be scattered in regular fashion as long as it’s clear which quantity value they are associated with. The scatter plot above has 100 points, and therefore we can see for instance that \\(\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\textrm{\\small urgent} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\textrm{\\small helicopter}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}_{\\text{H}}) = 0.03\\), since the corresponding region has 3 points out of 100."
  },
  {
    "objectID": "joint_probability.html#sec-joint-prob-densities",
    "href": "joint_probability.html#sec-joint-prob-densities",
    "title": "15  Joint probability distributions",
    "section": "15.3 Joint probability densities",
    "text": "15.3 Joint probability densities\nIf a joint quantity consists in several continuous interval quantities, then its joint probability distribution is usually represented by a joint probability density, which generalize the one-dimensional discussion of §  14.3 to several dimensions.\nFor instance, if \\(X\\) and \\(Y\\) are two continuous interval quantity, then the notation\n\\[\n\\mathrm{p}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = 0.001\n\\]\nmeans that the joint sentence “\\(X\\) has value between \\(x-\\epsilon/2\\) and \\(x+\\epsilon/2\\), and \\(Y\\) between \\(y-\\delta/2\\) and \\(y+\\delta/2\\)”, or in symbols\n\\[\n\\bigl(x-\\tfrac{\\epsilon}{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small&lt;}\\nonscript\\mkern 0mu}\\mathopen{} X \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small&lt;}\\nonscript\\mkern 0mu}\\mathopen{} x+\\tfrac{\\epsilon}{2}\\bigr)\n\\land\n\\bigl(y-\\tfrac{\\delta}{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small&lt;}\\nonscript\\mkern 0mu}\\mathopen{} Y \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small&lt;}\\nonscript\\mkern 0mu}\\mathopen{} y+\\tfrac{\\delta}{2}\\bigr)\n\\]\nhas probability \\(0.001\\cdot\\epsilon\\cdot\\delta\\), conditional on the background knowledge \\(\\mathsfit{I}\\). Said otherwise, the rectangular region of values around \\((x,y)\\) with widths \\(\\epsilon\\) and \\(\\delta\\) is assigned a probability \\(0.001\\cdot\\epsilon\\cdot\\delta\\).\nRemember that a density typically has physical units, as in the one-dimensional case (§  14.3). For instance, if \\(X\\) above is a temperature measured in kelvin (\\(\\mathrm{K}\\)) and \\(Y\\) a resistance measured in ohm (\\(\\Omega\\)), then we would write  \\(\\mathrm{p}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = \\frac{0.001}{\\mathrm{K}\\,\\Omega}\\)."
  },
  {
    "objectID": "joint_probability.html#sec-repr-joint-dens",
    "href": "joint_probability.html#sec-repr-joint-dens",
    "title": "15  Joint probability distributions",
    "section": "15.4 Representation of joint probability densities",
    "text": "15.4 Representation of joint probability densities\nFor one-dimensional densities we discussed line-based representations and scatter plots (§  14.4). The first of these representations can be generalized to two-dimensional densities, leading to a surface plot. Below is the surface density plot for the probability density given by the formula\n\n\\[\n\\mathrm{p}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) =\n\\tfrac{3}{8\\,\\pi}\\, \\mathrm{e}^{-\\frac{1}{2} (x-1)^2-(y-1)^2}+\n\\tfrac{3}{64\\,\\pi}\\,\\mathrm{e}^{-\\frac{1}{32} (x-2)^2-\\frac{1}{2} (y-4)^2}+ \\tfrac{1}{40\\,\\pi}\\,\\mathrm{e}^{-\\frac{1}{8} (x-5)^2-\\frac{1}{5} (y-2)^2}\n\\]\n\n\nThis kind of representation can be very neat, but it has three drawbacks: it sometimes hides features of the density from view, it cannot be extended to three-dimensional densities, and sometimes the analytical expression for the probability density (like the formula above) is not available.\nThe scatter plot instead does not hides features, can also be used for three-dimensional densities, and can be used in cases where we can at least obtain “representative” points from the probability density, even if the analytical expression of the latter is too complicated or not available. This representation is, however, quantitatively more imprecise. Here is a scatter plot, using 10 000 points, for the probability density given above:\n\n\n\nFigure 15.2: Scatter-plot representation of the joint probability density \\(\\mathrm{p}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\)\n\n\nAs usual, the probability of a small region is proportional to the density of points in that region. If we had a joint density for three continuous quantities, its scatter plot would consist of three-dimensional clouds of points instead.\nClearly both kinds of representation have advantages and disadvantages."
  },
  {
    "objectID": "joint_probability.html#sec-joint-mix-distr",
    "href": "joint_probability.html#sec-joint-mix-distr",
    "title": "15  Joint probability distributions",
    "section": "15.5 Joint mixed discrete-continuous probability distributions",
    "text": "15.5 Joint mixed discrete-continuous probability distributions\nFrequently occurring in engineering and data-science problems are joint quantities composed by some discrete and some continuous quantities. Their joint probability distribution is a density with respect to the continuous component quantity.\nSuppose for instance that \\(Z\\) is a binary quantity with domain \\(\\set{{\\small\\verb;low;}, {\\small\\verb;high;}}\\), \\(X\\) a continuous quantity with all real numbers \\(\\mathbf{R}\\) as domain, and together they form the joint quantity \\((Z,X) \\in \\set{{\\small\\verb;low;}, {\\small\\verb;high;}} \\times \\mathbf{R}\\). Then the probability expression\n\\[\n\\mathrm{p}(Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;low;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}3 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = 0.07\n\\]\nmeans that the agent with background information \\(\\mathsfit{I}\\) gives a \\(0.07\\cdot \\epsilon\\) degree of belief to the joint sentence “\\(Z\\) has value \\({\\small\\verb;low;}\\) and \\(X\\) has value between \\(3-\\epsilon/2\\) and \\(3+\\epsilon/2\\)”, for any small \\(\\epsilon\\). (As usual, if \\(X\\) has physical dimensions, say metres \\(\\mathrm{m}\\), then the probability density above has value \\(0.07\\,\\mathrm{m^{-1}}\\).)"
  },
  {
    "objectID": "joint_probability.html#sec-repr-mix-distr",
    "href": "joint_probability.html#sec-repr-mix-distr",
    "title": "15  Joint probability distributions",
    "section": "15.6 Representation of mixed probability distributions",
    "text": "15.6 Representation of mixed probability distributions\nMixed discrete-continuous probability distributions can be somewhat tricky to represent graphically. Here we consider line-based representations and scatter plots. We take as example the probability that the next patient who arrives at a particular hospital has a given age (positive continuous quantity) and arrives by \\({\\small\\verb;ambulance;}\\), \\({\\small\\verb;helicopter;}\\), or \\({\\small\\verb;other;}\\) transportation means.\n\nMulti-line plots\nA line plot can be used to represent the probability density for the continuous quantity and each specific value of the discrete quantity:\n\n\n\nFigure 15.3: [Line plot for the age-transportation joint probability]\n\n\nWith the plot above it’s important to keep in mind that the three curves are three pieces of the same probability density, not three different densities. This is also clear seeing that the three areas under them (which partly overlap) cannot each be \\(1\\), as would instead be expected for a probability density. The probability density is separated into three curves owing to the presence of the discrete quantity having three possible values.\nThe area under the solid blue curve is equal to \\(0.55\\), the area under the dashed red curve is \\(0.25\\), and the area under the dotted green curve is \\(0.20\\) . The total area under the three curves (counting also the overlapping regions) is equal to \\(1\\), as it should be.\nA possible disadvantage of this kind of plots is that some details, such as peaks, of the densities for some values of the discrete quantity may be barely discernible.\n\n\nScatter plots\nAs discussed before, in a scatter plot we represent the probability density by a cloud of “representative” objects, such as points, obtained from it: the density of these objects is approximately proportional to the probability density.\nAn example of scatter plot for the probability density of our example is the following (note that the blue colour is here no longer associated with \\({\\small\\verb;ambulance;}\\)):\n\nIn the plot above, the probability density is reflected by the density of vertical lines. Using points instead of vertical lines, the density would have been difficult to discern, since all points would be on one of three vertical positions.\nWe can use points if we give some variation to their vertical coordinate – keeping in mind that such vertical variation has no meaning. The idea is similar to the one of fig.  15.1. We obtain a plot like this:\n\n\n\nFigure 15.4: [Point-scatter plot for the age-transportation joint probability]\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nCompare the line plot of fig.  15.3 and the point-scatter plot of fig.  15.4, which represent the same joint probability density. Do some introspection, and analyse the contrasting impressions that the two kinds of representations may give you. For instance, does the line plot give you a wrong intuition about the sharpness of the peaks in the density?\nCompare with what you did in the exercise of §  14.4."
  },
  {
    "objectID": "joint_probability.html#sec-repr-general-distr",
    "href": "joint_probability.html#sec-repr-general-distr",
    "title": "15  Joint probability distributions",
    "section": "15.7 Representation of more general probability distributions and densities",
    "text": "15.7 Representation of more general probability distributions and densities\nProbability distributions for complex types of quantity can be quite tricky to represent in an informative way. They typically require a case-by-case approach.\nOften the idea behind the scatter plot works also in these complex cases: the distribution or density is represented by a “representative” sample of objects; and the objects can even depict the quantity itself.\nFor instance, imagine the quantity \\(L\\) defined as “the linear relationship between input voltage and output current of a specific electronic component”. The possible values of this quantity are not just simple numbers or categories, but straight lines, that is, functions of the form “\\(y=m\\,x + q\\)”, where \\(x\\) is the input voltage and \\(y\\) the output current. The possible values – straight lines – can differ in their angular coefficient \\(m\\) or in their intercept \\(q\\): one value could be the straight line \\(y= (2\\,\\mathrm{A/V})\\, x - 3\\,\\mathrm{A}\\), and another value could be straight line \\(y= (-1\\,\\mathrm{A/V})\\, x + 5\\,\\mathrm{A}\\), and so on. This is a continuous quantity, but it isn’t a quantity of a basic type.\n\n\n A voltage-current converter\nAn agent may be uncertain about which is the actual value of \\(L\\), that is, which is the straight line that correctly expresses the voltage-current relationship of the electronic component. The agent therefore assign a probability density over all possible values – over all possible straight lines. How to visually represent such a probability density?\nOne way is to use a scatter plot: the probability distribution is represented by a cloud of straight lines, whose density is approximately proportional to the probability density. Here is an example using 360 representative straight lines:\n\n\n\nFigure 15.5: [Scatter plot for the probability density over the voltage-current relationship]\n\n\nFrom this plot we can read some important semi-quantitative information about the agent’s probability density. For example, it’s most probable that the voltage-current relationship has a positive angular coefficient \\(m\\) around \\(0.5\\,\\mathrm{A/V}\\), and an intercept \\(q\\) around \\(3\\,\\mathrm{A}\\); it improbable, but not impossible, that the voltage-current relationship has a negative angular coefficient (the output current decreases as the input voltage is increased); and it’s practically impossible that the voltage-current relationship is almost vertical (say, changes in current larger than \\(\\sim 5\\,\\mathrm{A}\\) with changes in voltage smaller than \\(\\sim 0.2\\,\\mathrm{V}\\)).\n\n\n\n\n\n\n\n\n\n\n\n Exercises\n\n\n\n\nExplore datasets in a database such as the UC Irvine Machine Learning Repository, for example\n\nThe adult-income dataset\nThe heart-disease dataset\n\nAssume that the data given are representative “points” of a probability distribution or density (of which we don’t know the analytic formula). Plot the probability distributions and probability densities as scatter plots using some of these representative points.\nLook for the analytic formulae of some probability distributions and densities of simple and joint quantities, and plot them using different representations.\n\n\n\n\n\n\n\n\n\n\n\n Study reading\n\n\n\n\n§ 5.3.2 of Risk Assessment and Decision Analysis with Bayesian Networks\n§ 12.2.2 of Artificial Intelligence"
  },
  {
    "objectID": "marginal_probability.html#sec-marginal-probs",
    "href": "marginal_probability.html#sec-marginal-probs",
    "title": "16  Marginal probability distributions",
    "section": "16.1 Marginal probability: neglecting some quantities",
    "text": "16.1 Marginal probability: neglecting some quantities\nIn some situations an agent has a joint distribution of degrees of belief for the possible values of a joint quantity, but it needs to consider its belief in the value of one component quantity alone, irrespective of what the values for the other components quantities might be.\nConsider for instance the joint probability for the next-patient arrival scenario of table  15.1 from §  15.2, with joint quantity \\((U,T)\\). We may be interested in the probability that the next patient will need \\({\\small\\verb;urgent;}\\) care, independently of how the patient is transported to the hospital. This probability can be found, as usual, by analysing the problem in terms of sentences and using the basic rules of inference from §  8.4.\nThe sentence of interest is “The next patient will require urgent care”, or in symbols\n\\[U \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\]\nwhich is equivalent to “The next patient will require urgent care, and will arrive by ambulance, helicopter, or other means”, or in symbols\n\\[\nU \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\land\n(\nT \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\lor\nT \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\lor\nT \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;other;}\n)\n\\]\nUsing the derived rules of Boolean algebra of §  9.2 we can rewrite this sentence in yet another way:\n\\[\n(U \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\land T \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}) \\lor\n(U \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\land T \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}) \\lor\n(U \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\land T \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;other;})\n\\]\nThis last sentence is an or of mutually exclusive sentences. Its probability is therefore given by the or rule without the and terms (we now use the comma “\\(\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\)” for and):\n\n\\[\n\\begin{aligned}\n&\\mathrm{P}(U \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}})\n\\\\[1ex]\n&\\quad{}=\n\\mathrm{P}\\bigl[\n(U \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}) \\lor\n(U \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}) \\lor\n(U \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;other;})\n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}} \\bigr]\n\\\\[1ex]\n&\\quad{}=\\begin{aligned}[t]\n&\\mathrm{P}(U \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}}) +{}\n\\\\\n&\\quad\\mathrm{P}(U \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}}) +{}\n\\\\\n&\\quad\\mathrm{P}(U \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;other;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}})\n\\end{aligned}\n\\end{aligned}\n\\]\n\nWe have found that the probability for a value of the urgency quantity \\(U\\), independently of the value of the transportation quantity \\(T\\), can be calculated by summing all joint probabilities with all possible \\(T\\) values. Using the \\(\\sum\\)-notation we can write this compactly:\n\\[\n\\mathrm{P}(U \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}}) =\n\\sum_{t}\n\\mathrm{P}(U \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}t \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}})\n\\]\nwhere it’s understood that the sum index \\(t\\) runs over the values \\(\\set{{\\small\\verb;ambulance;}, {\\small\\verb;helicopter;}, {\\small\\verb;other;}}\\).\nThis is called a marginal probability.\n\n\n\n\n\n\n Exercise\n\n\n\nUsing the values from table 15.1, calculate:\n\nthe marginal probability that the next patient will need urgent care\nthe marginal probability that the next patient will arrive by helicopter\n\n\n\n\n\nConsidering now a more generic case of a joint quantity with component quantities \\(\\color[RGB]{34,136,51}X\\) and \\(\\color[RGB]{238,102,119}Y\\), the probability for a specific value of \\(\\color[RGB]{34,136,51}X\\), conditional on some information \\(\\mathsfit{I}\\) and irrespective of what the value of \\(\\color[RGB]{238,102,119}Y\\) might be, is given by\n\\[\n\\mathrm{P}({\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = \\sum_{\\color[RGB]{238,102,119}y} \\mathrm{P}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\]\nYou may notice the similarity with the expression for a combined probability from §  14.5. Indeed a marginal probability is just a special case of a combined probability: we are combining all probabilities that exhaust the possibilities for the sentence \\(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y\\).\n\n\n\n\n\n\n Exercise: test your understanding\n\n\n\nUsing again the values from table 15.1, calculate the probability that the next patient will need urgent care and will be transported either by ambulance or by helicopter."
  },
  {
    "objectID": "marginal_probability.html#sec-marginal-dens",
    "href": "marginal_probability.html#sec-marginal-dens",
    "title": "16  Marginal probability distributions",
    "section": "16.2 Marginal density distributions",
    "text": "16.2 Marginal density distributions\nIn the example of the previous section, suppose now that the quantities \\(\\color[RGB]{34,136,51}X\\) and \\(\\color[RGB]{238,102,119}Y\\) are continuous. Then the joint probability is expressed by a density:\n\\[\\mathrm{p}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\]\nwith the usual meaning. The marginal probability density for \\(\\color[RGB]{34,136,51}X\\) is still given by a sum, but this sum occurs over intervals of values of \\(\\color[RGB]{238,102,119}Y\\), intervals with very small widths; as a consequence the sum will have a very large number of terms. To remind ourselves of this fact, which can be very important in some situations, we use a different notation in terms of integrals:\n\\[\n\\mathrm{p}({\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = \\int_{\\color[RGB]{238,102,119}\\varUpsilon} \\mathrm{p}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\, \\mathrm{d}{\\color[RGB]{238,102,119}y}\n\\]\nwhere \\(\\color[RGB]{238,102,119}\\varUpsilon\\) represents the domain of the quantity \\(\\color[RGB]{238,102,119}Y\\).\nThe fact that integrals appear is in some cases extremely useful, because it allows us to use the theory of integration to calculate marginal probabilities quickly and precisely, instead of having to compute sums of a large numbers of small terms – a procedure that can be computationally expensive and lead to numerical errors owing to underflow or similar phenomena."
  },
  {
    "objectID": "marginal_probability.html#sec-marginal-scatter",
    "href": "marginal_probability.html#sec-marginal-scatter",
    "title": "16  Marginal probability distributions",
    "section": "16.3 Marginal probabilities and scatter plots",
    "text": "16.3 Marginal probabilities and scatter plots\nIn the previous chapters we have often discussed scatter plots for representing probability distributions of various kinds: discrete, continuous, joint, mixed, and so on.\nOne more advantage of the scatter plots for a joint distribution is that it can be quickly modified to represent any marginal, again with a scatter plot; whereas the use of a surface plot would require analytical calculations or approximations thereof.\nConsider for instance the joint probability density from §  15.4, represented by the formula\n\n\\[\n\\mathrm{p}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) =\n\\tfrac{3}{8\\,\\pi}\\, \\mathrm{e}^{-\\frac{1}{2} (x-1)^2-(y-1)^2}+\n\\tfrac{3}{64\\,\\pi}\\,\\mathrm{e}^{-\\frac{1}{32} (x-2)^2-\\frac{1}{2} (y-4)^2}+ \\tfrac{1}{40\\,\\pi}\\,\\mathrm{e}^{-\\frac{1}{8} (x-5)^2-\\frac{1}{5} (y-2)^2}\n\\]\n\nand suppose we would like to visualize the marginal density for \\(X\\),  \\(\\mathrm{p}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\). In order to represent it with a line plot, we would first have to calculate the integral of the formula above over all possible values of \\(Y\\):\n\n\\[\n\\mathrm{p}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) =\n\\int_{-\\infty}^{\\infty}\n\\Bigl[\n\\tfrac{3}{8\\,\\pi}\\, \\mathrm{e}^{-\\frac{1}{2} (x-1)^2-(y-1)^2}+\n\\tfrac{3}{64\\,\\pi}\\,\\mathrm{e}^{-\\frac{1}{32} (x-2)^2-\\frac{1}{2} (y-4)^2}+ \\tfrac{1}{40\\,\\pi}\\,\\mathrm{e}^{-\\frac{1}{8} (x-5)^2-\\frac{1}{5} (y-2)^2}\n\\Bigr]\n\\, \\mathrm{d}y\n\\]\n\nBut suppose, instead, that we have stored the points used to represent the density as a scatter plot, as in fig.  15.2. Each of these points is a pair of coordinates \\((x,y)\\), representing an \\(X\\)-value and a \\(Y\\)-value. It turns out that these same points can be used to make a scatter-plot of the marginal density for \\(X\\), by considering their \\(x\\)-coordinates only. Often we use a subsample (unsystematically chosen) of them, so that the resulting one-dimensional scatter plot doesn’t become too congested and difficult to read.\nAs an example, here is a scatter plot for the marginal density \\(\\mathrm{p}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\) above, obtained by selecting a subset of 400 points from the scatter plot (fig.  15.2) for \\(\\mathrm{p}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\); the points are replaced by vertical lines for better visibility:\n\n\n\nFigure 16.1: Scatter-plot representation of the marginal probability density \\(\\mathrm{p}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\)\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nThe points for the scatter plot of fig.  15.2 (§  15.4) are saved in the file scatterXY_samples.csv. Use them to represent the marginal probability density \\(\\mathrm{p}(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\), for the other quantity \\(Y\\), as a scatter plot."
  },
  {
    "objectID": "marginal_probability.html#sec-use-pitfall-marginal",
    "href": "marginal_probability.html#sec-use-pitfall-marginal",
    "title": "16  Marginal probability distributions",
    "section": "16.4 Uses and pitfalls of marginal probability distributions",
    "text": "16.4 Uses and pitfalls of marginal probability distributions\nAn agent’s probability distribution for a multi-dimensional joint quantity is not easily – or at all – visualizable. This shortcoming is even worse because, as discussed in §  10.1, our intuition often fails us badly in multi-dimensional problems.\nMarginal probability distributions for one or two of the component quantities are useful because they offer us a little glimpse of the multi-dimensional “monster”. In concrete engineering and data-science problem, when we need to discuss a multi-dimensional distribution it is good practice to visually report at least its one-dimensional marginal distributions.\nIn the machine-learning literature, one frequent application of this low-dimensional glimpse is for qualitatively assessing whether two multi-dimensional distributions are similar. Their one-dimensional marginals are visually compared and, if they overlap, one hopes (and some works in the literature even conclude) that the multi-dimensional distributions are somewhat similar as well.\n\n\nUnfortunately marginal distributions can also be quite deceiving:\n\n\n\n\n\n\n Exercise\n\n\n\nHere are three different joint probability densities for the joint quantity \\((X,Y)\\), each density represented by a scatter plot with 200 points. the files containing the coordinates of the scatter-plot points are also given:\nA. File scatterXY_A.csv:\n\nB. File scatterXY_B.csv:\n\nC. File scatterXY_C.csv:\n\n\n\n\nReproduce the three scatter plots above using the points from the three files, just to confirm that they are correct.\nFor each density, plot the marginal density for the quantity \\(X\\) as a scatter plot. Use the method described in §  16.3; do not subsample the points.\nWhat can you say about the three marginal densities you obtain?\n\nDo the same, but for the marginal densities for \\(Y\\).\nWhat can you say about the three marginal densities you obtain?\nIf two joint probability distributions have the same marginals, can we conclude that they are identical, or at least similar?\n\n\n\n\n\n\n\n\n\n\n\n Study reading\n\n\n\n\n§§ 5.3.2–5.3.3 of Risk Assessment and Decision Analysis with Bayesian Networks\n§ 12.3 of Artificial Intelligence\n§§ 5.1–5.5 of Probability"
  },
  {
    "objectID": "conditional_probability.html#sec-conditional-probs",
    "href": "conditional_probability.html#sec-conditional-probs",
    "title": "17  Conditional probability and learning",
    "section": "17.1 Conditional probability: augmenting knowledge",
    "text": "17.1 Conditional probability: augmenting knowledge\nWhen we introduced the notion of degree of belief – a.k.a. probability – in chapter  8, we stressed the fact that every probability is conditional on some state of knowledge or information. So the term “conditional probability” sounds like a pleonasm, just like saying “round circle”.\nThis term must be understood in a way analogous to “marginal probability”: it applies in situations where we have two or more sentences of interest. We speak of a “conditional probability” when we want to emphasize that additional sentences appear in the conditional (right side of “\\(\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\)”) of that probability, as compared to other probabilities. For instance, in a scenario in which these two probabilities appear:\n\\[\n\\mathrm{P}(\\mathsfit{A} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{B} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\qquad\n\\mathrm{P}(\\mathsfit{A} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\]\nwe call the first conditional probability of \\(\\mathsfit{A}\\) (given \\(\\mathsfit{B}\\)) to emphasize or point out that its conditional includes an additional sentence (\\(\\mathsfit{B}\\)), whereas the conditional of the second probability doesn’t.\nSuch emphasis is important because it also means that the “conditional” probability is based on some additional knowledge, information, or hypothesis with respect to the “non-conditional” one. This has obvious connections with the idea of “learning”. Indeed the calculation of “conditional” probabilities enters in all situations (even if hypothetical or counterfactual, see §  5.1) in which some knowledge is augmented by new knowledge. This can happens in several ways, which we now examine."
  },
  {
    "objectID": "conditional_probability.html#sec-conditional-joint-dis",
    "href": "conditional_probability.html#sec-conditional-joint-dis",
    "title": "17  Conditional probability and learning",
    "section": "17.2 Conditional from joint probability: dissimilar quantities",
    "text": "17.2 Conditional from joint probability: dissimilar quantities\nConsider once more the next-patient arrival scenario of §  15.2, with joint quantity \\((U,T)\\) and an agent’s joint probability distribution as in table  15.1. Suppose that the agent must forecast whether the next patient will require \\({\\small\\verb;urgent;}\\) or \\({\\small\\verb;non-urgent;}\\) care, so it needs to calculate the probability distribution for \\(U\\) (that is, the probabilities for \\(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\) and \\(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\)).\nIn the first exercise of §  16.1 you found that the marginal probability that the next patient will need urgent care is\n\\[\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}}) = 18\\%\\]\nthis is the agent’s degree of belief if it has the knowledge encoded in the sentence \\(\\mathsfit{I}_{\\text{H}}\\), nothing more and nothing less.\nBut now let’s imagine that the agent receives a new piece of information: it is told that the next patient is being transported by helicopter. In other words, the agent now knows that the sentence \\(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\) is true. The agent’s complete knowledge is then encoded in the anded sentence\n\\[T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\land \\mathsfit{I}_{\\text{H}}\\]\nwhich should therefore appear in the conditional. The agent’s belief that the next patient requires urgent care is therefore\n\\[\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{H}})\\]\nCalculation of this probability can be done by just one application of the and-rule:\n\n\\[\n\\begin{aligned}\n&\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}}) =\n\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{H}}) \\cdot\n\\mathrm{P}(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}})\n\\\\[3ex]\n&\\quad\\implies\\quad\n\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{H}})\n=\n\\frac{\n\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}})\n}{\n\\mathrm{P}(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}})\n}\n\\end{aligned}\n\\]\n\nWe do have the joint probability for \\(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\land T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\) that appears in the numerator of the fraction above. The probability in the denominator is just a marginal probability for \\(T\\), and we know how to calculate that too from §  16.1. Finally we find\n\\[\n\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{H}})\n=\\frac{\n\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}})\n}{\n\\sum_u\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}u \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}})\n}\n\\]\nwhere it’s understood that the sum index \\(u\\) runs over the values \\(\\set{{\\small\\verb;urgent;}, {\\small\\verb;non-urgent;}}\\).\nThis is called a conditional probability; in this case, the conditional probability of  \\(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\)  given  \\(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\).\nThe collection of probabilities for all possible values of the quantity \\(U\\), given a specific value of the quantity \\(T\\), say \\({\\small\\verb;helicopter;}\\):\n\\[\n\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{H}}) \\ ,\n\\qquad\n\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{H}})\n\\]\nis called the conditional probability distribution for \\(U\\)  given  \\(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\). It is indeed a probability distribution because the two probabilities sum up to \\(1\\).\n\n\n\n\n\n\n\n\n\n\nNote that the collection of probabilities for, say, \\(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\), but for different values of the conditional quantity \\(T\\), that is\n\\[\n\\begin{aligned}\n&\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{H}}) \\ ,\n\\\\[1ex]\n&\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{H}}) \\ ,\n\\\\[1ex]\n&\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;other;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{H}})\n\\end{aligned}\n\\]\nis not a probability distribution. Calculate the three probabilities above and check that indeed they do not sum up to one.\n\n\n\n\n\n\n\n\n Exercise\n\n\n\n\nUsing the values from table 15.1 and the formula for marginal probabilities, calculate:\n\nThe conditional probability that the next patient needs urgent care, given that the patient is being transported by helicopter.\nThe conditional probability that the next patient is being transported by helicopter, given that the patient needs urgent care.\n\nNow discuss and find an intuitive explanation for these comparisons:\n\nThe two probabilities you obtained above. Are they equal? why or why not?\nThe marginal probability that the next patient will be transported by helicopter, with the conditional probability that the patient will be transported by helicopter given that it’s urgent. Are they equal? if not, which is higher, and why?"
  },
  {
    "objectID": "conditional_probability.html#sec-conditional-joint-sim",
    "href": "conditional_probability.html#sec-conditional-joint-sim",
    "title": "17  Conditional probability and learning",
    "section": "17.3 Conditional from joint probability: similar quantities",
    "text": "17.3 Conditional from joint probability: similar quantities\nIn the previous section we examined how knowledge about one quantity of a particular kind can change an agent’s degree of belief about a quantity of a different kind, for example “transportation” about “urgency” or vice versa. This change is reflected in the value of the corresponding conditional probability.\nThis kind of change can also occur with quantities of a “similar kind”, that is, quantities that represent the same kind of phenomenon and have exactly the same domain. The maths and calculations are identical to those we have explored, but the interpretation and application can be somewhat different.\nAs an example, imagine a scenario similar to the next-patient one above, but now consider the next three patients to arrive, and their urgency. Define the following three quantities:\n\\(U_1\\) : urgency of the next patient\n\\(U_2\\) : urgency of the second future patient from now\n\\(U_3\\) : urgency of the third future patient from now\n\nevery one of these quantities has the same domain: \\(\\set{{\\small\\verb;urgent;},{\\small\\verb;non-urgent;}}\\).\nThe joint quantity \\((U_1, U_2, U_3, U_4)\\) has a domain with 23 = 8 possible values:\n\n\\(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\)\n\\(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\)\n. . .\n\\(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\)\n\\(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\)\n\nSuppose that an agent, with background information \\(\\mathsfit{I}\\), has a joint probability distribution for the joint quantity \\((U_1, U_2, U_3)\\); the distribution is implicitly given as follows: \n\nIf \\({\\small\\verb;urgent;}\\) appears 0 times out of 3: probability = \\(53.6\\%\\)\nIf \\({\\small\\verb;urgent;}\\) appears 1 times out of 3: probability = \\(11.4\\%\\)\nIf \\({\\small\\verb;urgent;}\\) appears 2 times out of 3: probability = \\(3.6\\%\\)\nIf \\({\\small\\verb;urgent;}\\) appears 3 times out of 3: probability = \\(1.4\\%\\)\n\nexamples:\n\\[\n\\begin{aligned}\n&\\mathrm{P}(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n= 0.036\n\\\\[1ex]\n&\\mathrm{P}(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n= 0.114\n\\\\[1ex]\n&\\mathrm{P}(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n= 0.036\n\\\\[1ex]\n\\end{aligned}\n\\]\n\n\n\n\n\n\n Exercise\n\n\n\n\nCheck that the joint probability distribution as defined above indeed sums up to \\(1\\).\nCalculate the marginal probability for \\(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\), that is,  \\(\\mathrm{P}(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\\). \nCalculate the marginal probability that the second and third patients are non-urgent cases, that is\n\n\\[\\mathrm{P}(U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}) \\ .\\] \n\n\nFrom this joint probability distribution the agent can calculate, among other things, its degree of belief that the third patient from now will require urgent care, regardless of the urgency of the preceding two patients. It’s the marginal probability\n\\[\n\\begin{aligned}\n\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})  &=\n\\sum_{u_1}\\sum_{u_2}\n\\mathrm{P}(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}u_1 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}u_2 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\\\[1ex]\n&= 0.114 + 0.036 + 0.036 + 0.014\n\\\\[1ex]\n&= \\boldsymbol{20.0\\%}\n\\end{aligned}\n\\]\nwhere the first term \\(0.114\\) in the sum corresponds to \\(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\), the second term \\(0.036\\) to \\(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\), and so on.\nTherefore the agent, right now, has a \\(20\\%\\) degree of belief that the third patient from now will require urgent care.\n\n\nNow fast-forward in time, after two patients have arrived and been taken good care of. Suppose that both were non-urgent cases, and the agent knows this. The agent needs to forecast whether the next (third) patient will require urgent care.\nIt wouldn’t be sensible to use  \\(\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\\),  calculated above, because this degree of belief represents an agent having only the background knowledge \\(\\mathsfit{I}\\). Now, instead, the agent has additional information about the first two patients, encoded in this anded sentence:\n\\[\nU_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\n\\]\nThe relevant degree of belief is therefore the conditional probability\n\\[\n\\begin{aligned}\n&\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\\\[2ex]\n&\\qquad{}=\n\\frac{\n\\mathrm{P}(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}{\n\\mathrm{P}(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}\n\\\\[1ex]\n&\\qquad{}=\\frac{0.114}{0.65}\n\\\\[2ex]\n&\\qquad{}\\approx\n\\boldsymbol{17.5\\%}\n\\end{aligned}\n\\]\nThis conditional probability of \\(17.5\\%\\) for \\(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\) is lower than the marginal one \\(20.0\\%\\) calculated previously. Observation of two patients has thus affected the agent’s degree of belief.\n\n\nLet’s also check how the agent’s belief changes in the case where the first two patients are both urgent. The calculation is completely analogous:\n\\[\n\\begin{aligned}\n&\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\\\[2ex]\n&\\qquad{}=\n\\frac{\n\\mathrm{P}(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}{\n\\mathrm{P}(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}\n\\\\[1ex]\n&\\qquad{}=\\frac{0.030}{0.107}\n\\\\[2ex]\n&\\qquad{}\\approx\n\\boldsymbol{28.0\\%}\n\\end{aligned}\n\\]\nIn this case the conditional probability \\(28.0\\%\\) for \\(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\) is higher than the marginal one \\(20.0\\%\\).\nOne possible intuitive explanation of these probability changes, in the present scenario, is that observation of two non-urgent cases makes the agent slightly more confident that “this is a day with few urgent cases”; whereas observation of two urgent cases makes the agent more confident that “this is a day with many urgent cases”.\n\n\n\n\n\n\n\n\n\n\nIn general we cannot say that the probability of a particular value (such as \\({\\small\\verb;urgent;}\\) in the scenario above) will decrease or increase as similar or dissimilar values are observed, nor how much the increase or decrease will be.\nIn a different situation the probability of \\({\\small\\verb;urgent;}\\) could actually increase as more and more \\({\\small\\verb;non-urgent;}\\) cases are observed. Imagine, for instance, a scenario where the agent initially knows that there are 10 urgent and 90 non-urgent cases ahead. Having observed 90 non-urgent cases, the agent will give a much higher probability – 100% – that the next case will be an urgent one.\nThe differences among such scenarios are reflected in differences of the joint probabilities, from which the conditional probabilities are calculated.\nAll these situations are correctly handled with the four fundamental rules of inference and the formula for conditional probability derived from them.\n\n\n\n\n\n\n\n\n Exercises\n\n\n\n\nUsing the same joint distribution above, calculate\n\\[\\mathrm{P}(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\]\nthat is, the probability that the next patient will require urgent care given that the agent knows the second and third patients will not-require urgent care.\n\nWhy is the value obtained different from  \\(\\mathrm{P}(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\\) ?\nDescribe a scenario in which the calculation above makes sense (and patients 2 and 3 still arrive after patient 1).\n\n\n\n\n\nDo an analysis completely analogous to the previous, three-patient one, but with different background information \\(\\mathsfit{J}\\) that gives a joint probability distribution for \\((U_1, U_2, U_3)\\) as follows:\n• If \\({\\small\\verb;urgent;}\\) appears 0 times out of 3: probability = \\(0\\%\\)\n• If \\({\\small\\verb;urgent;}\\) appears 1 times out of 3: probability = \\(24.5\\%\\)\n• If \\({\\small\\verb;urgent;}\\) appears 2 times out of 3: probability = \\(7.8\\%\\)\n• If \\({\\small\\verb;urgent;}\\) appears 3 times out of 3: probability = \\(3.1\\%\\)\n\nCalculate\n\\[\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{J})\\] \nand\n\\[\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{J})\\] and compare them.\nExplain why this particular change in degree of belief occurs, in this situation."
  },
  {
    "objectID": "conditional_probability.html#sec-conditional-joint-general",
    "href": "conditional_probability.html#sec-conditional-joint-general",
    "title": "17  Conditional probability and learning",
    "section": "17.4 General case: conditional from joint",
    "text": "17.4 General case: conditional from joint\nTake the time to review the two sections above, focusing on the application and meaning of the two scenarios and calculations, and noting the similarities and differences:\n\n The calculations were completely analogous; in particular, the conditional probability was obtained as the quotient of a joint probability and a marginal one.\n In the first (next-patient) scenario, information about one aspect of the situation changed the agent’s belief about another aspect; the two aspects were somewhat different (transportation and urgency). Whereas in the second (three-patient) scenario, information about analogous occurrences of an aspect of the situation changed the agent’s belief about a further occurrence.\n\n\n\nA third scenario is also possible, which combines the two above. Consider the case with three patients, where each patient can require \\({\\small\\verb;urgent;}\\) care or not, and can be transported by \\({\\small\\verb;ambulance;}\\), \\({\\small\\verb;helicopter;}\\), or \\({\\small\\verb;other;}\\) means. To describe this situation, introduce three pairs of quantities, which together form the joint quantity\n\\[\n(U_1, T_1, \\ U_2, T_2, \\ U_3, T_3)\n\\]\nwhose meaning should now be obvious. This joint quantity has \\((2\\cdot 3)^3 = 216\\) possible values, corresponding to all urgency & transportation combinations for the three patients.\nGiven the joint probability distribution for this joint quantity, it is possible to calculate all kinds of conditional probabilities, which reflect the knowledge that the agent may have acquired. For instance, suppose the agent has observed that\n\nthe first two patients have not required urgent care\nthe first patient was transported by ambulance\nthe second patient was transported by other means\nthe third patient is arriving by ambulance\n\nand needs to infer whether the third patient will require urgent care. The required probability is\n\n\\[\n\\begin{aligned}\n&\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nU_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nU_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;other;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathsfit{I})\n\\\\[2ex]\n&\\qquad{}=\n\\frac{\n\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nU_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nU_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;other;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\mathsfit{I})\n}{\n\\mathrm{P}(T_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nU_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nU_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;other;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathsfit{I})\n}\n\\end{aligned}\n\\]\n\nand is calculated in a way completely analogous to the ones already seen.\n\n\nAll three kinds of inference scenarios frequently occur in data science and engineering. In machine learning, the second scenario is connected to “unsupervised learning”; the third, mixed one to “supervised learning”. As you just saw, the probability calculus “sees” all of the them as analogous: information about something changes the agent’s belief about something else. And the handling of all three cases is perfectly covered by the four fundamental rules of inference.\n\n\nLet’s now consider a more generic case of a joint quantity with component quantities \\(\\color[RGB]{34,136,51}X\\) and \\(\\color[RGB]{238,102,119}Y\\), whose joint probability distribution is given. The two quantities could be complicated joint quantities themselves. The conditional probability for \\(\\color[RGB]{238,102,119}Y\\), given that \\(\\color[RGB]{34,136,51}X\\) has some specific value \\(\\color[RGB]{34,136,51}x^*\\), is then\n\\[\n\\mathrm{P}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x^*}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}) =\n\\frac{\n\\mathrm{P}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x^*}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\n}{\n\\sum_{\\color[RGB]{238,102,119}\\upsilon}\\mathrm{P}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\upsilon}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x^*}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\n}\n\\tag{17.1}\\]\nfor all possible values \\(\\color[RGB]{238,102,119}y\\)."
  },
  {
    "objectID": "conditional_probability.html#sec-conditional-conditional",
    "href": "conditional_probability.html#sec-conditional-conditional",
    "title": "17  Conditional probability and learning",
    "section": "17.5 Conditional from conditional probability",
    "text": "17.5 Conditional from conditional probability\nAs emphasized in §  5.2, probabilities are either obtained from other probabilities, or taken as given probabilities, maybe determined by symmetry requirements. This is also true when we want to calculate conditional probabilities.\nUp to now we have calculated conditional probabilities starting from the joint distribution as given, using the derived formula (17.1). In some situations, however, an agent may have given conditional probabilities together with given marginal probabilities.\nAs an example let’s consider a variation of our next-patient scenario one more time. The agent has background information \\(\\mathsfit{I}_{\\text{S}}\\) that provides the following set of probabilities:\n\nTwo conditional probability distributions  \\(\\mathrm{P}(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{S}})\\) for transportation \\(T\\) given urgency \\(U\\), as reported in the following table:\n\n\n\nTable 17.1: Probability distributions for transportation given urgency\n\n\n\n\n\n\n\n\n\n\\(\\mathrm{P}(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}t \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}u\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{S}})\\)\n\ntransportation at arrival  \\(T\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}{}\\)\n\n\n\n\nambulance\nhelicopter\nother\n\n\ngiven urgency  \\({}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}U\\)\nurgent\n0.61\n0.22\n0.17\n\n\nnon-urgent\n0.21\n0.01\n0.78\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis table has two probability distributions: on the first row, one conditional on \\(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\); on the second row, one conditional on \\(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\). Check that the probabilities on each row indeed sum up to one.\n\n\n\n\n\nMarginal probability distribution  \\(\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{S}})\\) for urgency \\(U\\):\n\n\\[\n\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{S}}) = 0.18 \\ ,\n\\quad\n\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{S}}) = 0.82\n\\tag{17.2}\\]\n\n\nWith this background information, the agent can also compute all joint probabilities simply using the and-rule. For instance\n\\[\n\\begin{aligned}\n&P(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{S}})\n\\\\[1ex]\n&\\quad{}=\nP(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{S}}) \\cdot\nP(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{S}})\n\\\\[1ex]\n&\\quad{}= 0.22 \\cdot 0.18 = \\boldsymbol{3.96\\%}\n\\end{aligned}\n\\]\n\n\nNote that the joint probabilities are slightly different compared with those from the previous background information \\(\\mathsfit{I}_{\\text{H}}\\).\nAnd from the joint probabilities, the marginal ones for transportation \\(T\\) can also be calculated. For instance\n\\[\n\\begin{aligned}\n&P(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{S}})\n\\\\[1ex]\n&\\quad{}=\n\\sum_u P(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}u \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{S}})\n\\\\[1ex]\n&\\quad{}=\n\\sum_u P(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}u \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{S}}) \\cdot\nP(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}u \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{S}})\n\\\\[1ex]\n&\\quad{}=\n0.22 \\cdot 0.18 +\n0.01 \\cdot 0.82\n\\\\[1ex]\n&\\quad{}= \\boldsymbol{4.78\\%}\n\\end{aligned}\n\\]\nSuppose that the agent knows that the next patient is being transported by \\({\\small\\verb;helicopter;}\\), and needs to forecast whether \\({\\small\\verb;urgent;}\\) care will be needed. This inference is the conditional probability  \\(\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{S}})\\), which can also be rewritten in terms of the set of probabilities initially given:\n\\[\n\\begin{aligned}\n&\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{H}})\n\\\\[2ex]\n&\\quad{}=\\frac{\n\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}})\n}{\n\\mathrm{P}(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}})\n}\n\\\\[1ex]\n&\\quad{}=\\frac{\nP(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{S}}) \\cdot\nP(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{S}})\n}{\n\\sum_u P(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}u \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{S}}) \\cdot\nP(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}u \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{S}})\n}\n\\\\[1ex]\n&\\quad{}=\\frac{0.0396}{0.0478}\n\\\\[2ex]\n&\\quad{}=\\boldsymbol{82.8\\%}\n\\end{aligned}\n\\]\nThis calculation has been slightly more involved than the one in §  17.2 because the joint probabilities were not directly available. Our calculation involved the steps  “ \\(T\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}U \\longrightarrow T\\land U \\longrightarrow U\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}T\\) ”.\n\n\nIf the agent were instead interested, say, in forecasting the transportation means knowing that the next patient requires urgent care, then the relevant degree of belief  \\(\\mathrm{P}(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{S}})\\) would be immediately available and no calculations would be needed."
  },
  {
    "objectID": "conditional_probability.html#sec-conditional-conditional-general",
    "href": "conditional_probability.html#sec-conditional-conditional-general",
    "title": "17  Conditional probability and learning",
    "section": "17.6 General case: conditional from conditional",
    "text": "17.6 General case: conditional from conditional\nThe example from the previous section can be easily generalized. Consider a joint quantity with component quantities \\(\\color[RGB]{34,136,51}X\\) and \\(\\color[RGB]{238,102,119}Y\\). The probabilities  \\(\\mathrm{P}({\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\)  and  \\(\\mathrm{P}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\)  are given.\nThe conditional probability for \\(\\color[RGB]{238,102,119}Y\\), given that \\(\\color[RGB]{34,136,51}X\\) has some specific value \\(\\color[RGB]{34,136,51}x^*\\), is then\n\\[\n\\mathrm{P}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x^*}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}) =\n\\frac{\n\\mathrm{P}( {\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x^*}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}) \\cdot\n\\mathrm{P}( {\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\n}{\n\\sum_{\\color[RGB]{238,102,119}\\upsilon}\n\\mathrm{P}( {\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x^*}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\upsilon} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}) \\cdot\n\\mathrm{P}( {\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\upsilon} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\n}\n\\tag{17.3}\\]\nfor all possible values \\(\\color[RGB]{238,102,119}y\\).\nIn the above formula we recognize Bayes’s theorem from §  9.5.\nThis formula is often exaggeratedly emphasized in the literature; some texts even present it as an “axiom” to be used in situations such as the present one. But we see that it is simply a by-product of the four fundamental rules of inference in a specific situation. An AI agent who knows the four fundamental inference rules, and doesn’t know what “Bayes’s theorem” is, will nevertheless arrive at this very formula."
  },
  {
    "objectID": "conditional_probability.html#sec-conditional-dens",
    "href": "conditional_probability.html#sec-conditional-dens",
    "title": "17  Conditional probability and learning",
    "section": "17.7 Conditional densities",
    "text": "17.7 Conditional densities\nThe discussion so far about conditional probabilities extends to conditional probability densities, in the usual way explained in §§ 15.3 and 16.2.\nIf \\(\\color[RGB]{34,136,51}X\\) and \\(\\color[RGB]{238,102,119}Y\\) are continuous quantities, the notation\n\\[\n\\mathrm{p}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}) = {\\color[RGB]{68,119,170}q}\n\\]\nmeans that, given background information \\(\\mathsfit{I}\\) and given the sentence “\\(\\color[RGB]{34,136,51}X\\) has value between \\(\\color[RGB]{34,136,51}x-\\delta/2\\) and \\(\\color[RGB]{34,136,51}x+\\delta/2\\)”, the sentence “\\(\\color[RGB]{238,102,119}Y\\) has value between \\(\\color[RGB]{238,102,119}y-\\epsilon/2\\) and \\(\\color[RGB]{238,102,119}y+\\epsilon/2\\)” has probability \\({\\color[RGB]{68,119,170}q}\\cdot{\\color[RGB]{238,102,119}\\epsilon}\\), as long as \\(\\color[RGB]{34,136,51}\\delta\\) and \\(\\color[RGB]{238,102,119}\\epsilon\\) are small enough. Note that the small interval \\(\\color[RGB]{34,136,51}\\delta\\) for \\(\\color[RGB]{34,136,51}X\\) is not multiplied by the density \\(\\color[RGB]{68,119,170}q\\).\nThe relation between a conditional density and a joint density or a different conditional density is given by\n\\[\n\\begin{aligned}\n&\\mathrm{p}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\\\[1ex]\n&\\quad{}=\n\\frac{\\displaystyle\n\\mathrm{p}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}{\\displaystyle\n\\int_{\\color[RGB]{238,102,119}\\varUpsilon}\\mathrm{p}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\upsilon} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) \\, \\mathrm{d}{\\color[RGB]{238,102,119}\\upsilon}\n}\n\\\\[1ex]\n&\\quad{}=\n\\frac{\\displaystyle\n\\mathrm{p}({\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}) \\cdot\n\\mathrm{p}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}{\\displaystyle\n\\int_{\\color[RGB]{238,102,119}\\varUpsilon} \\mathrm{p}({\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\upsilon} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}) \\cdot\n\\mathrm{p}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\upsilon} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\, \\mathrm{d}{\\color[RGB]{238,102,119}\\upsilon}\n}\n\\end{aligned}\n\\]\nwhere \\(\\color[RGB]{238,102,119}\\varUpsilon\\) is the domain of \\(\\color[RGB]{238,102,119}Y\\)."
  },
  {
    "objectID": "conditional_probability.html#sec-repr-conditional",
    "href": "conditional_probability.html#sec-repr-conditional",
    "title": "17  Conditional probability and learning",
    "section": "17.8 Graphical representation of conditional probability distributions and densities",
    "text": "17.8 Graphical representation of conditional probability distributions and densities\nConditional probability distributions and densities can be plotted in all the ways discussed in chapters 15 and 16. If we have two quantities \\(A\\) and \\(B\\), often we want to compare the different conditional probability distributions for \\(A\\) given different values of \\(B\\):\n\n\\(\\mathrm{P}(A\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} B\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;one-value;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\),\n\\(\\mathrm{P}(A\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} B\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;another-value;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\),\n\\(\\dotsc\\)\n\nand so on. This can be achieved by representing them by overlapping line plots, or side-by-side scatter plots, or similar ways.\n\n\nIn §  16.3 we saw that if we have the scatter plot for a joint probability density, then from its points we can often obtain a scatter plot for its marginal densities. Unfortunately no similar advantage exists for the conditional densities that can be obtained from a joint density. In theory, a conditional density for \\(Y\\), given that a quantity \\(X\\) has value in some small interval \\(\\delta\\) around \\(x\\), could be obtained by only considering scatter-plot points having \\(X\\) coordinate in a small interval between \\(x-\\delta/2\\) and \\(x+\\delta/2\\). But the number of such points is usually too small and the resulting scatter plot could be very misleading.\n\n\n\n\n\n\n\n\n Study reading\n\n\n\n\n§ 5.4 of Risk Assessment and Decision Analysis with Bayesian Networks\n§§ 12.2.1, 12.3, and 12.5 of Artificial Intelligence\n§§ 4.1–4.3 in Medical Decision Making\n§§ 5.1–5.5 of Probability – yes, once more!"
  },
  {
    "objectID": "information.html#sec-indep-sentences",
    "href": "information.html#sec-indep-sentences",
    "title": "18  Information, relevance, independence, association",
    "section": "18.1 Independence of sentences",
    "text": "18.1 Independence of sentences\nIn an ordinary situation represented by background information \\(\\mathsfit{I}\\), if you have to infer whether a coin will land heads, then knowing that it is raining outside has no impact on your inference: the information about rain is irrelevant for your inference. In other words, your degree of belief about the coin remains the same if you include the information about rain in the conditional.\nIn probability notation, representing “The coin lands heads” with \\(\\mathsfit{H}\\) and “It rains outside” with \\(\\mathsfit{R}\\), this irrelevance means\n\\[\n\\mathrm{P}(\\mathsfit{H} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = \\mathrm{P}(\\mathsfit{H} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{R} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\]\n\n\nMore generally two sentences \\(\\mathsfit{A}\\), \\(\\mathsfit{B}\\) are said to be mutually irrelevant or logically independent given information \\(\\mathsfit{I}\\) if any one of these three conditions holds:\n\n\n“independEnt” is written with an E, not with an A.\n\n\\(\\mathrm{P}(\\mathsfit{A}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{B}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}) = \\mathrm{P}(\\mathsfit{A}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\)\n\\(\\mathrm{P}(\\mathsfit{B}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{A}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}) = \\mathrm{P}(\\mathsfit{B}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\)\n\\(\\mathrm{P}(\\mathsfit{A}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{B}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = \\mathrm{P}(\\mathsfit{A}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) \\cdot \\mathrm{P}(\\mathsfit{B}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\)\n\nThese three conditions are equivalent to one another. In the first condition, \\(\\mathrm{P}(\\mathsfit{A}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{B}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\) is undefined if \\(\\mathrm{P}(\\mathsfit{B}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})=0\\), but in this case independence still holds; analogously in the second condition.\n\n\n\n\n\n\n\n\n\n\n\nIrrelevance or independence is not an absolute notion, but relative to some background knowledge. Two sentences may be independent given some background information, and not independent given another.\nIndependence as defined above is a logical, not physical, notion. It isn’t stating anything about physical dependence between phenomena related to the sentences \\(\\mathsfit{A}\\) and \\(\\mathsfit{B}\\). It’s simply stating that information about one does not affect an agent’s beliefs about the other."
  },
  {
    "objectID": "information.html#sec-indep-quantities",
    "href": "information.html#sec-indep-quantities",
    "title": "18  Information, relevance, independence, association",
    "section": "18.2 Independence of quantities",
    "text": "18.2 Independence of quantities\nThe notion of irrelevance of two sentences can be generalized to quantities. Take two quantities \\(X\\) and \\(Y\\). They are said to be mutually irrelevant or logically independent given information \\(\\mathsfit{I}\\) if any one of these three condition holds for all possible values \\(x\\) of \\(X\\) and \\(y\\) of \\(Y\\):\n\n\\(\\mathrm{P}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}) = \\mathrm{P}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\)   all \\(x,y\\)\n\\(\\mathrm{P}(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}) = \\mathrm{P}(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\)   all \\(x,y\\)\n\\(\\mathrm{P}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = \\mathrm{P}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) \\cdot \\mathrm{P}(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\)   all \\(x,y\\)\n\n\n\nNote the difference between independence of two sentences and independence of two quantities. The latter independence involves not just two, but many sentences: as many as the values of \\(X\\) and \\(Y\\).\nIn fact it may happen that for some particular values \\(x^*\\) of \\(X\\) and \\(y^*\\) \\(Y\\) the probabilities become independent, say:\n\\[\n\\mathrm{P}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x^* \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y^* \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}) = \\mathrm{P}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x^* \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\n\\]\nbut this equality does not occur for other values. In this case the quantities \\(X\\) and \\(Y\\) are not independent given information \\(\\mathsfit{I}\\). The general idea is that two quantities are independent if knowledge about one of them cannot change an agent’s beliefs about the other, no matter what their values might be.\n\n\n\n\n\n\n\n\n\n\n\nAlso in this case, irrelevance or independence is not an absolute notion, but relative to some background knowledge. Two quantities may be independent given some background information, and not independent given another.\nAlso in this case, independence is a logical, not physical, notion. It isn’t stating anything about physical dependence between phenomena related to the quantities \\(X\\) and \\(Y\\). It’s simply stating that information about one does not affect an agent’s beliefs about the other.\n\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nConsider our familiar next-patient inference problem with quantities urgency \\(U\\) and transportation \\(T\\). Assume a different background information \\(\\mathsfit{J}\\) that leads to the following joint probability distribution:\n\n\n\n\n\n\n\n\n\n\n\n\\(\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}u \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}t\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{J})\\)\n\ntransportation at arrival \\(T\\)\n\n\n\n\nambulance\nhelicopter\nother\n\n\nurgency \\(U\\)\nurgent\n0.15\n0.08\n0.02\n\n\nnon-urgent\n0.45\n0.04\n0.26\n\n\n\n\nCalculate the marginal probability distribution \\(\\mathrm{P}(U\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{J})\\) and the conditional probability distribution \\(\\mathrm{P}(U \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{J})\\), and compare them. Is the value \\(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\) relevant for inferences about \\(U\\)? \nCalculate the conditional probability distribution \\(\\mathrm{P}(U \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{J})\\), and compare it with the marginal \\(\\mathrm{P}(U\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{J})\\). Is the value \\(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\) relevant for inferences about \\(U\\)? \nAre the quantities \\(U\\) and \\(T\\) independent, given the background knowledge \\(\\mathsfit{J}\\)?"
  },
  {
    "objectID": "information.html#sec-info-uncertainty",
    "href": "information.html#sec-info-uncertainty",
    "title": "18  Information, relevance, independence, association",
    "section": "18.3 Information and uncertainty",
    "text": "18.3 Information and uncertainty\nThe definition of irrelevance given above appears to be very “black or white”: either two sentences or quantities are independent, or they aren’t. But in reality there is no such dichotomy. We can envisage some scenario \\(\\mathsfit{I}\\) where for instance the probabilities \\(\\mathrm{P}(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\) and \\(\\mathrm{P}(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\) are extremely close in value, although not exactly equal:\n\\[\n\\mathrm{P}(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n= \\mathrm{P}(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) + \\delta(x,y)\n\\]\nwith \\(\\delta(x,y)\\) very small. This would mean that knowledge about \\(X\\) modifies an agent’s belief just a little; and depending on the situation such modification could be unimportant. In this situation the two quantities would be “independent” for all practical purposes. Therefore there are degrees of relevance rather than a dichotomy “relevant vs irrelevant”.\nThis suggests that we try to quantify such degrees. This quantification would also give a measure of how “important” a quantity can be for inferences about another quantity.\nThis is the domain of Information Theory, which would require a course by itself to be properly explored. In this chapter we shall just get an overview of the main ideas and notions of this theory.\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\n\nInformation Theory, Inference, and Learning Algorithms\nElements of Information Theory\n\n\n\n\n\nThe notion of degree of relevance is important in data science and machine learning, because it rigorously quantifies two related, intuitive ideas often occurring in these fields:\n\n“Correlation” or “association”: in its general meaning, it’s the idea that if an agent’s knowledge about some quantity changes, then knowledge about another quantity may change as well.\n“Feature importance”: it’s the idea that knowledge about some aspects of a given problem may lead to improved inferences about other aspects.\n\nIn the next section we explore some tricky aspects and peculiarities of these ideas, which also tell us which kind of properties a quantitative measure for them should possess."
  },
  {
    "objectID": "information.html#sec-importance-scenarios",
    "href": "information.html#sec-importance-scenarios",
    "title": "18  Information, relevance, independence, association",
    "section": "18.4 Exploring “importance”: some scenarios",
    "text": "18.4 Exploring “importance”: some scenarios\nThe following examples are only meant to give an intuitive motivation of the characteristics that a metric for “importance” or “relevance” should have.\n\nFirst two characteristics: a lottery scenario\nA lottery comprises 1 000 000 tickets numbered from 000000 to 999999. One of these tickets is the winner. Its number is already known. You are allowed to choose any ticket you like, before the winner number is announced.\nBefore you choose, you have the possibility of getting (for free) some clues about the winning number. The clues are these:\n\nClue A:       \n\nThe first four digits of the winning number\n\nClue B:       \n\nThe 1st, 2nd, 3rd, and 5th digits of the winning number\n\nClue C:       \n\nThe last three digits of the winning number\n\n\nNow consider the following three “clue scenarios”.\n\nScenario 1: choose one clue\nYou have the possibility of choosing one of the three clues above. Which would you choose, in order of importance?\nObviously A or B are the most important, and equally important, because they increase your probability of winning from 1/1 000 000 to 1/100. C is the least important because it increases your probability of winning to 1/1000.\n\n\nScenario 2: discard one clue\nAll three clues are put in front of you (but you can’t see their digits). If you could keep all three, then you’d win for sure because they would give you all digits of the winning number.\nYou are instead asked to discard one of the three clues, keeping the remaining too. Which would you discard, in order of least importance?\nIf you discarded A, then B and C together would give you all digits of the winning number; so you would still win. Analogously if you discarded B. If you discarded C, then A and B together would give you all digits but the last; so you’d have a 1/10 probability of winning.\nObviously C is the most important clue to keep, and A and B least important.\n\n\nScenario 3: discard one more clue\nIn the previous Scenario 2, we saw that discarding A or B would not alter your 100% probability of winning. Either clue could therefore be said to have “importance = 0”.\nIf you had to discard both A and B, however, your situation would suddenly become worse, with only a 1/1000 probability of winning – like choosing C in Scenario 1. Clues A and B together can therefore be said to have high “importance &gt; 0”.\nNow let’s draw some conclusions from comparing the scenarios above.\n\n\nIn Scenario 1 we found that the “importance ranking” of the clues is\nA = B &gt; C\nwhereas in Scenario 2 we found the completely opposite ranking\nC &gt; A = B\nWe conclude that\n\n\n\n\n\n\nImportance is context-dependent\n\n\n\n\nIt doesn’t make sense to ask which aspect or feature is “most important” if we don’t specify the context of its use. Important if used alone? Important if used with others? and which others?\nDepending on the context, an importance ranking could be completely reversed. A quantitative measure of “importance” must therefore take the context into account.\n\n\n\n\n\nIn Scenario 3 we found that two clues may be completely unimportant if considered individually, but extremely important if considered jointly.\nWe conclude that\n\n\n\n\n\n\nImportance is non-additive\n\n\n\n\nA quantitative measure of importance cannot be additive, that is, quantify the importance of two or more features as the sum of their individual importance.\n\n\n\n\n\n\nThird characteristic: A two-quantity scenario\nSuppose we have a discrete quantities \\(X\\) with domain \\(\\set{1,2,3,4,5,6}\\) and another discrete quantity \\(Y\\) with domain \\(\\set{1,2,3,4}\\). We want to infer the value of \\(Y\\) after we are told the value of \\(X\\).\nThe conditional probabilities for \\(Y\\) given different values of \\(X\\) are as follows (each column sums up to \\(1\\))\n\n\nTable 18.1: Example conditional distribution for two discrete quantities\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\mathrm{P}(Y\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}X\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\)\n\n  \\({}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}X\\)\n\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n  \\(Y\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}{}\\)\n1\n1.00\n0.00\n0.00\n0.00\n0.00\n0.50\n\n\n2\n0.00\n1.00\n0.00\n0.00\n0.50\n0.00\n\n\n3\n0.00\n0.00\n1.00\n0.50\n0.00\n0.00\n\n\n4\n0.00\n0.00\n0.00\n0.50\n0.50\n0.50\n\n\n\n\nLet’s see what kind of inferences could occur.\nIf we observe \\(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}1\\), then we know for sure that \\(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}1\\); similarly for \\(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}2\\) and \\(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}3\\). These three values of \\(X\\) are therefore “most important” for inference about \\(Y\\). If we instead observe \\(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}4\\), then our uncertainty about \\(Y\\) is between two of its values; similarly for \\(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}5\\) and \\(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}6\\), These three values of \\(X\\) are therefore “least important” for inference about \\(Y\\).\nBut we usually speak of importance of a quantity or feature, not of a specific value. So what is the “overall importance” of \\(X\\)?\nConsider again three scenarios.\n\nIn the first, we have 33% probability each of observing values \\(1\\), \\(2\\), \\(3\\) of \\(X\\), for a total of 99%; and 0.33% probability of observing each of the remaining values, for a total of 1%.\nIn this scenario we expect to make an almost exact inference about \\(Y\\), and the quantity \\(X\\) has therefore great “overall importance”.\nIn the second, the reverse happens: we have 0.33% probability each of observing values \\(1\\), \\(2\\), \\(3\\) of \\(X\\), for a total of 1%; and 33% probability of observing each of the remaining values, for a total of 99%.\nIn this scenario we expect to uncertain roughly between two values of \\(Y\\), and the quantity \\(X\\) has therefore less “overall importance”.\nIn the third, we have around 16.7% probability of observing each of the values of \\(X\\).\nThis scenario is in between the first two: we expect to make an exact inference about \\(Y\\) half of the time, and to be undecided between two values half of the time. The quantity \\(X\\) has therefore some “overall importance”: neither zero, nor as much as in the first scenario.\n\nWhat determines the “overall importance” of the quantity or feature \\(X\\) is therefore its probability distribution.\nWe conclude that\n\n\n\n\n\n\nThe importance of a quantity depends on its probability distribution\n\n\n\n\nThe importance of a quantity is not only determined by the relation between its possible values and what we need to infer, but also by the probability with which its values can occur.\nA quantitative measure of “importance” of a quantity must therefore take the probability distribution for the latter into account."
  },
  {
    "objectID": "information.html#sec-entropy-mutualinfo",
    "href": "information.html#sec-entropy-mutualinfo",
    "title": "18  Information, relevance, independence, association",
    "section": "18.5 Entropies and mutual information",
    "text": "18.5 Entropies and mutual information\nThe thought-experiments above suggest that a quantitative measure of the importance of a quantity must have at least these three characteristics:\n\ntake the context somehow into account\nbe non-additive\ntake the probability distribution for the quantity into account\n\nDo measures with such properties exist?\nThey do. Indeed they are regularly used in Communication Theory and Information Theory, owing to the properties above. They even have international standards on their definition and measurement units. Before presenting them, let’s briefly present the mother of them all.\n\nShannon entropy\nAn agent with background knowledge \\(\\mathsfit{I}\\) has a belief distribution \\(\\mathrm{P}(Y\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\\) about the (finite) quantity \\(Y\\). The agent’s uncertainty about the value of \\(Y\\) can be quantified by the Shannon entropy:\n\\[\n\\mathrm{H}(Y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) \\coloneqq-\\sum_y \\mathrm{P}(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\, \\log_2 \\mathrm{P}(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\]\nwhose unit is the shannon (symbol \\(\\mathrm{Sh}\\)) when the logarithm is in base 2, as above.\nShannon entropy lies at the foundation of the whole fields of Information Theory and Communication Theory, and would require a lengthy discussion. Let’s just mention some of its properties and meanings:\n\nIt also quantifies the information gained by the agent, if it acquires knowledge about the value of \\(Y\\).\nIt is always positive or zero.\nIt is zero if, and only if, the agent knows the value of \\(Y\\), that is, if the probability distribution for \\(Y\\) gives 100% to one value and 0% to all others.\nIts maximum possible value is \\(\\log_2 N\\), where \\(N\\) is the number of possible values of \\(Y\\). This maximum is attained by the uniform distribution for \\(Y\\).\nThe value (in shannons) of the Shannon entropy can be interpreted as the number of binary digits that we lack for correctly identifying the value of \\(Y\\), if the possible values were listed as integers in binary format. Alternatively, a Shannon entropy equal to  \\(h\\,\\mathrm{Sh}\\)  is equivalent to being fully uncertain among \\(2^h\\) possible alternatives.\n\n\n\n Plot of the Shannon entropy for a binary quantity \\(Y\\in\\set{1,2}\\), for different distributions \\(\\mathrm{P}(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}1\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\)\n\n\nHere are the two most useful measures of “relevance” or “importance”, both based on the Shannon entropy:\n\n\nConditional entropy\nThe conditional entropy1 of a quantity \\(Y\\) given a quantity \\(X\\) and additional knowledge \\(\\mathsfit{I}\\), is defined as1 or “equivocation” according to ISO standard\n\n\\[\n\\mathrm{H}(Y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}) \\coloneqq\n-\\sum_x \\sum_y\n\\mathrm{P}( X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\cdot\n\\mathrm{P}(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\,\n\\log_2 \\mathrm{P}(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\]\n\nand, as defined above, is measured in shannons (symbol \\(\\mathrm{Sh}\\)).\nIt satisfies the three requirements: (1) different additional knowledge \\(\\mathsfit{I}\\), corresponding to different contexts, leads to different probabilities; (2) if the quantity \\(X\\) can be split into two component quantities, it can be proved that the conditional entropy given them jointly is more than the sum of the conditional entropies given them individually; (3) the probability distribution for \\(X\\) explicitly appears in its definition.\nIt has several remarkable properties:\n\nIf knowledge of \\(Y\\) is completely determined by that of \\(X\\), that is, if \\(Y\\) is a function of \\(X\\), then the conditional entropy \\(\\mathrm{H}(Y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\) is zero. Vice versa, if the conditional entropy is zero, then \\(Y\\) must be a function of \\(X\\).\nIf knowledge of \\(X\\) is irrelevant, in the sense of §  18.2, to knowledge of \\(Y\\), then the conditional entropy \\(\\mathrm{H}(Y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\) takes on its maximal value, determined by the marginal probability for \\(Y\\). Vice versa, if the conditional entropy takes on its maximal value, then \\(X\\) is irrelevant to \\(Y\\).\nThe value (in shannons) of the conditional entropy has the same meaning as for the Shannon entropy: if the conditional entropy amounts to \\(h\\,\\mathrm{Sh}\\), then after knowing \\(X\\) our uncertainty about \\(Y\\) is the same as if we were fully uncertain among \\(2^h\\) possible alternatives.\n\nFor instance, in the case of table  18.1, the conditional entropy has the following values in the three scenarios:\n\n\\(\\mathrm{H}(Y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_1) = 0.01\\,\\mathrm{Sh}\\) , almost zero; indeed \\(Y\\) can almost be considered a function of \\(X\\) in this case.\n\\(\\mathrm{H}(Y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_2) = 0.99\\,\\mathrm{Sh}\\) , almost 1; indeed in this case we are approximately uncertain between two values of \\(Y\\).\n\\(\\mathrm{H}(Y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_3) = 0.5\\,\\mathrm{Sh}\\) ; indeed this case is intermediate between the previous two.\n\n\n\nMutual information\nSuppose that, according to background knowledge \\(\\mathsfit{I}\\), for any value of \\(X\\) there’s a 100% probability that \\(Y\\) has one and the same value, say \\(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}1\\). The conditional entropy \\(\\mathrm{H}(Y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\) is then zero. In this case it is true that \\(Y\\) is formally a function of \\(X\\). But it is also true that we could perfectly predict \\(Y\\) without any knowledge of \\(X\\). The observed value of \\(X\\) didn’t really help us in forecasting \\(Y\\); in other words, \\(X\\) is not relevant for inference about \\(Y\\).22 There’s no contradiction with the second “remarkable property” previously discussed: in this case the maximal value that the conditional entropy can take is zero.\nIf we are interested in quantifying how much our knowledge about \\(X\\) “helped” in inferring \\(Y\\), we can subtract the conditional entropy for \\(Y\\) given \\(X\\) from the maximum value it would have if \\(X\\) were unavailable.\nThis is the mutual information3 of a quantity \\(Y\\) given a quantity \\(X\\) and additional knowledge \\(\\mathsfit{I}\\). It is defined as3 or “mean transinformation content” according to ISO standard\n\n\\[\n\\mathrm{H}(Y : X\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) \\coloneqq\n\\sum_x \\sum_y\n\\mathrm{P}(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\,\n\\log_2 \\frac{\\mathrm{P}(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})}{\n\\mathrm{P}(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\cdot\n\\mathrm{P}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}\n\\]\n\nand, as defined above, is also measured in shannons. It also satisfies the three requirements for a measure of “importance”. Its properties are somehow complementary to those of the conditional entropy:\n\nIf \\(Y\\) and \\(X\\) are independent, in the sense of §  18.2, then their mutual information is zero. Vice versa, if it is zero, then \\(Y\\) and \\(X\\) are independent.\nIf knowledge of \\(Y\\) is completely determined by that of \\(X\\), that is, if \\(Y\\) is a function of \\(X\\), then their mutual information attains its maximal value (which could be zero). Vice versa, if the mutual information attains its maximal value, then \\(Y\\) must be a function of \\(X\\).\nIf the mutual information between \\(Y\\) and \\(X\\) amounts to \\(h\\,\\mathrm{Sh}\\), then knowledge of \\(X\\) reduces, on average, \\(2^h\\)-fold times the possibilities regarding the value of \\(Y\\).\nIt is symmetric in the roles of \\(X\\) and \\(Y\\), that is, \\(\\mathrm{H}(Y : X\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = \\mathrm{H}(X : Y\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\).\n\nIn the case of table  18.1, the mutual information has the following values in the three scenarios:\n\n\\(\\mathrm{H}(Y : X\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_1) = 1.61\\,\\mathrm{Sh}\\) , almost equal to the maximal value achievable in this scenario (\\(1.62\\,\\mathrm{Sh}\\)); indeed \\(Y\\) can almost be considered a function of \\(X\\) in this case. Since \\(2^{1.61}\\approx 2.1\\), knowledge of \\(X\\) roughly halves the number of possible values of \\(Y\\).\n\\(\\mathrm{H}(Y : X\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_2) = 0.81\\,\\mathrm{Sh}\\) ; this means that knowledge of \\(X\\) reduces by \\(2^{0.81}\\approx 1.8\\) or almost 2 times the number of possible values of \\(Y\\).\n\\(\\mathrm{H}(Y : X\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_3) = 1.5\\,\\mathrm{Sh}\\) ; knowledge of \\(X\\) reduces by \\(2^{1.5} \\approx 2.8\\) or almost 3 times the number of possible values of \\(Y\\).\n\n\n\nUses\nWhether to use the conditional entropy \\(\\mathrm{H}(Y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\) or the mutual information \\(\\mathrm{H}(Y : X\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\) depends on the question we are asking.\nConditional entropy is the right choice if we want to quantify how much \\(Y\\) can be considered as a function of \\(X\\) – including the special case of a constant function. It is also the right choice if we want to know how many binary-search iterations it would take to find \\(Y\\), on average, once \\(X\\) is known.\nMutual information is the right choice if we want to quantify how much the knowledge of \\(X\\) helps, on average, on finding \\(Y\\), or equivalently how many additional binary-search iterations it would take to find \\(Y\\), if \\(X\\) were not known. This therefore includes quantifying “correlation” or “association”.\n\n\n\n\n\n\n\n\n\n\nThe widely used Pearson correlation coefficient is actually a very poor measure of correlation or association; it is more a measure of “linearity” than correlation. It can be very dangerous to rely on in data-science and machine-learning problems, where we can expect non-linearity and peculiar associations in wide dimensions. It is widely used not because it’s good, but because of intellectual inertia.\n\n\nIf we simply want to rank the relative importance of alternative quantities \\(X_1\\), \\(X_2\\), etc. in inferring \\(Y\\), then the two measures are equivalent and yield the same ranking, since they basically differ by a common zero point.\n\n\n\n\n\n\n Study reading\n\n\n\n\nChapter 8 of Information Theory, Inference, and Learning Algorithms\n§ 12.4 of Artificial Intelligence"
  },
  {
    "objectID": "information.html#sec-utility-importance",
    "href": "information.html#sec-utility-importance",
    "title": "18  Information, relevance, independence, association",
    "section": "18.6 Utility Theory to quantify relevance and importance",
    "text": "18.6 Utility Theory to quantify relevance and importance\nThe entropy-based measures discussed in the previous section originate from, have deep connections with, the problem of repeated communication or signal transmission. They do not require anything else beside joint probabilities. In a general decision problem – where an agent has probabilities and utilities – another approach is required, however.\nQuestions such as “What happens if I discard quantity \\(X\\) in my inference?” or “If I have to choose between quantity \\(U\\) and quantity \\(V\\) to condition my inference, which one should I choose?” are actually decision-making problems. They must therefore be solved using Decision Theory (this is an example of the recursive capabilities of Decision Theory, discussed in §  2.4).\nThe application of decision theory in these situations if often intuitively understandable. For example, if we need to rank the importance of quantities \\(U\\) and \\(V\\), we can calculate how much the expected utility would decrease if we discarded the one or the other.\nWe’ll come back to these questions toward the end of the course."
  },
  {
    "objectID": "connection-3-ML.html",
    "href": "connection-3-ML.html",
    "title": "19  Third connection with machine learning",
    "section": "",
    "text": "\\[\n\\DeclarePairedDelimiters{\\set}{\\{}{\\}}\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\\]\n\n\n\n\nIn chapter  11 we made a second tentative connection between the notions about probability explored until then, and notions from machine learning. We considered the possibility that a machine-learning algorithm is like an agent that has some built-in background information (corresponding to the algorithm’s architecture), has received pieces of information (corresponding to the data about perfectly known instances of the task, and possibly partial data about a new instance), and is assessing a not-previously known piece of information (other partial aspects of a new task instance):\n\\[\n\\mathrm{P}(\\underbracket[0ex]{\\color[RGB]{238,102,119}\\mathsfit{D}_{N+1}}_{\\mathclap{\\color[RGB]{238,102,119}\\text{outcome?}}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\\underbracket[0ex]{\\mathsfit{D}_N \\land \\dotsb \\land \\mathsfit{D}_2 \\land \\mathsfit{D}_1}_{\\mathclap{\\color[RGB]{34,136,51}\\text{training data?}}}\n\\color[RGB]{0,0,0}\\land \\underbracket[0ex]{\\color[RGB]{204,187,68}\\mathsfit{I}}_{\\mathrlap{\\color[RGB]{204,187,68}\\uparrow\\ \\text{architecture?}}})\n\\]\nThe correspondence about training data and architecture seems somewhat convincing, the one about outcome needs more exploration.\nHaving introduced the notion of quantity in the latest chapters 12 and 13, we recognize that “training data” are nothing else but quantities with given values. So a datum \\(\\mathsfit{D}_i\\) can be expressed by a sentence like \\(Z_i\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_i\\), where\n\n\\(i\\) is the instance: \\(1,2,\\dotsc,N, N+1\\)\n\\(Z_i\\), a quantity, describes the type of data at instance \\(i\\), for example “128 × 128 image with 24-bit colour depth, with a character label”\n\\(z_i\\) is the value of the quantity \\(Z_i\\) at instance \\(i\\), for example the specific image & label enclosed here:\n\n\n\n\n\n\nlabel = “Saitama”\n\n\nWe can therefore rewrite the correspondence above as follows:\n\\[\n\\mathrm{P}(\\underbracket[0ex]{\\color[RGB]{238,102,119}Z_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N+1}}_{\\mathclap{\\color[RGB]{238,102,119}\\text{outcome?}}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\\underbracket[0ex]{ Z_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_N \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Z_2 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_2 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Z_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_1}_{\\mathclap{\\color[RGB]{34,136,51}\\text{training data?}}}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\underbracket[0ex]{\\color[RGB]{204,187,68}\\mathsfit{I}}_{\\mathrlap{\\color[RGB]{204,187,68}\\uparrow\\ \\text{architecture?}}})\n\\]\nThis is the kind of inference that we explored in the “next-three-patients” scenario of §  17.3 and in some of its following sections. In chapter  27, after a review of conventional machine-learning methods and terminology, we shall discuss with more care what these inferences are about, what kind of information they use, and how they can be concretely calculated.\nBut we have been speaking of “task instances” and “instances of quantities” quite vaguely so far. These are important notions: the whole idea of “learning from examples” hinges on them. In the next few chapters we shall therefore make them more rigorous. The theory at makes them rigorous is Statistics. As a bonus we shall find out that a rigorous analysis of the notion of “instances” also leads to concrete formulae to calculate the probabilities discussed above."
  },
  {
    "objectID": "populations_variates.html#sec-collections",
    "href": "populations_variates.html#sec-collections",
    "title": "20  Populations and variates",
    "section": "20.1 Collections of similar quantities: motivation",
    "text": "20.1 Collections of similar quantities: motivation\nIn the latest chapters we gradually narrowed our focus on a particular kind of inference: inferences that involve collections of similar quantities (each of which can be simple, joint, or complex). “Similar” means that all such quantities have the same domain – for instance, each of them has possible values \\(\\set{{\\small\\verb;urgent;}, {\\small\\verb;non-urgent;}}\\); or each of them has possible values between \\(0\\,\\mathrm{\\textcelsius}\\) and \\(100\\,\\mathrm{\\textcelsius}\\) – and they have a similar meaning and measurement procedure. They can be considered different “instances” of the same quantity, so to speak. We saw an example in the three-patient hospital scenario of §  17.3, with the three “urgency” quantities \\(U_1\\), \\(U_2\\), \\(U_3\\), corresponding to the urgency of three consecutive patients. Here are other examples:\n\n\nStock exchange\n\nWe are interested in the daily change in closing price of a stock, during 1000 days. Each day the change can be positive (or zero), or negative.\n\n\nThe daily change on any day can clearly be considered as a binary quantity, say with domain \\(\\set{{\\color[RGB]{34,136,51}{\\small\\verb;+;}}, {\\color[RGB]{204,187,68}{\\small\\verb;-;}}}\\). The daily changes in 1000 days are a set of 1000 binary quantities with exactly the same domain – even if each one can have a different value.\n\n\n\n\n\n\n\n\nMars prospecting\n\nSome robot examines 1000 similar-sized rocks in a large crater on Mars. Each rock either contains haematite, or it doesn’t.\n\n\nThe haematite-content of any rock can be considered as a binary quantity, say with domain \\(\\set{{\\color[RGB]{34,136,51}{\\small\\verb;Y;}}, {\\color[RGB]{238,102,119}{\\small\\verb;N;}}}\\). The haematite contents of the 1000 rocks are again a set of 1000 binary quantities with exactly the same domain – even if each one can have a different value.\n\n\n\n\n\n\n\n\nGlass forensics\n\nA criminal forensics department has 215 glass fragments collected from many different crime scenes. Each fragment is characterized by a refractive index (between \\(1\\) and \\(\\infty\\)), a percentage of Calcium (between \\(0\\%\\) and \\(100\\%\\)), a percentage of Silicon (ditto), and a type of origin (for example “from window of building”, “from window of car”, and similar).\n\n\nThe refractive index, Calcium percentage, Silicon percentage, and type of the 215 fragments are a set of 215 joint quantities with exactly the same joint domain.\n\n\n\n\nIt is easy to think of many other and very diverse examples, with even more complex variates, such as images or words. We shall now try to abstract and generalize this similarity."
  },
  {
    "objectID": "populations_variates.html#sec-variates-populations",
    "href": "populations_variates.html#sec-variates-populations",
    "title": "20  Populations and variates",
    "section": "20.2 Units, variates, statistical populations",
    "text": "20.2 Units, variates, statistical populations\nConsider a large collection of entities that are somehow similar to one another. We call these entities units. These units could be, for instance:\n\nphysical objects such as cars, windmills, planets, or rocks from a particular place;\ncreatures such as animals of a particular species, or human beings, maybe with something in common such as geographical region; or plants of a particular kind;\nautomatons having a particular application;\nsoftware objects such as photos;\nabstract objects such as functions or graphs;\nthe rolls of a particular die or the tosses of a particular coin;\nthe weather conditions in several days.\n\nThese units are similar to one another in that they have some set of attributes1 common to all. These attributes can present themselves in a specific number of mutually-exclusive guises, which can be different from unit to unit. For instance, the attributes could be:1 The term features is frequently used in machine learning\n\n“colour”, each unit being, say, green, blue, or yellow;\n“mass”, each unit having a mass between \\(0.1\\,\\mathrm{kg}\\) and \\(10\\,\\mathrm{kg}\\);\n“health condition”, each unit (an animal or human in this case) being healthy or ill; or maybe being affected by one of a specific set of diseases;\ncontaining something, for instance a particular chemical substance;\n“having a label”, each unit having one of the labels A, B, C;\na complex combination of several simpler attributes like the ones above.\n\nThe units also have additional attributes (they must, otherwise we wouldn’t be able to distinguish each unit from all others), which we simply don’t consider or can’t measure. We’ll discuss this possibility later.\nFrom this description it’s clear that the attributes of each unit are a (possibly joint) quantity, as defined in §  12.1.1. Once the units and their attributes are specified, we have a set of as many quantities as there are units. All these quantities have identical domains.\nA quantity which is a collection of attributes of a set of units is called a variate. So when we speak about a variate it is understood that this is a quantity that appears, replicated, in some set of units.\n\n\nWe call a collection of units so defined a statistical population, or just population when there’s no ambiguity. The number of units is called the size of the population.\nThe notion of statistical population is extremely general and encompassing: many different things can be thought of as a population. In speaking of “data”, what is often meant is a particular statistical population. The specification of a population requires precision, especially when it is used to draw inferences, as we shall see later. A statistical population has not been properly specified until two things are precisely specified:\n\na way to determine whether something is a unit or not: inclusion and exclusion criteria, means of collection, and so on\na definition of the variate considered, its possible values, and how it is measured\n\n\n\n\n\n\n\n Exercises\n\n\n\n\nWhich of the following descriptions does properly define a statistical population? explain why it does or does not.\n\nPeople.\nElectronic components produced in a specific assembly line, since the line became operational until its discontinuation, and measured for their electric resistance, with possible values in \\([0\\,\\mathrm{\\Omega}, \\infty\\,\\mathrm{\\Omega}]\\), and for their result on a shock test, with possible values \\(\\set{{\\small\\verb;pass;}, {\\small\\verb;fail;}}\\).\nPeople born in Norway between 1st January 1990 and 31st December 2010.\nThe words contained in all websites of the internet.\nRocks, of volume between 1 cm³ and 1 m³, found in the Schiaparelli crater (as defined by contours on a map), and tested to contain haematite, with possible values \\(\\set{{\\small\\verb;Y;}, {\\small\\verb;N;}}\\).\n\nBrowse some datasets at the UC Irvine Machine Learning repository. Each dataset is a statistical population. The variate in most of these populations is a joint variate (to be discussed soon), that is, a collection of several variates.\nExamine and discuss the specification of some of those datasets:\n\nIs it well-specified what constitutes a “unit”? Are the criteria for including or excluding datapoints, their origin, and so on, well explained?\nAre the variates well-defined? Is it explained what they mean, how they were measured, what is their domain, and so on?\n\n\n\n\n\n\n\n\n\n\n\n\n Subtleties in the notion of statistical population\n\n\n\n\nA statistical population is only a conceptual device for simplifying and facing some decision or inference problem. There is no objectively-defined population “out there”.\nAny entity, object, person, and so on has some characteristics that makes it completely unique (say, its space-time coordinates), otherwise we wouldn’t be able to distinguish it from others. From this point of view any entity is just a one-member population in itself. If we consider two or more entities as being “similar” and belonging to the same population, it’s because we have decided to disregard some characteristics and focus on some others. This decision is arbitrary, a matter of convention, and depends on the specific inference and decision problem.\nTo “test” whether an entity belong to a given population, means to check if that entity satisfies the agreed-upon definition of that population.\nAny physical entity, object, person, etc. can represent different units in different and even non-overlapping statistical populations. For instance, a 100 cm³ rock found in the Schiaparelli crater on Mars could be a unit in these populations:\n\nRocks, of volume between 1 cm³ and 1 m³, found in the Schiaparelli crater and tested for haematite\nRocks, of volume between 10 cm³ and 200 cm³, found in the Schiaparelli crater and tested for haematite\nRocks, of volume between 10 cm³ and 200 m³, found in any crater on any planet of the solar system, and tested for haematite\nRocks, of volume between 1 cm³ and 1 m³, found in the Schiaparelli crater and measured for the magnitude of their magnetic field.\n\nPopulations a. b. c. above have the same variate but differ in their definition of “unit”. Populations a. and d. have the same definition of unit but different variates. Population b. is a subset of population a.: they have the same variate, and any unit in b. is also a unit in a.; but not every unit in a. is also a unit in b.. Populations a. and c. have some overlap: they have the same variate, and some units of a. are also units of c., and vice versa."
  },
  {
    "objectID": "populations_variates.html#sec-joint-variates",
    "href": "populations_variates.html#sec-joint-variates",
    "title": "20  Populations and variates",
    "section": "20.3 Populations with joint variates",
    "text": "20.3 Populations with joint variates\nThe definition of statistical population (§  20.2) makes it clear that the quantity associated with each unit can be of arbitrary complexity. In particular it could be a joint quantity (§  13.1), that is, a collection of quantities of a simpler type.\nWe saw an example at the beginning of this chapter, with a population relevant for glass forensics:\n\n\n\n\nunits: glass fragments (collected at specific locations)\nvariate: the joint variate \\((\\mathit{RI}, \\mathit{Ca}, \\mathit{Si}, \\mathit{Type})\\) consisting of four simple variates:\n\n\\(\\mathit{R}\\)efractive \\(\\mathit{I}\\)ndex of the glass fragment (interval continuous variate), with domain from \\(1\\) (included) to \\(+\\infty\\)\nweight percent of \\(\\mathit{Ca}\\)lcium in the fragment (interval discrete variate), with domain from \\(0\\) to \\(100\\) in steps of 0.01\nweight percent of \\(\\mathit{Si}\\)licon in the fragment (interval discrete variate), with domain from \\(0\\) to \\(100\\) in steps of 0.01\n\\(\\mathit{Type}\\) of glass fragment (nominal variate), with seven possible values building_windows_float_processed, building_windows_non_float_processed, vehicle_windows_float_processed, vehicle_windows_non_float_processed, containers, tableware, headlamps\n\n\nHere is a table with the values of the joint variate \\((\\mathit{RI}, \\mathit{Ca}, \\mathit{Si}, \\mathit{Type})\\) for ten units:\n\n\nTable 20.1: Glass fragments\n\n\n\n\n\n\n\n\n\nunit\n\\(\\mathit{RI}\\)\n\\(\\mathit{Ca}\\)\n\\(\\mathit{Si}\\)\n\\(\\mathit{Type}\\)\n\n\n\n\n1\n\\(1.51888\\)\n\\(9.95\\)\n\\(72.50\\)\ntableware\n\n\n2\n\\(1.51556\\)\n\\(9.41\\)\n\\(73.23\\)\nheadlamps\n\n\n3\n\\(1.51645\\)\n\\(8.08\\)\n\\(72.65\\)\nbuilding_windows_non_float_processed\n\n\n4\n\\(1.52247\\)\n\\(9.76\\)\n\\(70.26\\)\nheadlamps\n\n\n5\n\\(1.51909\\)\n\\(8.78\\)\n\\(71.81\\)\nbuilding_windows_float_processed\n\n\n6\n\\(1.51590\\)\n\\(8.22\\)\n\\(73.10\\)\nbuilding_windows_non_float_processed\n\n\n7\n\\(1.51610\\)\n\\(8.32\\)\n\\(72.69\\)\nvehicle_windows_float_processed\n\n\n8\n\\(1.51673\\)\n\\(8.03\\)\n\\(72.53\\)\nbuilding_windows_non_float_processed\n\n\n9\n\\(1.51915\\)\n\\(10.09\\)\n\\(72.69\\)\ncontainers\n\n\n10\n\\(1.51651\\)\n\\(9.76\\)\n\\(73.61\\)\nheadlamps\n\n\n\n\nThe joint-variate value for unit 4, for instance, is\n\\[\n\\mathit{RI}_{\\color[RGB]{204,187,68}4}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}1.52247 \\land\n\\mathit{Ca}_{\\color[RGB]{204,187,68}4}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}9.76 \\land\n\\mathit{Si}_{\\color[RGB]{204,187,68}4}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}70.26 \\land\n\\mathit{Type}_{\\color[RGB]{204,187,68}4}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;headlamps;}\n\\]\n\n\n\n\n\n\n\n\n\n\n\nIn the terminology we agreed upon, a variate is not a quantity. Remember that something is a quantity if it makes sense to ask “Does it have value . . . ?”. Now consider the variate \\(\\mathit{Ca}\\) (Calcium percentage) above. I say “\\(\\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}8.1\\)”. Can you check if what I said is true? No, because you don’t know which unit I’m referring to; you don’t even know if “\\(\\mathit{Ca}\\)” was referring to some unit.\nThe variate for a specific unit is a quantity instead. We can indicate this by appending the unit label to the variate symbol, as we did with “\\(\\mathit{Ca}_{\\color[RGB]{204,187,68}4}\\)” above. If I tell you “\\(\\mathit{Ca}_{\\color[RGB]{204,187,68}4}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}8\\)”, you can check that what i said is false; so \\(\\mathit{Ca}_{\\color[RGB]{204,187,68}4}\\) is a quantity.\n\n\n\n\nThe units’ IDs don’t need to be consecutive numbers; in fact they don’t even need to be numbers: any label that completely distinguishes all units will do.\n\n\n\n\n\n\n\n\n\n Exercises\n\n\n\n\nDownload the dataset2 income_data_nominal_nomissing.csv (4 MB):\n\nHow many variates does this population have?\nWhat types of variate (binary, nominal, etc.) do they seem to be?\nWhat are their domains?\n\nExplore datasets from the UC Irvine Machine Learning Repository, answering the three questions above.\n\n2 This is an adapted version of the UCI Adult dataset"
  },
  {
    "objectID": "statistics.html#sec-diff-prob-stat",
    "href": "statistics.html#sec-diff-prob-stat",
    "title": "21  Statistics",
    "section": "21.1 What’s the difference between Probability Theory and Statistics?",
    "text": "21.1 What’s the difference between Probability Theory and Statistics?\n“Probability theory” and “statistics” are often mentioned together. We shall soon see why, and what are the relationship between them. But first let’s try to define them more precisely:\n\nProbability theory\n\nis the theory that describes and norms the quantification and propagation of uncertainty, as we saw in §  8.1.\n\nStatistics\n\nis the study of collective properties of the variates of populations or, more generally, of collections of data.\n\n\nThere are clear and crucial differences between the two:\n\nThe fact that we are uncertain about something doesn’t mean that there are populations or replicas involved. We can apply probability theory without doing any statistics.\nIf we have full information about a population – the value of each variate for each unit – then we can calculate summaries and other properties of the variates. And there’s no uncertainty involved: at all times we can exactly calculate any information we like about any variates. So we do statistics, but probability theory plays no role (except perhaps in the form of propositional logic).\n\nMany texts do not clearly distinguish between probability and statistics. The distinction is important for us because we will have to solve problems involving the uncertainty about particular statistics, so the two must be kept clearly separate. This distinction was observed by James Clerk Maxwell who used it to develop the theories of statistical mechanics and kinetic theory.\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nMaxwell explains the statistical method and its use in the molecular description of matter:\n\nIntroductory Lecture on Experimental Physics\nMolecules\n\n\n\nIn many concrete problems, however, these do theories do go hand in hand and interact. This happens mainly in two non-mutually exclusive ways:\n\nthe statistics of a population give information that can be used in the conditional of an inference\nwe want to draw inferences about some statistics of a population, whose values we don’t know.\n\n\n\nLet’s now discuss some important statistics."
  },
  {
    "objectID": "statistics.html#sec-freq-distr",
    "href": "statistics.html#sec-freq-distr",
    "title": "21  Statistics",
    "section": "21.2 Frequencies and frequency distributions",
    "text": "21.2 Frequencies and frequency distributions\nConsider a statistical population of \\(N\\) units, with a variate \\(X\\) having a finite set of \\(K\\) values as domain. To keep things simple let’s just say these values are \\(\\set{1, 2, \\dotsc, K}\\) (without any ordering implied); the discussion applies for any finite set. The variate \\(X\\) could be of any non-continuous type: nominal, ordinal, interval, binary (§  12.2), or of a joint or complex type (§  13). Let’s denote the variate associated with unit \\(i\\) by \\(X_i\\). For instance, we express that unit #3 has \\(X\\)-variate value \\(5\\) and unit #7 has \\(X\\)-variate value \\(1\\) by writing\n\\[\nX_3 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}5 \\land X_7 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}1\n\\quad\\text{\\small or more compactly}\\quad\nX_3 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}5 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_7 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}1\n\\]\nFor each value \\(a\\) in the domain of the variate \\(X\\), we count how many units in the population have that particular value, let’s call the number we find \\(n_a\\). This is the absolute frequency of the value \\(a\\) in this population. Obviously \\(n_a\\) must be an integer between \\(0\\) (included) and \\(K\\) (included). The set of absolute frequencies of all values is called the absolute frequency distribution of the variate in the population. We must have\n\\[\\sum_{a=1}^K n_a = N \\ .\\]\nIt is often useful to give not the absolute count of how often the value \\(a\\) appears in a population, but the fraction with respect to the population size, which we denote by \\(f_a\\):\n\\[f_a \\coloneqq n_a/N\\]\nThis is called the relative frequency of the value \\(a\\). Obviously \\(0 \\le f_a \\le 1\\). The collection of relative frequencies for all values, \\(\\set{f_1, f_2, \\dotsc, f_K}\\), satisfies\n\\[\\sum_{a=1}^K f_a = 1 \\ .\\]\nWe call this collection the relative frequency distribution. We shall denote it with the boldface symbol \\(\\boldsymbol{f}\\) (boldface indicates that it is a tuple of numbers):\n\\(\\boldsymbol{f} \\coloneqq(f_1, f_2, \\dotsc, f_K)\\)\nwith an analogous convention if other letters are used instead of “\\(f\\)”.\n\n\n\n\n\n\n\n\n\n\nIn the following we shall call relative frequencies simply “frequencies”, and explicitly use the word “absolute” when we speak about absolute frequencies.\n\n\n\n\nThe frequency distribution of values in a population does not give us full information about the population, because it doesn’t tell which unit has which value. In many situations, however, this is all we need to know or we can hope to know.\nFrequencies and frequency distributions are quantities in the technical sense of §  12.1.1. In fact we can say, for instance, “The frequency of the value \\(7\\) is 0.3”, or “The frequency distribution for the values \\(1,2,3\\) is \\((0.2, 0.7, 0.1)\\)”. We shall denote the quantity, as separate from its value, by the corresponding capital letter, for example \\(F_1\\), so that we can write sentences about frequencies in our usual abbreviated form. For instance\n\\[\nF_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}f_3\n\\]\nmeans “The frequency of the variate value \\(3\\) is equal to \\(f_3\\)”, where \\(f_3\\) must be a specific number.\n\n\n\n\n\n\n Exercise\n\n\n\nConsider the statistical population defined as follows:\n\nunits: the bookings at a specific hotel during a specific time period\nvariate: the market segment of the booking\nvariate domain: the set of five values \\(\\set{{\\small\\verb;Aviation;}, {\\small\\verb;Complementary;}, {\\small\\verb;Corporate;}, {\\small\\verb;Offline;}, {\\small\\verb;Online;}}\\)\n\nThe population data is stored in the file hotel_bookings-market.csv. Each row of the file corresponds to a unit, and lists the unit id (this is not a variate in the present population) and the market segment.\nUse any method you like (a script in your favourite programming language, counting by hand, or whatever) to answer these questions:\n\nWhat is the size of the population?\nWhat are the absolute frequencies of the five values?\nWhat are their relative frequencies?\nWhich units have the value \\({\\small\\verb;Corporate;}\\)?\n\n\n\n\nDifferences between frequencies and probabilities\nThe fact that frequencies are non-negative and sum up to 1 makes them somewhat similar to probabilities, from a purely numerical point of view. The two notions, however, are completely different and have different uses. Here is a list of some important differences:\n\n\nNot few works in machine learning tend to call “probabilities” any set of positive numbers that sum up to one. Be careful when reading them. Mentally replace probability with degree of belief and see if the text mentioning “probabilities” still makes sense.\n\n\nA probability expresses a degree of belief.\nA frequency is just the count of how many times something appears.\n\n\nThe probability of a sentence depends on an agent’s state of knowledge and background information. Two agents can assign different probabilities to the same sentence.\nThe frequency of a value in a population is an objective physical quantity. All agents agree on the frequency (if they have the possibility to counting the units it).\n\n\n\n\nProbabilities refer to sentences.\nFrequencies refer to values in a population, not to sentences (unless we are speaking of how many times a sentence appears in, say, a book; but this is a completely different and peculiar case.)\n\n\nA probability can refer to a specific unit in a population. An agent can consider, for instance, the probability that a variate for unit #7 has value 3.\nA frequency cannot refer to a specific unit in a population. It is meaningless to “count how many times the value 3 appears in unit #7”."
  },
  {
    "objectID": "statistics.html#sec-joint-freq",
    "href": "statistics.html#sec-joint-freq",
    "title": "21  Statistics",
    "section": "21.3 Joint frequencies",
    "text": "21.3 Joint frequencies\nConsider the following population consisting of ten units with joint variate \\((\\mathit{age}, \\mathit{race}, \\mathit{sex}, \\mathit{income})\\), whose component variates have the following properties:\n\n\\(\\mathit{age}\\):   interval discrete with domain \\(\\set{17, 18, \\dotsc, 90+}\\)\n\\(\\mathit{race}\\):   nominal with domain \\(\\set{{\\small\\verb;Amer-Indian-Eskimo;}, {\\small\\verb;Asian-Pac-Islander;} , {\\small\\verb;Black;}, {\\small\\verb;Other;}, {\\small\\verb;White;}}\\)\n\\(\\mathit{sex}\\):   binary with domain \\(\\set{{\\small\\verb;F;}, {\\small\\verb;M;}}\\)\n\\(\\mathit{income}\\):   binary with domain \\(\\set{{\\small\\verb;`&lt;=50K';}, {\\small\\verb;`&gt;50K';}}\\)\n\n\n\nTable 21.1: Income\n\n\n\\(\\mathit{age}\\)\n\\(\\mathit{race}\\)\n\\(\\mathit{sex}\\)\n\\(\\mathit{income}\\)\n\n\n\n\n53\n\\({\\small\\verb;White;}\\)\n\\({\\small\\verb;M;}\\)\n\\({\\small\\verb;`&gt;50K';}\\)\n\n\n53\n\\({\\small\\verb;Black;}\\)\n\\({\\small\\verb;F;}\\)\n\\({\\small\\verb;`&lt;=50K';}\\)\n\n\n48\n\\({\\small\\verb;White;}\\)\n\\({\\small\\verb;M;}\\)\n\\({\\small\\verb;`&gt;50K';}\\)\n\n\n53\n\\({\\small\\verb;White;}\\)\n\\({\\small\\verb;F;}\\)\n\\({\\small\\verb;`&gt;50K';}\\)\n\n\n53\n\\({\\small\\verb;White;}\\)\n\\({\\small\\verb;M;}\\)\n\\({\\small\\verb;`&lt;=50K';}\\)\n\n\n26\n\\({\\small\\verb;White;}\\)\n\\({\\small\\verb;M;}\\)\n\\({\\small\\verb;`&lt;=50K';}\\)\n\n\n48\n\\({\\small\\verb;White;}\\)\n\\({\\small\\verb;F;}\\)\n\\({\\small\\verb;`&gt;50K';}\\)\n\n\n53\n\\({\\small\\verb;White;}\\)\n\\({\\small\\verb;M;}\\)\n\\({\\small\\verb;`&gt;50K';}\\)\n\n\n53\n\\({\\small\\verb;Black;}\\)\n\\({\\small\\verb;M;}\\)\n\\({\\small\\verb;`&lt;=50K';}\\)\n\n\n48\n\\({\\small\\verb;Amer-Indian-Eskimo;}\\)\n\\({\\small\\verb;M;}\\)\n\\({\\small\\verb;`&gt;50K';}\\)\n\n\n\n\nThe joint frequency distribution for the joint variate of the population above gives the frequencies of all possible joint variate values, for instance the value\n\\(\\mathit{age}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}53 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathit{race}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;Black;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathit{sex}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;F;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathit{income}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;`&lt;=50K';}\\)\nIn this population, most joint values appear each only once, and the remaining values never appear; this is because of the population’s small size and the large number of possible variate values. A couple of joint values appear twice. We have for example\n\\[\\begin{aligned}\n&f(\\mathit{age}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}53 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{race}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;White;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{sex}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;M;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{income}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;`&gt;50K';}\n) = \\frac{2}{10}\n\\\\[2ex]\n&f(\\mathit{age}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}53 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{race}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;Black;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{sex}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;F;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{income}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;`&lt;=50K';}\n) = \\frac{1}{10}\n\\\\[2ex]\n&f(\\mathit{age}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}48 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{race}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;Amer-Indian-Eskimo;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{sex}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;F;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{income}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;`&gt;50K';}\n) = 0\n\\end{aligned}\\]\n\n\n\n\n\n\n Exercise\n\n\n\nTry to write a function that takes as input a dataset with a small number of variates and outputs the joint frequency distribution for all combinations of variate values. The best output format is a multidimensional array having one dimension per variate, and for each dimension a length equal to the number of possible values of that variate. The value of the array in each cell is the corresponding frequency.\nFor instance, consider the case of the income dataset above but without the age variate. The output of the function would then be an array with \\(5 \\times 2 \\times 2\\) dimensions"
  },
  {
    "objectID": "statistics.html#sec-marginal-freq",
    "href": "statistics.html#sec-marginal-freq",
    "title": "21  Statistics",
    "section": "21.4 Marginal frequencies",
    "text": "21.4 Marginal frequencies\nWhen a population has a joint variate, we may be interested in only a subset of the simpler variates that constitute the joint one. In the population of the example above, for instance, we might be interested only in the \\(\\mathit{age}\\) and \\(\\mathit{income}\\) variates. These two variates together are then called marginal variates and define what we can call a marginal population of the original one. A marginal population has the same units as the original one, but only a subset of the variates of the original. It is a statistical population in its own right.\nThe notion of “marginalization” is a relative notion. Any population can often be considered as the marginal of a population with the same units but additional attributes.\n\n\nGiven a statistical population with joint variates \\({\\color[RGB]{34,136,51}X}, {\\color[RGB]{238,102,119}Y}\\), we define the marginal frequency of the value \\({\\color[RGB]{238,102,119}y}\\) of \\({\\color[RGB]{238,102,119}Y}\\) as the frequency of the value \\({\\color[RGB]{238,102,119}y}\\) in the marginal population with the variate \\({\\color[RGB]{238,102,119}Y}\\) alone. This frequency is simply written\n\\[\nf({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y})\n\\]\nA conditional frequency can be calculated as the sum of the joint frequencies for all values \\({\\color[RGB]{34,136,51}x}\\), in a way analogous to marginal probabilities (§  16.1):\n\\[\nf({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y}) = \\sum_{\\color[RGB]{34,136,51}x} f({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x})\n\\]\nFor example, if from the population of table  21.1 we consider the marginal population with variates \\((\\mathit{age}, \\mathit{income})\\), some of the marginal frequencies are\n\\[\\begin{aligned}\n&f(\\mathit{age}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}53 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{income}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;`&lt;=50K';}\n) = \\frac{3}{10}\n\\\\[2ex]\n&f(\\mathit{age}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}26 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{income}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;`&lt;=50K';}\n) = \\frac{1}{10}\n\\\\[2ex]\n&f(\\mathit{age}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}48 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{income}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;`&gt;50K';}\n) = \\frac{3}{10}\n\\end{aligned}\\]\n\n\n\n\n\n\n\n\n Exercises\n\n\n\n\nDownload again the dataset income_data_nominal_nomissing.csv:\n\nCalculate the marginal frequencies of some of its variates.\nDoes any variate have a value appearing with marginal absolute frequency equal to 1?"
  },
  {
    "objectID": "statistics.html#sec-summary-stat",
    "href": "statistics.html#sec-summary-stat",
    "title": "21  Statistics",
    "section": "21.5 Summary statistics",
    "text": "21.5 Summary statistics\nIn communicating statistics about a population it is always best to report and, when possible, visually show (for instance as marginal distributions) the full joint frequency distribution of the population’s variates.\nSometimes one wants to share some sort of “summary” of the frequency distribution, emphasizing particular aspects of it; because these are also aspects of the population. Different kinds of aspects can be chosen; some of them are only defined for specific types of variates. They are often called “summary statistics” or “descriptive statistics”. Below we give a brief description of some common ones, emphasizing when they are appropriate and when they are not. These summaries can also be used for probability distributions.\n\nMode\nThe mode is the value having the highest frequency (or probability, if we’re speaking about an agent’s beliefs rather than a population). There can be more than one mode.\nThe mode is defined for any distribution over discrete values, also for nominal quantities.\n\n\n\n\n\n\n\n\n\n\nBe careful in relying too much on the “mode” for a continuous quantity. Continuous quantities can be transformed in a one-to-one way into other, equivalent ones; and such a transformation also give the equivalent frequency or probability density for the new quantity. There is no general relationship between the modes of the densities for the two equivalent quantities. In fact, the density for one quantity can have one mode, whereas the density for the equivalent quantity can have no mode, or many modes. This is true for all kinds of distributions represented by densities, for example a continuous distribution of energy.\n\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nSome paradoxes, errors, and resolutions concerning the spectral optimization of human vision\n\n\n\n\nMedian and quartiles\nRecall (§  12.2) that an ordinal or an interval quantity or variate have values that can be ranked in a specific order. If there is a value for which the sum of the frequencies of all values of rank lower than that value equals the sum of the frequencies of all values of rank higher than that value, then that value is called the median of the distribution. See the following histogram as an example:\n The value \\({\\small\\verb;e;}\\) is the median of this frequency distribution, because \\(f({\\small\\verb;c;})+f({\\small\\verb;d;}) = f({\\small\\verb;f;})+f({\\small\\verb;g;})+f({\\small\\verb;h;})+f({\\small\\verb;i;})= 47.6\\%\\)\nIf there is no such separating value, then sightly different definitions of median exist in the literature; but the approximate idea is the same: a value that somehow divides the domain into two parts of roughly equal (50%) total frequency. This idea can be also applied to continuous distributions represented by densities.\nThe notion of median can be generalized to that of a value that separates the domain into a lower-rank part with total frequency 1/4, and a higher-rank part with total frequency 3/4; and also to that of a value separating into a 3/4 vs 1/4 proportion instead. These values are called the first quartile and third quartile. The two quartiles and the median (also called second quartile) divide the domain into four parts of roughly equal 25% frequencies.\nIf the variate or quantity under consideration is of interval type, then it’s possible to take the difference between the third and first quartile, called the interquartile range.\n\n\nMean and standard deviation\nFor an interval quantity \\(X\\) with values \\(\\set{x_1, x_2, \\dotsc}\\) for which it makes sense to take the sum, it is possible to define the mean and standard deviation:\n\\[\n\\bar{X} \\coloneqq\\sum_i x_i\\cdot f(x_i)\n\\qquad\n\\sigma(X) \\coloneqq\\sqrt{\\sum_i (x_i-\\bar{X})^2\\cdot f(x_i)}\n\\]\nwe assume that their meaning is more or less familiar to you.\n\n\nUses and pitfalls\nFor a nominal variate or quantity it doesn’t make sense to speak of median, quartiles, mean, standard deviation, because its possible values cannot be ranked or added.\nFor an ordinal variate or quantity it doesn’t make sense to speak of mean or standard deviation, because its possible values cannot be added.\n\n\nThe mean and standard deviation can make sense and can be useful in some circumstances. But note that even if the values of a quantity can be summed, their mean (and standard deviation) may not quite make sense.\nConsider the number of patients visiting a hospital in 100 consecutive days. It is possible to consider the mean number of patients per day. This number has a meaning: if this number of patients visited the hospital every day for 100 days, then the total number of visits would be equal to the actual total. The same reasoning can be made for the number of nurses working in the hospital every day for 100 days, and their mean.\nNow consider the daily ratios of patients to nurses, for those 100 days. These ratios are numbers, so we can take their mean. But what does such a mean represent? if we multiply it by 100, we don’t obtain the total of anything. Also, if we consider the total number of patients and total number of nurses in 100 days, their ratio will not be equal to the “mean ratio” we calculated.\nThe example above is not meant to say that a mean of ratios never makes sense, but to point out that mean and standard deviation are often overused. In chapter  23 we will discuss other problems that may arise in using mean and standard deviation.\nIn general, when in doubt, we recommend to use median and quartiles or median and interquartile range, which are more generally meaningful and enjoy several other properties (for example so-called “robustness”) useful in doing statistics.\n\n\nNote, in any case, that the present discussion regards the question of how to provide summary information besides the full frequency (or probability) distribution. If our problem is to choose one value out of the possible ones, then that’s a decision-making problem, which must be solved by specifying utilities and maximizing the expected utility, as preliminary discussed in chapter  2 and as will be discussed more in detail towards the final chapters.\n\n\n\n\n\n\n Study reading\n\n\n\n\n§ 2.6 of Risk Assessment and Decision Analysis with Bayesian Networks\n§ “The median estimate” of Meaningful expression of uncertainty in measurement\nThe Median Isn’t the Message"
  },
  {
    "objectID": "statistics.html#sec-outliers",
    "href": "statistics.html#sec-outliers",
    "title": "21  Statistics",
    "section": "21.6 Outliers vs out-of-population units",
    "text": "21.6 Outliers vs out-of-population units\nThe term “outlier” frequently appears in problems related to statistics and probability, often in conjunction with some summary statistics described above. Unfortunately the definitions of this term can be confusing or misleading. With the notion of outlier often there also comes a barrage of “methods” or rules meant to “deal” with outliers. Some such rules, for instance the rule of discarding any datapoints lying at more than three standard deviations from the mean, are often mindless and dangerous.\nSo let’s avoid the term “outlier” for the moment, and let’s take a different perspective.\n\n\nOne reason why we consider a population of units is that we are interested in making inferences about some units in this population, for which we lack the values of some variates. As we shall see in the forthcoming chapters, such inferences can be made if we first try to infer the full joint frequency distribution for the variates of the population of interest.\nThis kind of inference becomes more difficult if we have reckoned into the population some units that actually don’t belong there.\nSuppose for instance that a hospital is interested in the age of female patients admitted in a year. In collecting data, some male patients are counted in. Then obviously the age frequencies obtained from the collected data will not reflect the age frequencies among females. The problem is that some out-of-population units have been counted in by mistake.\nThe way out-of-population units affect and distort the frequency count can be different from problem to problem.\nIn our example, suppose that the wast majority of female patients could have age between 45–55 years, and that the male patients erroneously counted in also have age in the same range. Then the bulk of the frequency distribution will appear more inflated than it should be. Or suppose instead that the male patients erroneously reckoned have age between 80–90 years. In this case the old-age tail of the distribution will appear more inflated. As you see we can’t a priori point to any “tail” or “bulk” as a problem.\nLow frequencies are relatively affected by out-of-population units more than high frequencies. Suppose 10 female patients out of 100 have age 52; frequency 10%. If one 52-year-old male patient is now included by mistake, the frequency becomes 11/101 ≈ 10.1%, or a 1% relative error. But if one female patient out of 100 has age 96 (1% frequency), and a male patient of the same age is now included by mistake, the frequency becomes 2/101 ≈ 1.98%, with a 98% relative error. This is the reason some people focus on distribution tails and “outliers”, defined as data having with low-frequency values. (Note that this reasoning would concern any regions of low frequency, for example among two modes; not just tails.)\nYet we cannot mindlessly attack low-frequency regions and data just because they could be more affected by out-of-population units. In many problems of data science, engineering, medicine, low-frequency cases are the most important ones (think of rare diseases, rare mineral elements, rare astronomical events, and so on). So if we alter or eliminate low-frequency data only because they might be out-of-population units, then we have dangerously affected all our inferences about such rare events.\nMoreover, how could we judge what the “correct” frequency should be? Many outlier methods assume that the true frequency of the population has a Gaussian shape, and alter or cut the tails based on this assumption. But how can we know if such an assumption is correct? It turns out that the tails of a distribution are important for checking such assumption. Then you see the full circularity behind such mindless methods.\n\n\n\n\n\n\n Study reading\n\n\n\n§ 2.1 of Risk Assessment and Decision Analysis with Bayesian Networks\n\n\nWhich method should one use to face this problem, then? – The answer is that there’s no universal method. The approach depends on the specific problem. A data scientist must carefully examine all possible sources of out-of-population units, make inferences about them, and integrate these inferences in the general inference about the population of interest.\nThere is literature discussing first-principle approaches of this kind for different scenarios, but we cannot discuss them in the present course.\n\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nCh. 21 of Probability Theory"
  },
  {
    "objectID": "subpopulations.html#sec-subpopulations",
    "href": "subpopulations.html#sec-subpopulations",
    "title": "22  Subpopulations and conditional frequencies",
    "section": "22.1 Subpopulations",
    "text": "22.1 Subpopulations\nWhen we have a statistical population with a joint variate, it is often of interest to focus on a subset of units that share the same value of a particular variate.\nConsider for instance the following population, related to the glass-forensics example we encountered before:\n\n\n\n\nunits: glass fragments (collected at specific locations)\nvariate: the joint variate \\((\\mathit{Ca}, \\mathit{Si}, \\mathit{Type})\\) consisting of three simple variates:\n\nweight fraction of \\(\\mathit{Ca}\\)lcium in the fragment (ordinal variate), with three possible values \\(\\set{{\\small\\verb;low;}, {\\small\\verb;medium;}, {\\small\\verb;high;}}\\)\nweight fraction of \\(\\mathit{Si}\\)licon in the fragment (ordinal variate), with three possible values \\(\\set{{\\small\\verb;low;}, {\\small\\verb;medium;}, {\\small\\verb;high;}}\\)\n\\(\\mathit{Type}\\) of glass fragment (nominal variate), with seven possible values \\(\\{{\\small\\verb;building_windows_float_processed;}\\), \\({\\small\\verb;building_windows_non_float_processed;}\\), \\({\\small\\verb;containers;}\\), \\({\\small\\verb;tableware;}\\), \\({\\small\\verb;headlamps;}\\}\\)\n\n\n\n\nTable 22.1: Simplified glass-fragment population data\n\n\n\n\n\n\n\n\nunit\n\\(\\mathit{Ca}\\)\n\\(\\mathit{Si}\\)\n\\(\\mathit{Type}\\)\n\n\n\n\n1\n\\({\\small\\verb;low;}\\)\n\\({\\small\\verb;high;}\\)\n\\({\\small\\verb;headlamps;}\\)\n\n\n2\n\\({\\small\\verb;low;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;building_windows_non_float_processed;}\\)\n\n\n3\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;tableware;}\\)\n\n\n4\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;building_windows_float_processed;}\\)\n\n\n5\n\\({\\small\\verb;low;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;headlamps;}\\)\n\n\n6\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;containers;}\\)\n\n\n7\n\\({\\small\\verb;low;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;building_windows_non_float_processed;}\\)\n\n\n8\n\\({\\small\\verb;low;}\\)\n\\({\\small\\verb;high;}\\)\n\\({\\small\\verb;tableware;}\\)\n\n\n9\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;tableware;}\\)\n\n\n10\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;tableware;}\\)\n\n\n\n\nLet’s say we are interested only in units that have the \\(\\mathit{Type}\\) variate equal to \\({\\small\\verb;tableware;}\\). Discarding all others we obtain a new, smaller population with four units:\n\n\nTable 22.2: Selection according to \\(\\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;tableware;}\\)\n\n\n\n\n\n\n\n\nunit\n\\(\\mathit{Ca}\\)\n\\(\\mathit{Si}\\)\n\\(\\boldsymbol{\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}}\\mathit{Type}\\)\n\n\n\n\n3\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;tableware;}\\)\n\n\n8\n\\({\\small\\verb;low;}\\)\n\\({\\small\\verb;high;}\\)\n\\({\\small\\verb;tableware;}\\)\n\n\n9\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;tableware;}\\)\n\n\n10\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;tableware;}\\)\n\n\n\n\nwere a bar “\\(\\boldsymbol{\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}}\\)” indicates the variate used for the selection.\nAs another example, we could be interested instead in those units that have both \\(\\mathit{Ca}\\) and \\(\\mathit{Si}\\) variates equal to \\({\\small\\verb;medium;}\\). We obtain a smaller population with five units:\n\n\nTable 22.3: Selection according to \\(\\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;medium;}\\) and \\(\\mathit{Si}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;medium;}\\)\n\n\n\n\n\n\n\n\nunit\n\\(\\boldsymbol{\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}}\\mathit{Ca}\\)\n\\(\\boldsymbol{\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}}\\mathit{Si}\\)\n\\(\\mathit{Type}\\)\n\n\n\n\n3\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;tableware;}\\)\n\n\n4\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;building_windows_float_processed;}\\)\n\n\n6\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;containers;}\\)\n\n\n9\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;tableware;}\\)\n\n\n10\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;tableware;}\\)\n\n\n\n\nPopulations formed in this way are called subpopulations of the original one. They are statistical populations in their own right. The notion of “subpopulation” is a relative notion. Any population can often be considered as a subpopulation of some larger population having additional variates.\n\n\n\n\n\n\n Exercise\n\n\n\n\nFrom the population of table  22.1:\n\nConstruct the marginal population with variate \\(\\mathit{Ca}\\)\nReport the frequency distribution for the marginal population above (remember that \\(\\mathit{Ca}\\) has three possible values)\nConstruct the subpopulation with variate \\(\\mathit{Si}\\) equal to \\({\\small\\verb;high;}\\)\nConstruct the subpopulation with variate \\(\\mathit{Type}\\) equal to \\({\\small\\verb;headlamps;}\\) and the variate \\(\\mathit{Si}\\) equal to \\({\\small\\verb;medium;}\\)\n\n\nCheck your understanding of the reasoning behind the notions of marginal population and subpopulation with this exercise:\n\nFrom the population of table  22.1, construct the subpopulation with variate \\(\\mathit{Type}\\) equal to either \\({\\small\\verb;headlamps;}\\) or \\({\\small\\verb;tableware;}\\)."
  },
  {
    "objectID": "subpopulations.html#sec-conditional-freqs",
    "href": "subpopulations.html#sec-conditional-freqs",
    "title": "22  Subpopulations and conditional frequencies",
    "section": "22.2 Conditional frequencies",
    "text": "22.2 Conditional frequencies\nGiven a statistical population with joint variates \\({\\color[RGB]{34,136,51}X}, {\\color[RGB]{238,102,119}Y}\\) (and possibly others), we define the conditional frequency of the value \\({\\color[RGB]{238,102,119}y}\\) of \\({\\color[RGB]{238,102,119}Y}\\), given or “conditional on” the value \\({\\color[RGB]{34,136,51}x}\\) of \\({\\color[RGB]{34,136,51}X}\\), as the frequency of the value \\({\\color[RGB]{238,102,119}y}\\) in the subpopulation selected by \\({\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x}\\). This frequency is usually written\n\\[\nf({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x})\n\\]\nwhere \\(f\\) is the symbol for the joint frequency of the population.\nConsider for instance the glass-fragment population of table  22.1. The conditional frequency of \\({\\color[RGB]{238,102,119}\\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;low;}}\\) given \\({\\color[RGB]{34,136,51}\\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;tableware;}}\\) is the (marginal) frequency of \\(\\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;low;}\\) in the subpopulation of table  22.2, from which we find\n\\[\nf({\\color[RGB]{238,102,119}\\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;low;}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}\\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;tableware;}}) = \\frac{1}{4}\n\\]\nThe collection of these conditional frequencies for all values of \\({\\color[RGB]{238,102,119}Y}\\) constitutes the conditional frequency distribution of \\({\\color[RGB]{238,102,119}Y}\\) conditional on \\({\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x}\\). In our example this distribution has three conditional frequencies:\n\\[\\begin{aligned}\n&f({\\color[RGB]{238,102,119}\\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;low;}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}\\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;tableware;}}) = \\frac{1}{4}\n\\\\\n&f({\\color[RGB]{238,102,119}\\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;medium;}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}\\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;tableware;}}) = \\frac{3}{4}\n\\\\\n&f({\\color[RGB]{238,102,119}\\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;high;}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}\\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;tableware;}}) = 0\n\\end{aligned}\\]\nwhich sum up to \\(1\\) as they should.\n\n\n\n\n\n\n Conditional on a value of a variate\n\n\n\nIt doesn’t make sense to speak of the conditional frequency distribution of \\(Y\\) “conditional on \\(X\\)”. Conditional frequencies and frequency distributions are always conditional on some value of a variate. If we consider all possible values of \\(Y\\) and of \\(X\\) we obtain a collection of frequencies that is not a distribution.\n\n\nA conditional frequency can be calculated as the ratio of a joint and a marginal frequencies, in a way analogous to conditional probabilities (§  17.1):\n\\[\nf({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x}) =\n\\frac{f({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x})}{f({\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x})} =\n\\frac{f({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x})}{\n\\sum_{\\color[RGB]{238,102,119}y} f({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x})}\n\\]\n\n\n\n\n\n\n Exercises\n\n\n\nCalculate the conditional frequency distributions corresponding to the subpopulations of tables 22.2 and 22.3. For example, for table  22.2 this means calculating\n\\[\\begin{aligned}\n&f(\\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;low;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathit{Si}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;low;} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;tableware;})\\ ,\n\\\\\n&f(\\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;low;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathit{Si}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;medium;} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;tableware;})\\ ,\n\\\\\n&\\dotsc\\ ,\n\\\\\n&f(\\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;high;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathit{Si}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;high;} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;tableware;})\n\\end{aligned}\\]"
  },
  {
    "objectID": "subpopulations.html#sec-association",
    "href": "subpopulations.html#sec-association",
    "title": "22  Subpopulations and conditional frequencies",
    "section": "22.3 Associations",
    "text": "22.3 Associations\nThe analysis of subpopulations and conditional frequencies is important because it often reveals peculiar associations1 among different variates and groups of variates. Let’s illustrate what we mean by “association” with an example.1 In everyday language this is the same as “correlation”. The term “association” is used in statistics to avoid confusion with the Pearson correlation coefficient (see §  18.5)\nExtract the subpopulation having variate \\(\\mathit{Type}\\) equal to \\({\\small\\verb;headlamps;}\\) from the population of table  22.1:\n\n\nTable 22.4: Selection according to \\(\\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;headlamps;}\\)\n\n\n\n\n\n\n\n\nunit\n\\(\\mathit{Ca}\\)\n\\(\\mathit{Si}\\)\n\\(\\boldsymbol{\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}}\\mathit{Type}\\)\n\n\n\n\n1\n\\({\\small\\verb;low;}\\)\n\\({\\small\\verb;high;}\\)\n\\({\\small\\verb;headlamps;}\\)\n\n\n5\n\\({\\small\\verb;low;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;headlamps;}\\)\n\n\n\n\nwe notice that all units have variate \\(\\mathit{Ca}\\) equal to \\({\\small\\verb;low;}\\). In terms of conditional frequencies, this means\n\\[\n\\begin{aligned}\n&f(\\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;low;} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;headlamps;}) = 1\n\\\\\n&f(\\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;medium;} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;headlamps;}) = 0\n\\\\\n&f(\\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;high;} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;headlamps;}) = 0\n\\end{aligned}\n\\]\nIt is therefore impossible to observe other values of \\(\\mathit{Ca}\\) in this new population.22 We are not claiming that this fact will be true if new units are considered; this important question will be discussed later.\nOn the other hand, if we extract the subpopulation having variate \\(\\mathit{Ca}\\) equal to \\({\\small\\verb;low;}\\) we obtain\n\n\nTable 22.5: Selection according to \\(\\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;low;}\\)\n\n\n\n\n\n\n\n\nunit\n\\(\\boldsymbol{\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}}\\mathit{Ca}\\)\n\\(\\mathit{Si}\\)\n\\(\\mathit{Type}\\)\n\n\n\n\n1\n\\({\\small\\verb;low;}\\)\n\\({\\small\\verb;high;}\\)\n\\({\\small\\verb;headlamps;}\\)\n\n\n2\n\\({\\small\\verb;low;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;building_windows_non_float_processed;}\\)\n\n\n5\n\\({\\small\\verb;low;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;headlamps;}\\)\n\n\n7\n\\({\\small\\verb;low;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;building_windows_non_float_processed;}\\)\n\n\n8\n\\({\\small\\verb;low;}\\)\n\\({\\small\\verb;high;}\\)\n\\({\\small\\verb;tableware;}\\)\n\n\n\n\nwith conditional frequencies such as\n\\[\n\\begin{aligned}\n&f(\\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;headlamps;} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;low;}) = \\frac{2}{5}\n\\\\[1ex]\n&f(\\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;tableware;} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;low;}) = \\frac{1}{5}\n\\end{aligned}\n\\]\nand so on. The reverse is therefore not true: if \\(\\mathit{Ca}\\) is equal to \\({\\small\\verb;low;}\\), that does not mean that it’s impossible to observe other \\(\\mathit{Type}\\) values besides \\({\\small\\verb;headlamps;}\\). Note especially how these frequencies differ:\n\\[\n\\begin{gathered}\nf(\\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;low;} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;headlamps;}) = 1\n\\\\[1ex]\nf(\\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;headlamps;} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;low;}) = \\frac{2}{5}\n\\end{gathered}\n\\]\nIn the original population we have, figuratively speaking, the following interesting association:\n\\[\n\\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;headlamps;}\\ \\mathrel{\\color[RGB]{34,136,51}\\Rightarrow}\\\n\\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;low;}\n\\qquad\\text{\\small but}\\qquad\n\\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;low;}\\  \\mathrel{\\color[RGB]{238,102,119}\\nRightarrow}\\\n\\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;headlamps;}\n\\]\nThis kind of associations is often useful. Suppose for instance that you are asked to pick a unit with \\(\\mathit{Ca}\\) equal to \\({\\small\\verb;low;}\\) in the original population; but it’s difficult to measure a unit’s \\(\\mathit{Ca}\\) value, while it’s easy to measure its \\(\\mathit{Type}\\) value. Then you could instead search for a unit having \\(\\mathit{Type}\\) equal to \\({\\small\\verb;headlamps;}\\) (easier search), and you would be sure that the unit you found also has \\(\\mathit{Ca}\\) equal to \\({\\small\\verb;low;}\\).\n\n\nThe example above, where some values of a variate completely exclude some values of another, is a special one. More often we find that there are small or large changes in the frequency distribution of some variate, depending on the subpopulation considered.\n\n\n\n\n\n\n Exercise\n\n\n\n\nCalculate the (marginal) frequency distribution for the \\(\\mathit{Ca}\\) variate for the glass-fragment population of table  22.1. Is the value \\({\\small\\verb;low;}\\) more frequent than \\({\\small\\verb;medium;}\\)? or vice versa?\nCalculate the frequency distribution for \\(\\mathit{Ca}\\), conditional on \\(\\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;tableware;}\\) (see table  22.2). How does this frequency distribution differ from the one you calculated above? Come up with possible ways to exploit this difference in concrete applications.\n\n\n\n\nAssociations can be very counter-intuitive\nIt is usually best to assess associations by explicitly calculating all relevant conditional frequencies, rather than jumping to intuitive conclusions after having examined just a few. Here’s an example.\n\n\nConsider the statistical population defined as follows:\n\n\n\n\nunits: all reparations done by a repair company on a particular kind of electronic components, which is extremely delicate and usually very difficult to repair. The population has 26 units (every unit actually represents a batch 100 reparations, so the population really refers to 2600 reparations).\na joint variate, consisting in three binary ones:\n\n\\(\\mathit{\\color[RGB]{102,204,238}Location}\\) of the repair procedure, with values \\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\) and \\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\);\nrepair \\(\\mathit{\\color[RGB]{34,136,51}Method}\\), with values \\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\) and \\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\), representing a traditional reparation method and one introduced more recently;\n\\(\\mathit{\\color[RGB]{238,102,119}Outcome}\\) of the repair procedure, with values \\(\\color[RGB]{238,102,119}{\\small\\verb;success;}\\) and \\(\\color[RGB]{238,102,119}{\\small\\verb;fail;}\\).\n\n\n\n\nTable 22.6: Reparations (each row is one unit, representing 100 reparations).\nfile repair_data.csv\n\n\n\\(\\mathit{\\color[RGB]{238,102,119}Outcome}\\)\n\\(\\mathit{\\color[RGB]{34,136,51}Method}\\)\n\\(\\mathit{\\color[RGB]{102,204,238}Location}\\)\n\n\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;success;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;fail;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;fail;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;fail;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;fail;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;fail;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;success;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;success;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;success;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;success;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;success;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;success;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;fail;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;success;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;fail;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;fail;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;fail;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;fail;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;success;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;success;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;success;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;fail;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;fail;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;success;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;success;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;fail;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\)\n\n\n\n\nThe repair company claims that, in this population, the \\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\) repair method is more effective than the \\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\). Can you back up their claims?:\n\n\n\n\n\n\n Exercise (one of the most fun of the course!)\n\n\n\nUse the population data above. The calculations can be done with any tools you like.\n\nExamine the whole population first:\n\nCalculate the frequency distribution of the \\(\\mathit{\\color[RGB]{238,102,119}Outcome}\\) variate, conditional on \\(\\mathit{\\color[RGB]{34,136,51}Method}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\) (note that we are disregarding the \\(\\mathit{\\color[RGB]{102,204,238}Location}\\)).\nCalculate the frequency distribution of the \\(\\mathit{\\color[RGB]{238,102,119}Outcome}\\) variate, conditional on \\(\\mathit{\\color[RGB]{34,136,51}Method}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\).\nCompare the two conditional frequency distributions above. Which of the two repair methods seems more effective?\nAre the claims of the repair company justified?\n\nNow examine the reparations that have been done \\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\):\n\nBefore doing any calculations, what do you expect to find? should the \\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\) repair method be more effective than the \\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\) one, for onsite reparations?\nCalculate the frequency distribution of the \\(\\mathit{\\color[RGB]{238,102,119}Outcome}\\) variate, conditional on \\(\\mathit{\\color[RGB]{34,136,51}Method}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\) and \\(\\mathit{\\color[RGB]{102,204,238}Location}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\).\nCalculate the frequency distribution of the \\(\\mathit{\\color[RGB]{238,102,119}Outcome}\\) variate, conditional on \\(\\mathit{\\color[RGB]{34,136,51}Method}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\) and \\(\\mathit{\\color[RGB]{102,204,238}Location}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\).\nCompare the two conditional frequency distributions for this \\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\) case. Which of the two repair methods seems more effective?\nHow do you explain this result in the light of what you found in step 3.?\n\nNow examine the reparations that have been done \\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\)ly:\n\nBefore doing any calculations, what do you expect to find? should the \\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\) repair method be more effective than the \\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\) one, for reparations done remotely?\nCalculate the frequency distribution of the \\(\\mathit{\\color[RGB]{238,102,119}Outcome}\\) variate, conditional on \\(\\mathit{\\color[RGB]{34,136,51}Method}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\) and \\(\\mathit{\\color[RGB]{102,204,238}Location}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\).\nCalculate the frequency distribution of the \\(\\mathit{\\color[RGB]{238,102,119}Outcome}\\) variate, conditional on \\(\\mathit{\\color[RGB]{34,136,51}Method}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\) and \\(\\mathit{\\color[RGB]{102,204,238}Location}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\).\nCompare the two conditional frequency distributions for this \\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\) case. Which of the two repair methods seems more effective?\nHow do you explain this result in the light of what you found in steps 3. and 7.?\n\nSummarize and explain all your findings.\nCan the repair company claim that the \\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\) repair method is better than the \\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\)?\n\nSuppose you need to send an electronic component for repair to this company.\n\nIf you could choose both the \\(\\mathit{\\color[RGB]{102,204,238}Location}\\) and the \\(\\mathit{\\color[RGB]{34,136,51}Method}\\) of the repair, which would you choose? why?\nIf you could only choose the repair \\(\\mathit{\\color[RGB]{34,136,51}Method}\\), but have no control over the \\(\\mathit{\\color[RGB]{102,204,238}Location}\\), which method would you choose? why?\n\nIs there other information, missing from the description of the population, that should be known before answering the questions above?\n\n\n\n\n\n\n\n\n\n\n\n\n Study reading\n\n\n\n\n§§ 2.2–2.4 and 2.7–2.10 of Risk Assessment and Decision Analysis with Bayesian Networks\nThe role of exchangeability in inference This can be a difficult reading. Try to get the main message.\nSimpson’s paradox"
  },
  {
    "objectID": "samples.html#sec-infinite-populations",
    "href": "samples.html#sec-infinite-populations",
    "title": "23  Infinite populations and samples",
    "section": "23.1 Infinite populations",
    "text": "23.1 Infinite populations\nThe examples of populations that we explored so far comprised a small number of units, and all their data were exactly and fully known. In concrete inference and decision problems of the kind we have been focusing on in chapters 17 and 19, we usually deal with populations that are much larger or potentially infinite; and data are known only for a small collection of their units.\nIn the glass-forensic example (table  20.1), for instance, many more glass fragments could be examined beyond the 10 units reported there, with no clear bound on the total number. We could even extend that population considering glass fragments from past and future crime scenes:\n\n\n\nTable 23.1: Glass fragments, extended\n\n\n\n\n\n\n\n\n\n\nunit\n\\(\\mathit{RI}\\)\n\\(\\mathit{Ca}\\)\n\\(\\mathit{Si}\\)\n\\(\\mathit{Type}\\)\nnotes\n\n\n\n\n1\n\\(1.51888\\)\n\\(9.95\\)\n\\(72.50\\)\n\\({\\small\\verb;tableware;}\\)\n\n\n\n2\n\\(1.51556\\)\n\\(9.41\\)\n\\(73.23\\)\n\\({\\small\\verb;headlamps;}\\)\n\n\n\n3\n\\(1.51645\\)\n\\(8.08\\)\n\\(72.65\\)\n\\({\\small\\verb;building_windows_non_float_processed;}\\)\n\n\n\n4\n\\(1.52247\\)\n\\(9.76\\)\n\\(70.26\\)\n\\({\\small\\verb;headlamps;}\\)\n\n\n\n5\n\\(1.51909\\)\n\\(8.78\\)\n\\(71.81\\)\n\\({\\small\\verb;building_windows_float_processed;}\\)\n\n\n\n6\n\\(1.51590\\)\n\\(8.22\\)\n\\(73.10\\)\n\\({\\small\\verb;building_windows_non_float_processed;}\\)\n\n\n\n7\n\\(1.51610\\)\n\\(8.32\\)\n\\(72.69\\)\n\\({\\small\\verb;vehicle_windows_float_processed;}\\)\n\n\n\n8\n\\(1.51673\\)\n\\(8.03\\)\n\\(72.53\\)\n\\({\\small\\verb;building_windows_non_float_processed;}\\)\n\n\n\n9\n\\(1.51915\\)\n\\(10.09\\)\n\\(72.69\\)\n\\({\\small\\verb;containers;}\\)\n\n\n\n10\n\\(1.51651\\)\n\\(9.76\\)\n\\(73.61\\)\n\\({\\small\\verb;headlamps;}\\)\n\n\n\n…\n…\n…\n…\n…\n…\n\n\n351\n\\(1.52101\\)\n\\(8.75\\)\n\\(71.78\\)\n?\nfrom unsolved-crime scene in 1963\n\n\n…\n…\n…\n…\n…\n…\n\n\n1027\n\\(1.51761\\)\n\\(7.83\\)\n\\(72.73\\)\n?\ncrime scene in 2063\n\n\n…\n…\n…\n…\n…\n…\n\n\n\n\n\nthe imaginary example above also shows that the values of some variates for some units might be unknown; this is a situation we shall discuss in depth later.\n\n\nWe shall henceforth focus on statistical populations with a number of units that is in principle infinite, or so large that it can be considered practically infinite. “Practically” means that the number of units we’ll use as data or draw inferences about is a very small fraction, say less than 0.1%, of the total population size.\nThis is often the case. Consider for example (as in §  12.1.1) the collection of all possible 128 × 128 images with 24-bit colour depth. This collection has \\(2^{24 \\times 128 \\times 128} \\approx 10^{118 370}\\) units. Even if we used 100 billions of such images as data, and wanted to draw inferences on another 100 billions, these would constitute only \\(10^{-118 357}\\,\\%\\) of the whole collection. This collection is practically infinite.\nNote that we can’t say whether a population, per se, is “practically infinite” or not. It could be practically infinite for a particular inference problem, but not for another.\nWhen we use the term “population” it will often be understood that we’re speaking about a statistical population that is practically infinite with respect to the inference or decision problem under consideration."
  },
  {
    "objectID": "samples.html#sec-limit-freqs",
    "href": "samples.html#sec-limit-freqs",
    "title": "23  Infinite populations and samples",
    "section": "23.2 Limit frequencies",
    "text": "23.2 Limit frequencies\nIn §  21.2 we defined relative frequencies. Relative frequencies are ratios of two integers, the denominator being the population size \\(N\\). So a frequency \\(f\\) can only take on \\(N+1\\) rational values \\(0/N, \\dotsc, N/N\\) between \\(0\\) and \\(1\\). As the population size increases, the number of distinct, possible frequencies increases and eventually can be considered practically continuous. Frequencies in this case are sometimes called limit frequencies and they are treated as real numbers between \\(0\\) and \\(1\\)."
  },
  {
    "objectID": "samples.html#sec-samples",
    "href": "samples.html#sec-samples",
    "title": "23  Infinite populations and samples",
    "section": "23.3 Samples",
    "text": "23.3 Samples\n\nLearning from samples\nIn chapters 17 and 19 we considered an agent that must draw an inference about some units from a population. The agent’s degrees of belief in that inference relied (that is, were conditional on) units already observed in the population, the “learning” or “training” data. We saw that the agent’s degrees of belief changed, often becoming sharper, thanks to the information about the observed units.\nUnits for which we have full (or almost full) information, and that an agent can use to update its beliefs, are called a population sample or “sample” for short. Almost all data considered in engineering and data-science problems can be considered to be population samples.\nIt is extremely important to specify how a sample is extracted or collected from a population. For instance, if we consider table  20.1 to be a full population, we could extract a sample in such a way that \\(\\mathit{Type}\\) only has value \\({\\small\\verb;headlamps;}\\) (similarly to when we construct a subpopulation, §  22.1, but for a subpopulation we would select all units having that variate value). The marginal frequency of the value \\({\\small\\verb;headlamps;}\\) in the sample would then be \\(1\\), whereas in the original population it is \\(3/10 \\approx 0.333\\) – two very different frequencies.\n\n\n“Representative” and biased samples\nIf samples from a population are used as conditional information to calculate probabilities about other units, then they should of course be “relevant”, in some sense (not the technical sense of chapter  18), for the inference. The very definition of statistical population (§  20.2) is meant to have such a relevance built-in: the “similarity” of the units makes each of them relevant for inferences about any other.\nStill, the procedure with which samples are selected from a population may lead to quirky and unreasonable inferences. For instance suppose we are interested in prognosing a disease for a person from a particular population, having observed a sample of people from the same population. If the sample was chosen to consist only of people having the disease, then it is obviously meaningless for our inference.\nThe specific problem in this example is that our inference is based on guessing a frequency distribution in the full population (as we’ll see more in detail in later chapters), but the sample, owing to the way it was chosen, cannot show a frequency distribution similar to the full-population frequency distribution.\n\n\nA sampling procedure may generate a sample that is pointless for some inferences, but still useful for others.\nIn the inference and decision problems under our focus we would like to use a sample for which particular frequencies – most often the full joint frequency – don’t differ very much from those in the full population. We’ll informally call this a “representative sample”. This is a difficult notion; the International Organization for Standardization for instance warns (item 3.1.14):\n\nThe notion of representative sample is fraught with controversy, with some survey practitioners rejecting the term altogether.\n\n\n\nIn many cases it is impossible for a sample of given size to be fully “representative”:\n\n\n\n\n\n\n Exercise\n\n\n\nConsider the following population of 16 units, with four binary variates \\(W,X,Y,Z\\), each with values \\(0\\) and \\(1\\):\n\n\n\n\nTable 23.2: Four-bit population\n\n\n\\(W\\)\n\\(X\\)\n\\(Y\\)\n\\(Z\\)\n\n\n\n\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n\n\n0\n1\n0\n0\n\n\n1\n1\n0\n0\n\n\n0\n0\n1\n0\n\n\n1\n0\n1\n0\n\n\n0\n1\n1\n0\n\n\n1\n1\n1\n0\n\n\n0\n0\n0\n1\n\n\n1\n0\n0\n1\n\n\n0\n1\n0\n1\n\n\n1\n1\n0\n1\n\n\n0\n0\n1\n1\n\n\n1\n0\n1\n1\n\n\n0\n1\n1\n1\n\n\n1\n1\n1\n1\n\n\n\n\n\n\nThe joint variate \\((W,X,Y,Z)\\) has 16 possible values, from \\((0,0,0,0)\\) to \\((1,1,1,1)\\). Each of these values appear exactly once in the population, so it has frequency \\(1/16\\). The marginal frequency distribution for each binary variate is also uniform, with frequencies of 50% for both \\(0\\) and \\(1\\).\n\nExtract a representative sample of size four units. In particular, the marginal frequency distributions of the four variates should be as close to 50%/50% as possible.\n\n\n\nLuckily, the probability calculus allows an agent to draw inferences also when the sample is too small to correctly reflect full-population frequencies, if appropriate background information is provided.\n\n\nObviously we cannot expect a population sample to exactly reflect all frequency distributions – joint, marginal, conditional – of the original population; some discrepancy is to be expected. How much discrepancy should be allowed? And what is the minimal size for a sample not to exceed such discrepancy?\nInformation Theory, briefly mentioned in chapter  18, can give reasonable answers to these questions. Let us summarize some examples here.\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\n\nChapters 1–10 of Information Theory, Inference, and Learning Algorithms\nVideo lectures 1–9 from the Course on Information Theory, Pattern Recognition, and Neural Networks\n\n\n\nFirst we need to introduce the Shannon entropy of a discrete frequency distribution. It is defined in a way analogous to the Shannon entropy for a discrete probability distribution, discussed in §  18.5. Lets say the distribution is \\(\\boldsymbol{f} \\coloneqq(f_1,f_2, \\dotsc)\\). Its Shannon entropy \\(\\mathrm{H}(\\boldsymbol{f})\\) is\n\\[\n\\mathrm{H}(\\boldsymbol{f}) \\coloneqq-\\sum_{i} f_i\\ \\log_2 f_i\n\\qquad\\text{\\color[RGB]{119,119,119}\\small(with \\(0\\cdot\\log 0 \\coloneqq 0\\))}\n\\]\nand is measured in shannons when the base of the logarithm is 2.\nIf we have a population with joint frequency distribution \\(\\boldsymbol{f}\\), then a representative sample from it must have at least size\n\\[\n2^{\\mathrm{H}(\\boldsymbol{f})} \\equiv\n\\frac{1}{{f_1}^{f_1}\\cdot {f_2}^{f_2}\\cdot {f_3}^{f_3}\\cdot \\dotsb}\n\\] \nThis particular number has important practical consequences; for example it is related to the maximum rate at which a communication channel can send symbols (which can be considered as values of a variate) with an error as low as we please.\n\n\n\n\n\n\n Exercise\n\n\n\n\nCalculate the Shannon entropy of the joint frequency distribution for the four-bit population of table  23.2.\nCalculate the minimum representative-sample size according to the Shannon-entropy formula. Is the result intuitive?\n\n\n\n\n\nIf we are only interested in a smaller number of variates of a population, then the representative sample can be smaller as well: its size would be given by the entropy of the corresponding marginal frequency distribution of the variates of interest. In the example of table  23.2, if we are only interested in the variate \\(X\\), then any sample consisting of two units, one having \\(X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}0\\) and the other having \\(X\\mathord{\\nonscript\\mkern 1mu\\textrm{\\small=}\\nonscript\\mkern 1mu}\\mathopen{}1\\), would be a representative sample of the marginal frequency distribution \\(f(X)\\).\n\n\n\n\n\n\n Study reading\n\n\n\nRepresentative Sampling, I\n\n\nA sample that presents some aspects, such as frequency distributions, which are at variance with the original population, is sometimes called biased. This term is used in many different ways by different authors. Unfortunately, most samples are “biased” in this sense.\nThe only way to counteract the misleading information given by a biased sample is to specify appropriate background information, which comes not from data samples but from a general meta-analysis, often based on physical, medical, and similar principles, of the problem and population.\n\n\n\n\nQuirks of samples for mean and standard deviation\nFor some populations, the mean and standard deviation calculated in a sample can be wildly different from those of the full population – even when the sample comprises half of the full population! This does not happen with the median and quartiles. Here is a demonstration in R. Try it out in your favourite programming language.\n\n\n\n\n\n\n Guided exercise\n\n\n\nWe imagine to have a population of 1 000 000 units. These units have continuous interval variates \\(X\\) and \\(Y\\), each with an approximately standard Gaussian frequency distribution. These variates are not actual part of the population definition, however. Rather, an agent only has access to, or maybe it’s only interested in, the ratio of these two variates \\(T \\coloneqq X/Y\\).\nThe agent is in particular interested in the mean of the variate \\(T\\) in the full population, but has only access to the values of \\(T\\) in a sample. How does the mean calculated from a sample of increasing size compare with the actual mean of the full population? For comparison, we also study the median of the full population and of the samples.\nFirst let’s create the values of the variates \\(X\\) and \\(Y\\), and construct \\(T\\) from them:\n\n## Load custom plot functions\nsource('code/tplotfunctions.R')\nset.seed(1000) # to reproduce results\n\nN &lt;- 1000000 # population size\n\nX &lt;- rnorm(N) # variate invisible to agent\nY &lt;- rnorm(N) # variate invisible to agent\n\nT &lt;- X/Y # variate considered by agent\n\n## mean and median of T in the full population\npopmean &lt;- mean(T)\ncat('\\nThe full-population mean is', popmean, '(unknown to the agent)\\n')\n\n\nThe full-population mean is -8.04481 (unknown to the agent)\n\npopmedian &lt;- median(T)\ncat('\\nThe full-population median is', popmedian, '(unknown to the agent)\\n')\n\n\nThe full-population median is 0.00188701 (unknown to the agent)\n\n\nNow we imagine that the agent accumulates samples from the population, starting from 100, increasing by 100 units, until half of the population has been sampled. At each sample increase the agent calculates the sample mean. We plot how the sample mean changes with the sample size. We also plot indicate the full-population mean, which the agent doesn’t know and is trying to guess:\n\n## sizes of successive samples\nsamplesizes &lt;- seq(100, N/2, by=100)\n\n## empty vectors to contain the means and medians of the increasing samples\nsamplemeans &lt;- numeric(length(samplesizes))\nsamplemedians &lt;- numeric(length(samplesizes))\n\n## loop through the increasing samples, calculate mean for each\nfor(sample in 1:length(samplesizes)){\n    samplemeans[sample] &lt;- mean(T[1:samplesizes[sample]])\n    samplemedians[sample] &lt;- median(T[1:samplesizes[sample]])\n\n}\n\n## plot how sample means change with sample size, and the actual population mean\ncommonmax &lt;- 1.01*max(abs(c(popmean,popmedian,samplemeans,samplemedians)))\ntplot(x=samplesizes, y=samplemeans,\n      xlab='sample size', ylab='sample mean',\n      col=2, lwd=4,\n      ylim=c(-commonmax,commonmax))\nabline(h=popmean, lty=2, lwd=3, col=7)\ntext(y=popmean, x=max(samplesizes), labels='population mean (unknown to agent)', adj=c(1,1), col=7, cex=1.2)\n\n\n\n## plot how sample medians change with sample size, and the actual population median\ntplot(x=samplesizes, y=samplemedians,\n      xlab='sample size', ylab='sample median',\n      col=3, lwd=4,\n      ylim=c(-commonmax,commonmax))\nabline(h=popmedian, lty=2, lwd=3, col=7)\ntext(y=popmedian, x=max(samplesizes), labels='population median (unknown to agent)', adj=c(1,1), col=7, cex=1.2)\n\n\n\n\nTest this again with several pseudorandom seeds.\n\n\n\nNow try a similar exercise but for the standard deviation of \\(T\\)"
  },
  {
    "objectID": "machine_learning_overview.html#hyperparameters-and-model-complexity",
    "href": "machine_learning_overview.html#hyperparameters-and-model-complexity",
    "title": "24  Introduction to machine learning",
    "section": "24.1 Hyperparameters and model complexity",
    "text": "24.1 Hyperparameters and model complexity\nYou may have heard the quote by statistician George Box:\n\nAll models are wrong, but some are useful.\n\nAlthough coming off as a somewhat negative view on things, the quote still captures an important point about statistical modelling – our goal is not to make a complete, 100% accurate description of reality, but rather a simplified description that is meaningful in the context of our task at hand. The “correct” level of simplicity, in other words the optimal number of parameters \\(\\mathbf{w}\\), can be hard to find. Often it will be influenced by practical considerations such as time and computing power, but it is always governed by the amount of data we have available. We will look at the theory of model selection later, but let us first consider a visual example, from which we can define some important concepts.\nImagine that you don’t have a thermometer that shows the outside temperature. Never knowing whether you should wear a jacket or not when leaving the house, you get a great idea: If you can construct a mathematical model for the outside temperature as a function of the time of day, then a look at the clock yould be enough to decide for or against bringing a jacket. You manage to borrow a thermometer for a day, and make ten measurements at different times, which will form the basis for the model:\n\n\n\n\n\n\nFigure 24.1: Temperature measurements over the course of 24 hours.\n\n\n\n\nSource: over_and_underfitting.ipynb\nThe next step is to choose a function \\(f\\). For one-dimensional data like this, we could for instance select among the group of polynomials, of the form\n\\[\n    f (x, \\mathbf{w}) = w_0 + w_1 x + w_2 x^2 + \\dots + w_M x^M = \\sum_{j=0}^{M} w_j x^j \\,,\n\\]\nwhere \\(M\\) is the order of the polynomial. Recall that a zero’th order polymonial is just a constant value, so such a model would be represented with one single parameter. A first-order polynomial is a linear function (two parameters), and the higher in order (and number of parameters) we go, the more “curves” the function can have. This already presents us with a problem. Which order polynomial (i.e. which value of \\(M\\)) should we choose? Let us try different ones, and for each case, fit the parameters, meaning we find the parameter values that yield the smallest difference from the measured data points:\n\n\n\n\n\n\n\n\n(a) Zeroth-order polymonial\n\n\n\n\n\n\n\n(b) First-order polymonial\n\n\n\n\n\n\n\n\n\n(c) Fourth-order polymonial\n\n\n\n\n\n\n\n(d) Ninth-order polymonial\n\n\n\n\nFigure 24.2: Fitting polymonials (red lines) of different orders to the set of measurements (blue circles).\n\n\nSource: over_and_underfitting.ipynb\nObviously, the constant function does a poor job of describing our data, and likewise for the linear function. A fourth-order polynomial, on the other hand, looks very reasonable. Now consider the ninth-order polynomial: it matches the measurements perfectly, but surely, this does not match our expectation for what the temperature should look like.\nWe say that the first and second model underfit the data. This can happen for two reasons: Either the model has too little complexity to be able to describe the data (which is the case in this example), or, potentially, the optimal parameter values have not been found. The opposite case is overfitting, as shown for the last model, where the complexity is too high and the model adapts to artifacts in the data.\nThis concept is also called the bias-variance tradeoff. We will not go into too much detail on this yet, but qualitatively, we can say that bias (used in this setting) is the difference between the predicted value and the true value, when averaging over different data sets. Variance (again when used in this setting) indicates how big the change in parameters, and hence in model predictions, we get from fitting to different data sets. Let us say you measure the temperature on ten different days, and for each day, you fit a model, like before. These may be the results:\n\n\n\n\n\n\n\n\n(a) Large bias, low variance\n\n\n\n\n\n\n\n(b) Resonably low bias and variance\n\n\n\n\n\n\n\n(c) Low bias, large variance\n\n\n\n\nFigure 24.3: Red lines: Different models fitted to separate datasets, where each dataset is drawn from the same distribution as the original one. The original dataset shown in blue circles.\n\n\nSource: over_and_underfitting.ipynb\nThe blue dots are our “original” data points, plotted for reference, while the red lines corresponds to models fitted for each day’s measurements. Due to fluctuations in the measurements, they are different, but note how the difference is related to the model complexity. Our linear models (\\(M=1\\)) are all quite similar, but neither capture the pattern in the data very well, so they all have high bias. The overly complex models (\\(M=9\\)) have zero bias on their repective dataset, but high variance. The optimal choice is likely somewhere in-between (hence the tradeoff), as for the \\(M=4\\) models, which perform well without being overly sensitive to fluctuations in data. Since the value of \\(M\\) is chosen by us, we call it a hyperparameter, to separate it from the regular parameters which are optimised by minimising the loss function.\n\n\n\n\n\n\n Exercises\n\n\n\n\nOur simple temperature model relies on a series of assumptions, some which might be good, and some which might be (very) bad. State the ones you can think of, and evaluate if they are sensible. Hints: are polynomials a good choice for \\(f\\)? Is the data representative?\nFor the different models in figure 24.2 we started by choosing polymonial degree \\(M\\), and then computed the parameters that minimized the difference between the data points and the model. Then we had a look at the results, compared them to our expectations, and decided that \\(M=0\\) and \\(M=9\\) were both unlikely. Can you think of a way to incorporate our initial expectations into the computation?"
  },
  {
    "objectID": "machine_learning_overview.html#model-selection-in-practice",
    "href": "machine_learning_overview.html#model-selection-in-practice",
    "title": "24  Introduction to machine learning",
    "section": "24.2 Model selection (in practice)",
    "text": "24.2 Model selection (in practice)\nAs alluded to in the exercises above, there are ways of including both the data and our prior expectations when building a model, but it is, in fact, not very common to do so. In this section we will have a look at the typical data science approach, which relies on splitting the data in different sets. Starting from all our available data on a given problem that we wish to model, we divide it into\n\nthe training set, which is the data we will use to determine the model parameters \\(\\mathbf{w}\\),\nthe validation set, which is used to evaluate the model complexity (i.e. finding the optimal bias-variance tradeoff), and\nthe test set, which is used to evaluate the final performance of the model.\n\nThe benefit of this approach is that it is very simple to do. The downside, on the other hand, is that each set is necessarily smaller than the total, which subjects us to increased statistical uncertainty. The final parameters will be slightly less optimal than they could have been, and the measurement of the performance will be slightly less accurate. Still, it is common practice, and for the “standard” machine learning methods there is no direct way of simultaneously optimising for the model complexity."
  },
  {
    "objectID": "machine_learning_overview.html#machine-learning-taxonomy",
    "href": "machine_learning_overview.html#machine-learning-taxonomy",
    "title": "24  Introduction to machine learning",
    "section": "24.3 Machine learning taxonomy",
    "text": "24.3 Machine learning taxonomy\nMachine learning has been around for many decades, and the list of methods that fit under our simple definition of looking like equation 24.1 and learning their parameters from data, is very long. For a (non-exhaustive) systematic list, have a look at the methods that are implemented in scikit-learn, a Python library dedicated to data analysis.\nIn this course we will not try to go through all of them, but rather focus on the fundamentals of what they all have in common. With this fundamental understanding, learning about new methods is like learning a new programming language – each has their specific syntax and specific uses, but the underlying mechanism is the same. We will look at two important methods, that are inherently very different, but still accomplish the same end result. The first is decision trees (and the extension into random forests), while the second is neural networks."
  },
  {
    "objectID": "decision_trees.html#decision-trees-for-classification-on-categorical-values",
    "href": "decision_trees.html#decision-trees-for-classification-on-categorical-values",
    "title": "25  Decision trees",
    "section": "25.1 Decision trees for classification on categorical values",
    "text": "25.1 Decision trees for classification on categorical values\nFor a trivial example, lets us say you want to go and spend the day at the beach. There are certain criteria that should be fullfilled for that to be a good idea, and some of them depend on each other. A decision tree could look like this:\n\n\n\n\n\n\n\n\nG\n\n \n\nA\n\n   Go to the beach?    \n\nB\n\n Workday?  \n\nC\n\n Don’t go  \n\nB-&gt;C\n\n  Yes  \n\nD\n\n Sunny?  \n\nB-&gt;D\n\n  No  \n\nE\n\n Temp &gt; 18ºC  \n\nD-&gt;E\n\n  Yes  \n\nF\n\n Temp &gt; 23ºC  \n\nD-&gt;F\n\n  No  \n\nG\n\n  Go to the beach   \n\nE-&gt;G\n\n  Yes  \n\nH\n\n  Don’t go   \n\nE-&gt;H\n\n  No  \n\nI\n\n  Go to the beach   \n\nF-&gt;I\n\n  Yes  \n\nJ\n\n  Don’t go   \n\nF-&gt;J\n\n  No  \n\n\nFigure 25.1: A very simple decision tree.\n\n\n\n\nDepending on input data such as the weather, we end up following a certain path from the root node, along the branches, down to the leaf node, which returns the final decision for these given observations. The botany analogies are not strictly necessary, but at least we see where the name decision tree comes from.\nStudying the above tree structure more closely, we see that there are several possible ways of structuring it, that would lead to the same outcome. We can choose to first split on the Sunny node, and split on Workday afterwards. Drawing it out on paper, however, would show that this structure needs a larger total number of nodes, since we always need to split on Workday. Hence, the most efficient tree is the one that steps through the observables in order of descending importance.\nThe basic algorithm for buiding a decision tree (or growing it, if you prefer) on categorical data, can be written out quite compactly. Consider the following pseudo-code: \n\nfunction BuildTree(examples, target_feature, features)\n  \n  # examples is the training data\n  # target_feature is the feature we want to predict\n  # features is the list of features present in the data\n  \n  tree ← a single node, so far without any label\n  if all examples are of the same classification then\n    give tree a label equal to the classification\n    return tree\n  else if features is empty then\n    give tree a label equal the most common value of target_feature in examples\n    return tree\n  else\n    best_feature ← the feature from features with highest Importance(examples)\n\n    for each value v of best_feature do\n      examples_v ← the subset of examples where best_feature has the value v\n      subtree ← BuildTree(examples_v, target_feature, features - {best_feature})\n      add a branch with label v to tree, and below it, add the tree subtree\n    \n    return tree\n\nThis is the original ID3 algorithm (Quinlan 1986). Note how it works recursively – for each new feature, the function calls itself to build a subtree.\n\nQuinlan, J. Ross. 1986. “Induction of Decision Trees.” Machine Learning 1: 81–106.\n\nThe same algorithm is shown and explained in section 19.3 in Russell and Norvig, although they fail to specify that this is ID3.\n\nWe start by creating a node, which becomes a leaf node either if it classifies all examples correctly (no reason to split), or if there are no more features left (not possible to split). Otherwise, we find the most important feature by calling Importance(examples), and proceed to make all possible splits. Now, the magic happens in the Importance function. How can we quantify which feature is best to discriminate on? We have in sections Section 23.1 and Section 18.5 met a useful definition from information theory, which is the Shannon entropy:\n\\[\n    H(f) \\coloneqq-\\sum_{i} f_i\\ \\log_2 f_i\n    \\qquad\\text{\\color[RGB]{119,119,119}\\small(with \\(0\\cdot\\log 0 \\coloneqq 0\\))}\n\\]\nwhere the \\(f_i\\) are frequency distributions. If we stick to the simple example of our target features being “yes” or “no”, we can write out the summation like so:\n\\[\n    H = - f_{\\mathrm{yes}} \\log_2 f_{\\mathrm{yes}} - f_{\\mathrm{no}} \\log_2 f_{\\mathrm{no}}\n\\]\nLet us compute the entropy for two different cases, to see how it works. In the first case, we have 10 examples: 6 corresponding to “yes”, and four corresponding to “no”. The entropy is then\n\\[\n    H(6\\;\\text{yes}, 4\\;\\text{no}) = - (6/10) \\log_2 (6/10) - (4/10) \\log_2 (4/10) = 0.97\n\\]\nIn the second case, we still have 10 examples, but nearly all of the same class: 9 examples are “yes”, and 1 is “no”:\n\\[\n    H(9\\;\\text{yes}, 1\\;\\text{no}) = - (9/10) \\log_2 (9/10) - (1/10) \\log_2 (1/10) = 0.47\n\\]\nInterpreting the entropy as a measure of impurity in the set of examples, we can guess (or compute, using \\(0\\cdot \\log_2 0 = 0\\)) that the lowest possible entropy occurs for a set where all are of the same class. When doing classification, this is of course what we aim for – separating all examples into those corresponding to “yes” and those corresponding to “no”. A way of selecting the most important feature is then to choose the one where we expect the highest reduction in entropy, caused by splitting on this feature. This is called the information gain, and is generally defined as\n\\[\n    Gain(A, S) \\coloneqq H(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} H(S_v) \\,,\n\\tag{25.1}\\]\nwhere \\(A\\) is the feature under consideration, \\(Values(A)\\) are all the possible values that \\(A\\) can have. Further, \\(S\\) is the set of examples, and \\(S_v\\) is the subset containing examples where \\(A\\) has the value \\(v\\). Looking again at the binary yes/no case, it looks a little simpler. Using the feature Sunny as \\(A\\), we get:\n\\[\n    Gain(\\texttt{Sunny}, S) = H(S) - \\left(\\frac{S_{\\texttt{Sunny=yes}}}{S} H(S_{\\texttt{Sunny=yes}}) + \\frac{S_{\\texttt{Sunny=no}}}{S} H(S_{\\texttt{Sunny=no}})\\right) \\,.\n\\]\nThis equation can be read as “gain equals the original entropy before splitting on Sunny, minus the weighted entropy after splitting”, which is what we were after. One thing to note about equation 25.1: while it allows for splitting on an arbitrary number of values, we typically always want to split in two, resulting in binary trees. Non-binary trees tend to quickly overfit, which why few of the successors to the ID3 algorithm allow this. The extreme case would be if a feature is continuous instead of categorical. For a continuous feature it is unlikely that the data will contain values that are identical – probably many values are similar, but not identical to e.g. ten digits precision. ID3 would potentially split such a feature into as many branches as there are data points, which is maximal overfitting. Binary trees can of course overfit too (we will get back to this shortly), but first, let us introduce a similar algorithm that can deal with both continuous inputs, and continuous output."
  },
  {
    "objectID": "decision_trees.html#decision-trees-for-regression-in-addition-to-classification",
    "href": "decision_trees.html#decision-trees-for-regression-in-addition-to-classification",
    "title": "25  Decision trees",
    "section": "25.2 Decision trees for regression (in addition to classification)",
    "text": "25.2 Decision trees for regression (in addition to classification)\nThe problem of continuous features can be solved by requiring only binary splits, and then searching for the optimal threshold value for where to split. Each node will then ask “is the value of feature \\(A\\) larger or smaller than the threshold \\(x\\)”? Finding the best threshold involves going through all the values in data and computing the expected information gain. One could initially think that we need to consider all possible values that the data could take, but luckily we need only to consider the values the data does take, since the expected information gain has discrete steps for value where we move a data point from one branch to the other.\nA second thing we would like to solve, is to not only have categorical outputs (i.e. do classification), but also continuous values (i.e. do regression). Looking again at the pseudocode for the ID3 algorithm, we see that once we have “used up” all the features, the label assigned to the final branch will be the majority label among the remaining examples. For continuous target values, the fix is relatively simple – we instead just take the average of the values in the examples. These two solutions form the basis for the CART (Classification and Regression Trees) algorithm, which creates decision trees for any kind of inputs and outputs."
  },
  {
    "objectID": "decision_trees.html#preventing-overfitting",
    "href": "decision_trees.html#preventing-overfitting",
    "title": "25  Decision trees",
    "section": "25.3 Preventing overfitting",
    "text": "25.3 Preventing overfitting\nAgain from the pseudocode of the ID3 algorithm, we see that the basic rules for building a tree will keep making splits until we have perfect classification, or until no more features are available. Perfect classification surely sounds good, but is in practice rarely attainable, and these rules will typically create too many splits and thereby overfit to the training data.\n\nPruning and hyperparameter choices\nThe common approach to avoid this is in fact to just let it happen – and then afterwards, go in and remove the branches that give the least improvement in prediction. Sticking with the biologically inspired jargon, we call this pruning. We will not go into the details of this, but leave it as an illustration of how well-defined machine learning methods often need improvised heuristics to work. Other tweaks that are used in parallel include\n\nrequiring a minimum number of training events in each end node, and\nenforcing a maximum depth, i.e. allowing only a certain number of subsequent splits.\n\n\n\nRandom forests\nTraining several decision trees on similar data tends to end up looking like figure 24.3 (c): they are sensitive to small variations in the data and hence have high variance. But while each individual tree can be far off the target, the average is still good, since the variations often cancel out. This can be the case for many types of machine learning methods. We can improve performance by defining a new ensemble model \\(f(\\mathbf{x})\\) composed of several separate models \\(f_m(\\mathbf{x})\\),\n\\[\nf(\\mathbf{x}) = \\sum_{m=1}^M \\frac{1}{M} f_m(\\mathbf{x}) \\,,\n\\]\nwhere the \\(f_m\\) are trained on a randomly selected subset of the total data. Necessarily, the \\(f_m\\) models will be highly correlated, so one can also train them on randomly selected subsets of features, in order to reduce this correlation. In the case of decision trees, the ensemble is (obviously) called a random forest.\n\n\nBoosting\nA final trick for decision trees, which is boosting. This is an important method that all high-performing implementations of random forests use, but it also relies on a lot of tedious math, so we will mostly gloss over it. The point is that we can create the ensemble iteratively by adding new trees one at a time, where each new addition tries to improve on the prediction by the existing trees.\nStarting with a single tree \\(f_1\\), its predictions might be good, but not perfect. So when evaluated on the training data \\(\\mathbf{x}\\), there will be a difference between the targets \\(\\mathbf{y}\\) and the predictions \\(f(\\mathbf{x})\\), and this difference we typically call residuals \\(r\\):\n\\[\n\\mathbf{r}_1 = \\mathbf{y} - f_1(\\mathbf{x})\n\\]\nor if we re-write:\n\\[\n\\mathbf{y} = f_1(\\mathbf{x}) + \\mathbf{r}_1\n\\]\nWe want \\(\\mathbf{r}\\) to be as small as possible, but with a single tree, there is only so much we can do. The magic is to train a second tree \\(f_2\\), not to again predict \\(y\\), but to predict the residual \\(r_1\\). Then we get\n\\[\n\\mathbf{y} = f_1(\\mathbf{x}) + f_2(\\mathbf{x}) + \\mathbf{r}_2\n\\]\nwith \\(r_2 &lt; r_1\\) (hopefully), resulting in an improved prediction. This stagewise additive modelling can be done until we see no further improvement, resulting in an emsemble model that typically outperforms a standard random forest approach. Several variants of the boosting algorithm exists, some of which are discussed in chapter 16.4 of the Murphy book. Here, the explanation of the different boosting variants are based on arguments about loss functions. We have not started looking into loss funtions yet, but this will be a topic in the next chapter. It can be useful to come back to the Murphy book after having gone through next chapter’s material."
  },
  {
    "objectID": "decision_trees.html#software-frameworks",
    "href": "decision_trees.html#software-frameworks",
    "title": "25  Decision trees",
    "section": "25.4 Software frameworks",
    "text": "25.4 Software frameworks\nWe are about to start the programming exercises, so let us briefly discuss our options for the implementation. While coding a decision tree algorithm from scratch is not too technically challenging, it is an ineffective way to learn aboubt the its behaviour in practice, and we also do not have the time to so. Hence we will use ready-made frameworks.\nFor decent-performing and easy-to-use frameworks, we suggest to use scikit-learn for Python, and tidymodels for R. There are, however, plenty of alternatives, and you are free to choose whichever you like.\nFor state-of-the-art boosted decision trees, we suggest XGBoost, which regularly ranks among the best tree-based algorithms on the machine learning competitions at Kaggle, and is available for both Python and R.\n\n\n\n\n\n\n Exercises:\n\n\n\nIn these exercises we will start out with simple 2-dimensional data, just so we can visualise what is going on, and then move to real-world dataset afterwards.\n\nClassification\nLet us try to classify samples from two different populations, which we simply call positive class and negative class. Follow the code examples in this notebook: decision_tree_examples.ipynb. The exercises themselves are listed in the notebook, but are included here too:\n\nRecreate the data with either very big separation between the classes, or very small, and observe how the decision surface changes.\nIn the documentation for DecisionTreeClassifier, lots of options are described. You’ll noticed that we have already specified a non-default criterion. Try changing the other parameters, and again observe how the results differ. In particular, try setting max_depth and min_samples_split to small or big values.\nGenerate some separate test data, and plot those too. Does the default decision tree parameters give good results on the test data? Can you find better parameters to improve the class prediction for this example?\nFinally, print out the splitting thresholds and the leaf contents for the entire tree. Does it match your expectation from looking at the decision boundary?\n\n\n\nRegression\nNow, we want to try out decision trees for predicting continuous target values. We will leave you alone from the start, and only give you the recipe for generating the 1-dimensional data we want to predict:\n\nrng = np.random.default_rng()   # If not done already\nX = np.sort(5 * rng.uniform(size=(80, 1)), axis=0)\nY = np.sin(X).ravel()\nY[::5] += 3 * (0.5 - rng.uniform(size=16))\n\nThe execise is similar to the classification one:\n\nUsing DecisionTreeRegressor, vary the values of its parameters and observe the result.\nGenerate test data following the prescription above, and find optimal parameters that account for the fact that our data is prone to outliers.\n\n\n\nReal-world dataset classification\nNow that we understand how the tree-based models work, it is time to use them on an actual, higher-dimensional dataset. We will use the Adult Income dataset, which you have met already, to predict the binary outcome of people making more than $50,000 a year, depending on their education, line of work, and so on.\nThe training data are here: https://github.com/pglpm/ADA511/raw/master/datasets/train-income_data_nominal_nomissing.csv\nAs measure of how good the model’s predictions are, use the ratio of correct predictions (correct predictions divided by the number of examples), which is also known as accuracy.\nOnce you have trained your model and computed the accuracy on training data, compute the accuracy also for test data: https://github.com/pglpm/ADA511/raw/master/datasets/test-income_data_nominal_nomissing.csv\nWhich performs better – the DecisionTreeClassifier, the RandomForestClassifier or the GradientBoostingClassifier?\nOptional: To optimise the performance as far as possible, try one of the “latest” tree-based algoritm implementations, such as XGBoost, LightGBM, or TensorFlow Decision Forests."
  },
  {
    "objectID": "neural_networks.html",
    "href": "neural_networks.html",
    "title": "26  Neural networks",
    "section": "",
    "text": "27 Preventing overfitting - regularisation\nIn the previous chapter, we had several ways of limiting decision trees so they would not overfit to the training data. Many apply also here, but having introduced the concept of loss functions, we will add a general approach.\nFirst, limiting complexity by tuning hyperparameters, is always an option. For a standard network, we can adjust the number of layers and the number of nodes per layer. Making good guesses about the architecture of the network is difficult, so a hyperparameter search if often necessary to develop a performant model.\nSecondly, similar to random forests, one can make ensembles of models, trained on subsets (with replacement) of the data. For neural networks, however, this is less common, since the network is sort of an ensemble by itself. But remember that for random forests we also randomly dropped some of the features during training. The equivalent for neural networks is to randomly drop nodes, by zeroing their output for a single training iteration. This is called dropout.\nLastly, let us introduce general regularisation, which is based on modifying the loss function. This means it can be applied to any machine learning algorithm that minimises loss, and is not specific to the algorithm itself. What we want to do is to add a term in the loss function that penalises large parameter values. This way we do not need to restrict the explicit complexity of the model by changing the structure, but we rather restrict the “volume” the parameters can span out. The minimisation problem in equation {26.1} then becomes\n\\[\n\\underset{\\mathbf{w}}{\\mathrm{arg\\,min}}\\, L(\\mathbf{w}) + \\alpha ||\\mathbf{w}||_p \\,,\n\\tag{27.1}\\]\nwhere the last part is the regularisation term, whose strength is controlled by a new hyperparameter, \\(\\alpha\\). There are different ways of quantifying the values of the parameters \\(\\mathbf{w}\\) – in the above equation we have written it generally as an \\(p\\)-norm, but to actually compute the value, we need to decide on which norm to use. The options are\nThe scikit-learn User Guide has a nice example showing the effect of \\(L^2\\) regularisation for varying regularisation strength:"
  },
  {
    "objectID": "neural_networks.html#training-neural-networks",
    "href": "neural_networks.html#training-neural-networks",
    "title": "26  Neural networks",
    "section": "26.1 Training neural networks",
    "text": "26.1 Training neural networks\nFinding the optimal values of the model’s parameters \\(\\mathbf{w}\\) is usually called to train the model. When we looked at polynomials in the machine learning introduction (Chapter 24) we did not talk about this yet, partly because polynomial models have a closed-form solution for the best parameters, meaning they can be computed directly. Since neural networks are nonlinear by design, we need a different approach, which it to start with random parameter values and iteratively try to improve them.\n\nLoss functions\nThe first step towards improving the parameters, is to define what improvement is. Ultimately, our goal is to make predictions about the data that equal the true values, which is to say, we want to minimise the difference between predictions \\(f(\\mathbf{x}, \\mathbf{w}\\) and true values \\(y\\). This difference can be formulated in several different ways, but in the case of regression, the most common is the sum of squared errors:\n\\[\nL(\\mathbf{w}) = \\sum_{\\text{data points } i} (f(\\mathbf{x}_i, \\mathbf{w}) - y_i)^2\n\\]\n\\(L\\) is called a loss function, alternatively a cost function or an error function. With this in place, the training process becomes a minimisation problem:\n\\[\n\\underset{\\mathbf{w}}{\\mathrm{arg\\,min}}\\, L(\\mathbf{w}) \\,,\n\\tag{26.1}\\]\nTo minimise the loss function we still need to apply it to some data \\(\\mathbf{x}\\), but we have not made this dependence explicit, since the data are “unchanged” throughout the minimisation process.\nAs for any bounded function, the minimum can be found either where the gradient \\(\\nabla L(\\mathbf{w})\\) is zero, or where it does not exist. Solving this analytically is usually impossible, so we resort to a numerical solution – iteratively taking steps in the direction of smaller loss. The crucial point in neural network training is that the loss function is differentiable with respect to the network parameters, meaning we can compute \\(\\nabla L(\\mathbf{w})\\) and take steps in the negative (downwards) direction:\n\\[\n    \\mathbf{w}^{n+1} = \\mathbf{w}^{n} - \\eta \\nabla L(\\mathbf{w}^{n}) \\,.\n\\tag{26.2}\\]\nThis is the method of gradient descent. Here we have introduced a new hyperparameter \\(\\eta\\) called the learning rate, which controls how large each step will be.\nThe process of actually adjusting the parameters in the correct direction is called backpropagation, and involves first computing the value of \\(f(\\mathbf{x}, \\mathbf{w})\\), and then stepping backwards through each layer of the network, recursively updating the parameters by using the derivative. This sounds very tedious, but can be done efficiently by automatic differentiation, i.e. letting a computer do it. Modern frameworks for neural network models require only to know the layout of the network, and will, as we shall see in the exercises, figure out the rest automatically.\n\n\nGradient descent\nWhile we did not get into it earlier, the concept of defining a loss function and doing gradient descent, is in fact how the majority of machine learning algorithms are trained. Even for decision trees, which had their dedicated algorith, we ended up with a loss minimisation task once we introduced boosting.\nStraight-forward gradient descent as shown in equation {26.2} can work fine for relatively simple models, but will stop at the first minimum it encounters. For a reasonably complicated network, the loss function landscape can be expected to have several local minima or saddle points, causing the method to get stuck in places with suboptimal parameters. Several improved algorithms aim to tackle this.\n\nStochastic gradient descent updates the parameters through equation {26.2} for only a subset of data at a time. This is more computationally efficient, and the stochastic element helps against getting stuck in a local minimum, since a local minimum for some subset of data might not be a minimum for a different subset.\nAdaptive gradient methods use different learning rates per parameter, which is updated for each iteration.\nMomentum methods remember the previous gradients and keeps moving in the same direction even through flat or uphill parts, like a massive rolling ball.\n\nAll of the above can be combined, and the most common method of doing so is the Adaptive Moment Estimation (Adam) algorithm."
  },
  {
    "objectID": "connection-4-ML.html#sec-ML-birds-eye",
    "href": "connection-4-ML.html#sec-ML-birds-eye",
    "title": "27  Beyond machine learning",
    "section": "27.1 Machine learning from a bird’s-eye view",
    "text": "27.1 Machine learning from a bird’s-eye view\nThe last few chapters gave a brief introduction to and overview of popular machine-learning methods, their terminology, and the points of view that they typically adopt. Now let’s try to look at them keeping in mind our main goal in this course: exploring new methods, understanding their foundations, and thinking out of the box.\nIn this and the next few chapters we shall focus on the question: to what purpose do we use machine-learning algorithms?. After answering this question, we shall try to achieve that purpose in the optimal way, according to what our fundamental theory tells us we should do, without considering “machine learning”. But we shall keep an eye on where our optimal solution seems to be similar or dissimilar to machine-learning methods.\nThen, in the last chapters, we shall examine where the optimal solution and machine-learning methods converge and diverge, try to understand what machine-learning methods do from the point of view of the optimal solution, and think of ways to improve them."
  },
  {
    "objectID": "connection-4-ML.html#sec-cat-problems",
    "href": "connection-4-ML.html#sec-cat-problems",
    "title": "27  Beyond machine learning",
    "section": "27.2 A task-oriented categorization of some machine-learning problems",
    "text": "27.2 A task-oriented categorization of some machine-learning problems\nFor our goal, the common machine-learning categorization and terminology discussed in chapter  24 are somewhat inadequate. Distinctions such as “supervised learning” vs “unsupervised learning” are of secondary importance to a data engineer (as opposed to a “data mechanic”) for several reasons:\n\n  They group together some types of tasks that are actually quite different from an inferential or decision-making viewpoint; and conversely they separate types of tasks that are quite similar.\n  They focus on procedures rather than on purposes.\n\nThe important questions for us, in fact, are: What do we wish to infer or choose? and From which kind of information? These questions define the problem we want to solve. The procedure may then be chosen depending on the theory, resources and technologies, other contingent factors, and so on.\nLet’s introduce a different categorization that tries to focus on the purpose or task, on the types of desired information and of available information, rather than on the procedure.\nThe categorization below, of the types of task that machine-learning algorithms try to solve, is informal. It only provides a starting point from which to examine a new task. Many tasks will fall in between categories: every data-engineering or data-science problem is unique.\n\n\nWe exclude from the start all tasks that require an agent to continuously and actively interact with its environment for acquiring information, making choices, getting feedback, and so on. Clearly these tasks are the domain of Decision Theory in its most complex form, with ramified decisions, strategies, and possibly the interaction with other decision-making agents. To explore and analyse this kind of tasks is beyond the purpose of this course.\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\n\nDecision Analysis\nChapters 16–18 in Artificial Intelligence\nGames and Decisions\n\n\n\n\n\nWe focus on tasks where multiple “instances” with similar characteristics are involved, and the agent has some task related to a “new instance”, possibly to be repeated an indefinite number of times. According to the conceptual framework developed in part Data II, we can view these “instances” as units of a practically infinite population. The “characteristics” that the agent guesses or observed are variates common to all these units.\n\n\nRemember that you can adopt any terminology you like. If you prefer “instance” and “characteristics” or some other words to “unit” and “variate”, then use them. What’s important is that you understand the ideas and methods behind these words\n\nNew unit: given vs generated\nA first distinction can be made between\n\n   tasks where an agent must itself generate a new unit\n   tasks where a new unit is given to an agent, who must guess some of its variates\n\nAn example of the first type of task is image generation: an algorithm is given a collection of images and is asked to generate a new image based on them.\nWe shall see that these two types of task are actually quite close to each other, from the point of view of Decision Theory and Probability Theory.\n\nThe terms “discriminative” and “generative” are sometimes associated in machine learning with the two types of task. This association, however, is quite loose, because some tasks typically called “generative” actually belong to the first type. We shall therefore avoid these or other terms. It’s enough to keep in mind the distinction between the two types of task above.\n\n\n\nGuessing variates: all or some\nFocusing on the second type of task (a new unit is given to the agent), we can further divide it into two subtypes:\n\n  the agent must guess all variates of the new unit\n  the agent must guess some variates of the new unit, but can observe other variates of the new unit\n\nAn example of the first subtype of task is the “urgent vs non-urgent” problem of §  17.3: having observed incoming patients, some of which where urgent and some non-urgent, the agent must guess whether the next incoming patient will be urgent or not. No other kinds of information (transport, patient characteristics, and so on) are available for any patient.\nWe shall call predictands1 the variates that the agent must guess in a new unit, and predictors those that the agent can observe.2 The first subtype above can be viewed as a special case of the second where all variates are predictands, and there are no predictors.1 literally “what has to be predicted2 In machine learning and other fields, the terms “dependent variable”, “class” or “label” (for nominal variates) are often used for “predictand”; and the terms “independent variable” or “features” are often used for “predictor”.\n\nThe terms “unsupervised learning” and “supervised learning” are sometimes associated in machine learning with these two subtypes of task. But also in this case the association is loose and can be misleading. “Clustering” tasks, for example, are usually called “unsupervised” but they are examples of the second subtype above, where the agent has some predictors.\n\n\n\nInformation available in previous units\nFinally we can further divide the second subtype above into two or three subsubtypes, depending on the information available to the agent about previous units:\n\n   all predictors and predictands of previous units are known to the agent\n   all predictors of previous units, but not the predictands, are known to the agent\n   all predictands of previous units, but not the predictors, are known to the agent\n\n\n\nAn example of the first subsubtype of task is image classification. The agent is for example given the following 128 × 128-pixel images and character-labels from the One Punch Man series:\n\nand is then given one new 128 × 128-pixel image:\n\n\n\n\n\nof which it must guess the character-label.\nIn the example just given, the image is the predictor, the character-label is the predictand.\n\n\nA slight modification of the example above gives us a task of the second subsubtype. A different agent is given the images above, but without labels:\n\nand must then guess some kind of “label” or “group” for the new image above – and possibly even for the images already given. The kind of “group” requested depends on the specific problem.\nIn the new example above, the image is still the predictor, and the label or group is the predictand.\n\nThe term “supervised learning” typically refer to the first subsubtype above.\nThe term “unsupervised learning” can refer to the second subsubtype, for instance in “clustering” tasks. In a clustering task, the agent tries to guess which group or “cluster” a unit belong to, given a collection of similar units, whose groups are not known either. The cluster effectively is the predictand variate. In some cases the agent may want to guess the cluster not only of a new unit, but also of all previous units.\nThe third subsubtype is very rarely considered in machine learning, yet it is not an unrealistic task.\n\nThe types, subtypes, subsubtypes above are obviously not mutually exclusive or comprehensive. We can easily imagine scenarios where an agent has some predictors & predictands available about some previous units, but only predictors or only predictands available for other previous units. This scenario falls in between the three subsubtypes above. In machine learning, hybrid situations like these are categorized as “missing data” or “imputation”."
  },
  {
    "objectID": "connection-4-ML.html#sec-categ-probtheory",
    "href": "connection-4-ML.html#sec-categ-probtheory",
    "title": "27  Beyond machine learning",
    "section": "27.3 Flexible categorization using probability theory",
    "text": "27.3 Flexible categorization using probability theory\nWe have been speaking about the agent’s “guessing” the values of some variates. Guessing means that there’s a state of uncertainty; the agent can’t just say something like “the value is \\(42\\)”. Uncertainty means that the most honest thing that the agent can do is to express degrees of belief about each of the possible values. Probability theory must enter the scene.\nBut it also turns out that the categorization above into subtypes and subsubtypes of task can actually be presented in a more straightforward and flexible way using probability-theory notation.\n\nNotation\nFirst let’s introduce some symbol conventions to be used in the next chapters. We shall denote with \\({\\color[RGB]{68,119,170}Z}\\) all variates that are of interest to the agent: those to be guessed and those that may be known. The variates to be guessed in a new unit (the predictands) will be collectively denoted with \\({\\color[RGB]{68,119,170}Y}\\). The variates that can be observed in a new unit (the predictors) will be collectively denoted with \\({\\color[RGB]{68,119,170}X}\\). Therefore we have \\({\\color[RGB]{68,119,170}Z}= ({\\color[RGB]{68,119,170}Y}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{68,119,170}X})\\). In cases where there are no predictors, \\({\\color[RGB]{68,119,170}X}\\) is empty and we have \\({\\color[RGB]{68,119,170}Z}= {\\color[RGB]{68,119,170}Y}\\).\n\\({\\color[RGB]{68,119,170}Z}_i\\), \\({\\color[RGB]{68,119,170}Y}_i\\), \\({\\color[RGB]{68,119,170}X}_i\\) denote all variates, the predictands, and the predictors for unit #\\(i\\).  As usual we number from \\(i=1\\) to \\(i=N\\) the units that serve for learning, and \\(i=N+1\\) is the new unit of interest to the agent.\nRecall (§  5.3) that in probability notation \\(\\mathrm{P}({\\color[RGB]{238,102,119}\\boldsymbol{\\dotsb}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}{\\color[RGB]{34,136,51}\\boldsymbol{\\dotsb}} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\), the proposal contains what the agent’s belief is about, and the conditional contains what’s supposed to be known to the agent, together with the background information \\(\\mathsfit{I}\\).\n\n\n\n\n  The agent must guess all variates of the new unit\nThis kinds of guess is represented by the probability distribution\n\\[\n\\mathrm{P}(\\color[RGB]{238,102,119}\nZ_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\nZ_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{1}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\]\nfor all possible values \\(\\color[RGB]{238,102,119}z\\) in the domain of \\({\\color[RGB]{68,119,170}Z}\\). The specific values \\(\\color[RGB]{34,136,51}z_N, \\dotsc, z_1\\) of the variate \\({\\color[RGB]{68,119,170}Z}\\) for the previous units are known to the agent.\n\n\n\n\n  The agent must guess some variates of the new unit, but can observe other variates of the new unit\nThis kind of guess is represented by a probability distribution having\n\\[\n\\mathrm{P}(\\color[RGB]{238,102,119}\nY_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\nX_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\n\\dotsb \\,\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\]\nfor all possible values \\(\\color[RGB]{238,102,119}y\\) in the domain of the predictands \\({\\color[RGB]{68,119,170}Y}\\). The value \\(\\color[RGB]{34,136,51}x\\) of the predictors \\({\\color[RGB]{68,119,170}X}\\) for the new unit is known to the agent.\nThe remaining information contained in the conditional depends on the subsubtype of task previously discussed:\n\n\n\n   All predictors and predictands of previous units are known to the agent\nThis corresponds to the probability distribution\n\\[\n\\mathrm{P}(\\color[RGB]{238,102,119}\nY_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\nX_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\nY_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\]\nfor all possible \\(\\color[RGB]{238,102,119}y\\). All information about predictands and predictors for previous units appear in the conditional.\nIn the example with image classification, a pictorial representation of this probability would be\n\nwhere \\({\\color[RGB]{238,102,119}y} \\in \\set{\\color[RGB]{238,102,119}{\\small\\verb;Saitama;}, {\\small\\verb;Fubuki;}, {\\small\\verb;Genos;}, {\\small\\verb;MetalBat;}, \\dotsc \\color[RGB]{0,0,0}}\\).\n\n\n\n\n   All predictors of previous units, but not their predictands, are known to the agent\nThis corresponds to the probability distribution\n\\[\n\\mathrm{P}(\\color[RGB]{238,102,119}\nY_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\nX_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\nX_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\]\nfor all possible \\(\\color[RGB]{238,102,119}y\\). All information about predictors for the previous units, but not that about their predictands, appear in the conditional.\n\n\n\n\n\nMore general and hybrid tasks\nConsider a task that doesn’t fit into any of the types discussed above: The agent wants to guess the predictands for a new unit, say #3, after observing that its predictors have value \\(\\color[RGB]{34,136,51}x\\). Of two previous units, the agent knows the predictor value \\(\\color[RGB]{34,136,51}x_1\\) of the first, and the predictand value \\(\\color[RGB]{34,136,51}y_2\\) of the second. This task is expressed by the probability\n\\[\n\\mathrm{P}(\\color[RGB]{238,102,119}\nY_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\nX_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\nY_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{2}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\]\n\n\n\n\n\n\n\n\n\n Exercises\n\n\n\n\nWrite down the general probability expression for the task of subsubtype “all predictands of previous units, but not their predictors, are known to the agent”.\nWhat kind of task does the following probability express?:\n\\[\n\\mathrm{P}(\\color[RGB]{238,102,119}\nY_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N+1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Y_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{2}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1}\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\nX_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N+1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{2}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\]\nWhat kind of task could it represent in machine-learning terminology?\n\n\n\n\n\n\n   Tasks where an agent must itself generate a new unit\nOur first categorical division mentioned the task of generating a new unit, given previous examples. In this kind of task there are possible alternatives that the agent could generate. How should one alternative be chosen? A moment’s thought shows that the probabilities for the alternatives should enter the choice.\nSuppose, as a very simple example, that a unit-generating agent has been shown, in an unsystematic order, 30 copies of the symbol  and 10 copies of the symbol , and is asked to generate a new symbol out of these examples. Intuitively we expect it to generate , but cannot and don’t want to exclude the possibility that it could generate . These two generation possibilities should simply have different probabilities and, in the long run, appear with different frequencies.\nAlso in this kind of task, therefore, we have the probability distribution\n\\[\n\\mathrm{P}(\\color[RGB]{238,102,119}\nZ_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\nZ_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{1}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\]\nthe difference from before is that the sentence \\(\\color[RGB]{238,102,119}Z_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z\\) represents not the hypothesis that a given new unit has value \\(\\color[RGB]{238,102,119}z\\), but the possibility of generating a new unit with that value. In other words, the symbol “\\(\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\)” here means “must be set to…” rather than “would be observed to be…”; remember the discussion and warnings in §  6.3?\n\n\nThe general conclusion is that\n\n\n\n\n\n\n\n\n\n\n\nprobability distribution such as those discussed above must intrinsically enter all types of machine-learning algorithms."
  },
  {
    "objectID": "connection-4-ML.html#sec-underlying-distribution",
    "href": "connection-4-ML.html#sec-underlying-distribution",
    "title": "27  Beyond machine learning",
    "section": "27.4 The underlying distribution",
    "text": "27.4 The underlying distribution\nA remarkable feature of all the probabilities discussed in the above task categorization, even of those for “hybrid” types of task, is that they can all be calculated from one and the same probability distribution. We briefly discussed and used this feature in chapter  17.\nA conditional probability such as \\(\\mathrm{P}({\\color[RGB]{238,102,119}\\boldsymbol{\\dotsb}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}{\\color[RGB]{34,136,51}\\boldsymbol{\\dotsb}} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\) can always be written, by the and-rule, as the ratio of two probabilities:\n\\[\n\\mathrm{P}({\\color[RGB]{238,102,119}\\boldsymbol{\\dotsb}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}{\\color[RGB]{34,136,51}\\boldsymbol{\\dotsb}} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n=\n\\frac{\n\\mathrm{P}({\\color[RGB]{238,102,119}\\boldsymbol{\\dotsb}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{34,136,51}\\boldsymbol{\\dotsb}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}{\n\\mathrm{P}({\\color[RGB]{34,136,51}\\boldsymbol{\\dotsb}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}\n\\]\nTherefore we have, for the probabilities of some of the tasks above,\n\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\color[RGB]{238,102,119}\nZ_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\nZ_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{1}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n=\n\\frac{\n\\mathrm{P}(\\color[RGB]{238,102,119}\nZ_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\color[RGB]{34,136,51}\nZ_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{1}\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}{\n\\mathrm{P}(\n\\color[RGB]{34,136,51}\nZ_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{1}\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}\n\\\\[2em]\n&\\mathrm{P}(\\color[RGB]{238,102,119}\nY_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\nX_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\nY_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\\\[2ex]\n&\\qquad{}=\n\\frac{\n\\mathrm{P}(\\color[RGB]{238,102,119}\nY_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\color[RGB]{34,136,51}\nX_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\nY_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}{\n\\mathrm{P}(\n\\color[RGB]{34,136,51}\nX_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\nY_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}\n\\\\[2em]\n&\\mathrm{P}(\\color[RGB]{238,102,119}\nY_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\nX_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\nX_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\\\[2ex]\n&\\qquad{}=\n\\frac{\n\\mathrm{P}(\\color[RGB]{238,102,119}\nY_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\color[RGB]{34,136,51}\nX_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\nX_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}{\n\\mathrm{P}(\n\\color[RGB]{34,136,51}\nX_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\nX_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}\n\\end{aligned}\n\\]\n\n\n\nWe also know the marginalization rule (chapter  16.1): any quantity \\(\\color[RGB]{204,187,68}A\\) with values \\(\\color[RGB]{204,187,68}a\\) can be introduced into the proposal of a probability via the or-rule:\n\\[\n\\mathrm{P}({\\color[RGB]{238,102,119}\\boldsymbol{\\dotsb}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{34,136,51}\\boldsymbol{\\dotsb}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) =\n\\sum_{\\color[RGB]{204,187,68}a}\\mathrm{P}({\\color[RGB]{204,187,68}A\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}a} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{238,102,119}\\boldsymbol{\\dotsb}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{34,136,51}\\boldsymbol{\\dotsb}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\]\nUsing the marginalization rule we find these final expressions for the probabilities of tasks of various types:\n\n\n\n\n\n\n\n\n\n\n\n\n Guess all variates:\n\n\\[\n\\mathrm{P}(\\color[RGB]{238,102,119}\nZ_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\nZ_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{1}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n=\n\\frac{\n\\mathrm{P}(\\color[RGB]{238,102,119}\nZ_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\color[RGB]{34,136,51}\nZ_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{1}\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}{\n\\sum_{\\color[RGB]{170,51,119}z}\n\\mathrm{P}(\n\\color[RGB]{238,102,119}\nZ_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{170,51,119}z}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\color[RGB]{34,136,51}\nZ_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{1}\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}\n\\]\n\n\n\n  All previous predictors and predictands known:\n\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\color[RGB]{238,102,119}\nY_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\nX_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\nY_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\\\[2ex]\n&\\qquad{}=\n\\frac{\n\\mathrm{P}(\\color[RGB]{238,102,119}\nY_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\color[RGB]{34,136,51}\nX_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\nY_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}{\n\\sum_{\\color[RGB]{170,51,119}y}\n\\mathrm{P}(\\color[RGB]{238,102,119}\nY_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{170,51,119}y}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\color[RGB]{34,136,51}\nX_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\nY_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}\n\\end{aligned}\n\\]\n\n\n\n  Previous predictors known, previous predictands unknown:\n\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\color[RGB]{238,102,119}\nY_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\nX_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\nX_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\\\[2ex]\n&\\qquad{}=\n\\frac{\n\\sum_{\\color[RGB]{204,187,68}y_{N}, \\dotsc, y_{1}}\n\\mathrm{P}(\\color[RGB]{238,102,119}\nY_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\color[RGB]{34,136,51}\nX_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\color[RGB]{0,0,0}\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\n\\color[RGB]{204,187,68}\nY_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\color[RGB]{34,136,51}\nX_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\color[RGB]{204,187,68}\nY_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\color[RGB]{34,136,51}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}{\n\\sum_{{\\color[RGB]{170,51,119}y}, \\color[RGB]{204,187,68}y_{N}, \\dotsc, y_{1}}\n\\mathrm{P}(\\color[RGB]{238,102,119}\nY_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{170,51,119}y}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\color[RGB]{34,136,51}\nX_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\color[RGB]{0,0,0}\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\n\\color[RGB]{204,187,68}\nY_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\color[RGB]{34,136,51}\nX_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\color[RGB]{204,187,68}\nY_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\color[RGB]{34,136,51}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}\n\\end{aligned}\n\\]\n\n\n\nAll these formulae, even for hybrid tasks, involve sums and ratios of only one distribution:\n\\[\n\\mathrm{P}(\\color[RGB]{68,119,170}\nY_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\]\nStop for a moment and contemplate some of the consequences of this remarkable fact:\n\nAn agent that can perform one of the tasks above also can, in principle, perform all other tasks\nThis is why a perfect agent, working with probability, in principle does not have to worry about “supervised”, “unsupervised”, “missing data”, “imputation”, and similar situations. This also shows what was briefly mentioned before: all these task typologies are much closer to one another than it might look like from the perspective of current machine-learning methods.\nThe probability distribution above encodes the agent’s background knowledge and assumptions; different agents differ only in the values of that distribution.\nIf two agents yield different probability values in the same task, with the same variates and same training data, the difference must come from the joint probability distribution above. And, since the data given to the agents are exactly the same, the difference is in the agents’ background information \\(\\mathsfit{I}\\)."
  },
  {
    "objectID": "connection-4-ML.html#the-omnipresence-of-decision-making-in-machine-learning",
    "href": "connection-4-ML.html#the-omnipresence-of-decision-making-in-machine-learning",
    "title": "27  Beyond machine learning",
    "section": "27.5 The omnipresence of decision-making in machine learning",
    "text": "27.5 The omnipresence of decision-making in machine learning\nMany machine-learning textbooks say that\n\nin supervised learning the algorithm learns a functional relationship between some kind of input and some kind of output\n\nSuch statement is misleading, because it seems to imply that a functional relationship actually exists from input to output, or from predictor to predictand – it’s only waiting to be discovered and “learned”. As we discussed in chapter  24, in many important problems and applications this is actually not true: there isn’t any functional relationship between input and output at all. Even if any possible “noise” were removed, there would still not be any functional relationship between the denoised, actual predictor and predictand (here is an example).33 This is one more reason why we use the more general terms “predictor” & “predictand”, rather that “input” & “output”\nIn many important problems there’s only a statistical relationship between predictands and predictors. In other words, whenever the agent saw a predictor value \\(\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\\), the predictand value turned out to be \\(\\color[RGB]{34,136,51}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y'\\) in a given number of units, but also \\(\\color[RGB]{34,136,51}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y''\\) in a given number of other units, and so on.\nThe lack of an actual predictor-predictand function has a very important consequence. If a machine-learning algorithm outputs just one predictand value, among the possible ones that are consistent with the predictor, then it means that the algorithm must choose one of the possibilities. How is this choice made? This is obviously a decision-making problem. As we know from chapters 2 and 3, the optimal choice is determined by Decision Theory. Its determination requires:\n\n  the probabilities of the possible predictand values\n  the utilities of the possible choices of values\n\nThis point of view is also valid when there’s only one possible value: in this case the decision-making problem becomes trivial (one choice only), but it’s still correctly handled by decision theory.\n\n\nA decision-making step may also appear in tasks where the agent must generate a new unit. In some cases it may happen that a candidate alternative for generation should be discouraged, penalized, or even forbidden, because of particular circumstances; even if it would have been a probable candidate, given previous examples. Therefore also in this type of task there is an interplay between probabilities and utilities."
  },
  {
    "objectID": "exchangeable_probabilities.html#sec-recap-before-exchang",
    "href": "exchangeable_probabilities.html#sec-recap-before-exchang",
    "title": "28  Exchangeable beliefs",
    "section": "28.1 Recap",
    "text": "28.1 Recap\nIn the chapters of part Inference I we had an overview of how an agent can draw inferences and make predictions of the most general kind, expressed by general sentences, using the four fundamental rules of inference.\nThen, in the chapters of part Inference II, we successively narrowed our focus on more and more specialized kinds of inference typical of engineering and data-science problems and machine-learning algorithms: first, inferences about measurements and observations; and then inferences about multiple instances of similar measurements and observations. For these purposes we introduced a specialized language about quantities and data types (chapters of part Data I), and “populations” of similar data (chapters of part Data II).\nThe idea was that that an agent can arrive at sharper degrees of belief – that is, learn – by using information about “similar instances”. We also found the main formula, derived from the four fundamental rules, that takes care of learning and prediction (also in both “supervised” and “unsupervised” machine learning):\n\n\\[\n\\begin{aligned}\n    &\\mathrm{P}\\bigl(\n    {\\color[RGB]{238,102,119}Y_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N+1}}\n    \\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}}\n    {\\color[RGB]{34,136,51}X_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N+1}}\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\n    \\color[RGB]{34,136,51}Y_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_N \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_N \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    Y_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_1 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_1\n    \\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\mathsfit{I}} \\bigr)\n    \\\\[2ex]\n    &\\qquad{}=\n    \\frac{\n        \\mathrm{P}\\bigl(\n    \\color[RGB]{238,102,119}Y_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N+1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\color[RGB]{34,136,51}X_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N+1}\n    \\color[RGB]{0,0,0}\n        \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\color[RGB]{34,136,51}Y_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_N \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_N\n    \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\color[RGB]{34,136,51}Y_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_1 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_1\n    \\color[RGB]{0,0,0}\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}} {\\mathsfit{I}} \\bigr)\n}{\n     \\sum_{\\color[RGB]{238,102,119}y} \\mathrm{P}\\bigl(\n    {\\color[RGB]{238,102,119}Y_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    {\\color[RGB]{34,136,51}X_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N+1}}\n        \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\color[RGB]{34,136,51}Y_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_N \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_N \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    Y_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_1 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_1\n    \\color[RGB]{0,0,0}\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}}  {\\mathsfit{I}} \\bigr)\n}\n\\end{aligned}\n\\]\n\nTo calculate this formula, an agent needs the joint probability distribution\n\\[\n        \\mathrm{P}\\bigl(\n    \\color[RGB]{68,119,170}Y_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N+1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    X_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N+1}\n    \\color[RGB]{0,0,0}\n        \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\color[RGB]{68,119,170}Y_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_N \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_N\n    \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    Y_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_1 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_1\n    \\color[RGB]{0,0,0}\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}} {\\mathsfit{I}} \\bigr)\n\\]\nfor all possible values of \\(x_1,y_1\\), \\(x_2, y_2\\), \\(\\dotsc\\), and for a possibly unlimited number \\(N+1\\) of instances.\n\n\nWe shall now narrow our focus further for one last time, on inferences where this probability distribution satisfies a property that greatly simplifies the calculations. Such special inferences are also typical of many machine-learning applications, including “supervised” and “unsupervised” learning."
  },
  {
    "objectID": "exchangeable_probabilities.html#sec-excheable-beliefs",
    "href": "exchangeable_probabilities.html#sec-excheable-beliefs",
    "title": "28  Exchangeable beliefs",
    "section": "28.2 States of knowledge with symmetries",
    "text": "28.2 States of knowledge with symmetries\nAn agent’s degrees of belief about a particular population may satisfy a special symmetry, usually called “exchangeability”. This symmetry can be understood from different points of view. We start from one of these viewpoints, and then make connections with alternative ones.\nTake again the two populations briefly mentioned in §  20.1\n\n\nStock exchange\n\nThe daily change in closing price of a stock during 1000 consecutive days. Each day the change can be positive or zero: \\({\\color[RGB]{34,136,51}{\\small\\verb;+;}}\\), or negative: \\({\\color[RGB]{238,102,119}{\\small\\verb;-;}}\\).\n\n\n\n\n\n\n\n\nMars prospecting\n\nA collection of 1000 similar-sized rocks gathered from a specific, large crater on Mars. Each rock either contains haematite: \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\), or it doesn’t: \\({\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\).\n\n\n\n\n\n\n\n\nSuppose that, in each of these populations, you (the agent) don’t know the variate value for unit #735, and for some reason would like to infer it. You are given the variate values for 100 other units, which you can use to improve your inference. Now consider this question:\n\nHow much does the relative order of the 100 known units and the unknown unit matter to you, for drawing your inference?\n\nWe know, from information theory, that it never hurts having extra information, such as the units’ order. But you probably judge the units’ order to be much more important for your inference in the stock-exchange case than in the Mars-prospecting one. In the stock-exchange case it would be more informative to have data from units temporally close to unit #735; for example units #635–#734, or #736–#835, or #685–#734 & #736–#785, or similar ranges. But in the Mars-prospecting case you might find it acceptable if the 100 known units were picked up in some unsystematic way from the catalogue of remaining 999 units. There are reasons, boiling down to physics, behind this kind of judgement.\nThe question above could also be replaced by others, slightly different but still connected to the same issue. For example:\n\nHow strongly would you like to be able to choose which 100 units you can have data from, in order to draw your inference?\n\nor\n\nHow much would you be upset if the original order of the population units were destroyed by accidental shuffling?\n\nor\n\nWould it be acceptable to you if only the frequencies of the values (\\(\\set{{\\color[RGB]{34,136,51}{\\small\\verb;+;}},{\\color[RGB]{238,102,119}{\\small\\verb;-;}}}\\) in one case, \\(\\set{{\\color[RGB]{102,204,238}{\\small\\verb;Y;}},{\\color[RGB]{204,187,68}{\\small\\verb;N;}}}\\) in the other) for the 100 known units were given to you?\n\n\n\n\n\n\n\n Exercise\n\n\n\n\nFind examples of populations where the units have some kind of ordering that you think would be very important for drawing inferences about some units, given other units. Examine why you judge such ordering to be important. (The ordering doesn’t need to be one-dimensional. For instance, the pixel intensities of an image also have a two-dimensional relative order or position: is that important if you want to draw inferences about the intensities of some pixels from those of other pixels?)\nFind examples of populations where any potential ordering of the units would not be very important for drawing inferences about some units, given other units. Or, put it otherwise, you wouldn’t be excessively upset or worried if such order were lost owing to accidental shuffling of the units.\n\n\n\nMany kinds of inference considered in data science and engineering, and all inferences done with “supervised” or “unsupervised” machine-learning algorithms, are examples where any ordering of the data used for learning is deemed irrelevant and is, in fact, often lost. This irrelevance is clear from the data-shuffling involved in many procedures that accompany these algorithms.\nWe shall thus restrict our attention to situations and kinds of background information where this judgement of irrelevance is considered appropriate. In reality this is not a black-or-white situation: it is possible that some kind of ordering information would improve our inferences; what we are assuming here is that this improvement is so small that it can be neglected altogether."
  },
  {
    "objectID": "exchangeable_probabilities.html#sec-exchaneable-distr",
    "href": "exchangeable_probabilities.html#sec-exchaneable-distr",
    "title": "28  Exchangeable beliefs",
    "section": "28.3 Exchangeable probability distributions",
    "text": "28.3 Exchangeable probability distributions\nLet’s take the Mars-prospecting problem as a concrete example. Denote by \\(H\\) the variate expressing haematite presence, with domain \\(\\set{{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}, {\\color[RGB]{204,187,68}{\\small\\verb;N;}}}\\).\nIf an agent’s background information or assumption \\(\\mathsfit{I}\\) says that the relative order of units – rocks in this case – is irrelevant for inferences about other units, then it means that a probability such as\n\\[\n\\mathrm{P}(R_{735}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\nR_{734}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{733}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{732}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{731}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{730}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathsfit{I})\n\\]\nmust be equal to the probability\n\\[\n\\mathrm{P}(R_{356}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\nR_{587}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{877}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{316}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{562}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{988}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathsfit{I})\n\\]\nand in fact to any probability like\n\\[\n\\mathrm{P}(R_{i}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\nR_{j}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{k}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{l}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{m}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{n}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathsfit{I})\n\\]\nwhere \\(i\\), \\(j\\), and so on are different but otherwise arbitrary indices.\nIn other words, the probability depends on whether we are inferring \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\) or \\({\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\), and on how many \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\) and \\({\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\) appear in the conditional; in the example above, three \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\) and two \\({\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\). This property should also apply if the agent makes inferences about more than one unit, conditional on any number of units. It can be proven that this property is equivalent, in our present example, to this general requirement:\n\nThe value of a joint probability such as\n\\[\\mathrm{P}(R_{\\scriptscriptstyle\\dotso}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\\n\\dotso\\ \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{\\scriptscriptstyle\\dotso}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\ \\dotso \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\mathsfit{I})\n\\]\ndepends only on the total number of \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\) values and total number of \\({\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\) values that appear in it, or, equivalently, on the absolute frequencies of the values that appear in it.\n\n\n\nLeaving the Mars-specific example and generalizing, we can define the following property, called exchangeability:\n\n\n\n\n\n\n\n\n\n\n\nA joint probability distribution is called exchangeable if the probabilities for any number of units depend only on the absolute frequencies of the values appearing in them.\n\n\n\nLet’s see a couple more examples.\n\n\nDon’t forget that\n\\(\\mathrm{P}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\)\nand\n\\(\\mathrm{P}(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\)\nmean exactly the same, because and (symbol “\\(,\\)”) is commutative!\n\nConsider an infinite population with variate \\(Y\\) having domain \\(\\set{{\\color[RGB]{238,102,119}{\\small\\verb;low;}},{\\color[RGB]{204,187,68}{\\small\\verb;medium;}},{\\color[RGB]{34,136,51}{\\small\\verb;high;}}}\\). If the background information \\(\\mathsfit{J}\\) guarantees exchangeability, then these three joint probabilities must have the same value:\n\\[\\begin{aligned}\n&\\quad\\mathrm{P}(\nY_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;high;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{4}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;low;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{5}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;medium;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{6}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;low;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{J}\n)\n\\\\[2ex]\n&=\\mathrm{P}(\nY_{6}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;medium;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{5}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;high;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;low;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{4}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;low;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{J}\n)\n\\\\[2ex]\n&=\\mathrm{P}(\nY_{283}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;high;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{91}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;low;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{72}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;medium;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{1838}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;low;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{J}\n)\n\\end{aligned}\n\\]\nbecause they all have one \\({\\color[RGB]{34,136,51}{\\small\\verb;high;}}\\), one \\({\\color[RGB]{204,187,68}{\\small\\verb;medium;}}\\), two \\({\\color[RGB]{238,102,119}{\\small\\verb;low;}}\\). The same is true for these two probabilities:\n\\[\\begin{aligned}\n&\\quad\\mathrm{P}(\nY_{99}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;medium;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;medium;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{3024}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;low;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{J}\n)\n\\\\[2ex]\n&=\\mathrm{P}(\nY_{26}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;medium;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{611}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;low;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{78}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;medium;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{J}\n)\n\\end{aligned}\n\\]\nbecause both have zero \\({\\color[RGB]{34,136,51}{\\small\\verb;high;}}\\), two \\({\\color[RGB]{204,187,68}{\\small\\verb;medium;}}\\), one \\({\\color[RGB]{238,102,119}{\\small\\verb;low;}}\\).\n\n\n\nConsider an infinite population with variates \\((U,V)\\) having joint domain \\(\\set{{\\color[RGB]{238,102,119}{\\small\\verb;fail;}},{\\color[RGB]{34,136,51}{\\small\\verb;pass;}}} \\times \\set{{\\color[RGB]{102,204,238}-1}, {\\color[RGB]{119,119,119}0}, {\\color[RGB]{204,187,68}1}}\\)  (six possible joint values). If the background information \\(\\mathsfit{K}\\) guarantees exchangeability, then these two joint probabilities must have the same value:\n\\[\\begin{aligned}\n&\\quad\\mathrm{P}(\nU_{14}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;pass;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}V_{14}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}1}\\ \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\\nU_{337}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;pass;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}V_{337}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}-1}\\ \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\\nU_{8}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;fail;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}V_{8}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{119,119,119}0}\\ \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\\nU_{43}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;fail;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}V_{43}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}-1}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{}\n\\\\\n&\\qquad\\qquad\\qquad\\quad\nU_{825}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;pass;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}V_{825}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}-1}\\ \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\\nU_{66}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;pass;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}V_{66}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}-1}\\ \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\\nU_{700}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;fail;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}V_{700}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{119,119,119}0}\n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n\\\\[3ex]\n&=\\mathrm{P}(\nU_{421}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;pass;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}V_{421}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}-1}\\ \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\\nU_{55}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;fail;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}V_{55}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{119,119,119}0}\\ \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\\nU_{43}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;pass;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}V_{43}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}1}\\ \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\\nU_{14}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;fail;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}V_{14}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}-1}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{}\n\\\\\n&\\qquad\\qquad\\qquad\\quad\nU_{928}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;pass;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}V_{928}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}-1}\\ \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\\nU_{700}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;fail;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}V_{700}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{119,119,119}0}\\ \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\\nU_{39}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;pass;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}V_{39}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}-1}\n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n\\end{aligned}\n\\]\nbecause both have: one \\(({\\color[RGB]{238,102,119}{\\small\\verb;fail;}},{\\color[RGB]{102,204,238}-1})\\),  two \\(({\\color[RGB]{238,102,119}{\\small\\verb;fail;}},{\\color[RGB]{119,119,119}0})\\),  zero \\(({\\color[RGB]{238,102,119}{\\small\\verb;fail;}},{\\color[RGB]{204,187,68}1})\\),  three \\(({\\color[RGB]{34,136,51}{\\small\\verb;pass;}},{\\color[RGB]{102,204,238}-1})\\),  zero \\(({\\color[RGB]{34,136,51}{\\small\\verb;pass;}},{\\color[RGB]{119,119,119}0})\\),  one \\(({\\color[RGB]{34,136,51}{\\small\\verb;pass;}},{\\color[RGB]{204,187,68}1})\\). From this example, note that it’s important to count the occurrences of the joint values, not of the values of the single variates independently.\n\n\n\n\n\n\n\n\n Exercise\n\n\n\n\nFirst let’s check that you haven’t forgotten the basics about connectives (§  6.4), Boolean algebra §  9.2), and the four fundamental rules of inference (§  8.4):\n\nHow much is  \\(\\mathrm{P}(Y_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;low;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} Y_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;low;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{J})\\) ?\nSimplify the probability\n\n\\[\\mathrm{P}(X_{9}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_{28}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_{28}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\]\nwhat are the absolute frequencies of the values \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\) and \\({\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\) among the units in the probability above?\n\n\n\n\nFor each collection of probabilities below (the sentences \\(\\mathsfit{I'}, \\mathsfit{I''}, \\mathsfit{J'}\\dotsc\\) indicate different states of knowledge), say whether they cannot come from an exchangeable probability distribution, or if they might (to guarantee exchangeability, one has to check an infinite number of inequalities, so we can’t be sure about it unless they give us a general formula for the joint probabilities):\n\n\\(\\begin{aligned}[c]  &\\mathrm{P}(C_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}-1 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I'}) = 31.6\\%  \\\\  &\\mathrm{P}(C_{7}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}-1\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I'}) = 24.8\\%  \\end{aligned}\\)\n\\(\\begin{aligned}[c]  &\\mathrm{P}(Z_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;off;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Z_{53}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;on;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I''}) = 9.7\\%  \\\\  &\\mathrm{P}(Z_{3904}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;on;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Z_{29}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;off;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I''}) = 9.7\\%  \\end{aligned}\\)\n\\(\\begin{aligned}[c]  &\\mathrm{P}(A_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}A_{87}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}A_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{J'}) = 6.2\\%  \\\\  &\\mathrm{P}(A_{99}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}A_{10}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}A_{13}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{J'}) = 8.9\\%  \\end{aligned}\\)\n\\(\\begin{aligned}[c]  &\\mathrm{P}(W_{4}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;-;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}W_{97}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;+;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}W_{300}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;-;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{J''}) = 6.2\\%  \\\\  &\\mathrm{P}(W_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;-;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}W_{86}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;-;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}W_{107}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;-;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{J''}) = 8.9\\%  \\end{aligned}\\)\n\\(\\begin{aligned}[c]  &\\mathrm{P}(B_{1190}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;+;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}B_{1152}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;-;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}B_{233}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;-;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K'}) = 7.5\\%  \\\\  &\\mathrm{P}(B_{1185}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;+;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}B_{424}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;-;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}B_{424}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;-;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K'}) = 12.3\\%  \\end{aligned}\\)\n\\(\\begin{aligned}[c]  &\\mathrm{P}(S_{21}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;+;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_{21}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;low;}}\\ \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\  S_{33}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;-;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_{33}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;high;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K''}) = 5.0\\%  \\\\  &\\mathrm{P}(S_{5}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;-;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_{5}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;low;}}\\ \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\  S_{102}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;+;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_{102}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;high;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K''}) = 2.9\\%  \\end{aligned}\\)\n\n\n\n\n\nFurther constraints\nThe exchangeability property greatly reduces the number of probabilities that an agent needs to specify. For a population with a binary variate, a joint probability distribution for 1000 units would require the specification of around \\(2^{1000} \\approx 10^{300}\\) probabilities. But if this distribution is exchangeable, only \\(1000\\) probabilities need to be specified (the absolute frequency of one of the two values, ranging between 0 and 1000; minus one because of normalization).11 For the general case of a variate with \\(n\\) values, and \\(k\\) units, the number of independent probabilities is \\(\\binom{n+k-1}{k}\\).\nMoreover, the exchangeable joint distributions for different numbers of units satisfy additional restrictions, owing to the fact that each of them is the marginal distribution of all distributions with a larger number of units. In the Mars-prospecting case, for instance, if \\(\\mathrm{P}(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\\) is the degree of belief that rock #1 contains haematite, we must also have\n\\[\n\\begin{aligned}\n\\mathrm{P}(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\n&=\\mathrm{P}(R_{a}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\n\\\\\n&= \\mathrm{P}(R_{a}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_{b}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) + \\mathrm{P}(R_{a}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_{b}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\end{aligned}\n\\]\nfor any two different units #\\(a\\) and #\\(b\\). Therefore, if the agent has specified \\(\\mathrm{P}(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\\), then three of these four probabilities\n\\[\n\\begin{gathered}\n\\mathrm{P}(R_{a}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_{b}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\qquad\n\\mathrm{P}(R_{a}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_{b}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\\\[1ex]\n\\mathrm{P}(R_{a}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_{b}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\qquad\n\\mathrm{P}(R_{a}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_{b}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\end{gathered}\n\\]\nare completely determined if we specify just one of them.\n\n\n\n\n\n\n Exercise\n\n\n\nAssume that the state of knowledge \\(\\mathsfit{I}\\) implies exchangeability, and a population has binary variate \\(R\\in \\set{{\\color[RGB]{204,187,68}{\\small\\verb;N;}},{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}}\\).\nIf\n\\[\\mathrm{P}(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}) = 0.75 \\qquad \\mathrm{P}(R_{4}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_{9}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}) = 0.60\\]\nThen how much are the probabilities\n\\[\\mathrm{P}(R_{15}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}) = \\mathord{?} \\qquad\n\\mathrm{P}(R_{7}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_{11}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}) = \\mathord{?}\\]"
  },
  {
    "objectID": "inference_from_freqs.html#sec-pop-freq-known",
    "href": "inference_from_freqs.html#sec-pop-freq-known",
    "title": "29  Inferences from frequencies",
    "section": "29.1 If the population frequencies were known",
    "text": "29.1 If the population frequencies were known\nLet’s now see how the exchangeability of an agent’s degrees of belief allows it to calculate probabilities about the units of a population. We shall do this calculation in two steps. First, in the case where the agent knows the joint frequency distribution (§§ 21.2, 21.3, 23.2) for the full population. Second, in the more general case where the agent lacks this population-frequency information.\nWhen the full-population frequency distribution is known, the calculation of probabilities is very intuitive and analogous to the stereotypical “drawing balls from an urn”. We shall rely on this intuition; keep in mind, however, that the probabilities are not assigned “by intuition”, but actually fully determined by the two basic pieces of knowledge or assumptions: exchangeability and known population frequencies. Some simple proof sketches of this will also be given."
  },
  {
    "objectID": "inference_from_freqs.html#sec-pop-inference-notation",
    "href": "inference_from_freqs.html#sec-pop-inference-notation",
    "title": "29  Inferences from frequencies",
    "section": "29.2 Notation",
    "text": "29.2 Notation\nIn this and the following chapters we shall stick to a particular use of some symbols, and shall often take the simplified income dataset (file income_data_nominal_nomissing.csv and its underlying population as an example. This population has nine nominal variates. The variates, their domain sizes, and their possible values are listed at this link.\nWe consider an infinite population with any number of variates. For concreteness we assume these variates to have finite, discrete domains; but the formulae we obtain can be easily generalized to other kinds of variates. We shall denote all the population variates, jointly, with \\({\\color[RGB]{68,119,170}Z}\\). In the case of the income dataset, for instance, the variate \\({\\color[RGB]{68,119,170}Z}\\) stands for the joint variate with nine components:\n\\[\n\\begin{aligned}\n{\\color[RGB]{68,119,170}Z}&\\coloneqq(\\color[RGB]{68,119,170}\n\\mathit{workclass} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{education} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{marital\\_status} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{occupation} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n{}\\\\ &\\qquad\n\\color[RGB]{68,119,170}\\mathit{relationship} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{race} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{sex} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{native\\_country} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{income}\n\\color[RGB]{0,0,0})\n\\end{aligned}\n\\]\nWhen we write \\(\\color[RGB]{68,119,170}Z \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z\\), the symbol \\(\\color[RGB]{68,119,170}z\\) stands for some definite joint values, for instance \\(({\\color[RGB]{68,119,170}{\\small\\verb;Without-pay;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\small\\verb;Doctorate;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\small\\verb;Ireland;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\small\\verb;&gt;50K;}})\\).\nIn applications typical of “supervised learning” (§  19), the agent wants to draw inferences about some of the \\({\\color[RGB]{68,119,170}Z}\\) variates for a new unit, conditional on the values of the remaining variates. We call the joint variates to be inferred the predictand1, and we shall usually denote them jointly by the symbol \\({\\color[RGB]{68,119,170}Y}\\). We call the variates used to inform the inference the predictor2, and we shall usually denote them jointly by the symbol \\({\\color[RGB]{68,119,170}X}\\).1 literally “what has to be predicted”. In machine learning and other fields the terms “dependent variable”, “class” or “label” (for nominal variates) are often used.2 In machine learning and other fields the terms “independent variable” or “features” are often used\nIn the income problem, for instance, the agent (some USA census agency) would like to infer the \\(\\color[RGB]{68,119,170}\\mathit{income}\\) variate of a person from the other eight demographic characteristics \\(\\color[RGB]{68,119,170}\\mathit{workclass} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathit{education} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb\\) of that person. So in this inference problem we define\n\\[\n\\begin{aligned}\n{\\color[RGB]{68,119,170}Y}&\\coloneqq{\\color[RGB]{68,119,170}\\mathit{income}}\n\\\\[1ex]\n{\\color[RGB]{68,119,170}X}&\\coloneqq({\\color[RGB]{68,119,170}\\mathit{workclass} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathit{education} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathit{sex} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathit{native\\_country}})\n\\end{aligned}\n\\]\nWe shall, however, also consider slightly different inference problems, for example with \\(({\\color[RGB]{68,119,170}\\mathit{race} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathit{sex}})\\) as predictand and the remaining seven variates \\(({\\color[RGB]{68,119,170}\\mathit{workclass} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathit{income}})\\) as predictors."
  },
  {
    "objectID": "inference_from_freqs.html#sec-know-freq",
    "href": "inference_from_freqs.html#sec-know-freq",
    "title": "29  Inferences from frequencies",
    "section": "29.3 Knowing the full-population frequency distribution",
    "text": "29.3 Knowing the full-population frequency distribution\nNow suppose that the agent knows the full-population joint frequency distribution. Let’s make clearer what this means. In the income problem, for instance, consider these two different joint values for the joint variate \\({\\color[RGB]{68,119,170}Z}\\):\n\\[\n\\begin{aligned}\n{\\color[RGB]{68,119,170}z^{*}}&\\coloneqq(\n{\\small\\verb;Private;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\small\\verb;HS-grad;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\small\\verb;Married-civ-spouse;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\small\\verb;Machine-op-inspct;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{}\n\\\\\n&\\qquad{\\small\\verb;Husband;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\small\\verb;White;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\small\\verb;Male;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\small\\verb;United-States;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\small\\verb;&lt;=50K;}\n)\n\\\\[2ex]\n{\\color[RGB]{68,119,170}z^{**}}&\\coloneqq(\n{\\small\\verb;Self-emp-not-inc;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\small\\verb;HS-grad;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\small\\verb;Married-civ-spouse;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{}\n\\\\\n&\\qquad {\\small\\verb;Farming-fishing;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\small\\verb;Husband;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\small\\verb;White;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\small\\verb;Male;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\small\\verb;United-States;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\small\\verb;&lt;=50K;}\n)\n\\end{aligned}\n\\]\nThe agent knows that the value \\(\\color[RGB]{68,119,170}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z^{*}\\) occurs in the full population of interest (in this case all 340 millions or so USA citizens, considered in a short period of time) with a relative frequency \\(0.860 369\\%\\); it also knows that the value \\(\\color[RGB]{68,119,170}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z^{**}\\) occurs with a relative frequency \\(0.260 058\\%\\). We write this as follows:\n\\[\nf({\\color[RGB]{68,119,170}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z^{*}}) = 0.860 369\\% \\ ,\n\\qquad\nf({\\color[RGB]{68,119,170}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z^{**}}) = 0.260 058\\%\n\\]\nThe agent knows not only the frequencies of the two particular joint values \\(\\color[RGB]{68,119,170}z^{*}\\), \\(\\color[RGB]{68,119,170}z^{**}\\), but for all possible joint values, that is, for all possible combinations of values from the single variate. In the income example there are 54 001 920 possible combinations, and therefore just as many relative frequencies. All these frequencies together form the full-population frequency distribution for \\({\\color[RGB]{68,119,170}Z}\\), which we denote collectively with \\(\\boldsymbol{f}\\) (note the boldface). Let’s introduce the quantity \\(F\\), denoting the full-population frequency distribution. Knowledge that the frequencies are \\(\\boldsymbol{f}\\) is then expressed by the sentence \\(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\).\n\n\n\n\n\n\n Don’t confuse the full population with a sample from it\n\n\n\nNote that the frequencies reported above are not the ones found in the income_data_nominal_nomissing.csv dataset, because that dataset is only a sample from the full population, not the full population. The frequency values reported above are purely hypothetical (but not inconsistent with the frequencies observed in the sample).\n\n\nIn other cases, these hypothetically known frequencies would refer to the full population of units: maybe even past, present, future, if they span a possibly unlimited time range."
  },
  {
    "objectID": "inference_from_freqs.html#sec-1unit-freq-known",
    "href": "inference_from_freqs.html#sec-1unit-freq-known",
    "title": "29  Inferences from frequencies",
    "section": "29.4 Inference about a single unit",
    "text": "29.4 Inference about a single unit\nNow imagine that the agent, given the information \\(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\) about the frequencies and some background information \\(\\mathsfit{I}\\), must infer all \\({\\color[RGB]{68,119,170}Z}\\) variates for a specific unit \\(u\\). In the income case, it would be an inference about a specific USA citizen. This unit \\(u\\) could have any particular combination of variate values; in the income case it could have any one of the 54 001 920 possible combined values. The agent must assign a probability to each of these possibilities.3 Which probability values should it assign?3 Remember that this is what we mean when we say “drawing an inference”! (See chap.  5 and §  14.1)\nIntuitively we would say that the probability for a particular value \\(\\color[RGB]{68,119,170}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z\\) should be equal to the frequency of that value in the full population:\n\n\n\n\n\n\n\n\n\n\n\nif \\(\\mathsfit{I}\\) leads to an exchangeable probability distribution, then\n\\[\n\\mathrm{P}({\\color[RGB]{68,119,170}Z}_{u}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}) = f({\\color[RGB]{68,119,170}Z}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z})\n\\]\nfor any unit \\(u\\).\n\n\n\nFor instance, the probabilities that unit \\(u\\) has the values \\(\\color[RGB]{68,119,170}z^{*}\\) or \\(\\color[RGB]{68,119,170}z^{**}\\) above is\n\\[\n\\begin{aligned}\n&\\mathrm{P}({\\color[RGB]{68,119,170}Z}_{u}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z^{*}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}) =\nf({\\color[RGB]{68,119,170}Z}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z^{*}}) = 0.860 369\\%\n\\\\[1ex]\n&\\mathrm{P}({\\color[RGB]{68,119,170}Z}_{u}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z^{**}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}) =\nf({\\color[RGB]{68,119,170}Z}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z^{**}}) = 0.260 058\\%\n\\end{aligned}\n\\]\nThis intuition is the same as in drawing balls, which may have different sets of labels, from a collection, given that we know the proportion of balls with each possible label set.\nBut the equality above can actually be proven mathematically in this specific case: it follows from the assumption of exchangeability. Let’s examine a very simple case to get an idea of how this proof works.\n\nExact calculation of the probabilities in a simple case\nSuppose we have three rocks from our Mars-prospecting collection. They are marked #1, #2, #3. They look alike, but we know that two of them have haematite, so \\(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\) for them, and one doesn’t, so \\(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\) for that rock. This background information – let’s call it \\(\\mathsfit{K}_{\\textsf{3}}\\) – is a simple case of a finite population with three units and a binary variate \\(R\\). We know that the frequency distribution for this population is\n\n\n\n\\[f(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}) = 2/3 \\qquad f(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}) = 1/3\\]\nOur information \\(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\) about the frequencies corresponds to the following composite sentence:\n\n\\[\nF\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\ \\Longleftrightarrow\\\n(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}})\n\\lor\n(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}})\n\\lor\n(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}})\n\\]\n\nGiven \\(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\), we know that \\(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\) is true: \\(\\mathrm{P}( F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}})=1\\), which means\n\n\\[\n\\mathrm{P}\\bigl[(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}})\n\\lor\n(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}})\n\\lor\n(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}})\n\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}, \\mathsfit{K}_{\\textsf{3}}\\bigr] = 1\n\\]\n\nNow use the or-rule, considering that the three ored sentences are mutually exclusive:\n\n\\[\n\\begin{aligned}\n1&=\\mathrm{P}\\bigl[(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}})\n\\lor\n(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}})\n\\lor\n(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}})\n\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}}\\bigr]\n\\\\[2ex]\n&=\n\\mathrm{P}(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}})\n+{}\n\\\\&\\qquad\n\\mathrm{P}(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}})\n+{}\n\\\\&\\qquad\n\\mathrm{P}(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}})\n\\end{aligned}\n\\]\n\nAccording to our background information \\(\\mathsfit{K}_{\\textsf{3}}\\), our degrees of belief are exchangeable. This means that the three probabilities summed up above must all have the same value, because in each of them \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\) appears twice and \\({\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\) once. But if we are summing the same value thrice, and the sum is \\(1\\), that that value must be \\(1/3\\). Hence we find that\n\\[\n\\begin{aligned}\n&\\mathrm{P}(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}})\n= 1/3\n\\\\\n&\\mathrm{P}(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}})\n= 1/3\n\\\\\n&\\mathrm{P}(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}})\n= 1/3\n\\\\[1ex]\n&\\text{all other probabilities are zero}\n\\end{aligned}\n\\]\nNow let’s find the probability that a rock, say #1, has haematite (\\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\)), given that we haven’t observed any other rocks: \\(\\mathrm{P}(R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}})\\). This is a marginal probability (§  16.1), so it’s given by the sum\n\\[\n\\begin{aligned}\n\\mathrm{P}(R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}}) &=\n\\sum_{i={\\color[RGB]{102,204,238}{\\small\\verb;Y;}}}^{{\\color[RGB]{204,187,68}{\\small\\verb;N;}}}\\sum_{j={\\color[RGB]{102,204,238}{\\small\\verb;Y;}}}^{{\\color[RGB]{204,187,68}{\\small\\verb;N;}}}\n\\mathrm{P}(R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}i \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}j \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}})\n\\\\[1ex]\n&=\n\\mathrm{P}(R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}}) + {}\n\\\\ &\\qquad\n\\mathrm{P}(R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}}) + {}\n\\\\ &\\qquad\n\\mathrm{P}(R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}}) + {}\n\\\\ &\\qquad\n\\mathrm{P}(R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}})\n\\\\[1ex]\n&= 0 + 1/3 + 1/3 + 0\n\\\\[1ex]\n&= 2/3\n\\end{aligned}\n\\]\nwhich is indeed equal to \\(f(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}})\\).\n\n\nThis simple example gives you an idea why our intuition for equating – in specific circumstances – probability with full-population frequencies, is actually a mathematical theorem: it follows from (1) knowledge of the full-population frequencies, and (2) exchangeability.\n\n\n\n\n\n\n Exercises\n\n\n\n\nCalculate \\(\\mathrm{P}(R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}})\\), that is, the probability that rock #2 has haematite, given that we don’t know the haematite content of any other rock. Is it different from \\(\\mathrm{P}(R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}})\\), or not? Why?\nBuild a similar proof for a slightly different case; for example four rocks; or two units from a population with a variate having three possible values (instead of just the two \\(\\set{{\\color[RGB]{102,204,238}{\\small\\verb;Y;}},{\\color[RGB]{204,187,68}{\\small\\verb;N;}}}\\)).\nConsider the same calculation we did above, but in the case of background knowledge \\(\\mathsfit{K}_{\\text{NE}}\\) where our degrees of belief are \\(\\text{N}\\)ot \\(\\text{E}\\)xchangeable. For instance, give three different values to the probabilities\n\\[\n\\begin{gathered}\n\\mathrm{P}(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\text{NE}})\n\\\\\n\\mathrm{P}(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\text{NE}})\n\\\\\n\\mathrm{P}(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\text{NE}})\n\\end{gathered}\n\\]\nin such a way that they still sum up to \\(1\\). Then find by marginalization the probability that rock #1 contains haematite (\\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\)). Is this probability still equal to the relative frequency of \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\)?"
  },
  {
    "objectID": "inference_from_freqs.html#sec-moreunit-freq-known",
    "href": "inference_from_freqs.html#sec-moreunit-freq-known",
    "title": "29  Inferences from frequencies",
    "section": "29.5 Inference about several units",
    "text": "29.5 Inference about several units\nLet’s continue with the Mars-prospecting example of the previous section, with just three rocks. We found that the probability that rock #1 has haematite (\\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\)) was \\(2/3\\), given that we haven’t observed any other rocks. This probability was equal to the frequency of \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\)-rocks in the urn.\nNow suppose that we observe rock #2, and it turns out to have haematite (\\(R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\)). What is the probability that rock #1 has haematite?\nThe probability we are asking about is \\(\\mathrm{P}(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}})\\), and it can be calculated with the usual rules. The result is again the same as the frequency of the \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\)-rocks, but with respect to the new situation: there are now two rocks left in front of us, and one must contain haematite, while the other doesn’t. The probability is therefore \\(1/2\\), a value different from that we found before, \\(2/3\\):\n\\[\n\\mathrm{P}(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}}) = 2/3\n\\qquad\n\\mathrm{P}(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}}) = 1/2\n\\]\nThis situation is quite general: in a collection of many rocks, the probabilities for new observations change accordingly to information about previous observations (and also subsequent ones, if already known).\n\n\nBut consider now the case \\(\\mathsfit{K}\\) of a large collection of 3 000 000 rocks, 2 000 000 of which have haematite and the rest doesn’t.4 The population’s relative frequencies are exactly as in the case with three rocks, and for the probability that rock #1 contains haematite we still have4 Note how this scenario becomes very similar to that of coin tosses.\n\\[\n\\mathrm{P}(R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) =\nf(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}) = \\frac{2 000 000}{3 000 000} = 2/3\n\\]\nNow suppose we examine rock #2 and find haematite. What is the probability that rock #1 also contains haematite? In this case we find\n\\[\n\\mathrm{P}(R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) =\n\\frac{1 999 999}{2 999 999} \\approx 2/3\n\\]\nwith an absolute error of only \\(0.000 000 1\\). That is, the probability and frequency are almost the same as before examining rock #2. The reason is clear: the number of rocks is so large that observing some of them doesn’t practically change the content and proportions of the whole collection.\nThe joint probability that rock #2 contains haematite and rock #1 doesn’t is therefore, by the and-rule,\n\n\\[\n\\begin{aligned}\n\\mathrm{P}(R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) &=\n\\mathrm{P}(R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) \\cdot\n\\mathrm{P}(R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\[1ex]\n&\\approx f(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}) \\cdot f(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}})\n\\end{aligned}\n\\]\n\nthe approximation being the better, the larger the collection of rocks.\nIt is easy to see that this will hold for more observations, and for different and more complex variate domains, as long as the number of units considered is enough small compared with the population size. For instance\n\n\\[\n\\mathrm{P}(R_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\approx\nf(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}) \\cdot f(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}) \\cdot f(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}) \\cdot f(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}})\n\\]\n\nwhere \\(\\boldsymbol{f}\\) is the initial frequency distribution for the population.\n\n\nThis situation applies to more general populations: if the full-population frequencies are known, the agent’s beliefs are exchangeable, and the population is practically infinite, then the joint probability that some units have a particular set of values is equal to the product of the frequencies of those values.\n\n\n\n\n\n\n\n\n\n\n\n\nIf an agent:\n\nhas background information \\(\\mathsfit{I}\\) about a population saying that\n\nbeliefs about units are exchangeable\nthe population size is practically infinite\n\nhas full information \\(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\) about the population frequencies \\(\\boldsymbol{f}\\) for the variate \\({\\color[RGB]{68,119,170}Z}\\)\n\nthen\n\\[\n\\mathrm{P}(\n{\\color[RGB]{68,119,170}Z}_{u'}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z'} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n{\\color[RGB]{68,119,170}Z}_{u''}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z''} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n{\\color[RGB]{68,119,170}Z}_{u'''}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z'''} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\dotsb\n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\approx\nf({\\color[RGB]{68,119,170}Z}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z'}) \\cdot\nf({\\color[RGB]{68,119,170}Z}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z''}) \\cdot\nf({\\color[RGB]{68,119,170}Z}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z'''}) \\cdot\n\\dotsb\n\\]\nfor any (different) units \\(u', u'', u''', \\dotsc\\) and any (even equal) values \\(\\color[RGB]{68,119,170}z', z'', z''', \\dotsc\\).\n\n\n\n\nThe formula above solves our initial problem: How to calculate and encode the joint probability distribution for the full population?, although it does so only in the case where the full-population frequencies \\(\\boldsymbol{f}\\) are known. In this case this probability is encoded in the \\(\\boldsymbol{f}\\) itself (which can be represented as a multidimensional array), and can be calculated for any desired joint probability distribution just by a multiplication.\nIn the income example from §  29.3, the probability that two units (citizens) #\\(a\\), #\\(c\\) have value \\(\\color[RGB]{68,119,170}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z^{**}\\) and one unit #\\(b\\) has value \\(\\color[RGB]{68,119,170}Z \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z^{*}\\) is\n\n\\[\n\\begin{aligned}\n\\mathrm{P}(\n{\\color[RGB]{68,119,170}Z}_{a}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z^{**}} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n{\\color[RGB]{68,119,170}Z}_{b}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z^{*}} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n{\\color[RGB]{68,119,170}Z}_{c}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z^{**}}\n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n&\\approx\nf({\\color[RGB]{68,119,170}Z}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z^{**}}) \\cdot\nf({\\color[RGB]{68,119,170}Z}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z^{*}}) \\cdot\nf({\\color[RGB]{68,119,170}Z}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z^{**}})\n\\\\[1ex]\n&=\n0.260 058\\% \\cdot\n0.860 369\\% \\cdot\n0.260 058\\%\n\\\\\n&= 0.000 005 818 7\\%\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n Always check whether the approximate formula above is appropriate\n\n\n\nAs we have seen, the product formula above is strictly speaking only approximate. In situations where the full population has practically infinite size compared to (1) the number of units that the agent uses for learning, and (2) the number of units the agent will draw inferences about, then the formula can be used as if it were exact.\nBut how much is “practically infinite”? No general answer is possible: it depends on the precision required in the specific problem. In some problems, even if learning and inference involve 10% of the units from the full population, the approximation might still be acceptable; but in other problems it might not be. If learning and inference involve 50% or more units from the full population, then the formula above is hardly acceptable.\nThe probability calculus and the four fundamental rules allow us to handle problems with any population size exactly (see the Study reading below), but the exact computation becomes involved and expensive. This is why the approximate product formula above is valuable, when it can be properly used."
  },
  {
    "objectID": "inference_from_freqs.html#sec-no-learn-freqs",
    "href": "inference_from_freqs.html#sec-no-learn-freqs",
    "title": "29  Inferences from frequencies",
    "section": "29.6 No learning when full-population frequencies are known",
    "text": "29.6 No learning when full-population frequencies are known\nImagine an agent with exchangeable beliefs \\(\\mathsfit{I}\\) and knowledge \\(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\) of the full-population frequencies, who also has observed several units with values (possibly some identical) \\(\\color[RGB]{68,119,170}Z_{u'}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z' \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Z_{u''}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z'' \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Z_{u'''}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z''' \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb\\). What is this agent’s degree of belief that a new unit #\\(u\\) has value \\(\\color[RGB]{68,119,170}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z\\)?\nFrom our basic formula for this question,\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\color[RGB]{238,102,119}\nZ_{u}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\color[RGB]{34,136,51}\nZ_{u'}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z' \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_{u''}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z'' \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_{u'''}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z''' \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\dotsb \\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\\\[2ex]\n&\\qquad{}=\\frac{\n\\mathrm{P}(\\color[RGB]{238,102,119}\nZ_{u}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\color[RGB]{34,136,51}Z_{u'}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z' \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_{u''}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z'' \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_{u'''}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z''' \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\dotsb \\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n}{\n\\sum_{\\color[RGB]{170,51,119}z}\n\\mathrm{P}(\n\\color[RGB]{238,102,119}Z_{u}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\color[RGB]{170,51,119}z\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\color[RGB]{34,136,51}Z_{u'}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z' \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_{u''}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z'' \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_{u'''}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z''' \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\dotsb \\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n}\n\\\\[2ex]\n&\\qquad{}\\approx\\frac{\nf({\\color[RGB]{238,102,119}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z}) \\cdot\nf({\\color[RGB]{68,119,170}{\\color[RGB]{34,136,51}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z'}}) \\cdot\nf({\\color[RGB]{34,136,51}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z''}) \\cdot\nf({\\color[RGB]{34,136,51}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z'''}) \\cdot\n\\dotsb\n}{\n\\sum_{\\color[RGB]{170,51,119}z}\nf({\\color[RGB]{238,102,119}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\color[RGB]{170,51,119}z}) \\cdot\nf({\\color[RGB]{34,136,51}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z'}) \\cdot\nf({\\color[RGB]{34,136,51}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z''}) \\cdot\nf({\\color[RGB]{34,136,51}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z'''}) \\cdot\n\\dotsb\n}\n\\\\[2ex]\n&\\qquad{}=\\frac{\nf({\\color[RGB]{238,102,119}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z}) \\cdot\n\\cancel{f({\\color[RGB]{34,136,51}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z'})} \\cdot\n\\cancel{f({\\color[RGB]{34,136,51}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z''})} \\cdot\n\\cancel{f({\\color[RGB]{34,136,51}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z'''})} \\cdot\n\\cancel{\\dotsb}\n}{\n\\underbracket[0.2ex]{\\sum_{\\color[RGB]{170,51,119}z}\nf({\\color[RGB]{238,102,119}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\color[RGB]{170,51,119}z})}_{{}=1} \\cdot\n\\cancel{f({\\color[RGB]{34,136,51}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z'})} \\cdot\n\\cancel{f({\\color[RGB]{34,136,51}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z''})} \\cdot\n\\cancel{f({\\color[RGB]{34,136,51}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z'''})} \\cdot\n\\cancel{\\dotsb}\n}\n\\\\[2ex]\n&\\qquad{}=\nf({\\color[RGB]{238,102,119}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z})\n\\\\[3ex]\n&\\qquad{}\\equiv\n\\mathrm{P}(\\color[RGB]{238,102,119}Z_{u}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z \\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\end{aligned}\n\\]\nso the information from the units \\(u'\\), \\(u''\\), and so on is irrelevant to this agent. In other words, this agent’s inferences about some units are not affected by the observation of other units.\nThe reason for this irrelevance is that the agent already knows the full-population frequencies. So the observation of the frequencies of some values provides no new information to the agent.\nObviously this is not what we desire. But it is not a problem: the crucial point is that knowledge of full-population frequencies is only a hypothetical, idealized situation. In the next chapter we shall see that learning occurs when we go beyond this idealization.\n\n\n\n\n\n\n “Learning” about what?\n\n\n\nIn this and the following sections, and sometimes in the following chapters, when we say “the agent is learning” or “the agent is not learning” we mean specifically the change in an agent’s beliefs about observation of variates of some units which had not yet been observed.\nNote that there is always learning about something whenever we put new information in the conditional of a probability. In the Mars-prospecting example above, for example, we have\n\\[\n\\mathrm{P}(R_{1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}  \\mathsfit{K}) = 2/3\n\\qquad\n\\mathrm{P}(R_{1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) = 2/3\n\\]\nand the agent has (practically) not learned anything about the sentence \\(R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\) from the sentence \\(R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\).\nBut we also have\n\\[\n\\mathrm{P}(R_{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}  \\mathsfit{K}) = 2/3\n\\qquad\n\\mathrm{P}(R_{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) = 0\n\\]\nthe probability for the sentence \\(R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\) has changed. So the agent has learned something: that rock #2 doesn’t contain haematite (\\({\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\)).\n\n\n\n\n\n\n\n\n\n\n\n\n\n Study reading\n\n\n\nCh. 3 of Probability Theory. This chapter is extremely instructive in general to understand how probability theory works."
  },
  {
    "objectID": "inference_about_freqs.html#sec-freq-not-known",
    "href": "inference_about_freqs.html#sec-freq-not-known",
    "title": "30  Inference about frequencies",
    "section": "30.1 Inference when population frequencies aren’t known",
    "text": "30.1 Inference when population frequencies aren’t known\nIn chapter  29 we considered an agent that has exchangeable beliefs and that knows the full-population frequencies. The degrees of belief of such an agent have a very simple form: products of frequencies. But for such an agent the observations of units doesn’t give any useful information for drawing inferences about new units: such observations provide frequencies which the agent already knows.\nSituations where we have complete frequency knowledge can be common in engineering problems, where the physical laws underlying the phenomena involved are known and computable. They are far less common in data-science and machine-learning applications: here we must consider agents that do not know the full-population frequencies.\nHow does such an agent calculate probabilities about units? The answer is actually a simple application of the “extension of the conversation” (§  9.4, which boil down to applications of the and and or rules). A probability given that the frequency distribution is not known is equal to the average of the probabilities given each possible frequency distribution, weighted by the probabilities of the frequency distributions:\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\n{\\color[RGB]{68,119,170}Z}_{u'}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z'} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n{\\color[RGB]{68,119,170}Z}_{u''}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z''} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\dotsb\n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}\n)\n\\\\[2ex]\n&\\qquad{}=\n\\sum_{\\boldsymbol{f}}\n\\mathrm{P}(\n{\\color[RGB]{68,119,170}Z}_{u'}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z'} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n{\\color[RGB]{68,119,170}Z}_{u''}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z''} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\dotsb\n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}\n)\n\\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\end{aligned}\n\\]\nBut we saw in §  29.5 that the probability for a sequence of values given a known frequency is just the product of the value’s frequencies. We thus have our long-sought formula:\n\n\n\n\n\n\n\nde Finetti’s representation theorem\n\n\n\n\nIf an agent has background information \\(\\mathsfit{I}\\) about a population saying that\n\nbeliefs about units are exchangeable\nthe population size is practically infinite\n\nthen\n\\[\n\\mathrm{P}(\n{\\color[RGB]{68,119,170}Z}_{u'}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z'} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n{\\color[RGB]{68,119,170}Z}_{u''}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z''} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\dotsb\n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}\n)\n\\approx\n\\sum_{\\boldsymbol{f}}\nf({\\color[RGB]{68,119,170}Z}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z'}\\color[RGB]{0,0,0}) \\cdot\nf({\\color[RGB]{68,119,170}Z}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z''}\\color[RGB]{0,0,0}) \\cdot\n\\,\\dotsb\\\n\\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\]\nfor any (different) units \\(u', u'', \\dotsc\\) and any (even equal) values \\(\\color[RGB]{68,119,170}z', z'', \\dotsc\\).\nIn the sum above, \\(\\boldsymbol{f}\\) runs over all possible frequency distributions for the full population.\nProperly speaking the sum is an integral, because \\(F\\) is a continuous quantity. We should write \\(\\mathrm{P}( {\\color[RGB]{68,119,170}Z}_{u'}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z'} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = \\int f({\\color[RGB]{68,119,170}Z}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z'}) \\cdot \\,\\dotsb\\  \\cdot \\mathrm{p}(\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\,\\mathrm{d}\\boldsymbol{f}\\)\n\n\n\n\nThis result is called de Finetti’s representation theorem for exchangeable belief distributions. It must be emphasized that this result is actually independent of any real or imaginary population frequencies. We took a route to it through the idea of population frequencies only to help our intuition. If for any reason you find the idea of a “limit frequency for an infinite population” somewhat suspicious, then don’t worry: the formula above actually does not rely on it. The formula results from the assumption has exchangeable beliefs about a collection of units that can potentially be continued without end.\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nForesight: Its logical laws, its subjective sources. This essay gives much insight on our reasoning process in making forecasts and learning from experience.\n\n\n\n\nLet’s see how this formula works in the simple Mars-prospecting example (with 3 million rocks or more) from §  29.5. Suppose that the agent:\n\n\n\n\nknows that the rock collection consists of:\n\neither a proportion 2/3 of \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\)-rocks and 1/3 of \\({\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\)-rocks; denote these frequencies with \\(\\boldsymbol{f}'\\)\nor a proportion 1/2 of \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\)-rocks and 1/2 of \\({\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\)-rocks; denote these frequencies with \\(\\boldsymbol{f}''\\)\n\nassigns a \\(75\\%\\) degree of belief to the first hypothesis, and \\(25\\%\\) to the second (so the sentence \\(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}'\\lor F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}''\\) has probability \\(1\\)):\n\\[\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}'\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) = 75\\%\n\\qquad\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}''\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) = 25\\%\n\\]\n\nWhat is the agent’s degree of belief that rock #1 contains haematite? According to the derived rule of extension of the conversation, that is, the main formula written above, we find:\n\\[\n\\begin{aligned}\n\\mathrm{P}(R_{1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n&=\n\\sum_{\\boldsymbol{f}}\nf(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}) \\cdot \\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n\\\\[1ex]\n&=\nf'(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}) \\cdot \\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}'\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) +\nf''(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}) \\cdot \\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}''\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n\\\\[1ex]\n&=\n{\\color[RGB]{102,204,238}\\frac{2}{3}}\\cdot 75\\% +\n{\\color[RGB]{102,204,238}\\frac{1}{2}}\\cdot 25\\%\n\\\\[1ex]\n&= \\boldsymbol{62.5\\%}\n\\end{aligned}\n\\]\nIn an analogous way we can calculate, for instance, the agent’s belief that rock #1 contains haematite, rock #2 doesn’t, and rock #3 does:\n\n\\[\n\\begin{aligned}\n\\mathrm{P}(R_{1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_{3} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n&\\approx\n\\sum_{\\boldsymbol{f}}\nf(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}) \\cdot f(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}) \\cdot f(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}) \\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n\\\\[2ex]\n&=\nf'(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}) \\cdot f'(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}) \\cdot  f'(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}) \\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}'\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) + {}\n\\\\[1ex]\n&\\qquad\nf''(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}) \\cdot f''(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}) \\cdot  f''(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}) \\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}''\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n\\\\[2ex]\n&=\n{\\color[RGB]{102,204,238}\\frac{2}{3}}\\cdot {\\color[RGB]{204,187,68}\\frac{1}{3}}\\cdot {\\color[RGB]{102,204,238}\\frac{2}{3}}\\cdot 75\\% +\n{\\color[RGB]{102,204,238}\\frac{1}{2}}\\cdot {\\color[RGB]{204,187,68}\\frac{1}{2}}\\cdot {\\color[RGB]{102,204,238}\\frac{1}{2}}\\cdot 25\\%\n\\\\[2ex]\n&\\approx \\boldsymbol{14.236\\%}\n\\end{aligned}\n\\]\n\n\n\nThis formula generalizes to any population, any variates, and any number of hypotheses about the frequencies.\nMathematical and, even more, computational complications arise when we consider all possible frequency distributions, since there is a practically infinite number of them; they form a continuum in fact. But do not let these practical difficulties affect the intuitive picture behind them, which is simple to grasp once you’ve considered some simple examples.\n\n\n\n\n\n\n Exercise\n\n\n\nConsider a state of knowledge \\(\\mathsfit{K}'\\) according to which:\n\n\nThe rock collection may have a proportion \\(0/10\\) of \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\)-rocks (and \\(9/10\\) of \\({\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\)-rocks); call this frequency distribution \\(\\boldsymbol{f}_0\\)\nThe rock collection may have a proportion \\(1/10\\) of \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\)-rocks; call this \\(\\boldsymbol{f}_1\\)\nand so on… up to\na proportion \\(10/10\\) of \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\)-rocks; call this \\(\\boldsymbol{f}_{10}\\)\n\n\n\nThe probability of each of these frequency hypotheses is \\(1/11\\), that is:\n\\[\n  \\begin{aligned}\n  &\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}_{0} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}') = 1/11 \\\\[1ex]\n  &\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}_{1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}') = 1/11 \\\\[1ex]\n  &\\dotso\\\\[1ex]\n  &\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}_{10} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}') = 1/11\n  \\end{aligned}\n  \\]\n\nCalculate the probabilities\n\\[\n\\mathrm{P}(R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{K}') \\qquad\n\\mathrm{P}(R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{K}') \\qquad\n\\mathrm{P}(R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{K}')\n\\] \nDo they all have the same value? Try to explain why or why not."
  },
  {
    "objectID": "inference_about_freqs.html#sec-learning-general",
    "href": "inference_about_freqs.html#sec-learning-general",
    "title": "30  Inference about frequencies",
    "section": "30.2 Learning from observed units",
    "text": "30.2 Learning from observed units\nStaying with the same Mars-prospecting scenario, let’s now ask what’s the agent’s degree of belief that rock #1 contains haematite, given that the agent has found that rock #2 doesn’t contain haematite. In the case of an agent that knows the full-population frequencies we saw §  29.6 that this degree of belief is actually unaffected by other observations. What happens when the population frequencies are not known?\nThe calculation is straightforward:\n\n\\[\n\\begin{aligned}\n\\mathrm{P}(R_{1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n&=\n\\frac{\n\\mathrm{P}(R_{1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n}{\n\\mathrm{P}(R_{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n}\n\\\\[2ex]\n&\\approx\n\\frac{\n\\sum_{\\boldsymbol{f}}\nf(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}) \\cdot f(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}) \\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n}{\n\\sum_{\\boldsymbol{f}}\nf(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}) \\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n}\n\\\\[2ex]\n&=\n\\frac{\n{\\color[RGB]{102,204,238}\\frac{2}{3}}\\cdot {\\color[RGB]{204,187,68}\\frac{1}{3}}\\cdot 75\\% +\n{\\color[RGB]{102,204,238}\\frac{1}{2}}\\cdot {\\color[RGB]{204,187,68}\\frac{1}{2}}\\cdot 25\\%\n}{\n{\\color[RGB]{204,187,68}\\frac{1}{3}}\\cdot 75\\% +\n{\\color[RGB]{204,187,68}\\frac{1}{2}}\\cdot 25\\%\n}\n\\\\[2ex]\n&\\approx\\frac{\n22.9167\\%\n}{\n37.5000\\%\n}\n\\\\[2ex]\n&= \\boldsymbol{61.111\\%}\n\\end{aligned}\n\\]\n\nKnowledge that \\(R_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\) thus does affect the agent’s belief about \\(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\):\n\\[\n\\mathrm{P}(R_{1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}  \\mathsfit{K}) = 62.5\\%\n\\qquad\n\\mathrm{P}(R_{1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) \\approx 61.1\\%\n\\]\nIn particular, the observation of one \\({\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\)-rock has somewhat decreased the probability of observing a new \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\)-rock.\n\n\n\n\n\n\n Exercise\n\n\n\n\nCalculate the minimal number of \\({\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\) observations needed for lowering the agent’s degree of belief of observing a \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\) to \\(55\\%\\) or less. \nDoes it seem possible to lower the agent’s belief to less than \\(50\\%\\)? Explain why.\nCalculate the minimal number of \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\) observations needed for increasing the agent’s degree of belief of observing a \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\) to \\(65\\%\\) or more. \nDoes it seem possible to increase the agent’s belief to more than \\(2/3\\)? Explain why."
  },
  {
    "objectID": "inference_about_freqs.html#sec-learn-freqs",
    "href": "inference_about_freqs.html#sec-learn-freqs",
    "title": "30  Inference about frequencies",
    "section": "30.3 How learning works: learning about frequencies",
    "text": "30.3 How learning works: learning about frequencies\nAn agent having full-population frequency information does not learn1 from observation of units, whereas an agent not having such information does learn from observation of units. This fact shows how learning from observed to unobserved units actually works. Crudely speaking, observations do not directly affect the beliefs about unobserved units, but instead affect the beliefs about the population frequencies. And these in turn affect the beliefs about unobserved units. Graphically this could be represented as follows:1 remember the warning of §  29.6 about “learning”\n\n\n\n\nflowchart LR\n  A([observed units]) --&gt; B([population frequencies]) --&gt; C([unobserved units])\n  %%\n  linkStyle 0 stroke:#47a,fill:#47a\n  linkStyle 1 stroke:#47a,fill:#47a\n  style A fill:#283,color:#fff,stroke-width:0px\n  style B fill:#bbb,color:#000,stroke-width:0px\n  style C fill:#e67,color:#fff,stroke-width:0px\n\n\n\n\n\nas opposed to this:\n\n\n\n\n\n\n\n\nflowchart LR\n  A([observed units]) --&gt; B([population frequencies]) --&gt; C([unobserved units])\n  A ---&gt; C\n  %%\n  linkStyle 0 stroke:#47a\n  linkStyle 1 stroke:#47a\n  linkStyle 2 stroke:#47a\n  style A fill:#283,color:#fff,stroke-width:0px\n  style B fill:#bbb,color:#000,stroke-width:0px\n  style C fill:#e67,color:#fff,stroke-width:0px\n\n\n\n\n\n\n\n\n\n\n\n\n\n Information connections\n\n\n\nThe graphs above represent informational connections, not “causal”. The directed arrows roughly mean “…provides information about…”; they do not mean “…causes…”.\nIn the first graph, the lack of an arrow from observed units to unobserved units means that all information provided by the observed units for the unobserved ones is fully contained in the information about frequencies.\n\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nOn the notion of cause\n\n\n\n\nThe informational relation between observed units, frequencies, and unobserved units becomes clear if we check how the agent’s beliefs about the frequency hypotheses change as observations are made. In the Mars-prospecting example of §  30.1, the agent has initial probabilities\n\\[\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}'\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) = 75\\%\n\\qquad\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}''\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) = 25\\%\n\\]\nwhere \\(\\boldsymbol{f}'\\) gives frequency \\(2/3\\) to \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\), and \\(\\boldsymbol{f}''\\) gives frequency \\(1/2\\) to \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\). How do these probabilities change, conditional on the agent’s observing that rock #2 doesn’t contain haematite? We just need to use Bayes’s theorem. For the first hypothesis \\(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}'\\):\n\n\\[\n\\begin{aligned}\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}'\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) &=\n\\frac{\n\\mathrm{P}(R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}'\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) \\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}'\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n}{\n\\mathrm{P}(R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}'\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) \\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}'\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n+\n\\mathrm{P}(R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}''\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) \\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}''\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n}\n\\\\[1ex]\n&=\n\\frac{\nf'(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}) \\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}'\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n}{\nf'(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}) \\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}'\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n+\nf''(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}) \\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}''\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n}\n\\\\[1ex]\n&=\n\\frac{\n\\frac{1}{3} \\cdot\n75\\%\n}{\n\\frac{1}{3} \\cdot\n75\\%\n+\n\\frac{1}{2} \\cdot\n25\\%\n}\n\\\\[2ex]\n&=\n\\boldsymbol{66.667\\%}\n\\end{aligned}\n\\]\n\nand an analogous calculation yields \\(\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}'\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})=33.333\\%\\).\nThis result makes sense, because according to the hypothesis \\(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}''\\) there is a higher proportion of \\({\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\)-rocks than according to \\(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}'\\), and a \\({\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\)-rock has been observed. The hypothesis \\(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}''\\) therefore becomes slightly more plausible, and \\(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}'\\) slightly less.\nUsing the updated degree of belief above we also have another way to calculate the conditional probability \\(\\mathrm{P}(R_{1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\\), using the derived rule of “extension of the conversation” in a different manner:\n\n\\[\n\\begin{aligned}\n\\mathrm{P}(R_{1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n&=\n\\sum_{\\boldsymbol{f}}\n\\mathrm{P}(R_{1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\[2ex]\n\\text{\\color[RGB]{187,187,187}\\scriptsize(no learning if frequencies are known)}\\enspace\n&=\\sum_{\\boldsymbol{f}}\n\\mathrm{P}(R_{1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}  F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\[2ex]\n&=\n\\mathrm{P}(R_{1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}  F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}'\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}'\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n+{}\n\\\\[1ex]\n&\\qquad\\mathrm{P}(R_{1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}  F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}''\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}''\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\[2ex]\n&=\nf'(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}})\n\\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}'\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n+{}\n\\\\[1ex]\n&\\qquad\nf''(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}})\n\\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}''\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\[2ex]\n&=\n{\\color[RGB]{102,204,238}\\frac{2}{3}} \\cdot 66.667\\%\n+\n{\\color[RGB]{102,204,238}\\frac{1}{2}} \\cdot 33.333\\%\n\\\\[2ex]\n&= \\boldsymbol{61.111\\%}\n\\end{aligned}\n\\]\n\nThe result is exactly as in §  30.2 – as it should be: remember from chapter  8 that the four rules of inference are built so as to mathematically guarantee this kind of logical self-consistency.\n\n\nThe fact that the agent is actually learning about the full-population frequencies allows it to draw improved inferences not only about units, but also about characteristics intrinsic to the population itself, and also about its own performance in future inferences. For instance, the agent can even forecast the maximal accuracy that can be obtained in future inferences. We shall quickly explore these possibilities in a later chapter.\n\n\n\n\n\n\n Study reading\n\n\n\n\nCh. 4 of Probability Theory\n§§ 8.1–8.6 of Probability\nSkim through De finetti’s theorem on exchangeable variables"
  },
  {
    "objectID": "inference_about_freqs.html#sec-prob-for-freqs",
    "href": "inference_about_freqs.html#sec-prob-for-freqs",
    "title": "30  Inference about frequencies",
    "section": "30.4 How to assign the probabilities for the frequencies?",
    "text": "30.4 How to assign the probabilities for the frequencies?\nThe general formula we found for the joint probability:\n\\[\n\\mathrm{P}(\n{\\color[RGB]{68,119,170}Z}_{u'}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z'} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n{\\color[RGB]{68,119,170}Z}_{u''}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z''} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\dotsb\n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}\n)\n\\approx\n\\sum_{\\boldsymbol{f}}\nf({\\color[RGB]{68,119,170}Z}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z'}\\color[RGB]{0,0,0}) \\cdot\nf({\\color[RGB]{68,119,170}Z}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z''}\\color[RGB]{0,0,0}) \\cdot\n\\,\\dotsb\\\n\\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\]\nallows us to draw many kinds of predictions about units, which we’ll explore in the next chapter.\nBut how does the agent assign  \\(\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\) ,  that is, the probability distribution (in fact, a density) over all possible frequency distributions? There is no general answer to this important question, for two main reasons.\nFirst, a proper answer is obviously problem-dependent. In fact  \\(\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\)  is the place where the agent encodes any background information relevant to the problem.\nTake the simple example of the tosses of a coin. If you (the agent) examines the coin and the tossing method and they seem ordinary to you, then you might assign probabilities like these:\n\\[\n\\begin{aligned}\n&\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\textsf{\\small`always heads'} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{o}}) \\approx 0\n\\\\\n&\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\textsf{\\small`always tails'} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{o}}) \\approx 0\n\\\\\n&\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\textsf{\\small`50\\% heads 50\\% tails'} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{o}}) \\approx \\text{\\small very high}\n\\end{aligned}\n\\]\nBut if you are told that the coin is a magician’s one, with either two heads or two tails, and you don’t know which, then you might assign probabilities like these:\n\\[\n\\begin{aligned}\n&\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\textsf{\\small`always heads'} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{m}}) = 1/2\n\\\\\n&\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\textsf{\\small`always tails'} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{m}}) = 1/2\n\\\\\n&\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\textsf{\\small`50\\% heads 50\\% tails'} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{m}}) = 0\n\\end{aligned}\n\\]\n\n\n\n\n\n\n Exercise\n\n\n\nAssume the state of knowledge \\(\\mathsfit{I}_{\\text{m}}\\) above and calculate:\n\n\\(\\mathrm{P}(\\textsf{\\small`heads 1st toss'} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{m}})\\), the probability of heads at the first toss.\n\\(\\mathrm{P}(\\textsf{\\small`heads 2nd toss'} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\textsf{\\small`heads 1st toss'} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{m}})\\), the probability of heads at the second toss, given that heads was observed at the first.\n\nExplain your findings.\n\n\nSecond, for complex situations with many variates of different types it is may be mathematically and computationally difficult to write down and encode  \\(\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\) .  Moreover, the multidimensional characteristics and quirks of this belief distribution can be difficult to grasp and understand.\nYet it is a result of probability theory (§  5.2) that we cannot avoid specifying \\(\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\). Any “methods” that claim to avoid the specification of that probability distribution are covertly specifying one instead, and hiding it from sight. It is therefore best to have this distribution at least open to inspection rather than hidden.\n\n\nLuckily, if  \\(\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\)  is “open-minded”, that is, if it doesn’t exclude a priori any frequency distribution \\(\\boldsymbol{f}\\), or in other words if it doesn’t assign strictly zero belief to any \\(\\boldsymbol{f}\\), then with enough data the updated belief distribution   \\(\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{data} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\)  will actually converge to the true frequency distribution of the full population. The tricky word here is “enough”. In some problems a dozen observed units might be enough; in other problems a million observed units might not be enough yet.\n\n\n\nThe Dirichlet-mixture belief distribution for frequency distributions over nominal variates\nIn this course we sadly shall not examine in depth any mathematical expressions for belief distributions over frequencies. We briefly discuss here one belief distribution that is used in the prototype “optimal predictor machine” applied in the following chapters. We shall call it the Dirichlet-mixture belief distribution. The state of knowledge underlying this distribution will be denoted \\(\\mathsfit{K}\\).\nThe Dirichlet-mixture distribution is appropriate for frequency distributions over discrete, nominal variates, or joint variates with all nominal components. It is not appropriate to discrete ordinal variates, because it implicitly assumes that there is no natural order to the variate values.\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nSome of the theoretical basis for the choice of this belief distribution can be found in chapters 4–5 of The Estimation of Probabilities.\n\n\nSuppose we have a simple or joint nominal variate \\({\\color[RGB]{68,119,170}Z}\\) which can assume \\(M\\) different values (these can be joint values, as in the examples of §  29.3). As usual \\(\\boldsymbol{f}\\) denotes a specific frequency distribution for the variate values. For a specific value \\({\\color[RGB]{68,119,170}z}\\), \\(f({\\color[RGB]{68,119,170}z})\\) is the relative frequency with which that value occurs in the full population.\nThe Dirichlet-mixture distribution assigns to \\(\\boldsymbol{f}\\) a probability density proportional to the following formula:\n\\[\\mathrm{p}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) \\propto\n\\sum_{\\alpha}\\prod_{{\\color[RGB]{68,119,170}z}} f({\\color[RGB]{68,119,170}z})^{2^\\alpha -1}\n\\]\nThe proportionality constant is just the number that ensures that the integral of the density above over all possible frequency distributions equals 1.\nThe product “\\(\\prod_{{\\color[RGB]{68,119,170}z}}\\)” is over all \\(M\\) possible values of \\({\\color[RGB]{68,119,170}Z}\\). The sum “\\(\\sum_{\\alpha}\\)” is over an integer (positive or negative) parameter \\(\\alpha\\) that runs between a minimum and maximum value. In the applications of the next chapters the values are chosen as follows:\n\\[\n2^{\\alpha_{\\text{min}}} \\approx \\frac{1}{M}\n\\qquad\n2^{\\alpha_{\\text{max}}} = 4\n\\]\nThese minimum and maximum values turn out not to matter in most applications, even if we make \\(\\alpha_{\\text{min}}\\) even lower or \\(\\alpha_{\\text{max}}\\) even higher.\n\n\nLet’s see how this formula concretely looks like in the simple example of the Mars-prospecting scenario (which had many analogies with coin tosses).\nThe variate \\(R\\) can assume two values \\(\\set{{\\color[RGB]{102,204,238}{\\small\\verb;Y;}},{\\color[RGB]{204,187,68}{\\small\\verb;N;}}}\\), so \\(M=2\\) in this case. The frequency distribution consists in two frequencies:\n\\[f({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}) \\qquad f({\\color[RGB]{204,187,68}{\\small\\verb;N;}})\\]\nof which only one can be chosen independently, since they must sum up to 1.\nThen\n\\[\n2^{\\alpha_{\\text{min}}} \\approx \\frac{1}{2}\\ ,\n\\quad\n2^{\\alpha_{\\text{max}}} = 4\n\\qquad\\Longrightarrow\\qquad\n\\alpha_{\\text{min}}= -1 \\ ,\n\\quad\n\\alpha_{\\text{max}}= 2\n\\]\nand the agent’s belief distribution for the frequencies is proportional to22 the proportionality constant, which can be calculated exactly in this case, is \\(1/(\\pi+\\frac{493}{420}) \\approx 0.231 728\\).\n\\[\n\\begin{aligned}\n\\mathrm{p}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) &\\propto\n\\sum_{\\alpha}\\prod_{{\\color[RGB]{68,119,170}z}} f({\\color[RGB]{68,119,170}z})^{2^\\alpha -1}\n\\\\[1ex]\n&\\propto\nf({\\color[RGB]{102,204,238}{\\small\\verb;Y;}})^{2^{-1}-1}\\cdot f({\\color[RGB]{204,187,68}{\\small\\verb;N;}})^{2^{-1}-1}\n+ f({\\color[RGB]{102,204,238}{\\small\\verb;Y;}})^{2^{0}-1}\\cdot f({\\color[RGB]{204,187,68}{\\small\\verb;N;}})^{2^{0}-1}\n+{} \\\\&\\qquad\nf({\\color[RGB]{102,204,238}{\\small\\verb;Y;}})^{2^{1}-1}\\cdot f({\\color[RGB]{204,187,68}{\\small\\verb;N;}})^{2^{1}-1}\n+ f({\\color[RGB]{102,204,238}{\\small\\verb;Y;}})^{2^{2}-1}\\cdot f({\\color[RGB]{204,187,68}{\\small\\verb;N;}})^{2^{2}-1}\n\\\\[1ex]\n&\\propto\n\\frac{1}{\\sqrt{f({\\color[RGB]{102,204,238}{\\small\\verb;Y;}})}}\\cdot \\frac{1}{\\sqrt{f({\\color[RGB]{204,187,68}{\\small\\verb;N;}})}}\n+ 1\n+ f({\\color[RGB]{102,204,238}{\\small\\verb;Y;}})\\cdot f({\\color[RGB]{204,187,68}{\\small\\verb;N;}})\n+ f({\\color[RGB]{102,204,238}{\\small\\verb;Y;}})^{3}\\cdot f({\\color[RGB]{204,187,68}{\\small\\verb;N;}})^{3}\n\\end{aligned}\n\\]\nWe can visualize this distribution with a generalized scatter plot (§  15.7) of 100 frequencies, each represented by a line histogram (§  14.2):\n\nwe see that all possible frequency distributions are considered by this belief distribution.\n\n\n\n\n\n\n Exercise\n\n\n\nCalculate the formula above (disregard the missing proportionality constant) for these three frequency distributions:\n\n\\(f({\\color[RGB]{102,204,238}{\\small\\verb;Y;}})=0.5\\quad f({\\color[RGB]{204,187,68}{\\small\\verb;N;}})=0.5\\) \n\\(f({\\color[RGB]{102,204,238}{\\small\\verb;Y;}})=0.75\\quad f({\\color[RGB]{204,187,68}{\\small\\verb;N;}})=0.25\\) \n\\(f({\\color[RGB]{102,204,238}{\\small\\verb;Y;}})=0.99\\quad f({\\color[RGB]{204,187,68}{\\small\\verb;N;}})=0.01\\) \n\n\n\n\n\nJoint probabilities about units with the Dirichlet-mixture distribution\nA mathematical advantage of the Dirichlet-mixture belief distribution is that some formulae where it enters can be computed exactly. The most important is the formula in de Finetti’s representation theorem (§  30.1).\nTake a sequence of observations for \\(N\\) units, for which the variate \\({\\color[RGB]{68,119,170}Z}\\) is seen to have values \\(\\color[RGB]{68,119,170}z_1, z_2, \\dotsc, z_N\\). Some or even all of these \\(N\\) values might be identical; denote by \\(\\#{\\color[RGB]{68,119,170}z}\\) the multiplicity with which value \\({\\color[RGB]{68,119,170}z}\\) occurs in the sequence. For instance, in the Mars-prospecting example, the sequence\n\\[\nR_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\n\\]\nhas \\(\\#{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}= 3\\) and \\(\\#{\\color[RGB]{204,187,68}{\\small\\verb;N;}}= 1\\), whereas the sequence33 Remember that the agent has exchangeable beliefs, so the units’ IDs don’t matter (§  28.3)!\n\\[\nR_{32}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_{102}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_{8}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\n\\]\nhas \\(\\#{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}= 0\\) and \\(\\#{\\color[RGB]{204,187,68}{\\small\\verb;N;}}= 3\\).\nWith the Dirichlet-mixture distribution we have the following general equality:\n\\[\n\\begin{aligned}\n\\mathrm{P}(\n\\color[RGB]{68,119,170}Z_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_1 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\dotsb\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_N\n\\color[RGB]{0,0,0}\n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}\n)\n&=\n\\int\nf({\\color[RGB]{68,119,170}Z}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z_{1}}\\color[RGB]{0,0,0}) \\cdot\n\\,\\dotsb\\, \\cdot\nf({\\color[RGB]{68,119,170}Z}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z_{N}}\\color[RGB]{0,0,0}) \\cdot\n\\mathrm{p}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n\\,\\mathrm{d}\\boldsymbol{f}\n\\\\[2ex]\n&=\n\\sum_{\\alpha=\\alpha_{\\text{min}}}^{\\alpha_{\\text{max}}}\n\\frac{\n\\prod_{{\\color[RGB]{68,119,170}z}} \\bigl(2^{\\alpha} + \\#{\\color[RGB]{68,119,170}z}- 1\\bigr)!\n}{\n\\bigl(M\\,2^{\\alpha} + N -1 \\bigr)!\n}\n\\ \\cdot\\\n\\sum_{\\alpha=\\alpha_{\\text{min}}}^{\\alpha_{\\text{max}}}\n\\frac{\n\\bigl(M\\,2^{\\alpha} -1 \\bigr)!\n}{\n{\\bigl(2^{\\alpha} - 1\\bigr)!}^M\n}\n\\end{aligned}\n\\]\nThe second factor (where no \\(\\#{\\color[RGB]{68,119,170}z}\\) or \\(N\\) appear) is the same whenever we deal with the same population (it only depends on \\(M\\)), so it disappears whenever we take ratios of probabilities like the one above.\nAs a concrete numerical example,\n\n\\[\n\\begin{aligned}\n\\mathrm{P}(\n\\underbracket[0.1ex]{R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}}_{\n\\color[RGB]{187,187,187}N=4\\quad \\#{\\small\\verb;Y;}=3\\quad \\#{\\small\\verb;N;}=1\n}\n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}\n)\n&=\n\\sum_{\\alpha=-1}^{2}\n\\frac{\n\\bigl(2^{\\alpha} + {\\color[RGB]{102,204,238}3} - 1\\bigr)! \\cdot\n\\bigl(2^{\\alpha} + {\\color[RGB]{204,187,68}1} - 1\\bigr)!\n}{\n\\bigl(2\\cdot 2^{\\alpha} + 4 -1 \\bigr)!\n}\n\\ \\cdot\\\n\\sum_{\\alpha=-1}^{2}\n\\frac{\n\\bigl(2\\cdot 2^{\\alpha} -1 \\bigr)!\n}{\n{\\bigl(2^{\\alpha} - 1\\bigr)!}^2\n}\n\\\\[1ex]\n&=\n\\boldsymbol{0.042 331}\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n Exercise\n\n\n\nUsing the formula above, calculate:\n\n\\(\\mathrm{P}(R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\\) ; does the result make sense?\n\\(\\mathrm{P}(R_{32}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_{102}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_{8}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\\)"
  },
  {
    "objectID": "general_inference.html#sec-recap-general-formulae",
    "href": "general_inference.html#sec-recap-general-formulae",
    "title": "31  Final inference formulae",
    "section": "31.1 Recap of the main inference formulae",
    "text": "31.1 Recap of the main inference formulae\nAll inferences about units of a population rely on the joint probability for any number of units, which is given by the following formula (§  30.1):\n\n\n\n\n\n\nde Finetti’s representation\n\n\n\n\\[\n\\begin{aligned}\n&\\mathrm{P}\\bigl(\n\\color[RGB]{68,119,170}Z_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N+1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_N\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_1\n\\color[RGB]{0,0,0}\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}} \\mathsfit{I}\\bigr)\n\\\\[2ex]\n&\\qquad{}=\n\\sum_{\\boldsymbol{f}}\nf({\\color[RGB]{68,119,170}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N+1}}\\color[RGB]{0,0,0}) \\cdot\nf({\\color[RGB]{68,119,170}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N}}\\color[RGB]{0,0,0}) \\cdot\n\\, \\dotsb\\, \\cdot\nf({\\color[RGB]{68,119,170}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{1}}\\color[RGB]{0,0,0})\n\\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\end{aligned}\n\\]\n\n\nwhere \\(\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\\) is problem-dependent and must be specified by the agent. In the case of a single or joint nominal variate \\({\\color[RGB]{68,119,170}Z}\\) we have an explicit mathematical formula for an agent that adopts a Dirichlet-mixture belief distribution (§  30.4).\nThe formula above can be used to draw two main kinds of inferences:\n\n\n\n\n\n\nInferences about all variates \\(Z\\) of a new unit, given observed units (“unsupervised learning”)\n\n\n\n\\[\n\\begin{aligned}\n    &\\mathrm{P}\\bigl(\n    {\\color[RGB]{238,102,119}Z_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N+1}}\n    \\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}}\n    \\color[RGB]{34,136,51}Z_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_N \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    Z_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_1\n    \\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\mathsfit{I}} \\bigr)\n    \\\\[2ex]\n    &\\qquad{}\n    =\n    \\frac{\n        \\mathrm{P}\\bigl(\n    \\color[RGB]{238,102,119}Z_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N+1}\n    \\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\color[RGB]{34,136,51}Z_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_N\n    \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\color[RGB]{34,136,51}Z_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_1\n    \\color[RGB]{0,0,0}\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}} {\\mathsfit{I}} \\bigr)\n}{\n     \\sum_{\\color[RGB]{170,51,119}z} \\mathrm{P}\\bigl(\n    {\\color[RGB]{238,102,119}Z_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{170,51,119}z}}\n        \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\color[RGB]{34,136,51}Z_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_N \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    Z_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_1\n    \\color[RGB]{0,0,0}\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}}  {\\mathsfit{I}} \\bigr)\n}\n\\end{aligned}\n\\]\n\n\nAnd, if we split the variate \\({\\color[RGB]{68,119,170}Z}\\) into two sets of predictand and predictor variates \\(\\color[RGB]{68,119,170}(Y \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X)\\)  (§  29.2):\n\n\n\n\n\n\n\nInferences about some variates \\(Y\\) of a new unit, given other of its variates \\(X\\) and given observed units (“supervised learning”)\n\n\n\n\\[\n\\begin{aligned}\n    &\\mathrm{P}\\bigl(\n    \\color[RGB]{238,102,119}Y_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N+1}\n\\color[RGB]{0,0,0}\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}}\n    \\color[RGB]{34,136,51}X_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N+1}\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\n    Y_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_N \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_N \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    Y_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_1 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_1\n    \\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}\\bigr)\n    \\\\[2ex]\n    &\\qquad{}=\n    \\frac{\n        \\mathrm{P}\\bigl(\n    \\color[RGB]{238,102,119}Y_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N+1} \\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\color[RGB]{34,136,51}X_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N+1}\n        \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_N \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_N\n    \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_1 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_1\n    \\color[RGB]{0,0,0}\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}} {\\mathsfit{I}} \\bigr)\n}{\n     \\sum_{\\color[RGB]{170,51,119}y} \\mathrm{P}\\bigl(\n    \\color[RGB]{238,102,119}Y_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{170,51,119}y} \\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\color[RGB]{34,136,51}X_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N+1}\n        \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_N \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_N \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    Y_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_1 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_1\n    \\color[RGB]{0,0,0}\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}}  {\\mathsfit{I}} \\bigr)\n}\n\\end{aligned}\n\\]\n\n\n\n\n\nIf \\({\\color[RGB]{68,119,170}Z}\\), \\({\\color[RGB]{68,119,170}Y}\\), \\({\\color[RGB]{68,119,170}X}\\) are each a single or joint nominal variate, and we consider an agent having a Dirichlet-mixture belief distribution (§  30.4), the formulae above takes on concrete mathematical expressions, which can be rewritten as follows:\n\n\n\n\n\n\n\nCase of purely nominal variates and agent with Dirichlet-mixture belief distribution\n\n\n\n\\[\n\\begin{aligned}\n&\\mathrm{P}\\bigl(\n\\color[RGB]{68,119,170}Z_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N+1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_N\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_1\n\\color[RGB]{0,0,0}\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}} \\mathsfit{K}\\bigr)\n=\n\\sum_{\\alpha=\\alpha_{\\text{min}}}^{\\alpha_{\\text{max}}}\n\\frac{\n\\prod_{{\\color[RGB]{68,119,170}z}} \\bigl(2^{\\alpha} + {\\color[RGB]{68,119,170}\\#z} - 1\\bigr)!\n}{\n\\bigl(M\\,2^{\\alpha} + N \\bigr)!\n}\n\\ \\cdot\\\n\\sum_{\\alpha=\\alpha_{\\text{min}}}^{\\alpha_{\\text{max}}}\n\\frac{\n\\bigl(M\\,2^{\\alpha} -1 \\bigr)!\n}{\n{\\bigl(2^{\\alpha} - 1\\bigr)!}^M\n}\n\\\\[3em]\n&\\mathrm{P}\\bigl(\n    \\color[RGB]{238,102,119}Z_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N+1}\n\\color[RGB]{0,0,0}\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}}\n    \\color[RGB]{34,136,51}Z_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_N \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    Z_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_1\n    \\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}\\bigr)\n    =\n    \\frac{\\displaystyle{}\\enspace\n    \\sum_{\\alpha=\\alpha_{\\text{min}}}^{\\alpha_{\\text{max}}}\n\\frac{\n(2^{\\alpha} + {\\color[RGB]{34,136,51}\\# z_{N+1}} )\n\\cdot\n\\prod_{\\color[RGB]{34,136,51}z} \\bigl(2^{\\alpha} + {\\color[RGB]{34,136,51}\\# z} - 1\\bigr)!\n}{\n\\bigl(M\\,2^{\\alpha} + N \\bigr)!\n}\n    \\enspace{}}{\\displaystyle{}\\enspace\n    \\sum_{\\color[RGB]{34,136,51}z^*}\n    \\sum_{\\alpha=\\alpha_{\\text{min}}}^{\\alpha_{\\text{max}}}\n\\frac{\n(2^{\\alpha} + {\\color[RGB]{34,136,51}\\# z^*})\n\\cdot\n\\prod_{\\color[RGB]{34,136,51}z} \\bigl(2^{\\alpha} + {\\color[RGB]{34,136,51}\\# z} - 1\\bigr)!\n}{\n\\bigl(M\\,2^{\\alpha} + N \\bigr)!\n}\n    \\enspace{}}\n\\\\[3em]\n    &\\mathrm{P}\\bigl(\n    \\color[RGB]{238,102,119}Y_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N+1}\n\\color[RGB]{0,0,0}\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}}\n    \\color[RGB]{34,136,51}X_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N+1}\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\nY_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_N \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_N \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    Y_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_1 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_1\n    \\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}\\bigr)\n    \\\\[2ex]\n    &\\qquad\\qquad\\qquad{}=\n    \\frac{\\displaystyle{}\\enspace\n    \\sum_{\\alpha=\\alpha_{\\text{min}}}^{\\alpha_{\\text{max}}}\n\\frac{\n\\prod_{\\color[RGB]{34,136,51}x}\\bigl(2^{\\alpha} + {\\color[RGB]{34,136,51}\\#(y_{N+1}, x)}\\bigr)\n\\cdot\n\\prod_{\\color[RGB]{34,136,51}y}\n\\prod_{\\color[RGB]{34,136,51}x}\n\\bigl(2^{\\alpha} + {\\color[RGB]{34,136,51}\\#(y, x)} - 1\\bigr)!\n}{\n\\bigl(M\\,2^{\\alpha} + N \\bigr)!\n}\n    \\enspace{}}{\\displaystyle{}\\enspace\n    \\sum_{\\color[RGB]{34,136,51}y^*}\n    \\sum_{\\alpha=\\alpha_{\\text{min}}}^{\\alpha_{\\text{max}}}\n\\frac{\n\\prod_{\\color[RGB]{34,136,51}x}\\bigl(2^{\\alpha} + {\\color[RGB]{34,136,51}\\#(y^*, x)}\\bigr)\n\\cdot\n\\prod_{\\color[RGB]{34,136,51}y}\n\\prod_{\\color[RGB]{34,136,51}x}\n\\bigl(2^{\\alpha} + {\\color[RGB]{34,136,51}\\#(y, x)} - 1\\bigr)!\n}{\n\\bigl(M\\,2^{\\alpha} + N \\bigr)!\n}\n    \\enspace{}}\n\\end{aligned}\n\\]\nIn the last two formulae, the counts \\(\\color[RGB]{34,136,51}\\#z\\), \\(\\color[RGB]{34,136,51}\\#(y,x)\\), and similar refer to the values present in the conditional.\n\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nThe last three formulae above look complicated. They do because they contain abbreviations (“\\(\\sum\\)”, “\\(\\prod\\)”) of sums or products of many terms. But once you have written down a couple of concrete examples, their meaning should be clear.\nFor the second formula, try to write it down for a concrete case in the Mars-prospecting scenario; say with \\(N=2\\) and specific values for \\(z_1, z_2, z_3\\), and verify that it is the correct expression of the more general formulae previously given. Use the property of the factorial\n\\[(a+1)! = (a+1) \\cdot a!\\]\n\n\n\nExample\nLet’s see a simple step-by-step application of the formula for “unsupervised learning” in the Mars-prospecting scenario.\nThe agent has exchangeable beliefs represented by a Dirichlet-mixture distribution. The variate of the population of interest has two possible values, so in the formulae above we have  \\(\\alpha_{\\text{min}}=-1\\), \\(\\alpha_{\\text{max}}=2\\)  (§  30.4).\nThe agent has collected three rocks, and upon examination two of them contain haematite, one doesn’t. The agent’s data are therefore\n\\[R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\]\n(remember that it doesn’t matter how we label the rocks, because the agent’s beliefs are exchangeable).\nWhat probability should the agent give to finding haematite in a newly collected rock? That is, what value should it assign to\n\\[\\mathrm{P}(R_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) \\ ?\\]\nThere are different but logically equivalent ways of breaking down this calculation into steps. They also correspond to different ways of calculating the mathematical formulae. Here we examine one possibility.\n\n\n    For the computation of the probability for \\(R_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\) we need to consider all alternative hypotheses. In this case there are only two alternatives, including the one of interest:\n\\[R_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\qquad R_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\]\n\n\n    We need to calculate the joint probabilities for the agent’s data (about the three previous rocks) and each hypothesis in turn. In the present case, they are the two joint probabilities\n\\[\n\\begin{aligned}\n\\mathrm{P}(R_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\, R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n\\\\[1ex]\n\\mathrm{P}(R_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\, R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n\\end{aligned}\n\\]\nThese probabilities will appear, summed together, in the denominator of a final fraction; whereas the probability containing the hypothesis of interest will appear, by itself, in the numerator.\n\na   In the first joint probability, \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\) appears thrice and \\({\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\) appears once, so\n\\[\\#{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}= 3 \\qquad \\#{\\color[RGB]{204,187,68}{\\small\\verb;N;}}= 1 \\qquad \\text{\\color[RGB]{119,119,119}\\small total} = 4\\]\nThe de Finetti representation formula then gives, besides a constant factor,\n\\[\\begin{aligned}\n\\mathrm{P}(R_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\, R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n&\\propto\n\\sum_{\\alpha=-1}^{2}\n\\frac{\n\\bigl(2^{\\alpha} + {\\color[RGB]{102,204,238}3} - 1\\bigr)! \\cdot\n\\bigl(2^{\\alpha} + {\\color[RGB]{204,187,68}1} - 1\\bigr)!\n}{\n\\bigl(2\\cdot 2^{\\alpha} + {\\color[RGB]{119,119,119}4} -1 \\bigr)!\n}\n\\\\[1ex]\n&\\propto\n\\boldsymbol{0.182 675}\n\\end{aligned}\n\\]\nb   In the second joint probability, \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\) appears twice and \\({\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\) appears twice, so\n\\[\\#{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}= 2 \\qquad \\#{\\color[RGB]{204,187,68}{\\small\\verb;N;}}= 2 \\qquad \\text{\\color[RGB]{119,119,119}\\small total} = 4\\]\nWe find, again besides the same constant factor,\n\\[\\begin{aligned}\n\\mathrm{P}(R_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\, R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n&\\propto\n\\sum_{\\alpha=-1}^{2}\n\\frac{\n\\bigl(2^{\\alpha} + {\\color[RGB]{102,204,238}2} - 1\\bigr)! \\cdot\n\\bigl(2^{\\alpha} + {\\color[RGB]{204,187,68}2} - 1\\bigr)!\n}{\n\\bigl(2\\cdot 2^{\\alpha} + {\\color[RGB]{119,119,119}4} -1 \\bigr)!\n}\n\\\\[1ex]\n&\\propto\n\\boldsymbol{0.114 468}\n\\end{aligned}\n\\]\n\n\n\n    The probability for the hypothesis of interest is given by the fraction\n\n\\[\n\\begin{aligned}\n&\\mathrm{P}(R_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\[1ex]\n&\\qquad{}=\n\\frac{\n\\mathrm{P}(R_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\, R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n}{\n\\mathrm{P}(R_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\, R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) +\n\\mathrm{P}(R_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\, R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n}\n\\\\[1ex]\n&\\qquad{}=\n\\frac{0.182 675 }{0.182 675 + 0.114 468}\n\\\\[1ex]\n&\\qquad{}=\n\\boldsymbol{61.477\\%}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "general_inference.html#how-typical-machine-learning-problems-are-solved-by-the-optimal-predictor-machine",
    "href": "general_inference.html#how-typical-machine-learning-problems-are-solved-by-the-optimal-predictor-machine",
    "title": "31  Final inference formulae",
    "section": "31.2 How typical machine-learning problems are solved by the “optimal predictor machine”",
    "text": "31.2 How typical machine-learning problems are solved by the “optimal predictor machine”\n\nMissing values in training data\n\n\n“Generative” use: probability of predictor given the possible predictand values"
  },
  {
    "objectID": "OPM_application_nominal.html#example-population-and-data",
    "href": "OPM_application_nominal.html#example-population-and-data",
    "title": "32  The optimal predictor machine for glass forensics",
    "section": "32.1 Example population and data",
    "text": "32.1 Example population and data\nConsider the following population, which we consider to be exchangeable:\n\nUnits: glass fragments collected at particularly defined crime scenes.\nVariates:\n\n\\(\\mathit{RI}\\), ordinal, domain \\(\\set{1,\\dotsc,4}\\): Refractive Index of the fragment\n\\(\\mathit{Na}\\), ordinal, domain \\(\\set{1,\\dotsc,4}\\): Natrium content of the fragment\n\\(\\mathit{Mg}\\), ordinal, domain \\(\\set{1,\\dotsc,4}\\): Magnesium content of the fragment\n\\(\\mathit{Al}\\), ordinal, domain \\(\\set{1,\\dotsc,4}\\): Aluminium content of the fragment\n\\(\\mathit{Si}\\), ordinal, domain \\(\\set{1,\\dotsc,4}\\): Silicon content of the fragment\n\\(\\mathit{K}\\), ordinal, domain \\(\\set{1,\\dotsc,4}\\): Potassium content of the fragment\n\\(\\mathit{Ca}\\), ordinal, domain \\(\\set{1,\\dotsc,4}\\): Calcium content of the fragment\n\\(\\mathit{Ba}\\), ordinal, domain \\(\\set{1,\\dotsc,4}\\): Barium content of the fragment\n\\(\\mathit{Fe}\\), ordinal, domain \\(\\set{1,\\dotsc,4}\\): Iron content of the fragment\n\\(\\mathit{Type}\\), nominal, domain \\(\\set{{\\small\\verb;T1;},\\dotsc,{\\small\\verb;T7;}}\\): Type or origin of the glass fragment\n\nThe values for the \\(\\mathit{RI}\\) and content variates represent ranges of numeric values or percentages, which you can find in the metadata file glass_metadata-4_lev.csv. In the same file you also find the description of the glass types.\n\nWe have a sample of 214 units from this population; their variate values are stored in the file glass_data-4_lev.csv. Here are the first five units:\n\nprint(head(fread('datasets/glass_data-4_lev.csv'), 5))\n\n   Id RI Na Mg Al Si K Ca Ba Fe Type\n1:  1  2  2  4  2  3 1  2  1  1   T2\n2:  2  2  1  3  2  3 1  2  1  1   T2\n3:  3  2  2  4  2  2 1  2  1  1   T1\n4:  4  1  2  2  2  3 1  2  1  1   T4\n5:  5  2  2  3  2  3 1  2  1  1   T1\n\n\n\n\nNow we imagine to prepare an agent for drawing inferences about the full population of glass fragments – which also means fragments from future or unsolved crime scenes. The agent uses – or is the embodiment of – the universal exchangeable-inference machine."
  },
  {
    "objectID": "OPM_application_nominal.html#the-agents-initial-state-of-knowledge",
    "href": "OPM_application_nominal.html#the-agents-initial-state-of-knowledge",
    "title": "32  The optimal predictor machine for glass forensics",
    "section": "32.2 The agent’s initial state of knowledge",
    "text": "32.2 The agent’s initial state of knowledge\n\nLoad the machine’s apparatus\nThe calls\nsource('code/tplotfunctions.R')\nsource('code/optimal_predictor_machine-nominal.R')\nat the beginning of this chapter loaded several R functions that implement the universal machine: they draw inferences, calculate marginal and conditional probabilities, and plot probability distributions.\n\n\nLearning the background information\nOur agent at the moment doesn’t know anything at all, not even about the existence of the population above. If we were to ask it anything, we would just get a blank stare back.\nLet us give it the basic background information about the population: the variates’ names and domains. We do this through the function finfo(): it has a data argument, which we omit for the moment, and a metadata argument. The latter can simply be the name of the file containing the metadata (NB: this file must have a specific format):\n\npriorknowledge &lt;- buildK(metadata='datasets/glass_metadata-4_lev.csv')\n\nThe agent now possesses this basic background knowlege, encoded in the priorknowledge object. The encoding uses a particular mathematical representation which, however, is of no interest to us1. Other representations could also be used, but the knowledge would be the same. Think of this as encoding an image into a png or other lossless format: the representation of the image would be different, but the image would be the same.1 If you’re curious you can have a glimpse at it with the command str(priorknowledge), which displays structural information about an object.\n\n\nPreliminary inferences about the population\nNow the agent knows about the population, variates, and domains. But it has not seen any data, that is, the variate values for some units. Yet we can ask it some questions and to draw some inferences. Remember that the answer to a question is not just a value: it is the collection of all possible values, with a probability assigned to each. If the actual value is known, then it will have probability 1, and all others probability 0.\nLet’s ask the agent: what is the marginal frequency distribution for the variate \\(\\mathit{Type}\\), in the full (infinite!) population? Obviously the agent doesn’t know what the actual distribution is, nor do we. It will calculate a probability distribution over all possible marginal frequency distributions.\nThis probability distribution for the \\(\\mathit{Type}\\) variate is calculated by the function fmarginal(). It has arguments finfo: the agent’s information; and variates: the names of the variates of which we want the marginal frequencies:\n\npriorknowledge_type &lt;- fprobability(K=priorknowledge, marginal='Type', Kout=TRUE)\n\nThe answer is stored in the object priorknowledge_type, which now contains only information pertinent to the \\(\\mathit{Type}\\) variate.\nWe would like to visualize this probability distribution over marginal frequency distributions. A complication is that we would need infinite dimensions to visualize this faithfully. One approximate way to represent this probability distribution is by showing, say, 100 representative samples from it. The idea is the same as for a scatter plot (§  14.4). In this case we would then have 100 different frequency distributions for the variate \\(\\mathit{Type}\\).\nThe function plotsamples1D() does this kind of visual representation. It has arguments finfo: the object encoding the probability distribution; n (default 100): the number of samples to show; and predict, which for the moment we set to FALSE and discuss in a moment.\nHow do you think this probability distribution will look like? what kind of marginal frequencies we do expect in the full population?\n\nplotsamples1D(K=priorknowledge_type, n=100, predict=FALSE)\n\n\n\n\nYou see that anything goes: Some frequency distributions give frequency almost 1 to a specific value, and almost 0 to the others. Other frequency distributions spread out the frequencies more evenly, with some peaks here or there.\nThis is a meaningful answer, because the agent hasn’t seen any data. From its point of view, everything is possible in this population.\n@@ TODO: add representation as quantiles\n\n\nPreliminary inferences about units\nUp to now the agent has drawn an inference regarding the full population, not regarding any specific unit. Now let’s ask it: what will be the value of the \\(\\mathit{Type}\\) variate in the next unit, or glass fragment, we observe? As usual, an answer consists in a probability distribution over all possible values.\n\n\n\n\n\n\n Exercise\n\n\n\nBefore continuing, ask yourself the same question: which probabilities would you give to the \\(\\mathit{Type}\\) values for the next unit, given that you only know that this quantity has seven possible values?\n\n\nThe agent’s answer this time is a probability distribution over seven values, which we can draw faithfully. The function plotsamples1D() can draw this probability as well, if we give the argument predict=TRUE (default):\n\nplotsamples1D(K=priorknowledge_type)\n\n\n\n\nThis plot shows the probability distribution for the next unit in blue, together with a sample of 100 possible frequency distributions for the \\(\\mathit{Type}\\) variate over the full population. Note that samples are drawn anew every time, so they can look somewhat differently from time to time.22 To have reproducible plots, use set.seed(314) (or any integer you like) before calling the plot function.\nThe agent’s answer is that in the next unit we can observe any \\(\\mathit{Type}\\) value with equal probability. Do you think this is a reasonable answer?\nThe plot above and the information it represents are very useful for inference purposes: not only they give the probability for the next observation, but also an idea of how such a probability could change in the future, as more data are collected and knowledge of the population’s frequency distribution becomes more precise.\n\n\n\n\n\n\n Exercise\n\n\n\nInspect the agent’s inferences for other variates."
  },
  {
    "objectID": "OPM_application_nominal.html#the-agent-learns-from-data",
    "href": "OPM_application_nominal.html#the-agent-learns-from-data",
    "title": "32  The optimal predictor machine for glass forensics",
    "section": "32.3 The agent learns from data",
    "text": "32.3 The agent learns from data\n\nLearning from the sample data\nNow let’s give the agent the data from the sample of 214 glass fragments. This is done again with the buildK() function, but providing the data argument, which can be the name of the data file:\n\npostknowledge &lt;- buildK(data='datasets/glass_data-4_lev.csv', metadata='datasets/glass_metadata-4_lev.csv')\n\nThe postknowledge object contains the agent’s knowledge from the metadata and the sample data. This object can be used in the same way as the object representing the agent’s background knowledge.\n\n\nInferences about the population\nNow that the agent has learned from the data, we can ask it again what is the marginal frequency distribution for the variate \\(\\mathit{Type}\\), in the full population.\nWe calculate the probability for the possible marginal frequency distributions, and then plot it as a set of 100 representative samples:\n\npostknowledge_type &lt;- fprobability(K=postknowledge, marginal='Type', Kout=TRUE)\n\nplotsamples1D(K=postknowledge_type, predict=FALSE)\n\n\n\n\nThis plot shows two important aspects of this probability distribution and of the agent’s current state of knowledge:\n\nNot anything goes anymore. Some frequency distributions are clearly “excluded”, or more precisely they are extremely improbable. The most probable frequency distributions have a maximum at the \\({\\small\\verb;T2;}\\) value, another lower peak for the value \\({\\small\\verb;T4;}\\), and several other qualitative features that can be glimpsed from the plot.\nYet, the agent still has a degree of uncertainty, qualitatively shown by the width of the “bands” of frequency distributions. For example, the frequency for the \\({\\small\\verb;T2;}\\) value could be \\(0.40\\) as well as \\(0.30\\)\n\n@@ TODO: add code with function that reports the exact probabilities of the frequencies.\nAnd there are other more specific aspects that can be found by visual inspection. For instance:\n\nSome frequency distributions have their absolute maximum at \\({\\small\\verb;T1;}\\), and a lower value at \\({\\small\\verb;T2;}\\). Vice versa, others have their absolute maximum at \\({\\small\\verb;T2;}\\) and a lower value at \\({\\small\\verb;T1;}\\). So there’s still uncertainty as to which value is the most frequent in the full population.\nThe agent gives a very small but non-zero frequency to the value \\({\\small\\verb;T7;}\\). Yet, the data have no units at all with the \\({\\small\\verb;T7;}\\) value. Even if the agent has never seen this value in the data it was given, it knows nevertheless, from the metadata, that this is a possible value. So the agent doesn’t dogmatically say that its frequency in the full population should be zero (as in the sample); only that it should be extremely low.\n\n\n\n\n\n\n\n Exercise\n\n\n\nGiven that the value \\({\\small\\verb;T7;}\\) has not been observed in 214 units, what approximate upper bound to its frequency would you give? Does the agent’s inference agree with your intuition?\n\n\n\n\nInferences about units\nFinally we ask the agent what \\(\\mathit{Type}\\) value we should observe in the next glass fragment. The probability distribution answering this question is plotted by the same function with the argument predict=TRUE (default), as before:\n\nplotsamples1D(K=postknowledge_type)\n\n\n\n\nFigure 32.1: Frequency distributions for full population, and probability distribution for next unit"
  },
  {
    "objectID": "OPM_application_nominal.html#conditional-discriminative-or-supervised-learning-inferences",
    "href": "OPM_application_nominal.html#conditional-discriminative-or-supervised-learning-inferences",
    "title": "32  The optimal predictor machine for glass forensics",
    "section": "32.4 Conditional, “discriminative” or “supervised-learning” inferences",
    "text": "32.4 Conditional, “discriminative” or “supervised-learning” inferences\nThe inferences about a new units that the agent has made so far were of an “unsupervised-learning” or “generative” kind ([§ sec-3-connection-ML]): the agent did not receive or use any partial information about a new unit. Let’s now try a “supervised-learning” or “discriminative” kind of inference.\nImagine that we are at a new crime scene, a glass fragment is recovered, and tests are made about its refractive index and chemical composition. The following values are found, referred to the levels of our variates:\n\n\n\n\nnewfragment &lt;- c(RI=2, Na=2,  Mg=3,  Al=2,  Si=3,  K=1, Ca=2,  Ba=1,  Fe=1)\n\nThe detectives would like to know what’s the possible origin of this fragment, that is, its \\(\\mathit{Type}\\). Our agent can draw this inference.\nFirst, the agent can calculate the probability distribution over the conditional frequencies (§  22.2) of the \\(\\mathit{Type}\\) values for the subpopulation (§  22.1) of units having the specific variate values above. This calculation is done with the function fconditional(), with arguments finfo: the agent’s current knowledge, and unitdata: the partial data obtained from the unit.\n\ncondknowledge_type &lt;- fprobability(K=postknowledge, marginal='Type', conditional=newfragment, Kout=TRUE)\n\nThe condknowledge object contains the agent’s knowledge conditional on the variates given; this knowledge is about the remaining variates, which in this case are the single variate \\(\\mathit{Type}\\) (so the fmarginal() calculation is actually redundant in this case).\nSecond, the agent can calculate the probability distribution for the \\(\\mathit{Type}\\) values of this particular glass fragment, given the above information.\nBoth inferences can be visualized in the usual way:\n\nplotsamples1D(K=condknowledge_type)\n\n\n\n\nFigure 32.2: Conditional frequency distributions for full population, and conditional probability distribution for next unit\n\n\n\n\nThe agent thus gives a probability around \\(80\\%\\) to the fragment’s being of \\({\\small\\verb;T1;}\\) type, around \\(10\\%\\) of being \\({\\small\\verb;T2;}\\) type, and around \\(5\\%\\) of being \\({\\small\\verb;T5;}\\) type. It also shows that further training data could change these probabilities by even \\(\\pm 10\\%\\) or even \\(\\pm 15\\%\\).\nNote how the possible conditional frequency distributions for \\(\\mathit{Type}\\) and the probability distribution differ from the unconditional ones shown in fig.  32.1. The global maximum, \\(80\\%\\), is now sharper than the one for the unconditional case, \\(35\\%\\). This means that knowledge of the other variates for the present fragment has decreased the agent’s uncertainty as regards its type.\nShould the crime investigation proceed on the assumption that the fragment is of \\({\\small\\verb;T1;}\\) type? No, not necessarily. The best decision depends on the gains and costs involved in making correct or wrong assumptions. To this last decision-making problem we turn next.\n\n\n\n\n\n\n Exercise\n\n\n\n\nPerform the “discriminative” inferences above, but omitting from the newfragment data one variate in turn; so first omit only \\(\\mathit{RI}\\) and do the inferences, then omit only \\(\\mathit{Na}\\) and do the inferences, and so on.\nIn each of these inferences you will find that the agent becomes more ore less “confident” about the fragment type. Which of the variates above seem to be most important for making a confident inference?"
  },
  {
    "objectID": "making_decisions.html#decisions-possible-situations-and-consequences",
    "href": "making_decisions.html#decisions-possible-situations-and-consequences",
    "title": "33  Making decisions",
    "section": "33.1 Decisions, possible situations, and consequences",
    "text": "33.1 Decisions, possible situations, and consequences"
  },
  {
    "objectID": "making_decisions.html#gains-and-losses-utilities",
    "href": "making_decisions.html#gains-and-losses-utilities",
    "title": "33  Making decisions",
    "section": "33.2 Gains and losses: utilities",
    "text": "33.2 Gains and losses: utilities\n\nFactors that enter utility quantification\nUtilities can rarely be assigned a priori."
  },
  {
    "objectID": "making_decisions.html#making-decisions-under-uncertainty-maximization-of-expected-utility",
    "href": "making_decisions.html#making-decisions-under-uncertainty-maximization-of-expected-utility",
    "title": "33  Making decisions",
    "section": "33.3 Making decisions under uncertainty: maximization of expected utility",
    "text": "33.3 Making decisions under uncertainty: maximization of expected utility"
  },
  {
    "objectID": "missing_parts.html",
    "href": "missing_parts.html",
    "title": "To be deleted",
    "section": "",
    "text": "A general inference problem\nSo far we have been jumping back and forth between “inference” and “data”, keeping the two topics somewhat separated. Now we shall finally combine them, to solve inference problems typically encountered in machine learning. Before continuing, let us summarize what we have learned so far, also to motivate why those two topics have been kept separated."
  },
  {
    "objectID": "missing_parts.html#induction",
    "href": "missing_parts.html#induction",
    "title": "To be deleted",
    "section": "Induction",
    "text": "Induction\nIt is a fact of everyday experience that we expect things to happen in particular ways because of our previous experience with similar things. If we observe that ten electronic components in a row come out damaged from an assembly line, then we expect that the next one will be damaged as well. If you regularly go hiking in a particular area and often see deer, and sometimes hares, then you think it likely to see deer also at your next hike and, if you’re lucky, a hare. You would probably be surprised to see a moose if you had never seen one in those areas. This doesn’t only concern the future: if someone asks you whether you saw a deer during a hike from long time ago, which you don’t quite remember anymore, you’d say “probably I did”.\nThis familiar experience is called induction, especially in the philosophical literature. It has generated a lot of thought and research since the times of Hume (18th century), who apparently was the first to ask how and why this experience is possible.\nBut this kind of regularities does not have clear-cut circumstances. In some cases it is not clear what should be considered “similar” things – in fact, sometimes we circularly reverse the reasoning: if something does not respect an observed regularity then we say it wasn’t “similar”. Also the circumstances in which this regularity should occur are often not-well-defined: should you expect to see deer if you hike in a somewhat different area?\nAnd sometimes this kind of regularities simply fails. Something expected doesn’t happen any longer, even if the circumstances and the “similarity” are clearly the same. Jeffreys aptly said:\n\nA common argument for induction is that induction has always worked in the past and therefore may be expected to hold in the future. It has been objected that this is itself an inductive argumentand cannot be used in support of induction. What is hardly ever mentioned is that induction has often failed in the past and that progress in science is very largely the consequence of direct attention to instances where the inductive method has led to incorrect predictions.\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nChapter I of Scientific Inference (3rd ed.) is an extremely insightful reading about inference, probability, and science.\n\n\n\nInduction and data science\nInduction, with the mentioned caveats that accompany it, is at the basis of how we approach and solve many engineering problems, and at the heart of data science: from data about particular things or phenomena we try to infer new or old instances of “similar” things or phenomena.\nWe shall now see how this is done in a quantitative manner with the probability calculus. It is important to clarify what the probability calculus, and all approximate inference methods derived from it, can and cannot do:\n\n It allows us to make inductive inferences in a quantitative and guaranteed self-consistent way.\n It does not explain why there are regularities and why induction in some cases works.\n It does not tell us which things or phenomena should be considered “similar”. In fact, what’s similar and what’s not is something that we must input into the probability calculus.\n If we formulate the “similarity” and “non-similarity” of an instance as well-defined hypotheses, then the probability calculus allows us to calculate their probabilities (and make a decision as to accept, if necessary)."
  },
  {
    "objectID": "missing_parts.html#inferences-and-populations",
    "href": "missing_parts.html#inferences-and-populations",
    "title": "To be deleted",
    "section": "Inferences and populations",
    "text": "Inferences and populations\n\nStock exchange and Mars prospecting, again\nConsider the following two sketches of inference problems, related to the scenarios of §  20.1:\n\n\nStock exchange\n\nIn 100 days, the daily change in closing price of a stock has been positive 74 times, and negative 26 times, according a particular sequence; for instance:\n\n\n\nIn which of the subsequent 3 days will the closing-price change be positive, and in which negative?\n\n\n\n\n\n\n\n\nMars prospecting\n\nOf the last 100 similar-sized rocks examined in a large crater on Mars, 74 contained haematite (Y), and 26 did not (N). For instance, the data could be:\n\n\n\nWhich, among the next 3 rocks that will be examined, will contain haematite, and which will be haematite-free?\n\n\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nDiscuss:\n\nWhich of the two inferences above seems more difficult?…\n…Why? Speculate on which factors make one inference more difficult than the other.\nWhich differences and similarities do you find between the two inferences?\nWhich additional information could be important for drawing more precise inferences?\nWhich type of quantities appear in the two inferences?\n\n\n\n\n\nTranslation into sentences\nLet us remember chapter 8 that in order to set up and solve an inference problem we must first define appropriate sentences which we’ll use in probability supposals and conditionals.\nFor the two problems above it looks obvious that we can use sentences involving the language of statistical populations. We must then try to define the units and the variates. Tentatively let’s try the following definitions:\n\nStock exchange\n\n\nUnits: the problem is only sketched, so let’s assume that a precise definition can be found somewhere. There are at least 100 + 3 units, but the problem suggests that we might consider an infinite population.\nVariates: the change in closing price seems an obvious binary variate for the population. Let’s denote it \\(C\\), with domain \\(\\set{\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}},\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}}\\).\n\n\nMars prospecting\n\n\nUnits: again the problem is only sketched, so let’s assume that a precise description exists of how the rocks to be examined should be chosen. There are at least 100 + 3 units, but the problem suggests that we might consider a practically infinite population.\nVariates: the presence of haematite is an obvious binary variate. Let’s denote it with \\(H\\), with domain \\(\\set{\\color[RGB]{34,136,51}{\\small\\verb;Y;},\\color[RGB]{238,102,119}{\\small\\verb;N;}}\\).\n\n\n\nThe population data can be represented as follows, where question marks ? indicate the units about which we want to draw inferences, and the ellipses “…” indicate that the populations possibly extend to infinite other units:\n\n\n\n\nTable 1: Stock exchange\n\n\nunit\n\\(C\\)\n\n\n\n\n…\n…\n\n\n1\n\\(\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\)\n\n\n2\n\\(\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\)\n\n\n3\n\\(\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\)\n\n\n4\n\\(\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\)\n\n\n5\n\\(\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\)\n\n\n6\n\\(\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\)\n\n\n(omitted)\n(omitted)\n\n\n95\n\\(\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\)\n\n\n96\n\\(\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\)\n\n\n97\n\\(\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\)\n\n\n98\n\\(\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\)\n\n\n99\n\\(\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\)\n\n\n100\n\\(\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\)\n\n\n101\n?\n\n\n102\n?\n\n\n103\n?\n\n\n…\n…\n\n\n\n\n\n\n\n\n\nTable 2: Mars prospecting\n\n\nunit\n\\(H\\)\n\n\n\n\n…\n…\n\n\n1\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n2\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n3\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\)\n\n\n4\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n5\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n6\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n(omitted)\n(omitted)\n\n\n95\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\)\n\n\n96\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n97\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\)\n\n\n98\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n99\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n100\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n101\n?\n\n\n102\n?\n\n\n103\n?\n\n\n…\n…\n\n\n\n\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nWould you define these two populations in a different way? Do you think other variates should be included, for example?\n\n\n\n\nDesired probabilities\nThe stock-exchange problem asks “In which of the subsequent 3 days will the closing-price change be positive, and in which negative?”. In terms of the populations just introduced, this can be translated into:\n\n“will the \\(C\\) variate for units #101, #102, #103 have value \\(\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\) or \\(\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\)?”\nIn other words, we are asking which of the two following mutually exclusive sentences, expressed symbolically, will be true:\n\\[\nC_{101}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\qquad C_{101}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\n\\]\nSince one of them must be true, their probabilities form a probability distribution (§  14), in which for the moment we omit the conditional:\n\\[\n\\mathrm{P}\\bigl(C_{101}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}}  \\quad\\quad\n\\mathrm{P}\\bigl(C_{101}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}}\n\\]\nAnalogously for units #102 and #103, leading to two more probability distributions.\nWhen we and together sentences for the three units we obtain a joint probability distribution (§  15) over 2³ = 8 mutually exclusive and exhaustive composite sentences:\n\\[\\begin{aligned}\n&\\mathrm{P}\\bigl(C_{101}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}C_{102}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}C_{103}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\n\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}}\n\\\\[1ex]\n&\\mathrm{P}\\bigl(C_{101}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}C_{102}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}C_{103}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\n\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}}\n\\\\[1ex]\n&\\dotso\n\\\\[1ex]\n&\\mathrm{P}\\bigl(C_{101}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}C_{102}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}C_{103}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\n\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}}\n\\end{aligned}\\]\n\n\nNow let’s focus on the conditional. The information we are given consists of the values of the \\(C\\) variate for units #1 to #100, which we can and together. We must also and all other information implicit in the stock-exchange problem, which we denote \\(\\mathsfit{I}_{\\text{s}}\\):\n\\[\n\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}} C_{100}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}C_{99}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsc \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nC_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}C_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}C_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{s}} \\bigr)\n\\]\nWe finally arrive at the eight probabilities (which constitute a probability distribution) that we want to calculate:\n\n\\[\\begin{aligned}\n&\\mathrm{P}\\bigl(C_{101}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}C_{102}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}C_{103}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\n\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}} C_{100}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}C_{99}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsc \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nC_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}C_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}C_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{s}} \\bigr)\n\\\\[1ex]\n&\\mathrm{P}\\bigl(C_{101}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}C_{102}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}C_{103}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\n\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}} C_{100}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}C_{99}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsc \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nC_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}C_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}C_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{s}} \\bigr)\n\\\\[1ex]\n&\\dotso\n\\\\[1ex]\n&\\mathrm{P}\\bigl(C_{101}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}C_{102}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}C_{103}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\n\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}} C_{100}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}C_{99}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsc \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nC_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}C_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}C_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{s}} \\bigr)\n\\end{aligned}\\]\n\n\n\n\n\n\n\n Exercise\n\n\n\nDo a similar analysis for the Mars-prospecting problem, and write down the final probability distribution that we wish to calculate.\n\n\n\n\nThe next – fundamental – question is: how do we calculate these probabilities? Which other probabilities can we take as our starting point?"
  },
  {
    "objectID": "missing_parts.html#sec-recap",
    "href": "missing_parts.html#sec-recap",
    "title": "To be deleted",
    "section": "Recap",
    "text": "Recap\nOn the “inference side” we gave a concrete and operational explanation of what “drawing an inference” means: it is the calculation of the probabilities – the degrees of belief – of some sentences, from the probabilities of others. This means that in principle we can draw inferences and apply the probability calculus to literally anything that can be expressed by language.\nWe also saw the following points:\n\nThe calculation of probabilities only uses four fundamental – and mathematically quite simple – rules (§  8.4). Any inference, even those make by the most complex machine-learning algorithms, is just a repeated application of those four rules (sometimes with approximating shortcuts for the sake of speed).\nNo inference can be drawn unless some probabilities are first posited as a starting point. This is just another face of the formal-logic fact that no theorem (besides tautologies) can be derived by logic, unless some axioms are given first.\nThe four fundamental rules are determined by basic requirements of logical consistency. Modifying the rules would lead to inconsistent inferences and sub-optimal decisions.\n\n\n\n\n\n\n\n\n\nflowchart BT\n  A[quantity] --o B([sentences]) --&gt; C{{probability calculus}}\n  D[value] ---o B\n  E[population] --o B\n  F[...] -..-o B\n  G[(problem)] --x A & D & E\n  G[(problem)] -.-x F\n\n\n\n\n\nOn the “data side” we introduced a handful of notions, such as “quantity”, “having a value”, “domain”, “quantity type”, “population”, “variate”, “frequency”, and others. We learned how to use them, and paid attention to some of their counter-intuitive properties.\nThese notions allow us to speak, in a precise way, about typical situations and problems that arise in engineering and other scientific contexts. In essence we have introduced a particular kind of sentences to express engineering problems – so that we can apply the probability calculus to them and draw inferences about them.\nThis specific language has its roots in physics and mathematics, where it has been successfully used and refined for several centuries. But it might evolve in different directions in the future, to make allowance for new inferential problems. Later we shall indeed expand it in a couple of directions to cover particular operations that we do with data.\n\n\nTo solve any inference problem, or to design an AI agent for solving an inference problem, all we have to do in principle is to repeatedly use the fundamental laws of inference of §  8.4.\nIn many cases an in-principle application of the inference laws is computationally impossible, however. Approximate calculations and premises are then used, which are sometimes quite drastic. The approximations used depend on the nature of the inference problem. This leads to many “recipes”, which may look extremely different from one another, being used in specialized fields and applications. People working in those fields are trained to use those recipes.\n\n\n Technological advances such as quantum computing may make the use of more powerful exact inference methods possible – for those who know them.\nFor a data-science engineer it is important to keep in mind that these recipes are only approximations, and that there are only a couple of principles underlying all of them, despite their diversity. The principled, maximally optimal solution should always be kept in sight. Technological advances continually allow us to make computations that were previously impossible – think of “quantum computers” these days. A truly optimal in-principle solution, preferable to a sub-optimal approximation, can suddenly become accessible – to those who know it!\n\n\nWe shall now introduce a very broad class of inference problems. They include tasks performed by most common machine-learning algorithms: classification and regression, “supervised” and “unsupervised” learning, and similar. After studying the principled solution to this class of problems we’ll discuss present-day approximations to it."
  },
  {
    "objectID": "missing_parts.html#induction-1",
    "href": "missing_parts.html#induction-1",
    "title": "To be deleted",
    "section": "Induction",
    "text": "Induction\nIt is a fact of everyday experience that we expect things to happen in particular ways because of our previous experience with similar things. If we observe that ten electronic components in a row come out damaged from an assembly line, then we expect that the next one will be damaged as well. If you regularly go hiking in a particular area and often see deer, and sometimes hares, then you think it likely to see deer also at your next hike and, if you’re lucky, a hare. You would probably be surprised to see a moose if you had never seen one in those areas. This doesn’t only concern the future: if someone asks you whether you saw a deer during a hike from long time ago, which you don’t quite remember anymore, you’d say “probably I did”.\nThis familiar experience is called induction, especially in the philosophical literature. It has generated a lot of thought and research since the times of Hume (18th century), who apparently was the first to ask how and why this experience is possible.\nBut this kind of regularities does not have clear-cut circumstances. In some cases it is not clear what should be considered “similar” things – in fact, sometimes we circularly reverse the reasoning: if something does not respect an observed regularity then we say it wasn’t “similar”. Also the circumstances in which this regularity should occur are often not-well-defined: should you expect to see deer if you hike in a somewhat different area?\nAnd sometimes this kind of regularities simply fails. Something expected doesn’t happen any longer, even if the circumstances and the “similarity” are clearly the same. Jeffreys aptly said:\n\nA common argument for induction is that induction has always worked in the past and therefore may be expected to hold in the future. It has been objected that this is itself an inductive argumentand cannot be used in support of induction. What is hardly ever mentioned is that induction has often failed in the past and that progress in science is very largely the consequence of direct attention to instances where the inductive method has led to incorrect predictions.\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nChapter I of Scientific Inference (3rd ed.) is an extremely insightful reading about inference, probability, and science.\n\n\n\nInduction and data science\nInduction, with the mentioned caveats that accompany it, is at the basis of how we approach and solve many engineering problems, and at the heart of data science: from data about particular things or phenomena we try to infer new or old instances of “similar” things or phenomena.\nWe shall now see how this is done in a quantitative manner with the probability calculus. It is important to clarify what the probability calculus, and all approximate inference methods derived from it, can and cannot do:\n\n It allows us to make inductive inferences in a quantitative and guaranteed self-consistent way.\n It does not explain why there are regularities and why induction in some cases works.\n It does not tell us which things or phenomena should be considered “similar”. In fact, what’s similar and what’s not is something that we must input into the probability calculus.\n If we formulate the “similarity” and “non-similarity” of an instance as well-defined hypotheses, then the probability calculus allows us to calculate their probabilities (and make a decision as to accept, if necessary)."
  },
  {
    "objectID": "missing_parts.html#sec-two-populations",
    "href": "missing_parts.html#sec-two-populations",
    "title": "To be deleted",
    "section": "Inferences and populations",
    "text": "Inferences and populations\n\nStock exchange and Mars prospecting, again\nConsider the following two sketches of inference problems, related to the scenarios of §  20.1:\n\n\nStock exchange\n\nIn 100 days, the daily change in closing price of a stock has been positive 74 times, and negative 26 times, according a particular sequence; for instance:\n\n\n\nIn which of the subsequent 3 days will the closing-price change be positive, and in which negative?\n\n\n\n\n \n\n\nMars prospecting\n\nOf the last 100 similar-sized rocks examined in a large crater on Mars, 74 contained haematite (Y), and 26 did not (N). For instance, the data could be:\n\n\n\nWhich, among the next 3 rocks that will be examined, will contain haematite, and which will be haematite-free?\n\n\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nDiscuss:\n\nWhich of the two inferences above seems more difficult?…\n…Why? Speculate on which factors make one inference more difficult than the other.\nWhich differences and similarities do you find between the two inferences?\nWhich additional information could be important for drawing more precise inferences?\nWhich type of quantities appear in the two inferences?\n\n\n\n\n\nTranslation into sentences\nLet us remember chapter 8 that in order to set up and solve an inference problem we must first define appropriate sentences which we’ll use in probability supposals and conditionals.\nFor the two problems above it looks obvious that we can use sentences involving the language of statistical populations. We must then try to define the units and the variates. Tentatively let’s try the following definitions:\n\nStock exchange\n\n\nUnits: the problem is only sketched, so let’s assume that a precise definition can be found somewhere. There are at least 100 + 3 units, but the problem suggests that we might consider an infinite population.\nVariates: the change in closing price seems an obvious binary variate for the population. Let’s denote it \\(C\\), with domain \\(\\set{\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}},\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}}\\).\n\n\nMars prospecting\n\n\nUnits: again the problem is only sketched, so let’s assume that a precise description exists of how the rocks to be examined should be chosen. There are at least 100 + 3 units, but the problem suggests that we might consider a practically infinite population.\nVariates: the presence of haematite is an obvious binary variate. Let’s denote it with \\(H\\), with domain \\(\\set{\\color[RGB]{34,136,51}{\\small\\verb;Y;},\\color[RGB]{238,102,119}{\\small\\verb;N;}}\\).\n\n\n\nThe population data can be represented as follows, where question marks ? indicate the units about which we want to draw inferences, and the ellipses “…” indicate that the populations possibly extend to infinite other units:\n\n\n\n\nTable 1.1: Stock exchange\n\n\nunit\n\\(C\\)\n\n\n\n\n…\n…\n\n\n1\n\\(\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\)\n\n\n2\n\\(\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\)\n\n\n3\n\\(\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\)\n\n\n4\n\\(\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\)\n\n\n5\n\\(\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\)\n\n\n6\n\\(\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\)\n\n\n(omitted)\n(omitted)\n\n\n95\n\\(\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\)\n\n\n96\n\\(\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\)\n\n\n97\n\\(\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\)\n\n\n98\n\\(\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\)\n\n\n99\n\\(\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\)\n\n\n100\n\\(\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\)\n\n\n101\n?\n\n\n102\n?\n\n\n103\n?\n\n\n…\n…\n\n\n\n\n\n\n\n\n\nTable 1.2: Mars prospecting\n\n\nunit\n\\(H\\)\n\n\n\n\n…\n…\n\n\n1\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n2\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n3\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\)\n\n\n4\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n5\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n6\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n(omitted)\n(omitted)\n\n\n95\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\)\n\n\n96\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n97\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\)\n\n\n98\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n99\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n100\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n101\n?\n\n\n102\n?\n\n\n103\n?\n\n\n…\n…\n\n\n\n\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nWould you define these two populations in a different way? Do you think other variates should be included, for example?\n\n\n\n\nDesired probabilities\nThe stock-exchange problem asks “In which of the subsequent 3 days will the closing-price change be positive, and in which negative?”. In terms of the populations just introduced, this can be translated into:\n\n“will the \\(C\\) variate for units #101, #102, #103 have value \\(\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\) or \\(\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\)?”\nIn other words, we are asking which of the two following mutually exclusive sentences, expressed symbolically, will be true:\n\\[\nC_{101}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\qquad C_{101}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\n\\]\nSince one of them must be true, their probabilities form a probability distribution (§  14), in which for the moment we omit the conditional:\n\\[\n\\mathrm{P}\\bigl(C_{101}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}}  \\quad\\quad\n\\mathrm{P}\\bigl(C_{101}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}}\n\\]\nAnalogously for units #102 and #103, leading to two more probability distributions.\nWhen we and together sentences for the three units we obtain a joint probability distribution (§  15) over 2³ = 8 mutually exclusive and exhaustive composite sentences:\n\\[\\begin{aligned}\n&\\mathrm{P}\\bigl(C_{101}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}C_{102}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}C_{103}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\n\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}}\n\\\\[1ex]\n&\\mathrm{P}\\bigl(C_{101}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}C_{102}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}C_{103}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\n\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}}\n\\\\[1ex]\n&\\dotso\n\\\\[1ex]\n&\\mathrm{P}\\bigl(C_{101}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}C_{102}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}C_{103}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\n\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}}\n\\end{aligned}\\]\n\n\nNow let’s focus on the conditional. The information we are given consists of the values of the \\(C\\) variate for units #1 to #100, which we can and together. We must also and all other information implicit in the stock-exchange problem, which we denote \\(\\mathsfit{I}_{\\text{s}}\\):\n\\[\n\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}} C_{100}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}C_{99}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsc \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nC_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}C_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}C_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{s}} \\bigr)\n\\]\nWe finally arrive at the eight probabilities (which constitute a probability distribution) that we want to calculate:\n\n\\[\\begin{aligned}\n&\\mathrm{P}\\bigl(C_{101}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}C_{102}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}C_{103}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\n\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}} C_{100}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}C_{99}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsc \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nC_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}C_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}C_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{s}} \\bigr)\n\\\\[1ex]\n&\\mathrm{P}\\bigl(C_{101}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}C_{102}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}C_{103}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\n\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}} C_{100}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}C_{99}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsc \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nC_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}C_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}C_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{s}} \\bigr)\n\\\\[1ex]\n&\\dotso\n\\\\[1ex]\n&\\mathrm{P}\\bigl(C_{101}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}C_{102}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}C_{103}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\n\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}} C_{100}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}C_{99}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsc \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nC_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}C_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}C_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{s}} \\bigr)\n\\end{aligned}\\]\n\n\n\n\n\n\n\n Exercise\n\n\n\nDo a similar analysis for the Mars-prospecting problem, and write down the final probability distribution that we wish to calculate.\n\n\n\n\nThe next – fundamental – question is: how do we calculate these probabilities? Which other probabilities can we take as our starting point?\n\n\nThe stock-exchange and Mars-prospecting inference problems of §  1.3 differ in some aspects and are similar in others. Time, for example, seems an important aspect of the first inference. Yet, the second inference involves or could involve time as well: the rocks could be inspected sequentially. So more precisely it is the time ordering that seems more relevant in the first inference than the second.\nAn important difference between the two inference problems is revealed by the following thought-experiments. Consider how you would answer in the stock-exchange case and in the Mars-prospecting case:\n\nIt turns out that there was an error in the sequence of positive and negative datapoints, although not in the total amounts of positive and negative ones. In the stock-exchange inference, for instance, days 1 (\\(-\\)) and 3 (\\(+\\)) were erroneously swapped, as were days 100 (\\(+\\)) and 85 (\\(-\\)), and many other days; in the Mars-prospecting inference, rocks #1 (Y) and #3 (N) were erroneously swapped, as were rocks #100 (Y) and #97 (N), and many other rocks.\n\nShould the final inference about the next 3 datapoints be changed?\n\nThe information about the exact sequence of data is lost, and only the total amount of positive and negative datapoints is preserved (74 vs 26).\n\nWould the final inference about the next 3 datapoints be different, compared to the situation where the exact data sequence is known?\n\nA new inference is requested: not about the 3 datapoints following the known data, but the 3 datapoints preceding the known data: the day before day 1 and so on, or the rock before rock #1 and so on. (Or, generalizing: an inference about 3 datapoints interspersed among the known ones is requested.)\n\nWould the inference for the preceding 3 datapoints be different from the original one about the subsequent 3 datapoints?\n\n\nIn all these thought-experiments, our intuition is that the inference for the stock-exchange problem would be different, but the one for the Mars-prospecting problem would stay the same or not change appreciably. Swapping days in a stock course, matters; swapping rocks in a crater, doesn’t. There are good reasons, based on physics and dynamical-system theory, for this intuition.\nAn inference which does not change if data and unknown quantities are freely reordered, like the Mars-prospecting one, is called exchangeable. Whereas an inference in which the ordering of data and unknowns is relevant, like the stock-exchange one, is called non-exchangeable. There is not a dichotomy between the two cases; rather, there are continuous varying degrees of exchangeability or non-exchangeability. But for the moment we shall simplify this gradation into a binary distinction.\nWe shall now make this definition more precise, and then study and exploit the remarkable consequences that it has for an agent’s inferences and probability calculations."
  },
  {
    "objectID": "missing_parts.html#sec-exch-populations",
    "href": "missing_parts.html#sec-exch-populations",
    "title": "To be deleted",
    "section": "Infinitely exchangeable populations",
    "text": "Infinitely exchangeable populations\n\nDefinition\nConsider a (practically) infinite statistical population with variate \\(X\\). This variate could be a joint one, consisting of variates \\(U,V,W,\\dots\\) of arbitrary types. For simplicity let’s assume that the domain of this variate is discrete. For instance, the variate could be of a binary type with domain \\(\\set{{\\small\\verb;Yes;}, {\\small\\verb;No;}}\\); or it could be the combination of such a binary variate and an another ordinal variate with domain \\(\\set{{\\small\\verb;low;}, {\\small\\verb;medium;}, {\\small\\verb;high;}}\\); so the domain of the joint variable would be the set of 2 × 3 values \\(\\set[\\big]{({\\small\\verb;Yes;}, {\\small\\verb;low;}),\\ ({\\small\\verb;No;}, {\\small\\verb;low;}),\\ \\dotsc,\\ ({\\small\\verb;No;},{\\small\\verb;high;})}\\).\nThis variate associates a quantity to each unit in the population. We denote by \\(X_1\\) the variate for unit #1, and so on.\nNow consider an agent, with background knowledge \\(\\mathsfit{I}\\), which must draw inferences about some population units.\nWe call the infinite population exchangeable if the agent’s inferences are unaffected by the units’ identities, and only depend on the units’ variate values. Said otherwise, exchanges among units that have the same values don’t matter for inference purposes.\n\n\nExample\nLet’s make this clear with a simplified version of the Mars-prospecting example.\nSuppose the agent knows the values of units #95–#99, and needs the probability that unit #101 has variate value \\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\), unit #102 value \\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\), and unit #103 value \\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\), as schematized below:\n\n\n\n\n\nagent’s 1st inference\n\n\nunit\n\\(H\\)\n\n\n\n\n…\n…\n\n\n1\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n2\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n3\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\)\n\n\n4\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n5\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n6\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n7\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n8\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\)\n\n\n…\n…\n\n\n95\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\) \\(\\mathrlap{\\quad\\qquad\\textbf{given}}\\)\n\n\n96\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\) \\(\\mathrlap{\\quad\\qquad\\textbf{given}}\\)\n\n\n97\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\) \\(\\mathrlap{\\quad\\qquad\\textbf{given}}\\)\n\n\n98\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\) \\(\\mathrlap{\\quad\\qquad\\textbf{given}}\\)\n\n\n99\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\) \\(\\mathrlap{\\quad\\qquad\\textbf{given}}\\)\n\n\n100\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n101\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\) \\(\\mathrlap{\\quad\\qquad\\textbf{?}}\\)\n\n\n102\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\) \\(\\mathrlap{\\quad\\qquad\\textbf{?}}\\)\n\n\n103\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\) \\(\\mathrlap{\\quad\\qquad\\textbf{?}}\\)\n\n\n…\n…\n\n\n\n\n\n\n\n\\[\n\\mathrm{P}\\bigl(H_{101}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\color[RGB]{238,102,119}{\\small\\verb;N;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}H_{102}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}H_{103}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\color[RGB]{238,102,119}{\\small\\verb;N;}\n\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}}\nH_{99}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}H_{98}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nH_{97}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\color[RGB]{238,102,119}{\\small\\verb;N;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}H_{96}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nH_{95}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\color[RGB]{238,102,119}{\\small\\verb;N;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}\\bigr)\n\\]\n\n\nIf the population is exchangeable, then the inference above is exactly the same – the probability values are identical – as the following one:\n\n\n\n\n\nagent’s 2nd inference\n\n\nunit\n\\(H\\)\n\n\n\n\n…\n…\n\n\n1\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\) \\(\\mathrlap{\\quad\\qquad\\textbf{given}}\\)\n\n\n2\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n3\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\) \\(\\mathrlap{\\quad\\qquad\\textbf{?}}\\)\n\n\n4\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n5\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\) \\(\\mathrlap{\\quad\\qquad\\textbf{given}}\\)\n\n\n6\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\) \\(\\mathrlap{\\quad\\qquad\\textbf{given}}\\)\n\n\n7\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\) \\(\\mathrlap{\\quad\\qquad\\textbf{?}}\\)\n\n\n8\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\)\n\n\n…\n…\n\n\n95\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\) \\(\\mathrlap{\\quad\\qquad\\textbf{given}}\\)\n\n\n96\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n97\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\) \\(\\mathrlap{\\quad\\qquad\\textbf{?}}\\)\n\n\n98\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n99\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n100\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n101\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\)\n\n\n102\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n103\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\) \\(\\mathrlap{\\quad\\qquad\\textbf{given}}\\)\n\n\n…\n…\n\n\n\n\n\n\n\n\\[\n\\mathrm{P}\\bigl(H_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\color[RGB]{238,102,119}{\\small\\verb;N;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}H_{7}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}H_{97}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\color[RGB]{238,102,119}{\\small\\verb;N;}\n\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}}\nH_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}H_{5}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nH_{95}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\color[RGB]{238,102,119}{\\small\\verb;N;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}H_{6}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nH_{103}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\color[RGB]{238,102,119}{\\small\\verb;N;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}\\bigr)\n\\]\nWhy? Because both inferences have one \\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\) and two \\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\) in their supposals, and both have three \\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\) and two \\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\) in their conditionals. Or, from a different point of view, both inferences look the same if we reorder the units (each keeping its value), as shown below:\n\n\n\n\n2nd inference\n\n\nunit\n\\(H\\)\n\n\n\n\n…\n…\n\n\n1\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\) \\(\\mathrlap{\\quad\\quad\\textbf{given}}\\)\n\n\n2\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n3\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\) \\(\\mathrlap{\\quad\\quad\\textbf{?}}\\)\n\n\n4\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n5\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\) \\(\\mathrlap{\\quad\\quad\\textbf{given}}\\)\n\n\n6\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\) \\(\\mathrlap{\\quad\\quad\\textbf{given}}\\)\n\n\n7\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\) \\(\\mathrlap{\\quad\\quad\\textbf{?}}\\)\n\n\n8\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\)\n\n\n…\n…\n\n\n95\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\) \\(\\mathrlap{\\quad\\quad\\textbf{given}}\\)\n\n\n96\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n97\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\) \\(\\mathrlap{\\quad\\quad\\textbf{?}}\\)\n\n\n98\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n99\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n100\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n101\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\)\n\n\n102\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n103\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\) \\(\\mathrlap{\\quad\\quad\\textbf{given}}\\)\n\n\n…\n…\n\n\n\n\n\n\n\n2nd inference reordered\n\n\nunit\n\\(H\\)\n\n\n\n\n…\n…\n\n\n99\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n2\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n101\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\)\n\n\n4\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n98\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n96\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n102\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n8\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\)\n\n\n…\n…\n\n\n95\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\) \\(\\mathrlap{\\quad\\quad\\textbf{given}}\\)\n\n\n6\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\) \\(\\mathrlap{\\quad\\quad\\textbf{given}}\\)\n\n\n103\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\) \\(\\mathrlap{\\quad\\quad\\textbf{given}}\\)\n\n\n5\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\) \\(\\mathrlap{\\quad\\quad\\textbf{given}}\\)\n\n\n1\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\) \\(\\mathrlap{\\quad\\quad\\textbf{given}}\\)\n\n\n100\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n3\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\) \\(\\mathrlap{\\quad\\quad\\textbf{?}}\\)\n\n\n7\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\) \\(\\mathrlap{\\quad\\quad\\textbf{?}}\\)\n\n\n97\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\) \\(\\mathrlap{\\quad\\quad\\textbf{?}}\\)\n\n\n…\n…\n\n\n\n\n\n\n\n1st inference\n\n\nunit\n\\(H\\)\n\n\n\n\n…\n…\n\n\n1\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n2\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n3\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\)\n\n\n4\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n5\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n6\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n7\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n8\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\)\n\n\n…\n…\n\n\n95\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\) \\(\\mathrlap{\\quad\\quad\\textbf{given}}\\)\n\n\n96\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\) \\(\\mathrlap{\\quad\\quad\\textbf{given}}\\)\n\n\n97\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\) \\(\\mathrlap{\\quad\\quad\\textbf{given}}\\)\n\n\n98\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\) \\(\\mathrlap{\\quad\\quad\\textbf{given}}\\)\n\n\n99\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\) \\(\\mathrlap{\\quad\\quad\\textbf{given}}\\)\n\n\n100\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\)\n\n\n101\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\) \\(\\mathrlap{\\quad\\quad\\textbf{?}}\\)\n\n\n102\n\\(\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\) \\(\\mathrlap{\\quad\\quad\\textbf{?}}\\)\n\n\n103\n\\(\\color[RGB]{238,102,119}{\\small\\verb;N;}\\) \\(\\mathrlap{\\quad\\quad\\textbf{?}}\\)\n\n\n…\n…\n\n\n\n\n\n\n\n\n\n\\[\\begin{aligned}\n&\\mathrm{P}\\bigl(H_{101}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\color[RGB]{238,102,119}{\\small\\verb;N;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}H_{102}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}H_{103}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\color[RGB]{238,102,119}{\\small\\verb;N;}\n\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}}\nH_{99}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}H_{98}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nH_{97}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\color[RGB]{238,102,119}{\\small\\verb;N;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}H_{96}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nH_{95}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\color[RGB]{238,102,119}{\\small\\verb;N;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}\\bigr)\n\\\\[1ex]\n&=\\mathrm{P}\\bigl(H_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\color[RGB]{238,102,119}{\\small\\verb;N;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}H_{7}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}H_{97}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\color[RGB]{238,102,119}{\\small\\verb;N;}\n\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}}\nH_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}H_{5}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nH_{95}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\color[RGB]{238,102,119}{\\small\\verb;N;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}H_{6}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nH_{103}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\color[RGB]{238,102,119}{\\small\\verb;N;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}\\bigr)\n\\end{aligned}\\]\nThis equality under exchanges holds no matter how many units we consider in the supposal and in the conditional (the conditional could even be empty).\nAs two additional examples with the same population,\n\\[\\begin{gathered}\n\\mathrm{P}(H_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} H_{101}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\color[RGB]{238,102,119}{\\small\\verb;N;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n= \\mathrm{P}(H_{98}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} H_{8}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\color[RGB]{238,102,119}{\\small\\verb;N;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\\\[1ex]\n\\mathrm{P}(H_{99}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}H_{7}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}H_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\color[RGB]{238,102,119}{\\small\\verb;N;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n=\\mathrm{P}(H_{4}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}H_{102}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\color[RGB]{34,136,51}{\\small\\verb;Y;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}H_{95}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\color[RGB]{238,102,119}{\\small\\verb;N;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\end{gathered}\\]\n\n\nThe definition of exchangeability extends in an obvious way to populations with variates having arbitrary discrete domains. As an example consider a population with an ordinal variate \\(X\\) having domain of three values \\(\\set{\\texttt{\\small\\color[RGB]{34,136,51}low},\\texttt{\\small\\color[RGB]{102,204,238}med},\\texttt{\\small\\color[RGB]{204,187,68}high}}\\). If this population is exchangeable for an agent with state of knowledge \\(\\mathsfit{J}\\), then we must have for instance\n\\[\\begin{aligned}\n&\\mathrm{P}( X_5\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\texttt{\\small\\color[RGB]{102,204,238}med}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\texttt{\\small\\color[RGB]{204,187,68}high}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{14}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\texttt{\\small\\color[RGB]{102,204,238}med}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X_{7}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\texttt{\\small\\color[RGB]{34,136,51}low}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\texttt{\\small\\color[RGB]{102,204,238}med}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{J})\n\\\\[1ex]\n&=\n\\mathrm{P}( X_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\texttt{\\small\\color[RGB]{102,204,238}med}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_{90}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\texttt{\\small\\color[RGB]{204,187,68}high}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\texttt{\\small\\color[RGB]{102,204,238}med}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X_{7}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\texttt{\\small\\color[RGB]{34,136,51}low}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_{5}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\texttt{\\small\\color[RGB]{102,204,238}med}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{J})\n\\end{aligned}\\]\n\n\n\n\n\n\n Important points about exchangeability\n\n\n\n\nStrictly speaking, exchangeability is not an intrinsic property of a population. It is a property of an agent’s state of knowledge about the population. For instance, if an agent knows that the units’ indices reflect some temporal order, then that agent generally won’t consider that population as exchangeable. Another agent, oblivious of the fact that the units’ indices carry information, may instead consider that population as exchangeable.\nThe probability calculus doesn’t tell us if a population is exchangeable; in fact, it requires exchangeability (or non-exchangeability) as an input.\n…However, if the possibility of exchangeability and non-exchangeability are formulated as two well-defined hypotheses, the probability calculus can tell us their probabilities.\nThere isn’t any clear-cut dichotomy between exchangeable and non-exchangeable populations. Rather, the discrepancy between probabilities under exchanges of units gradually increases from practically acceptable levels to unacceptable ones. What’s acceptable depends on the particular problem.\n\n\n\n\n\n\n\n\n\n Exercise\n\n\n\n@@ TODO\n\n\n\n\nWe can finally derive a concrete way of drawing inferences in problems involving exchangeable populations. We shall approach the derivation in an intuitive way, in two steps."
  },
  {
    "objectID": "missing_parts.html#sec-inference-known-freq",
    "href": "missing_parts.html#sec-inference-known-freq",
    "title": "To be deleted",
    "section": "Inference when population frequencies are known",
    "text": "Inference when population frequencies are known\nSuppose we have a practically infinite statistical population with variate \\(X\\) having a discrete and finite domain. For simplicity let’s assume the domain consists in the integers \\(\\set{1,2,\\dotsc,K}\\) (we can always rename the actual domain values so as to put them into correspondence with a set of integers).\nAn agent with state of knowledge \\(\\mathsfit{I}\\) needs to draw inferences about some units, given the values of other units, as in the examples of the previous §  1.3 and…\nNow we suppose that the agent additionally knows the joint frequency distribution for the variate \\(X\\) in the population. We express this knowledge with the sentence \\(\\boldsymbol{F}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\), where \\(\\boldsymbol{f}\\) is a \\(K\\)-tuple of numerical frequencies. We denote the frequencies of the values \\(1,2,\\dotsc, K\\) by \\(f_1, f_2, \\dotsc, f_K\\).\nIf someone gave you the frequencies above (their exact numerical values), and asked you your degree of belief that the variate for some unit, say unit #47, has value \\(2\\), what would you answer?\nThis scenario has a very strong symmetry. A fraction \\(f_2\\) of all the units have value \\(2\\). Unit #47 could be one among those in this fraction, or one among those in the remaining  \\(1-f_2\\)  fraction. We are compelled to give probability \\(f_2\\) that unit #47 has value \\(2\\):\n\\[\n\\mathrm{P}(X_{47}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}2 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\boldsymbol{F}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}) = f_2\n\\]\nThis is just one way of looking at this scenario. Alternative ways show similar symmetries, which lead to the same degree of belief.\nMore generally, the agent’s degree of belief on some unit #\\(u\\) to have value \\(i\\), when the joint frequency distribution \\(f\\) is known, must be\n\\[\n\\mathrm{P}(X_u \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}i \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\boldsymbol{F}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}) = f_i\n\\]\n\n\nWe can extend this reasoning to more than one unit. A crucial point here is that the population is practically infinite. If we know that the frequency of value \\(2\\) is \\(f_2\\), and then we observe a unit with that value, then we know that the total number of units having value \\(2\\) has slightly decreased. In a practically infinite population, however, this decrease is negligible, so we can think of \\(f_2\\) as remaining the same.\n\n\n\n\n\n\n Exercise\n\n\n\nSuppose the population is finite, with \\(N\\) units. The absolute frequency (§  21.2) of value \\(2\\) is then \\(Nf_2\\).\n\nCalculate the new absolute and relative frequencies of value \\(2\\) after we remove one unit with that value.\nCalculate the difference between the previous and new frequency.\nDo the same calculations also for the other variate values (\\(1,3,\\dotsc\\)).\n\n\n\nWe therefore find that the agent’s degree of belief on some unit #\\(u\\) to have value \\(i\\), and some other unit #\\(v\\) to have value \\(j\\), when the joint frequency distribution \\(f\\) is known, must be\n\\[\n\\mathrm{P}(X_u \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}i \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_v\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}j \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\boldsymbol{F}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}) = f_i \\cdot f_j\n\\]\nAnd so on for more units.\n\n@@ TODO to be continued"
  }
]