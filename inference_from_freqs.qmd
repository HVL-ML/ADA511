# [Inferences from frequencies]{.green} {#sec-inference-from-freqs}
{{< include macros.qmd >}}
{{< include macros_exchangeability.qmd >}}

## If the population frequency were known {#sec-pop-freq-known}

Let's now see how the exchangeability of an agent's degrees of belief allows it to calculate probabilities about the units of a population. We shall do this calculation in two steps. First, in the case where *the agent knows the joint frequency distribution* (§§ [-@sec-freq-distr], [-@sec-joint-freq], [-@sec-limit-freqs]) *for the full population*. Second, in the more general case where the agent lacks this population-frequency information.

When the full-population frequency distribution is known, the calculation of probabilities is very intuitive and analogous to the stereotypical "drawing balls from an urn". We shall rely on this intuition; keep in mind, however, that the probabilities are not assigned "by intuition", but actually fully determined by the two basic pieces of knowledge or assumptions: *exchangeability* and *known population frequencies*. Some simple proof sketches of this will also be given.

## Notation {#sec-pop-inference-notation}

In this and the following chapters we shall stick to a particular use of some symbols, and shall often take the simplified [income]{.midgrey} dataset (file [`income_nominal_data_all.csv`](datasets/income_nominal_data_all.csv) and its underlying population as an example.

We consider an infinite population with any number of variates. For concreteness we assume these variates to have finite, discrete domains; but the formulae we obtain can be easily generalized to other kinds of variates. *We shall denote all the population variates, jointly, with $\bZ$*. In the case of the income dataset, for instance, the variate $\bZ$ stands for the joint variate with nine components:

$$
\begin{aligned}
\bZ &\defd (\blue
\mathit{workclass} \and 
\mathit{education} \and 
\mathit{marital\_status} \and 
\mathit{occupation} \and 
{}\\ &\qquad
\blue\mathit{relationship} \and
\mathit{race} \and 
\mathit{sex} \and 
\mathit{native\_country} \and 
\mathit{income}
\black)
\end{aligned}
$$

When we write $\blue Z \mo z$, the symbol $\blue z$ stands for some definite joint *values*, for instance $({\blue\cat{Without-pay} \and \cat{Doctorate} \and \dotsb \and \cat{Ireland} \and \cat{>50K}})$.

In applications typical of "supervised learning" ([§ @sec-3-connection-ML]), the agent wants to draw inferences about *some* of the  $\bZ$ variates for a new unit, conditional on the values of the *remaining* variates. We call the joint variates to be inferred the [**predictand**]{.blue}^[literally "what has to be predicted". In machine learning and other fields the terms "dependent variable", "class" or "label" (for nominal variates) are often used.], and we shall usually denote them jointly by the symbol $\rY$. We call the variates used to inform the inference the [**predictor**]{.blue}^[In machine learning and other fields the terms "independent variable" or "features" are often used], and we shall usually denote them jointly by the symbol $\gX$.


In the [income]{.midgrey} problem, for instance, the agent (some USA census agency) would like to infer the $\red\mathit{income}$ variate of a person from the other eight demographic characteristics $\green\mathit{workclass} \and \mathit{education} \and \dotsb$ of that person. So in this inference problem we define

$$
\begin{aligned}
\rY &\defd {\red\mathit{income}}
\\[1ex]
\gX &\defd ({\green\mathit{workclass} \and \mathit{education} \and \dotsb \and \mathit{sex} \and \mathit{native\_country}})
\end{aligned}
$$

We shall, however, also consider slightly different inference problems, for example with $({\red\mathit{race} \and \mathit{sex}})$ as predictand and the remaining seven variates $({\green\mathit{workclass} \and \dotsb \and \mathit{income}})$ as predictors.


## Knowing the full-population frequency distribution {#sec-know-freq}

Now suppose that the agent knows the *full-population joint frequency distribution*. Let's make clearer what this means. In the [income]{.midgrey} problem, for instance, consider these two different joint values for the joint variate $\bZ$:

<!-- `example_freqdistr.rds`, items 1 and 26 in reverse-ordered frequencies -->
$$
\begin{aligned}
{\blue z'}&\defd (
\cat{Private} \and \cat{HS-grad} \and \cat{Married-civ-spouse} \and \cat{Machine-op-inspct} \and {}
\\
&\qquad\cat{Husband} \and \cat{White} \and \cat{Male} \and \cat{United-States} \and \cat{<=50K}
)
\\[2ex]
{\blue z''}&\defd (
\cat{Self-emp-not-inc} \and \cat{HS-grad} \and \cat{Married-civ-spouse} \and \cat{Farming-fishing} \and {}
\\
&\qquad\cat{Husband} \and \cat{White} \and \cat{Male} \and \cat{United-States} \and \cat{<=50K}
)
\end{aligned}
$$

The agent knows that the value $\blue Z\mo z'$ occurs in the full population of interest (in this case all 340 millions or so USA citizens, considered in a short period of time) with a relative frequency $0.860 369\%$; it also knows that the value $\blue Z\mo z''$ occurs with a relative frequency $0.260 058\%$. We write this as follows:

$$
f({\blue Z\mo z'}) = 0.860 369\% \ ,
\qquad
f({\blue Z\mo z''}) = 0.260 058\%
$$

The agent knows not only the frequencies of the two particular joint values $\blue z'$, $\blue z''$, but for *all possible* joint values, that is, for all possible combinations of values from the single variate. In the [income]{.midgrey} example there are 54 001 920 possible combinations, and therefore just as many relative frequencies. All these frequencies together form the full-population frequency distribution for $\bZ$, which we denote collectively with $\vf$ (note the boldface). This complete frequency information is denoted by the sentence $\if$.

:::{.callout-warning}
## {{< fa exclamation-circle >}} Don't confuse the full population with a sample from it
Note that the frequencies reported above are *not* the ones found in the [`income_nominal_data_all.csv`](datasets/income_nominal_data_all.csv) dataset, because that dataset is only a *sample* from the full population, not the full population. The frequency values reported above are purely hypothetical (but not inconsistent with the frequencies observed in the sample).
:::



In other cases, these hypothetically known frequencies would refer to the full population of units: maybe even past, present, future, if they span a possibly unlimited time range.

## Inference about a single unit {#sec-1unit-freq-known}

Now imagine that the agent, given the information $\if$ about the frequencies and some background information $\yI$, must infer all $\bZ$ variates for a specific unit $\yellow u$. In the [income]{.midgrey} case, it would be an inference about a specific USA citizen. This unit $\yellow u$ could have any particular combination of variate values; in the [income]{.midgrey} case it could have any one of the 54 001 920 possible combined values. The agent must assign a probability to each of these possibilities.^[Remember that this is what we mean when we say "drawing an inference"! (See [chap. @sec-what-inference] and [§ @sec-distribute-prob])] Which probability values should it assign?

Intuitively we would say that the probability for a particular value $\blue Z\mo z$ should be equal to the frequency of that value in the full population:


:::{.callout-note style="font-size:120%"}
##
::::{style="font-size:120%"}

if $\yI$ leads to an exchangeable probability distribution, then

$$
\P({\blue Z_{\yellow u}\mo z} \| \if \and \yI) = f({\blue Z\mo z})
$$

for any unit $\yellow u$.
::::
:::



For instance, the probabilities that unit $\yellow u$ has the values $\blue z'$ or $\blue z''$ above would be

$$
\begin{aligned}
&\P(\bZ_{\yellow u}\mo {\blue z'} \| \if \and \yI) =
f(\bZ \mo {\blue z'}) = 0.860 369\%
\\[1ex]
&\P(\bZ_{\yellow u}\mo {\blue z''} \| \if \and \yI) =
f(\bZ \mo {\blue z''}) = 0.260 058\%
\end{aligned}
$$

This intuition is the same as in drawing balls, which may have different sets of labels, from an urn, given that we know the proportion of balls with each possible label set.

we 

But the equality above can actually be proven mathematically in this specific case: it follows from the assumption of exchangeability. Let's examine a very simple case to get an idea of how this proof works.

### Exact calculation of the probabilities in a simple case

Suppose we have an urn with three balls: two of them are marked $\yy$, and one $yn$. This background information $\yi[U]$ is a simple case of finite population with three units and a binary variate which we can call $B$. We know that the frequency distribution is

$$f(U\mo\yy) = 2/3 \qquad f(U\mo\yn) = 1/3$$

Let's label the first ball we'll draw #1, the second #2, and the third #3. Our information $\if$ about the frequencies corresponds to the following composite sentence:

:::{.column-page-inset-right}
$$
\if \ \equiv\ 
(B_{1}\mo\yy \and 
B_{2}\mo\yy \and 
B_{3}\mo\yn)
\lor
(B_{1}\mo\yy \and 
B_{2}\mo\yn \and 
B_{3}\mo\yy)
\lor
(B_{1}\mo\yn \and 
B_{2}\mo\yy \and 
B_{3}\mo\yy)
$$
:::

Given $\if$, we know that $\if$ is true: $\P(\if \| \if \and \yi[U])=1$, which means

:::{.column-page-inset-right}
$$
\P\bigl[(B_{1}\mo\yy \and 
B_{2}\mo\yy \and 
B_{3}\mo\yn)
\lor
(B_{1}\mo\yy \and 
B_{2}\mo\yn \and 
B_{3}\mo\yy)
\lor
(B_{1}\mo\yn \and 
B_{2}\mo\yy \and 
B_{3}\mo\yy)
\|[\big] \if, \yi[U]\bigr] = 1
$$
:::

Now use the `or`-rule, considering that the three `or`ed sentences are mutually exclusive:

:::{.column-page-right}
$$
\begin{aligned}
1&=\P\bigl[(B_{1}\mo\yy \and 
B_{2}\mo\yy \and 
B_{3}\mo\yn)
\lor
(B_{1}\mo\yy \and 
B_{2}\mo\yn \and 
B_{3}\mo\yy)
\lor
(B_{1}\mo\yn \and 
B_{2}\mo\yy \and 
B_{3}\mo\yy)
\|[\big] \if \and \yi[U]\bigr]
\\[2ex]
&=
\P(B_{1}\mo\yy \and 
B_{2}\mo\yy \and 
B_{3}\mo\yn \|\if \and \yi[U])
+{}
\\&\qquad
\P(B_{1}\mo\yy \and 
B_{2}\mo\yn \and 
B_{3}\mo\yy \|\if \and \yi[U])
+{}
\\&\qquad
\P(B_{1}\mo\yn \and 
B_{2}\mo\yy \and 
B_{3}\mo\yy \|\if \and \yi[U])
\end{aligned}
$$
:::

According to our background information $\yi[U]$, our degrees of belief are exchangeable. This means that the three probability summed up above must all have the same value, because in each of them $\yy$ appears twice and $\yn$ once. But if we are summing the same values thrice, and the sum is $1$, that that value must be $1/3$. Hence we find that

$$
\begin{aligned}
&\P(B_{1}\mo\yy \and 
B_{2}\mo\yy \and 
B_{3}\mo\yn \|\if \and \yi[U])
= 1/3
\\
&\P(B_{1}\mo\yy \and 
B_{2}\mo\yn \and 
B_{3}\mo\yy \|\if \and \yi[U])
= 1/3
\\
&\P(B_{1}\mo\yn \and 
B_{2}\mo\yy \and 
B_{3}\mo\yy \|\if \and \yi[U])
= 1/3
\\[1ex]
&\text{all other probabilities are zero}
\end{aligned}
$$

Finally let's find the probability that the first ball we draw has mark $\yy$, that is, $\P(B_1\mo\yy \| \if\and \yi[U])$. It's a marginal probability ([§ @sec-marginal-probs]), so it's given by the sum

$$
\begin{aligned}
\P(B_1\mo\yy \| \if\and \yi[U]) &=
\sum_{i=\yy}^{\yn}\sum_{j=\yy}^{\yn}
\P(B_1\mo\yy \and B_2\mo i \and B_3\mo j \| \if\and \yi[U])
\\[1ex]
&=
\P(B_1\mo\yy \and B_2\mo\yy \and B_3\mo\yy \| \if\and \yi[U]) + {}
\\ &\qquad
\P(B_1\mo\yy \and B_2\mo\yy \and B_3\mo\yn \| \if\and \yi[U]) + {}
\\ &\qquad
\P(B_1\mo\yy \and B_2\mo\yn \and B_3\mo\yy \| \if\and \yi[U]) + {}
\\ &\qquad
\P(B_1\mo\yy \and B_2\mo\yn \and B_3\mo\yn \| \if\and \yi[U])
\\[1ex]
&= 0 + 1/3 + 1/3 + 0
\\[1ex]
&= 2/3
\end{aligned}
$$

which is indeed equal to $f(B\mo\yy)$.

\

This simple example gives you an idea why our intuition for equating -- in specific circumstances -- probability with full-population frequencies, is actually a mathematical theorem: it follows from (1) knowledge of the full-population frequencies, and (2) **exchangeability**.

:::{.callout-caution}
## {{< fa user-edit >}} Exercises
- Build a similar proof for a slightly different case; for example two balls with three possible marks each, or four balls with two possible marks each.

- Consider the same problem, but in the case of background knowledge $\yJ$ where our degrees of belief are **not** exchangeable. For instance, give three *different* values to the probabilities
    
    $$
\begin{gathered}
\P(B_{1}\mo\yy \and 
B_{2}\mo\yy \and 
B_{3}\mo\yn \|\if \and \yJ)
\\
\P(B_{1}\mo\yy \and 
B_{2}\mo\yn \and 
B_{3}\mo\yy \|\if \and \yJ)
\\
\P(B_{1}\mo\yn \and 
B_{2}\mo\yy \and 
B_{3}\mo\yy \|\if \and \yJ)
\end{gathered}
$$
    
    in such a way that they still sum up to $1$. Then find by marginalization the probability that the first drawn ball has mark $\yy$. Is this probability still equal to the relative frequency of $\yy$?
:::


## Inference about several units {#sec-moreunit-freq-known}

Let's continue with the urn & balls example of the previous section. We found that the probability of drawing a first ball with mark $\yy$ was $2/3$, the same value as the frequency of $\yy$-balls in the urn.

Now suppose that we draw the first ball, and it turns out to be $\yy$. What is the probability that the second drawn ball will be $\yy$?

The probability we are asking about is $\P(B_{2}\mo\yy \| B_{1}\mo\yy \and \if \and \yi[U])$, and it can be calculated with the usual rules. The result is again the same as the frequency of the $\yy$ balls, but *with respect to the new urn contents*: there are now two balls left, one marked $\yy$ and the other $\yn$. The probability is therefore $1/2$, a value different from that for the first ball, $2/3$. This situation is quite general: in an urn with many balls, the probabilities for new draws change accordingly to information about previous draws (and also subsequent draws, if available).

\

But consider now the case $\yi[U]'$ of an urn with 3 000 000 balls, 2 000 000 of which are marked $\yy$ and the rest $\yn$. The population's relative frequencies are exactly as in the case with three balls, and for the probability that the first drawn ball is $\yy$ we still have

$$
\P(B_1\mo\yy \| \if\and \yi[U]') = 
f(B\mo\yy) = \frac{2 000 000}{3 000 000} = 2/3
$$

If the first drawn ball is $\yy$, what is the probability that the second is $\yy$? In this case we find

$$
\P(B_2\mo\yy \| B_1\mo\yy \and \if\and \yi[U]') = 
\frac{1 999 999}{2 999 999} \approx 2/3
$$

with an absolute error of only $0.000 000 1$. That is, the probability and frequency are almost the same as for the first draw. The reason is clear: the number of balls is so large that drawing some of them doesn't practically change the content and its proportions.

The joint probability that the first ball is $\yy$ and the second is $\yy$ is therefore, by the `and`-rule,

$$
\begin{aligned}
\P(B_2\mo\yy \and B_1\mo\yy \| \if\and \yi[U]') &=
\P(B_2\mo\yy \| B_1\mo\yy \and \if\and \yi[U]') \cdot
\P(B_1\mo\yy \| \if\and \yi[U]')
\\[1ex]
&\approx f(B\mo\yy) \cdot f(B\mo\yy)
\end{aligned}
$$

the approximation being the better, the larger the number of balls in the urn.

It's easy to see that this will hold for more balls and different values, as long as the number of balls considered is enough small compared with the population size. For instance

:::{.column-page-inset-right}
$$
\P(B_4\mo\yy \and B_3\mo\yn \and B_2\mo\yn \and B_1\mo\yy \| \if\and \yi[U]')
\approx 
f(B\mo\yy) \cdot f(B\mo\yn) \cdot f(B\mo\yn) \cdot f(B\mo\yy)
$$
:::

where $\vf$ is the *initial* frequency distribution for the urn.

\

It is easy to see that this situation applies to more general populations: if the full-population frequencies are known, the agent's beliefs are exchangeable, and the population is practically infinite, then the joint probability that some units have a particular set of values is equal to the product of the frequencies of those values.

:::::{.column-page-right}
:::{.callout-note style="font-size:120%"}
##
::::{style="font-size:120%"}

If an agent:

- has background information $\yI$ about a population saying that
    + beliefs about units are exchangeable
    + the population size is practically infinite
- has full information $\if$ about the population frequencies $\vf$ for the variate $\bZ$

then

$$
\P(
{\blue Z_{\yellow u'}\mo z'} \and 
{\blue Z_{\yellow u''}\mo z''} \and 
{\blue Z_{\yellow u'''}\mo z'''} \and 
\dotsb
\| \if \and \yI) 
\approx
f({\blue Z\mo z'}) \cdot
f({\blue Z\mo z''}) \cdot
f({\blue Z\mo z'''}) \cdot
\dotsb
$$

for any (different) units $\yellow u', u'', u''', \dotsc$ and any (even equal) values $\blue z', z'', z''', \dotsc$.
::::
:::
:::::


The formula above solves our initial problem: *How to calculate and encode the joint probability distribution for the full population?*, although it does so only in the case where the full-population frequencies $\vf$ are known. In this case this probability is encoded in the $\vf$ itself (which can be represented as a multidimensional array), and can be calculated for any desired joint probability distribution just by a multiplication.


## No learning when full-population frequencies are known {#sec-no-learn-freqs}



::: {.callout-caution}
## {{< fa book >}} Study reading
Ch. 3 of [*Probability Theory*](https://hvl.instructure.com/courses/25074/modules/items/660090)
:::
