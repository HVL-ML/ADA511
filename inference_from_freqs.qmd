# [Inferences from frequencies]{.green} {#sec-inference-from-freqs}
{{< include macros.qmd >}}
{{< include macros_exchangeability.qmd >}}

## If the population frequency were known {#sec-pop-freq-known}

Let's now see how the exchangeability of an agent's degrees of belief allows it to calculate probabilities about the units of a population. We shall do this calculation in two steps. First, in the case where *the agent knows the joint frequency distribution* (§§ [-@sec-freq-distr], [-@sec-joint-freq], [-@sec-limit-freqs]) *for the full population*. Second, in the more general case where the agent lacks this population-frequency information.

When the full-population frequency distribution is known, the calculation of probabilities is very intuitive and analogous to the stereotypical "drawing balls from an urn". We shall rely on this intuition; keep in mind, however, that the probabilities are not assigned "by intuition", but actually fully determined by the two basic pieces of knowledge or assumptions: *exchangeability* and *known population frequencies*. Some simple proof sketches of this will also be given.

## Notation {#sec-pop-inference-notation}

In this and the following chapters we shall stick to a particular use of some symbols, and shall often take the simplified [income]{.midgrey} dataset (file [`income_nominal_data_all.csv`](datasets/income_nominal_data_all.csv) and its underlying population as an example.

We consider an infinite population with any number of variates. For concreteness we assume these variates to have finite, discrete domains; but the formulae we obtain can be easily generalized to other kinds of variates. *We shall denote all the population variates, jointly, with $\bZ$*. In the case of the income dataset, for instance, the variate $\bZ$ stands for the joint variate with nine components:

$$
\begin{aligned}
\bZ &\defd (\blue
\mathit{workclass} \and 
\mathit{education} \and 
\mathit{marital\_status} \and 
\mathit{occupation} \and 
{}\\ &\qquad
\blue\mathit{relationship} \and
\mathit{race} \and 
\mathit{sex} \and 
\mathit{native\_country} \and 
\mathit{income}
\black)
\end{aligned}
$$

When we write $\blue Z \mo z$, the symbol $\blue z$ stands for some definite joint *values*, for instance $({\blue\cat{Without-pay} \and \cat{Doctorate} \and \dotsb \and \cat{Ireland} \and \cat{>50K}})$.

In applications typical of "supervised learning" ([§ @sec-3-connection-ML]), the agent wants to draw inferences about *some* of the  $\bZ$ variates for a new unit, conditional on the values of the *remaining* variates. We call the joint variates to be inferred the [**predictand**]{.blue}^[literally "what has to be predicted". In machine learning and other fields the terms "dependent variable", "class" or "label" (for nominal variates) are often used.], and we shall usually denote them jointly by the symbol $\rY$. We call the variates used to inform the inference the [**predictor**]{.blue}^[In machine learning and other fields the terms "independent variable" or "features" are often used], and we shall usually denote them jointly by the symbol $\gX$.


In the [income]{.midgrey} problem, for instance, the agent (some USA census agency) would like to infer the $\red\mathit{income}$ variate of a person from the other eight demographic characteristics $\green\mathit{workclass} \and \mathit{education} \and \dotsb$ of that person. So in this inference problem we define

$$
\begin{aligned}
\rY &\defd {\red\mathit{income}}
\\[1ex]
\gX &\defd ({\green\mathit{workclass} \and \mathit{education} \and \dotsb \and \mathit{sex} \and \mathit{native\_country}})
\end{aligned}
$$

We shall, however, also consider slightly different inference problems, for example with $({\red\mathit{race} \and \mathit{sex}})$ as predictand and the remaining seven variates $({\green\mathit{workclass} \and \dotsb \and \mathit{income}})$ as predictors.


## Knowing the full-population frequency distribution {#sec-know-freq}

Now suppose that the agent knows the *full-population joint frequency distribution*. Let's make clearer what this means. In the [income]{.midgrey} problem, for instance, consider these two different joint values for the joint variate $\bZ$:

<!-- `example_freqdistr.rds`, items 1 and 26 in reverse-ordered frequencies -->
$$
\begin{aligned}
{\blue z'}&\defd (
\cat{Private} \and \cat{HS-grad} \and \cat{Married-civ-spouse} \and \cat{Machine-op-inspct} \and {}
\\
&\qquad\cat{Husband} \and \cat{White} \and \cat{Male} \and \cat{United-States} \and \cat{<=50K}
)
\\[2ex]
{\blue z''}&\defd (
\cat{Self-emp-not-inc} \and \cat{HS-grad} \and \cat{Married-civ-spouse} \and \cat{Farming-fishing} \and {}
\\
&\qquad\cat{Husband} \and \cat{White} \and \cat{Male} \and \cat{United-States} \and \cat{<=50K}
)
\end{aligned}
$$

The agent knows that the value $\blue Z\mo z'$ occurs in the full population of interest (in this case all 340 millions or so USA citizens, considered in a short period of time) with a relative frequency $0.860 369\%$; it also knows that the value $\blue Z\mo z''$ occurs with a relative frequency $0.260 058\%$. We write this as follows:

$$
f({\blue Z\mo z'}) = 0.860 369\% \ ,
\qquad
f({\blue Z\mo z''}) = 0.260 058\%
$$

The agent knows not only the frequencies of the two particular joint values $\blue z'$, $\blue z''$, but for *all possible* joint values, that is, for all possible combinations of values from the single variate. In the [income]{.midgrey} example there are 54 001 920 possible combinations, and therefore just as many relative frequencies. All these frequencies together form the full-population frequency distribution for $\bZ$, which we denote collectively with $\vf$ (note the boldface). This complete frequency information is denoted by the sentence $\if$.

:::{.callout-warning}
## {{< fa exclamation-circle >}} Don't confuse the full population with a sample from it
Note that the frequencies reported above are *not* the ones found in the [`income_nominal_data_all.csv`](datasets/income_nominal_data_all.csv) dataset, because that dataset is only a *sample* from the full population, not the full population. The frequency values reported above are purely hypothetical (but not inconsistent with the frequencies observed in the sample).
:::



In other cases, these hypothetically known frequencies would refer to the full population of units: maybe even past, present, future, if they span a possibly unlimited time range.

## Inference about a single unit {#sec-1unit-freq-known}

Now imagine that the agent, given the information $\if$ about the frequencies and some background information $\yI$, must infer all $\bZ$ variates for a specific unit $\yellow u$. In the [income]{.midgrey} case, it would be an inference about a specific USA citizen. This unit $\yellow u$ could have any particular combination of variate values; in the [income]{.midgrey} case it could have any one of the 54 001 920 possible combined values. The agent must assign a probability to each of these possibilities.^[Remember that this is what we mean when we say "drawing an inference"! (See [chap. @sec-what-inference] and [§ @sec-distribute-prob])] Which probability values should it assign?

Intuitively we would say that the probability for a particular value $\blue Z\mo z$ should be equal to the frequency of that value in the full population. For instance, the probabilities that unit $\yellow u$ has the values $\blue z'$ or $\blue z''$ above would be

$$
\begin{aligned}
&\P(\bZ_{\yellow u}\mo {\blue z'} \| \if \and \yI) =
f(\bZ \mo {\blue z'}) = 0.860 369\%
\\[1ex]
&\P(\bZ_{\yellow u}\mo {\blue z''} \| \if \and \yI) =
f(\bZ \mo {\blue z''}) = 0.260 058\%
\end{aligned}
$$

This intuition is the same as in drawing balls, which may have different sets of labels, from an urn, given that we know the proportion of balls with each possible label set.

But the equality above can actually be proven mathematically in this specific case: it follows from the assumption of exchangeability. Let's examine a very simple case to get an idea of how this proof works.

### Exact calculation of the probabilities in a simple case

Suppose we have an urn with three balls: two of them are marked $\yy$, and one $yn$. This is the simple case of a finite population with three units and a binary variate, let's call it $B$. We know that the frequency distribution is

$$f(U\mo\yy) = 2/3 \qquad f(U\mo\yn) = 1/3$$

Let's label the first ball we'll draw #1, the second #2, and the third #3. Our information $\if$ about the frequencies corresponds to the following composite sentence:

:::{.column-page-inset-right}
$$
\if \ \equiv\ 
(B_{1}\mo\yy \and 
B_{2}\mo\yy \and 
B_{3}\mo\yn)
\lor
(B_{1}\mo\yy \and 
B_{2}\mo\yn \and 
B_{3}\mo\yy)
\lor
(B_{1}\mo\yn \and 
B_{2}\mo\yy \and 
B_{3}\mo\yy)
$$
:::

Given $\if$, we know that $\if$ is true: $\P(\if \| \if \and \yI)=1$, which means

:::{.column-page-inset-right}
$$
\P\bigl[(B_{1}\mo\yy \and 
B_{2}\mo\yy \and 
B_{3}\mo\yn)
\lor
(B_{1}\mo\yy \and 
B_{2}\mo\yn \and 
B_{3}\mo\yy)
\lor
(B_{1}\mo\yn \and 
B_{2}\mo\yy \and 
B_{3}\mo\yy)
\|[\big] \if, \yI\bigr] = 1
$$
:::

Now use the `or`-rule, considering that the three `or`ed sentences are mutually exclusive:

:::{.column-page-right}
$$
\begin{aligned}
1&=\P\bigl[(B_{1}\mo\yy \and 
B_{2}\mo\yy \and 
B_{3}\mo\yn)
\lor
(B_{1}\mo\yy \and 
B_{2}\mo\yn \and 
B_{3}\mo\yy)
\lor
(B_{1}\mo\yn \and 
B_{2}\mo\yy \and 
B_{3}\mo\yy)
\|[\big] \if \and \yI\bigr]
\\[2ex]
&=
\P(B_{1}\mo\yy \and 
B_{2}\mo\yy \and 
B_{3}\mo\yn \|\if \and \yI)
+
\P(B_{1}\mo\yy \and 
B_{2}\mo\yn \and 
B_{3}\mo\yy \|\if \and \yI)
+
\P(B_{1}\mo\yn \and 
B_{2}\mo\yy \and 
B_{3}\mo\yy \|\if \and \yI)
\end{aligned}
$$
:::

According to our background information $\yI$, our degrees of belief are exchangeable. This means that the three probability summed up above must all have the same value, because in each of them $\yy$ appears twice and $\yn$ once. But if we are summing the same values thrice, and the sum is $1$, that that value must be $1/3$. Hence we find that

$$
\begin{aligned}
&\P(B_{1}\mo\yy \and 
B_{2}\mo\yy \and 
B_{3}\mo\yn \|\if \and \yI)
= 1/3
\\
&\P(B_{1}\mo\yy \and 
B_{2}\mo\yn \and 
B_{3}\mo\yy \|\if \and \yI)
= 1/3
\\
&\P(B_{1}\mo\yn \and 
B_{2}\mo\yy \and 
B_{3}\mo\yy \|\if \and \yI)
= 1/3
\\[1ex]
&\text{all other probabilities are zero}
\end{aligned}
$$

Finally let's find the probability that the first ball we draw has mark $\yy$, that is, $\P(B_1\mo\yy \| \if\and \yI)$. It's a marginal probability ([§ @sec-marginal-probs]), so it's given by the sum

$$
\begin{aligned}
\P(B_1\mo\yy \| \if\and \yI) &=
\sum_{i=\yy}^{\yn}\sum_{j=\yy}^{\yn}
\P(B_1\mo\yy \and B_2\mo i \and B_3\mo j \| \if\and \yI)
\\[1ex]
&=
\P(B_1\mo\yy \and B_2\mo\yy \and B_3\mo\yy \| \if\and \yI) + {}
\\ &\qquad
\P(B_1\mo\yy \and B_2\mo\yy \and B_3\mo\yn \| \if\and \yI) + {}
\\ &\qquad
\P(B_1\mo\yy \and B_2\mo\yn \and B_3\mo\yy \| \if\and \yI) + {}
\\ &\qquad
\P(B_1\mo\yy \and B_2\mo\yn \and B_3\mo\yn \| \if\and \yI)
\\[1ex]
&= 0 + 1/3 + 1/3 + 0
\\[1ex]
&= 2/3
\end{aligned}
$$

which is indeed equal to $f(B\mo\yy)$.

\

This simple example gives you an idea why our intuition for equating -- in specific circumstances -- probability with full-population frequencies, is actually a mathematical theorem: it follows from (1) knowledge of the full-population frequencies, and (2) **exchangeability**.

:::{.callout-caution}
## {{< fa user-edit >}} Exercises
- Build a similar proof for a slightly different case; for example two balls with three possible marks each, or four balls with two possible marks each.

- Consider the same problem, but in the case of background knowledge $\yJ$ where our degrees of belief are **not** exchangeable. For instance, give three *different* values to the probabilities
    
    $$
\begin{gathered}
\P(B_{1}\mo\yy \and 
B_{2}\mo\yy \and 
B_{3}\mo\yn \|\if \and \yJ)
\\
\P(B_{1}\mo\yy \and 
B_{2}\mo\yn \and 
B_{3}\mo\yy \|\if \and \yJ)
\\
\P(B_{1}\mo\yn \and 
B_{2}\mo\yy \and 
B_{3}\mo\yy \|\if \and \yJ)
\end{gathered}
$$
    
    in such a way that they still sum up to $1$. Then find by marginalization the probability that the first drawn ball has mark $\yy$. Is this probability still equal to the relative frequency of $\yy$?

:::
