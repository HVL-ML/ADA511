# [Code and computations]{.red} {#sec-code-compute-OPM}
{{< include macros.qmd >}}
{{< include macros_exchangeability.qmd >}}
{{< include macros_opm.qmd >}}

[**{{< fa person-digging >}} under construction {{< fa person-digging >}}**]{.yellow}

----

Before starting, let's agree on some terminology in order not to get confused in the discussion below.

- We shall call [*task*]{.blue} a repetitive inference problem with a specified set of units and variates. For instance, a task could be the consecutive prediction of the urgency of incoming patients, given their mean of transportation. We assume that the details of the variates, such as their domain, are well specified. Possibly also a set of data from other patients is available, which we call "training data".
- We shall call [*application*]{.blue} or [*instance*]{.blue} of the task a single inference about a specific new unit, for instance a new incoming patient, known to be arriving by helicopter.


## Range of use of the code {#sec-code-range}

The concrete formulae discussed in the previous [chapter @sec-dirichlet-mix] can be put into code, for use in different tasks involving only nominal variates. Software of this kind can in principle be written to allow for some or all of the versatility discussed in §§ [-@sec-categ-probtheory]--[-@sec-underlying-distribution], for example the possibility of taking care (in a first-principled way!) of partially missing training data. But the more versatile we make the software, the more memory, processing power, and computation time it will require.

Roughly speaking, more versatility corresponds to calculations of the joint probability

::::{.column-page-right}
:::{.callout-note}
##

$$
\P(
\blue 
Z_{L}\mo  z_{L}
\and
\dotsb \and
Z_{1}\mo z_1
\black
\| \yD
)
=
\frac{1}{\amax-\amin+1}
\sum_{\ya=\amin}^{\amax}
\frac{
\prod_{\bz} \bigl(\frac{2^{\ya}}{M} + \#\bz - 1\bigr)!
}{
\bigl(2^{\ya} + L -1 \bigr)!
}
\cdot
\frac{
\bigl(2^{\ya} -1 \bigr)!
}{
{\bigl(\frac{2^{\ya}}{M} - 1\bigr)!}^M
}
\quad
$$ {#eq-main-joint}

:::
::::

for more values of the quantities $\blue Z_1, Z_2, \dotsc$. For instance, if data about unit #4 are missing, then we need to calculate the joint probability above for several (possibly all) values of $\blue Z_4$. If data about two units are missing, then we need to do an analogous calculation for all possible *combinations* of values; and so on.

For our prototype, let's forgo versatility about units used as training data. From now on we abbreviate the set of training data as

:::{.column-margin}
Recall that $\bZ$ denotes all (nominal) variates of the population
:::

$$
\data \defd
(\green
Z_{1}\mo z_1 \land 
Z_{2}\mo z_2 \land \dotsb 
\black)
$$

where $\blue z_1, z_2, \dotsc$ are specific values, stored in some training dataset. No values are missing.

Since the training $\data$ are given and fixed in a task, we omit the suffix "${}_{N+1}$" that we have often used to indicate a "new" unit. So "$\blue Z\mo z$" simply refers to the variate $\bZ$ in a new application of the task.

We allow for full versatility in every new instance. This means that we can accommodate, *on the spot at each new instance*, what the predictand variates are, and what the predictor variates (if any) are. For example, if the population has three variates $\bZ=(\bA \and \bB \and \bC)$, our prototype can calculate, at each new application, inferences such as

- $P(\bB\mo\dotso\|\data \and \yD)$: any one predictand variate, no predictors

- $P(\bA\mo\dotso \and \bC\mo\dotso\|\data \and \yD)$: any two predictand variates, no predictors

- $P(\bA\mo\dotso \and \bB\mo\dotso \and \bC\mo\dotso\|\data \and \yD)$: all three variates

- $P(\bB\mo\dotso\|\bA\mo\dotso \and \data \and \yD)$: any one predictand variate, any other one predictor

- $P(\bB\mo\dotso\| \bA\mo\dotso \and \bC\mo\dotso \and\data \and  \yD)$: any one predictand variate, any other two predictors

- $P(\bA\mo\dotso \and \bC\mo\dotso\|\bB\mo\dotso \and \data \and \yD)$: any two predictand variates, any other one predictor


## Computations needed in the code {#sec-code-computations}


To enjoy the versatility discussed above, the code needs to compute

:::{.callout-note}
##

$$
\P(
\blue Z \mo z
\and
\green\data
\black \| \yD)
$$ {#eq-objectP}

for all possible values $\bz$.
:::

But these computations must be done *only once* for a given task. For a new application we only need to combine these already-computed probabilities via sums and fractions. For example, in the three-variate case above, if in a new application we need to forecast $\red A\mo a$ given $\green C\mo c$, then we calculate

::::{.column-page-inset-right}
:::{.callout-note}
##

(example)

$$
P(\red A\mo a \black \|\green C\mo c \black \and \data \and \yD)
=
\frac{
\sum_{\blue b}
P(\red A\mo a \black \and \blue B\mo b \black \and \green C\mo c \black \and \data \| \yD)
}{
\sum_{\purple \alpha}\sum_{\blue b}
P(\red A\mo {\purple \alpha} \black \and \blue B\mo b \black \and \green C\mo c \black \and \data \| \yD)
}
\quad
$$ {#eq-forecast}

:::
::::

where all $P(\red A\mo\dotso \black \and \blue B\mo\dotso \black \and \green C\mo\dotso \black \and \data \| \yD)$ are already computed.

::::{.column-body-outset-right}
:::{.callout-caution}
## {{< fa user-edit >}} Exercise

Using the `and`-rule, prove (pay attention to the conditional "$\|$" bar):

$$
\frac{
\sum_{\blue b}
P(\red A\mo a \black \and \blue B\mo b \black \and \green C\mo c \black \and \data \| \yD)
}{
\sum_{\purple \alpha}\sum_{\blue b}
P(\red A\mo {\purple \alpha} \black \and \blue B\mo b \black \and \green C\mo c \black \and \data \| \yD)
}
=
\frac{
\sum_{\blue b}
P(\red A\mo a \black \and \blue B\mo b \black \and \green C\mo c \black \| \data \and \yD)
}{
\sum_{\purple \alpha}\sum_{\blue b}
P(\red A\mo {\purple \alpha} \black \and \blue B\mo b \black \and \green C\mo c \black \| \data \and \yD)
}
$$

:::
::::

The exercise above shows that instead of $\P(\blue Z \mo z \black \and \green\data \black \| \yD)$ we could calculate

$$
\P(
\blue Z \mo z
\black \|
\green\data
\black \and  \yD)
$$

once for all possible values $\bz$, and use that instead. Mathematically and logically the two ways are completely equivalent. Numerically they can be different as regards precision or possible overflow errors. Using $\P( \blue Z \mo z \black \| \green\data \black \and \yD)$ would be convenient if our basic formula (@eq-main-joint) didn't contain the sum $\sum_k$ over the $k$ index. Our code shall instead use $\P(\blue Z \mo z \black \and \green\data \black \| \yD)$ because it leads to slightly more precision in some situations.

\

Our prototype software must therefore include two main functions:

- One function computes (@eq-objectP) once and for all in a given task, using the training $\data$ and the metadata $\yD$ provided. The result can be stored in an array or similar object, let's call it `P`.

- One function computes and outputs probabilities such as (@eq-forecast) at each new instance, using the stored object `P` as well as the predictor variates and values provided with that instance, and the predictand variates requested at that instance.

----

----

*below: old*



We have a choice of how a general-purpose code to build. For instance

A prototype R code for the Optimal Predictor Machine with Dirichlet-mixture background information is available at

[`https://github.com/pglpm/ADA511/tree/master/code/OPM-nominal`](https://github.com/pglpm/ADA511/tree/master/code/OPM-nominal)

It consists of two main functions [`buildP()`](https://github.com/pglpm/ADA511/blob/master/code/OPM-nominal/buildP.R) and [`forecastP()`](https://github.com/pglpm/ADA511/blob/master/code/OPM-nominal/buildP.R) and some helper functions.

The code implements the formulae of the previous [chapter @sec-dirichlet-mix] in a way that allows some of the versatility of the Optimal Predictor Machine discussed in §§ [-@sec-categ-probtheory]--[-@sec-underlying-distribution].

The basic idea behind the code is to calculate the probability distribution for a new unit


$$
\P(
\blue Z_{N+1} \mo z
\and
\green\data
\black \pmb{\|[\big]} \yD\bigr)
\qquad\text{\small for all values }\bz
$$

Where $\data$ represents all variate values of previous units.

The crucial point here is that we calculate the distribution above *for all values $\bz$* but **not** for all possible alternative values that the data could have. We would need the latter in order to be able to handle missing variates in the data; so this flexibility is lost. But we're still retaining the flexibility of choosing whatever predictand/predictor division we like.

From this distribution, the code can then draw all possible inferences about the new unit. For instance, if the population has three variates $\bZ=(\bA \and \bB \and \bC)$, the code can calculate inferences such as

- $P(\bB\|\data \and \yD)$: any one predictand variate, no (that is, unknown) predictors

- $P(\bA \and \bC\|\data \and \yD)$: any two predictand variates, no predictors

- $P(\bA \and \bB \and \bC\|\data \and \yD)$: all three variates

- $P(\bB\|\bA \and \data \and \yD)$: any one predictand variate, any other one predictor

- $P(\bB\| \bA \and \bC \and\data \and  \yD)$: any one predictand variate, any other two predictors

- $P(\bA \and \bC\|\bB \and \data \and \yD)$: any two predictand variates, any other one predictor

\

### Numerics

The formulae of [chapter @sec-dirichlet-mix] cannot be computed as they're written. First, the factorials in the formulae generate very large numbers which lead to overflow and then `NaN`s in the ratios. Second, the sums over some variates may involve so many terms as to require a long computation time. In the end we would have to wait a long time just to see a string of `NaN`s.

The first problem is dealt with, in the code, by rewriting the formulae in terms for logarithms and renormalizing numerators and denominators of fractions. The second problem is dealt with by reorganizing the sums as multiples of identical summands. These details are not discussed here.

## Description of the main functions {#opm-functions}

In typical order of use, the functions are the following. They may have some extra arguments that are omitted in this description:

`guessmetadata(data, file)`
: This function helps building a metadata file from a file of data.

    - Argument `data` is either the file name of a dataset, or a dataset itself (`data.table`).
    - Argument `file` is the name of the desired metadata file. If missing, and the `data` argument is a file name, a new file is created with same name but soffix `meta_`; otherwise the metadata go to `stdout`.


`buildP(metadata, data=NULL)`
: This is the main function. It creates an object (called "`P`") that encodes the probability distribution $\P(
\blue Z \mo z
\and
\green\data
\black \pmb{\|[\big]} \yD\bigr)$. The object consists of a list of a multidimensional array and three vectors.

    - Argument `metadata` is either the name of a metadata file, or a metadata object itself (`data.table`).
	- Argument `data` is either the file name of a dataset, or a dataset itself (`data.table`). If missing or `NULL`, No data are given to the machine, which therefore shows the initial beliefs of the agents.


`forecastP(P, conditional=NULL)`
: This function outputs the probability distribution for a given set of variates


----

[*To be continued*]{.grey}
