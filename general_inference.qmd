# [Final inference formulae]{.green} {#sec-final-inference}
{{< include macros.qmd >}}
{{< include macros_exchangeability.qmd >}}

We finally have all ingredients and formulae to use the probability calculus to make many kinds of inferences about some units in a population, given observations from other units. Keep in mind the minimal assumptions we are making: (1) beliefs about units are exchangeable, (2)the population size is practically infinite. Remember that these assumptions also underlie all machine-learning algorithms for "supervised" and "unsupervised" learning.

We still use the general scenario and notation of [§ @sec-pop-inference-notation].

## Recap of the main inference formulae {#sec-recap-general-formulae}

All inferences about units of a population rely on the joint probability for any number of units, which is given by the following formula ([§ @sec-freq-not-known]):

:::{.callout-note}
## de Finetti's representation

$$
\begin{aligned}
&\P\bigl(
\blue Z_{N+1} \mo z_{N+1}
\and
Z_N \mo z_N
\and
\dotsb \and 
Z_1 \mo z_1 
\black \pmb{\|[\big]} \yI\bigr)
\\[2ex]
&\qquad{}=
\sum_{\vf}
f({\blue Z\mo z_{N+1}}\black) \cdot
f({\blue Z\mo z_{N}}\black) \cdot
\, \dotsb\, \cdot
f({\blue Z\mo z_{1}}\black)
\cdot
\P(F\mo\vf \| \yI)
\end{aligned}
$$

<!-- $$ -->
<!-- \P( -->
<!-- \bZ_{}\mo {\blue z'} \and  -->
<!-- \bZ_{u''}\mo {\blue z''} \and  -->
<!-- \dotsb -->
<!-- \| \yI -->
<!-- ) -->
<!-- \approx -->
<!-- \sum_{\vf} -->
<!-- f(\bZ\mo {\blue z'}\black) \cdot -->
<!-- f(\bZ\mo {\blue z''}\black) \cdot -->
<!-- \,\dotsb\  -->
<!-- \cdot -->
<!-- \P(F\mo\vf \| \yI) -->
<!-- $$ -->

:::

where $\P(F\mo\vf\|\yI)$ is problem-dependent and must be specified by the agent. In the case of a single or joint nominal variate $\bZ$ we have an explicit mathematical formula for an agent that adopts a Dirichlet-mixture belief distribution ([§ @sec-prob-for-freqs]).


The formula above can be used to draw two main kinds of inferences:

:::{.callout-note}
## Inferences about all variates $Z$ of a new unit, given observed units ("unsupervised learning")

$$
\begin{aligned}
    &\P\bigl(
	{\red Z_{N+1} \mo z_{N+1}}
	\pmb{\|[\big]} 
    \green Z_N \mo z_N \and 
	\dotsb \and 
	Z_1 \mo z_1 
    \black\and {\yI} \bigr)
	\\[2ex]
	&\qquad{}
	=
	\frac{
	    \P\bigl(
	\red Z_{N+1} \mo z_{N+1}
	\black \and
    \green Z_N \mo z_N
	\and
	\dotsb \and 
	\green Z_1 \mo z_1 
    \black\pmb{\|[\big]} {\yI} \bigr)
}{
	 \sum_{\purple z} \P\bigl(
	{\red Z_{N+1} \mo {\purple z}} 
		\and
    \green Z_N \mo z_N \and 
	\dotsb \and 
	Z_1 \mo z_1 
    \black\pmb{\|[\big]}  {\yI} \bigr)
}
\end{aligned}
$$
:::

And, if we split the variate $\bZ$ into two sets of [predictand]{.red} and [predictor]{.green} variates $(\rY \and \gX)$\ \ ([§ @sec-pop-inference-notation]):

::::{.column-body-outset-right}
:::{.callout-note}
## Inferences about some variates $Y$ of a new unit, given other of its variates $X$ and given observed units ("supervised learning")

$$
\begin{aligned}
    &\P\bigl(
	\red Y_{N+1} \mo y_{N+1}
\black \pmb{\|[\big]} 
	\green X_{N+1} \mo x_{N+1}\, \and\,
    Y_N \mo y_N \and X_N \mo x_N \and
	\dotsb \and 
	Y_1 \mo y_1 \and X_1 \mo x_1 
    \black\and \yI \bigr)
	\\[2ex]
	&\qquad{}=
	\frac{
	    \P\bigl(
	\red Y_{N+1} \mo y_{N+1} \black\and
	\green X_{N+1} \mo x_{N+1}
		\and
 Y_N \mo y_N \and X_N \mo x_N
	\and
	\dotsb \and 
 Y_1 \mo y_1 \and X_1 \mo x_1 
    \black\pmb{\|[\big]} {\yI} \bigr)
}{
	 \sum_{\purple y} \P\bigl(
	\red Y_{N+1} \mo {\purple y} \black\and
	\green X_{N+1} \mo x_{N+1}
		\and
 Y_N \mo y_N \and X_N \mo x_N \and
	\dotsb \and 
	Y_1 \mo y_1 \and X_1 \mo x_1 
    \black\pmb{\|[\big]}  {\yI} \bigr)
}
\end{aligned}
$$

:::
::::

\

If $\bZ$, $\rY$, $\gX$ are each a single or joint nominal variate, and we consider an agent having a Dirichlet-mixture belief distribution ([§ @sec-prob-for-freqs]), the formulae above takes on concrete mathematical expressions:

::::{.column-page-inset-right}
:::{.callout-note}
## Case of purely nominal variates and agent with Dirichlet-mixture belief distribution

$$
\begin{aligned}
&\P\bigl(
\blue Z_{N+1} \mo z_{N+1}
\and
Z_N \mo z_N
\and
\dotsb \and 
Z_1 \mo z_1 
\black \pmb{\|[\big]} \yK\bigr)
=
\sum_{\alpha=\alpha_{\text{min}}}^{\alpha_{\text{max}}}
\frac{
\prod_{\bz} \bigl(2^{\alpha} + {\blue\#z} - 1\bigr)!
}{
\bigl(M\,2^{\alpha} + N \bigr)!
}
\ \cdot\ 
\sum_{\alpha=\alpha_{\text{min}}}^{\alpha_{\text{max}}}
\frac{
\bigl(M\,2^{\alpha} -1 \bigr)!
}{
{\bigl(2^{\alpha} - 1\bigr)!}^M
}
\\[3em]
&\P\bigl(
	\red Z_{N+1} \mo z_{N+1}
\black \pmb{\|[\big]} 
    \green Z_N \mo z_N \and 
	\dotsb \and 
	Z_1 \mo z_1 
    \black\and \yK \bigr)
	=
	\frac{\displaystyle{}\enspace
	\sum_{\alpha=\alpha_{\text{min}}}^{\alpha_{\text{max}}}
\frac{
(2^{\alpha} + {\green\# z_{N+1}} )
\cdot
\prod_{\green z} \bigl(2^{\alpha} + {\green\# z} - 1\bigr)!
}{
\bigl(M\,2^{\alpha} + N \bigr)!
}
	\enspace{}}{\displaystyle{}\enspace
	\sum_{\green z^*}
	\sum_{\alpha=\alpha_{\text{min}}}^{\alpha_{\text{max}}}
\frac{
(2^{\alpha} + {\green\# z^*})
\cdot
\prod_{\green z} \bigl(2^{\alpha} + {\green\# z} - 1\bigr)!
}{
\bigl(M\,2^{\alpha} + N \bigr)!
}
	\enspace{}}
\\[3em]
    &\P\bigl(
	\red Y_{N+1} \mo y_{N+1}
\black \pmb{\|[\big]} 
	\green X_{N+1} \mo x_{N+1}\, \and\,
 Y_N \mo y_N \and X_N \mo x_N \and
	\dotsb \and 
	Y_1 \mo y_1 \and X_1 \mo x_1 
    \black\and \yK \bigr)
	\\[2ex]
	&\qquad\qquad\qquad{}=
	\frac{\displaystyle{}\enspace
	\sum_{\alpha=\alpha_{\text{min}}}^{\alpha_{\text{max}}}
\frac{
\prod_{\green x}\bigl(2^{\alpha} + {\green\#(y_{N+1}, x)}\bigr)
\cdot
\prod_{\green y} 
\prod_{\green x} 
\bigl(2^{\alpha} + {\green\#(y, x)} - 1\bigr)!
}{
\bigl(M\,2^{\alpha} + N \bigr)!
}
	\enspace{}}{\displaystyle{}\enspace
	\sum_{\green y^*}
	\sum_{\alpha=\alpha_{\text{min}}}^{\alpha_{\text{max}}}
\frac{
\prod_{\green x}\bigl(2^{\alpha} + {\green\#(y^*, x)}\bigr)
\cdot
\prod_{\green y} 
\prod_{\green x} 
\bigl(2^{\alpha} + {\green\#(y, x)} - 1\bigr)!
}{
\bigl(M\,2^{\alpha} + N \bigr)!
}
	\enspace{}}
\end{aligned}
$$

In the last two formulae, the counts $\green\#z$, $\green\#(y,x)$, and similar [*refer to the values present in the conditional*]{.green}.

:::
::::



### Examples

Let's see a simple application


## How typical machine-learning problems are solved by the "optimal predictor machine"

### Missing values in training data


### "Generative" use: probability of predictor given the possible predictand values
