# [Final inference formulae]{.green} {#sec-final-inference}
{{< include macros.qmd >}}
{{< include macros_exchangeability.qmd >}}

We finally have all ingredients and formulae to use the probability calculus to make many kinds of inferences about some units in a population, given observations from other units. Keep in mind the minimal assumptions we are making: (1) beliefs about units are exchangeable, (2)the population size is practically infinite. Remember that these assumptions also underlie all machine-learning algorithms for "supervised" and "unsupervised" learning.

We still use the general scenario and notation of [§ @sec-pop-inference-notation].

## Recap of the main inference formulae {#sec-recap-general-formulae}

All inferences about units of a population rely on the joint probability for any number of units, which is given by the following formula ([§ @sec-freq-not-known]):

:::{.callout-note}
## de Finetti's representation

$$
\begin{aligned}
&\P\bigl(
\blue Z_{N+1} \mo z_{N+1}
\and
Z_N \mo z_N
\and
\dotsb \and 
Z_1 \mo z_1 
\black \pmb{\|[\big]} \yI\bigr)
\\[2ex]
&\qquad{}=
\sum_{\vf}
f({\blue Z\mo z_{N+1}}\black) \cdot
f({\blue Z\mo z_{N}}\black) \cdot
\, \dotsb\, \cdot
f({\blue Z\mo z_{1}}\black)
\cdot
\P(F\mo\vf \| \yI)
\end{aligned}
$$

<!-- $$ -->
<!-- \P( -->
<!-- \bZ_{}\mo {\blue z'} \and  -->
<!-- \bZ_{u''}\mo {\blue z''} \and  -->
<!-- \dotsb -->
<!-- \| \yI -->
<!-- ) -->
<!-- \approx -->
<!-- \sum_{\vf} -->
<!-- f(\bZ\mo {\blue z'}\black) \cdot -->
<!-- f(\bZ\mo {\blue z''}\black) \cdot -->
<!-- \,\dotsb\  -->
<!-- \cdot -->
<!-- \P(F\mo\vf \| \yI) -->
<!-- $$ -->

:::

where $\P(F\mo\vf\|\yI)$ is problem-dependent and must be specified by the agent. In the case of a single or joint nominal variate $\bZ$ we have an explicit mathematical formula for an agent that adopts a Dirichlet-mixture belief distribution ([§ @sec-prob-for-freqs]).


The formula above can be used to draw two main kinds of inferences:

:::{.callout-note}
## Inferences about all variates $Z$ of a new unit, given observed units ("unsupervised learning")

$$
\begin{aligned}
    &\P\bigl(
	{\red Z_{N+1} \mo z_{N+1}}
	\pmb{\|[\big]} 
    \green Z_N \mo z_N \and 
	\dotsb \and 
	Z_1 \mo z_1 
    \black\and {\yI} \bigr)
	\\[2ex]
	&\qquad{}
	=
	\frac{
	    \P\bigl(
	\red Z_{N+1} \mo z_{N+1}
	\black \and
    \green Z_N \mo z_N
	\and
	\dotsb \and 
	\green Z_1 \mo z_1 
    \black\pmb{\|[\big]} {\yI} \bigr)
}{
	 \sum_{\purple z} \P\bigl(
	{\red Z_{N+1} \mo {\purple z}} 
		\and
    \green Z_N \mo z_N \and 
	\dotsb \and 
	Z_1 \mo z_1 
    \black\pmb{\|[\big]}  {\yI} \bigr)
}
\end{aligned}
$$
:::

And, if we split the variate $\bZ$ into two sets of predictand and predictor variates $\blue(Y \and X)$\ \ ([§ @sec-pop-inference-notation]):

::::{.column-body-outset-right}
:::{.callout-note}
## Inferences about some variates $Y$ of a new unit, given other of its variates $X$ and given observed units ("supervised learning")

$$
\begin{aligned}
    &\P\bigl(
	\red Y_{N+1} \mo y_{N+1}
\black \pmb{\|[\big]} 
	\green X_{N+1} \mo x_{N+1}\, \and\,
    Y_N \mo y_N \and X_N \mo x_N \and
	\dotsb \and 
	Y_1 \mo y_1 \and X_1 \mo x_1 
    \black\and \yI \bigr)
	\\[2ex]
	&\qquad{}=
	\frac{
	    \P\bigl(
	\red Y_{N+1} \mo y_{N+1} \black\and
	\green X_{N+1} \mo x_{N+1}
		\and
 Y_N \mo y_N \and X_N \mo x_N
	\and
	\dotsb \and 
 Y_1 \mo y_1 \and X_1 \mo x_1 
    \black\pmb{\|[\big]} {\yI} \bigr)
}{
	 \sum_{\purple y} \P\bigl(
	\red Y_{N+1} \mo {\purple y} \black\and
	\green X_{N+1} \mo x_{N+1}
		\and
 Y_N \mo y_N \and X_N \mo x_N \and
	\dotsb \and 
	Y_1 \mo y_1 \and X_1 \mo x_1 
    \black\pmb{\|[\big]}  {\yI} \bigr)
}
\end{aligned}
$$

:::
::::

\

If $\bZ$, $\bY$, $\bX$ are each a single or joint nominal variate, and we consider an agent having a Dirichlet-mixture belief distribution ([§ @sec-prob-for-freqs]), the formulae above takes on concrete mathematical expressions, which can be rewritten as follows:

::::{.column-page-inset-right}
:::{.callout-note}
## Case of purely nominal variates and agent with Dirichlet-mixture belief distribution

$$
\begin{aligned}
&\P\bigl(
\blue Z_{N+1} \mo z_{N+1}
\and
Z_N \mo z_N
\and
\dotsb \and 
Z_1 \mo z_1 
\black \pmb{\|[\big]} \yK\bigr)
=
\sum_{\alpha=\amin}^{\amax}
\frac{
\prod_{\bz} \bigl(2^{\alpha} + {\blue\#z} - 1\bigr)!
}{
\bigl(M\,2^{\alpha} + N \bigr)!
}
\ \cdot\ 
\sum_{\alpha=\amin}^{\amax}
\frac{
\bigl(M\,2^{\alpha} -1 \bigr)!
}{
{\bigl(2^{\alpha} - 1\bigr)!}^M
}
\\[3em]
&\P\bigl(
	\red Z_{N+1} \mo z_{N+1}
\black \pmb{\|[\big]} 
    \green Z_N \mo z_N \and 
	\dotsb \and 
	Z_1 \mo z_1 
    \black\and \yK \bigr)
	=
	\frac{\displaystyle{}\enspace
	\sum_{\alpha=\amin}^{\amax}
\frac{
(2^{\alpha} + {\green\# z_{N+1}} )
\cdot
\prod_{\green z} \bigl(2^{\alpha} + {\green\# z} - 1\bigr)!
}{
\bigl(M\,2^{\alpha} + N \bigr)!
}
	\enspace{}}{\displaystyle{}\enspace
	\sum_{\green z^*}
	\sum_{\alpha=\amin}^{\amax}
\frac{
(2^{\alpha} + {\green\# z^*})
\cdot
\prod_{\green z} \bigl(2^{\alpha} + {\green\# z} - 1\bigr)!
}{
\bigl(M\,2^{\alpha} + N \bigr)!
}
	\enspace{}}
\\[3em]
    &\P\bigl(
	\red Y_{N+1} \mo y_{N+1}
\black \pmb{\|[\big]} 
	\green X_{N+1} \mo x_{N+1}\, \and\,
 Y_N \mo y_N \and X_N \mo x_N \and
	\dotsb \and 
	Y_1 \mo y_1 \and X_1 \mo x_1 
    \black\and \yK \bigr)
	\\[2ex]
	&\qquad\qquad\qquad{}=
	\frac{\displaystyle{}\enspace
	\sum_{\alpha=\amin}^{\amax}
\frac{
\prod_{\green x}\bigl(2^{\alpha} + {\green\#(y_{N+1}, x)}\bigr)
\cdot
\prod_{\green y} 
\prod_{\green x} 
\bigl(2^{\alpha} + {\green\#(y, x)} - 1\bigr)!
}{
\bigl(M\,2^{\alpha} + N \bigr)!
}
	\enspace{}}{\displaystyle{}\enspace
	\sum_{\green y^*}
	\sum_{\alpha=\amin}^{\amax}
\frac{
\prod_{\green x}\bigl(2^{\alpha} + {\green\#(y^*, x)}\bigr)
\cdot
\prod_{\green y} 
\prod_{\green x} 
\bigl(2^{\alpha} + {\green\#(y, x)} - 1\bigr)!
}{
\bigl(M\,2^{\alpha} + N \bigr)!
}
	\enspace{}}
\end{aligned}
$$

In the last two formulae, the counts $\green\#z$, $\green\#(y,x)$, and similar [*refer to the values present in the conditional*]{.green}.

:::
::::

:::{.callout-caution}
## {{< fa user-edit >}} Exercise
The last three formulae above look complicated. They do because they contain abbreviations ("$\sum$", "$\prod$") of sums or products of many terms. But once you have written down a couple of concrete examples, their meaning should be clear.

For the second formula, try to write it down for a concrete case in the Mars-prospecting scenario; say with $N=2$ and specific values for $z_1, z_2, z_3$, and verify that it is the correct expression of the more general formulae previously given. Use the property of the factorial

$$(a+1)! = (a+1) \cdot a!$$

:::


### Example

Let's see a simple step-by-step application of the formula for "unsupervised learning" in the Mars-prospecting scenario.

The agent has exchangeable beliefs represented by a Dirichlet-mixture distribution. The variate of the population of interest has two possible values, so in the formulae above we have\ \ $\amin=-1$, $\amax=2$\ \ ([§ @sec-prob-for-freqs]).

The agent has collected three rocks, and upon examination two of them contain haematite, one doesn't. The agent's data are therefore

$$\yR_1\mo\yy \and \yR_2\mo\yy \and \yR_3\mo\yn$$

(remember that it doesn't matter how we label the rocks, because the agent's beliefs are exchangeable).

What probability should the agent give to finding haematite in a newly collected rock? That is, what value should it assign to

$$\P(\yR_4\mo\yy \| \yR_1\mo\yy \and \yR_2\mo\yy \and \yR_3\mo\yn \and \yK) \ ?$$

There are different but logically equivalent ways of breaking down this calculation into steps. They also correspond to different ways of calculating the mathematical formulae. Here we examine one possibility.

\

[{{< fa 1 >}} {{< fa hand-point-right >}}]{.green}\ \ \ For the computation of the probability for $\yR_4\mo\yy$ we need to consider all alternative hypotheses. In this case there are only two alternatives, including the one of interest:

$$\yR_4\mo\yy \qquad \yR_4\mo\yn$$

\

[{{< fa 2 >}} {{< fa hand-point-right >}}]{.green}\ \ \ We need to calculate the joint probabilities for the agent's data (about the three previous rocks) `and` each hypothesis in turn. In the present case, they are the two joint probabilities

$$
\begin{aligned}
\P(\yR_4\mo\yy \, \and\, \yR_1\mo\yy \and \yR_2\mo\yy \and \yR_3\mo\yn \| \yK) 
\\[1ex]
\P(\yR_4\mo\yn \, \and\, \yR_1\mo\yy \and \yR_2\mo\yy \and \yR_3\mo\yn \| \yK) 
\end{aligned}
$$

These probabilities will appear, summed together, in the denominator of a final fraction; whereas the probability containing the hypothesis of interest will appear, by itself, in the numerator.

:::{.column-body-outset-right}

[{{< fa 2 >}}a {{< fa hand-point-right >}}]{.green}\ \ In the first joint probability, $\yy$ appears thrice and $\yn$ appears one, so

$$\#\yy = 3 \qquad \#\yn = 1 \qquad \text{\midgrey\small total} = 4$$

The de Finetti representation formula then gives, besides a constant factor,

$$\begin{aligned}
\P(\yR_4\mo\yy \, \and\, \yR_1\mo\yy \and \yR_2\mo\yy \and \yR_3\mo\yn \| \yK) 
&\propto
\sum_{\alpha=-1}^{2}
\frac{
\bigl(2^{\alpha} + {\lblue 3} - 1\bigr)! \cdot
\bigl(2^{\alpha} + {\yellow 1} - 1\bigr)!
}{
\bigl(2\cdot 2^{\alpha} + {\midgrey 4} -1 \bigr)!
}
\\[1ex]
&\propto
\boldsymbol{0.182 675}
\end{aligned}
$$


[{{< fa 2 >}}b {{< fa hand-point-right >}}]{.green}\ \ In the second joint probability, $\yy$ appears twice and $\yn$ appears twice, so

$$\#\yy = 2 \qquad \#\yn = 2 \qquad \text{\midgrey\small total} = 4$$

We find, again besides the same constant factor,

$$\begin{aligned}
\P(\yR_4\mo\yn \, \and\, \yR_1\mo\yy \and \yR_2\mo\yy \and \yR_3\mo\yn \| \yK) 
&\propto
\sum_{\alpha=-1}^{2}
\frac{
\bigl(2^{\alpha} + {\lblue 2} - 1\bigr)! \cdot
\bigl(2^{\alpha} + {\yellow 2} - 1\bigr)!
}{
\bigl(2\cdot 2^{\alpha} + {\midgrey 4} -1 \bigr)!
}
\\[1ex]
&\propto
\boldsymbol{0.114 468}
\end{aligned}
$$

:::

\

[{{< fa 3 >}} {{< fa hand-point-right >}}]{.green}\ \ \ The probability for the hypothesis of interest is given by the fraction

:::{.column-page-inset-right}

$$
\begin{aligned}
&\P(\yR_4\mo\yy \| \yR_1\mo\yy \and \yR_2\mo\yy \and \yR_3\mo\yn \and \yK)
\\[1ex]
&\qquad{}=
\frac{
\P(\yR_4\mo\yy \, \and\, \yR_1\mo\yy \and \yR_2\mo\yy \and \yR_3\mo\yn \| \yK)
}{
\P(\yR_4\mo\yy \, \and\, \yR_1\mo\yy \and \yR_2\mo\yy \and \yR_3\mo\yn \| \yK) +
\P(\yR_4\mo\yn \, \and\, \yR_1\mo\yy \and \yR_2\mo\yy \and \yR_3\mo\yn \| \yK)
}
\\[1ex]
&\qquad{}=
\frac{0.182 675 }{0.182 675 + 0.114 468}
\\[1ex]
&\qquad{}=
\boldsymbol{61.477\%}
\end{aligned}
$$

:::







## How typical machine-learning problems are solved by the "optimal predictor machine"

### Missing values in training data


### "Generative" use: probability of predictor given the possible predictand values
