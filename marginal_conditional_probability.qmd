# [Marginal and conditional probabilities]{.green}
{{< include macros.qmd >}}

## Marginal probability distributions {#sec-marginal-probs}

In some situations an agent has a joint distribution of degrees of belief for the possible values of a joint quantity, but it needs to consider its belief in the value of *one* component quantity alone, *irrespective of what the values for the other components quantities might be*.

Consider for instance the joint probability for the next-patient arrival scenario of [table @tbl-urgent-arrival] from [§ @sec-repr-joint-prob], with joint quantity $(U,T)$. We may be interested in the probability that the next patient will need $\cat{urgent}$ care, *independently of how the patient is transported to the hospital*. This probability can be found, as usual, by analysing the problem in terms of *sentences* and using the [basic rules of inference from § @sec-fundamental].

The sentence of interest is "[The next patient will require urgent care]{.midgrey}", or in symbols

$$U \mo \cat{urgent}$$

which is equivalent to "[The next patient will require urgent care, and will arrive by ambulance, helicopter, or other means]{.midgrey}", or in symbols

$$
U \mo \cat{urgent} \land
(
T \mo \cat{ambulance} \lor
T \mo \cat{helicopter} \lor
T \mo \cat{other}
)
$$

Using the [derived rules of Boolean algebra of § @sec-boolean] we can rewrite this sentence in yet another way:

$$
(U \mo \cat{urgent} \land T \mo \cat{ambulance}) \lor
(U \mo \cat{urgent} \land T \mo \cat{helicopter}) \lor
(U \mo \cat{urgent} \land T \mo \cat{other})
$$


This last sentence is an `or` of *mutually exclusive* sentences. Its probability is therefore given by the `or` rule without the `and` terms  (we now use the comma "$\and$" for `and`):

:::{.column-page-right}
$$
\begin{aligned}
&\P(U \mo \cat{urgent} \| \yi[H])
\\[1ex]
&\quad{}=
\P\bigl[
(U \mo \cat{urgent} \and T \mo \cat{ambulance}) \lor
(U \mo \cat{urgent} \and T \mo \cat{helicopter}) \lor
(U \mo \cat{urgent} \and T \mo \cat{other})
\| \yi[H] \bigr]
\\[1ex]
&\quad{}=\begin{aligned}[t]
&\P(U \mo \cat{urgent} \and T \mo \cat{ambulance} \| \yi[H]) +{}
\\
&\quad\P(U \mo \cat{urgent} \and T \mo \cat{helicopter} \| \yi[H]) +{}
\\
&\quad\P(U \mo \cat{urgent} \and T \mo \cat{other} \| \yi[H])
\end{aligned}
\end{aligned}
$$
:::

We have found that the probability for a value of the urgency quantity $U$, independently of the value of the transportation quantity $T$, can be calculated by summing all joint probabilities with all possible $T$ values. Using the $\sum$-notation we can write this compactly:

$$
\P(U \mo \cat{urgent} \| \yi[H]) =
\sum_{t}
\P(U \mo \cat{urgent} \and T\mo t \| \yi[H])
$$

where it's understood that the sum index $t$ runs over the values $\set{\cat{ambulance}, \cat{helicopter}, \cat{other}}$.


This is called a [**marginal probability**]{.blue}.

:::{.callout-caution}
## {{< fa user-edit >}} Exercise
Using the values from [table @tbl-urgent-arrival], calculate:

- the marginal probability that the next patient will need urgent care
- the marginal probability that the next patient will arrive by helicopter
:::

\

Considering now a more abstract case for a bivariate quantity with component quantities $X$ and $Y$, the probability for a specific value of $X$, conditional on some information $\yI$ and  irrespective of what the value of $Y$ might be, is given by

$$
\P({\green X\mo x}\| \yI) = \sum_{\red y} \P({\red Y\mo y} \and {\green X\mo x} \| \yI)
$$

You may notice the similarity with the expression for a *combined probability* from [§ @sec-combined-probs]. Indeed a marginal probability is just a special case of a combined probability: we are combining all probabilities that exhaust the possibilities for the sentence $Y\mo y$.

:::{.callout-caution}
## {{< fa user-edit >}} Exercise: test your understanding

Using again the values from [table @tbl-urgent-arrival], calculate the probability that *the next patient will need urgent care and will be transported either by ambulance or by helicopter*.
:::

## Marginal density distributions {#sec-marginal-dens}

In the example of the previous section, suppose now that the quantities $\green X$ and  $\red Y$ are continuous. Then the joint probability is expressed by a density:

$$\p({\red Y\mo y} \and {\green X\mo x} \| \yI)$$

with the usual meaning. The marginal probability density for $\green X$ is still given by a sum, but  this sum occurs over intervals  of values of $\red Y$, intervals with very small widths; as a consequence the sum will have a very large number of terms. To remind ourselves of this fact, which can be very important in some situations, we use a different notation in terms of integrals:

$$
\p({\green X\mo x}\| \yI) = \int_{\red \varUpsilon} \p({\red Y\mo y} \and {\green X\mo x} \| \yI)\, \di{\red y}
$$

where $\red\varUpsilon$ represents the domain of the quantity $\red Y$.

The fact that integrals appear is in some cases extremely useful, because it allows us to use the theory of integration to calculate marginal probabilities quickly and precisely, instead of having to compute sums of a large numbers of small terms -- a procedure that can be computationally expensive and lead to numerical errors owing to underflow or similar phenomena.

\

## Marginal probabilities and scatter plots {#sec-marginal-scatter}

In the previous chapters we have often discussed scatter plots for representing probability distributions of various kinds: discrete, continuous, joint, mixed, and so on.

One more advantage of the scatter plots for a joint distribution is that it can be quickly modified to represent any marginal, again with a scatter plot; whereas the use of a surface plot would require analytical calculations or approximations thereof.

Consider for instance the joint probability density from [§ @sec-repr-joint-dens], represented by the formula

:::{.column-page-inset-right}
$$
\p(X\mo x \and Y\mo y \| \yI) =
\tfrac{3}{8\,\pi}\, \e^{-\frac{1}{2} (x-1)^2-(y-1)^2}+
\tfrac{3}{64\,\pi}\,\e^{-\frac{1}{32} (x-2)^2-\frac{1}{2} (y-4)^2}+ \tfrac{1}{40\,\pi}\,\e^{-\frac{1}{8} (x-5)^2-\frac{1}{5} (y-2)^2}
$$
:::

and suppose we would like to visualize the marginal density for $X$,\ \ $\p(X\mo x \| \yI)$. In order to represent it with a line plot, we would first have to calculate the integral of the formula above over all possible values of $Y$:

:::{.column-page-inset-right}
$$
\p(X\mo x \| \yI) =
\int_{-\infty}^{\infty}
\Bigl[
\tfrac{3}{8\,\pi}\, \e^{-\frac{1}{2} (x-1)^2-(y-1)^2}+
\tfrac{3}{64\,\pi}\,\e^{-\frac{1}{32} (x-2)^2-\frac{1}{2} (y-4)^2}+ \tfrac{1}{40\,\pi}\,\e^{-\frac{1}{8} (x-5)^2-\frac{1}{5} (y-2)^2}
\Bigr]
\, \di y
$$
:::

But suppose, instead, that we have the points used to represent the density as a scatter plot, as in [fig. @fig-scatterXY], stored somewhere.

![](scatter_2d_marginalX.png){width=100%}

## Conditional probability distributions {#sec-conditional-probs}

A joint probability distribution quantifies an agent's uncertainty about all the quantities that compose a joint quantity. This means that in general the agent has no sure certainty about any of them (there are of course special cases where probabilities can be $0$ or $1$, but they are not the general case). Suppose that the agent has a joint probability for the quantities $X$ and $Y$, conditional on a state of knowledge $\yI$:

$$ \P({\green X\mo x} \and {\red Y\mo y} \|\yI) $$

<!-- :::{.callout-warning} -->
<!-- ## -->
<!-- {{< fa exclamation-circle >}} Always keep in mind that these are just convenient shorthands for -->
<!-- $$ -->
<!-- \P(\pr{The quantity \(X\) has value \(x\)} \land  -->
<!-- \pr{The quantity \(Y\) has value \(y\)} -->
<!-- \| \yI) -->
<!-- $$ -->
<!-- ::: -->

Now consider these two situations:

1. the agent acquires knowledge that $X$ has true value $x$, so its state of knowledge has changed;

2. for inference purposes, the agents needs to *hypothetically* assume that the quantity $X$ has value $x$ (even if that might not be the case in reality);

and, in either situation, the agent needs the probability that $Y$ has value $y$. This probability is written, in either situation as

$$
\P({\red Y\mo y} \| {\green X\mo x} \and \yI)
$$

It is usually called a [**conditional probability**]{.blue}. It is somewhat a misnomer, because *all* probabilities are conditional on something. The implicit understanding here is that *new* information has been added to the conditional with respect to some base state of knowledge.

What is the numerical value of the conditional probability above? We simply use the `and`-rule:

$$
\P({\red Y\mo y} \| {\green X\mo x} \and \yI) =
\frac{\P({\green X\mo x} \and {\red Y\mo y} \|\yI)}{\P({\green X\mo x} \| \yI)}
$$

The denominator is the *marginal probability* for $X$, which can also be calculated from the joint distribution as discussed in [§ @sec-marginal-probs]:

$$
\P({\green X\mo x} \| \yI) = \sum_{\red y} \P({\red Y\mo y} \and {\green X\mo x} \|\yI)
$$

We can thus rewrite the conditional probability as

$$
\P({\red Y\mo y} \| {\green X\mo x} \and \yI) =
\frac{\P({\red Y\mo y} \and {\green X\mo x} \|\yI)}{\sum_{\red y} \P({\red Y\mo y} \and {\green X\mo x} \|\yI)}
$$

:::{.callout-caution}
## {{< fa user-edit >}} Exercise
Consider the next-patient problem with the joint probability of [table @tbl-urgent-arrival]:

1. Calculate the probability that the next patient needs urgent care, knowing that the patient will arrive by helicopter.

2. Compare the conditional probability you just found with the *marginal* probability that the next patient needs urgent care, independently of transportation means (calculated in a previous exercise). Which is higher? Why?

3. Calculate the probability that the next patient arrives by helicopter, knowing that the patient will need urgent care.

4. Compare the conditional probability from 3. with that from 1. Why are they so different? Why is the probability "$\prq{helicopter}\|\prq{urgent}$" so low, when "$\prq{urgent}\|\prq{helicopter}$" is so high instead? find an intuitive explanation.
:::


## The importance of marginal distributions for drawing inferences

[@@ TODO]{.small .grey}
