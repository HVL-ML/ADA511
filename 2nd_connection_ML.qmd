# A second connection with machine learning {#sec-2nd-connection-ML}
{{< include macros.qmd >}}
{{< include macros_prob_inference.qmd >}}

In [chapter @sec-1st-connection-ML] we made a first tentative connection between the notions explored until then and notions from machine learning. We found that a machine-learning algorithm is like an agent that has some [built-in background information (corresponding to the algorithm's architecture)]{.yellow}, has received [additional pieces of information (corresponding to the training data)]{.green}, and is assessing a [not-previously known piece of information (the outcome)]{.red}:

$$
\P(\underbracket[0ex]{\red\se{D}_{N+1}}_{\mathclap{\red\text{outcome?}}} \| 
\green\underbracket[0ex]{\se{D}_N \land \dotsb \land \se{D}_2 \land \se{D}_1}_{\mathclap{\green\text{training data?}}} 
\black\land \underbracket[0ex]{\yellow\yI}_{\mathrlap{\yellow\uparrow\ \text{architecture?}}})
$$

We feel quite sure about the correspondence about [training data]{.green} and [architecture]{.yellow}, but we'll need to explore more the one about [outcome]{.red}, which must additionally involve some decision process.

Having introduced the notion of quantity ([chapters @sec-data-types-basic] [and @sec-data-types-multi]), we recognize that a training datum of a machine-learning algorithm also concern some quantity and its value, so it can be expressed by a sentence like $D\mo d$, where $D$ describes the kind of data, and $d$ represents a concrete, say numerical, value. Similarly for the outcome. So we can rewrite the correspondence above as follows:

$$
\P(\underbracket[0ex]{\red D_{N+1} \mo d_{N+1}}_{\mathclap{\red\text{outcome?}}} \| 
\green\underbracket[0ex]{ D_N \mo d_N \and \dotsb \and  D_2 \mo d_2 \and  D_1 \mo d_1}_{\mathclap{\green\text{training data?}}} 
\black\and \underbracket[0ex]{\yellow\yI}_{\mathrlap{\yellow\uparrow\ \text{architecture?}}})
$$

Let's extend these tentative connections further.

\

Machine-learning textbook usually make a distinction between "supervised learning" and "unsupervised learning". Unfortunately the explanation given for this distinction is often misleading:

- {{< fa meh-rolling-eyes >}}\ \ Some books say that in supervised learning the algorithm "learns a functional relationship between some kind of input and some kind of output". This is usually *not* true: in the vast majority of applications there isn't any *functional* relationship between input and output at all; at most only a *statistical* or *probabilistic* one. This is clear also from the fact that two training datapoints can have identical inputs but different outputs; and you remember from Calculus I that we can't speak of a function in this case. It's unclear how something that doesn't exist can be learned.

    Books that give this kind of explanation are unfortunately oversimplifying things to the point of being incorrect^[Paraphrasing [M. W. Zemansky](https://hvl.instructure.com/courses/25074/modules/items/679142):\
	*"Teaching machine learning\
	Is as easy as a song:\
	You think you make it simpler\
	When you make it slightly wrong!"*]. The algorithm is actually doing something more complex -- which we shall analyse in detail later!

- {{< fa meh >}}\ \ Yet other books say that the distinction rests in the kind of data used: "input-output" pairs for supervised learning, and only "inputs" for unsupervised learning. It's good that this description doesn't mention "functions", but it is still quite poor, because it confuses the means with the purpose. It's a little like saying that the difference between car and aeroplane is that the latter has wings. Sure -- but *why?* This description misses the essential difference between these two transportation means: they operate through different material media and exploit different kinds of physics; that's why the second has wings.

    From this point of view, the very terms "supervised learning" and "unsupervised learning" are somewhat poor, suffering from the same drawback.

- {{< fa smile >}}\ \ More enlightening books explain that the distinction rests in what the algorithm needs for each new application: in supervised learning, it uses features -- that is, additional information -- available at each new application; in unsupervised learning, it doesn't use any new information at each new application.

    From this point of view, the distinction between "supervised" and "unsupervised" becomes less sharp: we can imagine to increase the information that's used at each new instance, from zero ("unsupervised") to larger and larger amounts ("supervised").


Going back to our tentative correspondence with inference and decision-making agents, we see a strong similarity between unsupervised and supervised learning and two kinds of inference:

- In the unsupervised case, even if the quantities ${\green D_1}, {\green D_2}, {\green \dotsc}, {\green D_N}$  in the known instances and in the new application ${\red D_{N+1}}$ consist of joint or complex quantities ([chapter @sec-data-types-multi]), we are not interested in their possible decomposition into component quantities. So we still the tentative connection above:

    $$
    \P(\underbracket[0ex]{\red D_{N+1} \mo d_{N+1}}_{\mathclap{\red\text{outcome?}}} \| 
    \green\underbracket[0ex]{ D_N \mo d_N \and \dotsb \and  D_2 \mo d_2 \and  D_1 \mo d_1}_{\mathclap{\green\text{training data?}}} 
    \black\and \underbracket[0ex]{\yellow\yI}_{\mathrlap{\yellow\uparrow\ \text{architecture?}}})
    $$

    For example, the agent has been given information about a collection of images, and then tries to guess what the next image could be.


- In the supervised case, the quantities in the known instances and in the new application are joint quantities: ${\green D_1} = ({\green Y_1}, {\green X_1}), {\green\dotsc}$ and ${\red D_{N+1}} = ({\red Y_{N+1}}, {\red X_{N+1}})$, and we are interested in the $X$ and $Y$ component quantities separately. The reason is that, upon application to a new instance, one of these component quantities, say $\red X_{N+1}$, *can actually be observed by the agent*, so it's known. It's the other component quantity, $\red Y_{N+1}$, that the agent is uncertain about. The agent therefore needs to draw the following inference:

    $$
    \P\bigl(
	{\red Y_{N+1} \mo y_{N+1}}
	\|[\big] 
	{\red X_{N+1} \mo x_{N+1}}\, \and\,
    \green Y_N \mo y_N \and X_N \mo x_N \and
	\dotsb \and 
	Y_1 \mo y_1 \and X_1 \mo x_1 
    \black\and {\yellow\yI} \bigr)
    $$




