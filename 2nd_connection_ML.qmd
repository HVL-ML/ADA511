# A second connection with machine learning {#sec-2nd-connection-ML}
{{< include macros.qmd >}}
{{< include macros_prob_inference.qmd >}}

In [chapter @sec-1st-connection-ML] we made a first tentative connection between the notions then explored, and notions from machine learning. We found that a machine-learning algorithm is like an agent that has some [built-in background information (corresponding to the algorithm's architecture)]{.yellow}, has received [additional pieces of information (corresponding to the training data)]{.green}, and is assessing a [not-previously known piece of information (the output)]{.red}:

$$
\P(\underbracket[0ex]{\red\se{D}_{N+1}}_{\mathclap{\red\text{output?}}} \| 
\green\underbracket[0ex]{\se{D}_N \land \dotsb \land \se{D}_2 \land \se{D}_1}_{\mathclap{\green\text{training data?}}} 
\black\land \underbracket[0ex]{\yellow\yI}_{\mathrlap{\yellow\uparrow\ \text{architecture?}}})
$$

We feel quite sure about the correspondence about [training data]{.green} and [architecture]{.yellow}, but we'll need to explore more the one about [output]{.red}, which must additionally involve some decision process.

Let's extend these tentative connections further.

\

Machine-learning textbook usually make a distinction between "supervised learning" and "unsupervised learning". Unfortunately the explanation given for this distinction is often misleading:

- {{< fa meh-rolling-eyes >}}\ \ Some books say that in supervised learning the algorithm "learns a functional relationship between input and output". This is usually *not* true, because in the vast majority of applications there isn't any *functional* relationship between input and output at all, at most only a *statistical* or *probabilistic* one. This is clear also from the fact that two training datapoints can have identical inputs but different outputs; you remember from Calculus I that we can't speak of a function in this case. It's unclear how something that doesn't exist can be learned.

    The algorithm is actually doing something more complex, which we'll analyse in detail later.

- {{< fa meh >}}\ \ Yet other books say that the distinction rests in the kind of data used: "input-output" pairs for supervised learning, and only "inputs" for unsupervised learning. It's good that this description doesn't mention "functions", but it is still quite poor, because it confuses the means with the purpose. It's a little like saying that the difference between car and aeroplane is that the latter has wings. Sure -- but *why?* This description misses their essential difference: the two transportation means operate through different material media and exploit different kinds of physics; that's why the second has wings.

    From this point of view, the very terms "supervised learning" and "unsupervised learning" are somewhat poor, suffering from the same drawback.

- {{< fa smile >}}\ \ More enlightening books explain that the distinction rests in what the algorithm needs for each new application: in supervised learning, it uses features (that is, information), that is available at each new application; in unsupervised learning, it doesn't use any new information at each new application.

    Considered from this point of view, the distinction between "supervised" and "unsupervised" becomes also less sharp: we can imagine to increase the information that's used at each new instance, from zero ("unsupervised") to larger and larger amounts ("supervised").


Going back to our tentative correspondence with inference and decision-making agents, we see a strong similarity with two kinds of inference:

- In the first case the data and output correspond to simple or complex quantities, and we still have the tentative connection above:

    $$
    \P(\underbracket[0ex]{\red\se{D}_{N+1}}_{\mathclap{\red\text{output?}}} \| 
    \green\underbracket[0ex]{\se{D}_N \land \dotsb \land \se{D}_2 \land \se{D}_1}_{\mathclap{\green\text{training data?}}} 
    \black\land \underbracket[0ex]{\yellow\yI}_{\mathrlap{\yellow\uparrow\ \text{architecture?}}})
    $$

- In the second case the data correspond to joint quantities:



