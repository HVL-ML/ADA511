# [A second connection with machine learning]{.red} {#sec-2nd-connection-ML}
{{< include macros.qmd >}}
{{< include macros_prob_inference.qmd >}}

In [chapter @sec-1st-connection-ML] we made a first *tentative* connection between the notions about probability explored until then, and notions from machine learning. We considered the possibility that a machine-learning algorithm is like an agent that has some [built-in background information (corresponding to the algorithm's architecture)]{.yellow}, has received [pieces of information (corresponding to the data about perfectly known instances of the task)]{.green}, and is assessing a [not-previously known piece of information (the outcome in a new task instance)]{.red}:

$$
\P(\underbracket[0ex]{\red\se{D}_{N+1}}_{\mathclap{\red\text{outcome?}}} \| 
\green\underbracket[0ex]{\se{D}_N \land \dotsb \land \se{D}_2 \land \se{D}_1}_{\mathclap{\green\text{training data?}}} 
\black\land \underbracket[0ex]{\yellow\yI}_{\mathrlap{\yellow\uparrow\ \text{architecture?}}})
$$

The correspondence about [training data]{.green} and [architecture]{.yellow} seems somewhat convincing, the one about [outcome]{.red} will need more exploration, because it seems to involve some decision process -- and we haven't fully explored the machinery of decision-making yet.

Having introduced the notion of quantity in the latest chapters [-@sec-quantities-types-basic] and [-@sec-quantities-types-multi], we recognize that training data about a task instance concern some quantity and its value, so they can be expressed by a sentence like $D_i\mo d_i$, where

- $i$ is the instance: $1,2,\dotsc,N$
- $D_i$ describes the kind of data at instance $i$, for example "128 × 128 image with 24-bit colour depth"
- $d_i$ is the value of the data at instance $i$, for example the one here at the margin

:::{.column-margin}
![](muppet.jpg){width=128px}
:::

And similarly for the outcome of a new task instance where the algorithm is applied for real, which we consider as instance $N+1$. So we can rewrite the correspondence above as follows:

$$
\P(\underbracket[0ex]{\red D_{N+1} \mo d_{N+1}}_{\mathclap{\red\text{outcome?}}} \| 
\green\underbracket[0ex]{ D_N \mo d_N \and \dotsb \and  D_2 \mo d_2 \and  D_1 \mo d_1}_{\mathclap{\green\text{training data?}}} 
\black\and \underbracket[0ex]{\yellow\yI}_{\mathrlap{\yellow\uparrow\ \text{architecture?}}})
$$

Let's extend this tentative connection even further.

\

Machine-learning textbooks usually make a distinction between "supervised learning" and "unsupervised learning". Unfortunately the explanation given for this distinction is sometimes misleading:

- [{{< fa meh-rolling-eyes >}}]{.purple}\ \ Some books say that in supervised learning the algorithm "learns a functional relationship between some kind of input and some kind of output". This is usually *not* true: in the vast majority of applications there isn't any *functional* relationship between input and output at all; at most only a *statistical* or *probabilistic* one. This is clear from the fact that two training datapoints can have identical inputs but different outputs ([here is an example](steven-seagal-emotion-chart.jpg){.external}); and you remember from Calculus I that we can't speak of a function in this case. It's unclear how something that doesn't exist can be learned.

    Books that give this kind of explanation are unfortunately oversimplifying things, to the point of being incorrect^[Paraphrasing [M. W. Zemansky](https://hvl.instructure.com/courses/25074/modules/items/679142):\
	*"Teaching machine learning\
	Is as easy as a song:\
	You think you make it simpler\
	When you make it slightly wrong!"*]. The algorithm is actually doing something more complex -- which we shall analyse in detail later.


- [{{< fa meh >}}]{.purple}\ \ Yet other books say that the distinction rests in the kind of data used for training: "input-output" pairs for supervised learning, and only "inputs" for unsupervised learning. It's good that this description doesn't mention "functions", but it is still unsatisfactory, because it confuses the means with the purpose. It's a little like saying that the difference between car and aeroplane is that the latter has wings. Sure -- but *why?* This description misses the essential difference between these two means of transportation: they operate through different material media and exploit different kinds of physics; that's why the second has wings.

    Books that give this kind of explanation focus on a more "operational" kind of knowledge, which is sufficient for their specific goals. The very terms "supervised learning" and "unsupervised learning" indeed emphasize this operational side. But for our goal we need to look beyond operational differences and to understand their reasons.

- [{{< fa smile >}}]{.lightblue}\ \ More enlightening books explain that the distinction rests in what the algorithm needs for each new application: in supervised learning, it uses features -- that is, additional information -- available at each new application instance; whereas in unsupervised learning it doesn't: no new instance-dependent information is given.

    From this point of view, we also see that the distinction between "supervised" and "unsupervised" becomes less sharp: we can imagine to increase the information that's used at each new instance from zero ("unsupervised") to larger and larger amounts ("supervised").


Going back to our tentative correspondence with inference and decision-making agents, we see a strong similarity between unsupervised & supervised learning and two kinds of inference:

- In the unsupervised case, even if the quantities ${\green D_1}, {\green D_2}, {\green \dotsc}, {\green D_N}$  in the known instances and in the new instance ${\red D_{N+1}}$ might consist of joint or complex quantities ([chapter @sec-quantities-types-multi]), we are not interested in their possible decomposition into component quantities. So we still have the tentative connection above:

    $$
    \P(\underbracket[0ex]{\red D_{N+1} \mo d_{N+1}}_{\mathclap{\red\text{outcome?}}} \| 
    \green\underbracket[0ex]{ D_N \mo d_N \and \dotsb  \and  D_1 \mo d_1}_{\mathclap{\green\text{training data?}}} 
    \black\and \underbracket[0ex]{\yellow\yI}_{\mathrlap{\yellow\uparrow\ \text{architecture?}}})
    $$

    For example, the agent has been given information about a collection of images, and then tries to guess what the next image could be.


- In the supervised case, the quantities in the known instances and in the new application are joint quantities: 

    $$
	\begin{gathered}
	{\green D_N} = ({\green Y_N}, {\green X_N}),
	\ {\green\dotsc},\
	{\green D_1} = ({\green Y_1}, {\green X_1})
	\\[1ex]
	{\red D_{N+1}} = ({\red Y_{N+1}}, {\red X_{N+1}})
	\end{gathered}
	$$
	
	and we are interested in the $X$ and $Y$ component quantities separately. For instance, the $X$-quantity could be an image, as in the example above, and the $Y$-quantity could be a label with domain $\set{\cat{Muppet}, \cat{non-Muppet}}$. The reason we make this separation is that, upon applying the algorithm in a new task instance, *one of these component quantities*, say $\red X_{N+1}$, *can actually be observed by the agent*; so it is known. It's the other component quantity, $\red Y_{N+1}$, that the agent is uncertain about. The agent therefore needs to draw the following inference:

    $$
    \P\bigl(
	{\red Y_{N+1} \mo y_{N+1}}
	\|[\big] 
	{\red X_{N+1} \mo x_{N+1}}\, \and\,
    \green Y_N \mo y_N \and X_N \mo x_N \and
	\dotsb \and 
	Y_1 \mo y_1 \and X_1 \mo x_1 
    \black\and {\yellow\yI} \bigr)
    $$

An interesting aspect of this new tentative correspondence is that there isn't any essential difference between unsupervised and supervised cases. Consider the last probability above; using the formula for a conditional probability ([§ @sec-conditional-probs]) we have

:::{.column-page-right}
$$
\begin{aligned}
    &\P\bigl(
	{\red Y_{N+1} \mo y_{N+1}}
	\|[\big] 
	{\red X_{N+1} \mo x_{N+1}}\, \and\,
    \green Y_N \mo y_N \and X_N \mo x_N \and
	\dotsb \and 
	Y_1 \mo y_1 \and X_1 \mo x_1 
    \black\and {\yellow\yI} \bigr)
	\\[2ex]
	&\qquad{}=
	\frac{
	    \P\bigl(
	\grey\overbracket[1px]{
	{\red Y_{N+1} \mo y_{N+1}} \and
	{\red X_{N+1} \mo x_{N+1}}
	}^\mathclap{{\red D_{N+1}\mo d_{N+1}}}
	\black
		\|[\big] 
		\grey\overbracket[1px]{
    \green Y_N \mo y_N \and X_N \mo x_N
		}^\mathclap{{\green D_{N}\mo d_{N}}}\green
	\and
	\dotsb \and 
		\grey\overbracket[1px]{
	\green Y_1 \mo y_1 \and X_1 \mo x_1 
		}^\mathclap{{\green D_{1}\mo d_{1}}}
    \black\and {\yellow\yI} \bigr)
}{
	 \sum_{\red y_{N+1}} \P\bigl(
	{\red Y_{N+1} \mo y_{N+1}} \and
	{\red X_{N+1} \mo x_{N+1}}
		\|[\big] 
    \green Y_N \mo y_N \and X_N \mo x_N \and
	\dotsb \and 
	Y_1 \mo y_1 \and X_1 \mo x_1 
    \black\and {\yellow\yI} \bigr)
}
\end{aligned}
$$
:::

This is a fraction between the probability for the "unsupervised case" (which we wrote above compactly in terms of $D$ rather than $(Y,X)$), and the same probability summed up for all possible values of $\red Y_{N+1}$, which in the image-example above would be a sum over $\red Y_{N+1}\mo \cat{Muppet}$ and $\red Y_{N+1}\mo \cat{non-Muppet}$.

So if the agent has the probability for the "unsupervised case", then with a simple computation it also has the probability for the "supervised case".

\

One important question arise: how are these probabilities calculated by the agent? This is what we discuss in the next chapters.



<!-- ---- -->

<!-- :::{.columns} -->

<!-- ::::{.column width=25%} -->
<!-- :::: -->

<!-- ::::{.column width=50%} -->
<!-- ![](steven-seagal-emotion-chart.jpg){width=100%} -->
<!-- [Example of dataset in which the same input (facial expression) is associated with different outputs (mood labels)]{.small} -->
<!-- :::: -->

<!-- ::::{.column width=25%} -->
<!-- :::: -->
<!-- ::: -->
