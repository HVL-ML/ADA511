
\

\

\






# Dear student<br> and aspiring data engineer {.unnumbered}

The goal of this course is not to help you learn how to tune the parameters of the latest kind of deep network, or how to efficiently handle large amounts of data, or what is the latest improvement that has been made on a random-forest algorithm.

The goal of this course is to help you learn the principles to build the machine-learning algorithms and AI devices *of the future*.

How can such a goal be achieved?

There is a small set of rules and one method that is *mathematically* guaranteed to output the optimal solution of any inference, prediction, classification, regression, and decision problem. You can think of it as "the unbeatable machine-learning algorithm". Or, from an AI point of view, as the "laws of robotics" that any ideal AI designed to draw inferences and make decisions should be built upon.

This method is quite easy to grasp and understand, but is computationally extremely expensive; the more so, the more data points and data dimensions there are. Current machine-learning algorithms, from support-vector machines to large language models (chatGPT), are all *approximations* to this exact, ideal method; each one uses a different kind of approximation. The upside of these approximations is that they allow for much faster computations; their downside, however, is that they give *sub-optimal* results.

Approximations evolve toward the exact, maximally optimal method. The approximations used at any given time in history exploit the computational technologies then available. Deep networks, for instance, would have been useless 50 years ago, before the introduction of Graphical Processing Units.

Every new technological advance opens up possibilities for new approximations that get us closer to the ideal optimum. To *see* and *realize* these possibilities, a data scientist needs at the very least:

- to *know* the exact, maximally optimal method

- to think out of the box

Without the first requirement, how do you know what is *the target* to approximate towards? You risks making an approximation that leads to worse results than before.

@@ Add how this affect evaluation of improvement as well.

Without the second requirement you risks not taking full advantage of the new technological possibilities. Consider the evolution of transportation: if you keep thinking in terms of how to improve a horse's hoofs, you'll never conceive a combustion engine; if you keep thinking in terms of how to improve combustion fuel, you'll never conceive an electric motor.

If you want make advances in machine learning and AI, you must know how the ideal machine-learning algorithm looks like, and you must not limit yourself to thinking of "training sets", "test sets", "supervised learning", "models", and similar notions. In this course you'll see for yourself that such notions are anchored to particular approximations, not to the ideal method.


