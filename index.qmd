
\

\

\






# Dear student<br> and aspiring data engineer {.unnumbered}

The goal of this course is not to help you learn how to tune the parameters of the latest kind of deep network, or how to efficiently handle large amounts of data, or how to do cross-validation in an efficient way, or update you on the latest improvement in random-forest algorithms.

The goal of this course is to help you learn the principles to build the machine-learning algorithms and AI devices *of the future*. And at the end of the course you'll also be able to concretely improve present-day algorithms, as a side effect.

How can such a goal be achieved?

There is a small set of rules and one method that is *mathematically* guaranteed to output the optimal solution of any inference, prediction, classification, regression, and decision problem. You can think of it as "the unbeatable universal machine-learning algorithm". Or, from an AI point of view, you can think of these rules as the "laws of robotics" that any ideal AI designed to draw inferences and make decisions should be built upon.

These rules and method are quite easy to grasp and understand, but are computationally extremely expensive; the more so, the more data points and data dimensions we need to deal with. Current machine-learning algorithms, from support-vector machines to large language models (chatGPT), are all *approximations* to this ideal universal method; each one uses a different kind of approximation. The upside of these approximations is that they allow for much faster computations; their downside is that they give *sub-optimal* results.

Approximations evolve toward the maximally optimal ideal method. The approximations used at any given time in history exploit the computational technologies then available. Deep networks, for instance, would have been a  useless approximation 50 years ago, before the introduction of Graphical Processing Units.

Every new technological advance opens up possibilities for new approximations that get us closer to the ideal optimum. To *see* and *realize* these possibilities, a data scientist needs at the very least:

[- {{< fa binoculars >}}\ \ to *know* the exact, maximally optimal method]{.blue}

[- {{< fa rocket >}}\ \ to think outside the box]{.green}

Without the first requirement, how do you know what is *the target* to approximate towards, and how far you are from it? You risk: 1. making an approximation that leads to worse results than before; and 2. evaluating the approximation in the wrong way, so you don't even realize it's worse than before.

Without the second requirement you risk missing to take full advantage of the new technological possibilities. Consider the evolution of transportation: if you keep thinking in terms of how to improve a horse's hoofs, you'll never conceive a combustion engine; if you keep thinking in terms of how to improve combustion fuel, you'll never conceive an electric motor. Existing approximations may of course be good starting points; but you need to clearly understand how they approximate the ideal optimum -- so we're back to the first requirement.

If you want to make advances in machine learning and AI, you must know how the ideal universal algorithm looks like, and you must not limit yourself to thinking of "training sets", "test sets", "cross-validation", "supervised learning", "models", and similar notions. In this course you'll see for yourself that such notions are anchored to the present-day box of approximations.

And we want to think outside that box.

But don't worry: this course does not only want to prepare you for the future. Towards its end you should be able to *propose and implement concrete improvements* to present-day methods.

## Your role; bugs & features

This course is still in an experimental, "alpha" version. So will not only learn something from it (hopefully), but also test it together with us, and help improving it for future students -- thank you for this in advance!

For this reason it's good to clarify the goals and guidelines of this course. Some aspects that you might consider *bugs* are actually *features*:

[{{< fa square-root-alt >}}\ \ Light maths requirements]{.green}
: We believe that the fundamental rules and methods can be understood and also used (at least in not too complex applications) without complex mathematics. Indeed the basic laws of inference and decision-making involve only the four basic operations [$+ - \times /$]{.green}. So this course only requires maths of a beginning first-year undergraduate level.\
Â 


[{{< fa language >}}\ \ Names don't constitute knowledge]{.green}
:   :::{.column-margin}
    {{< video https://www.youtube.com/watch?v=lFIYKmos3-s >}}
    :::
    In these course notes you'll often stumble upon [**blue boldface terms**]{.blue} and [blue definitions]{.blue}. This typographic emphasis does **not** mean that those terms and definitions should be **memorized**. It means that there are important *ideas* that you must try to *understand* and *use*. We don't care which terminology you adopt. Instead of the term [**statistical population**]{.blue}, feel free to use the term [**pink apple**]{.blue} if you like, as long you can explain a term by means of a discussion and examples.^[Some standard technical terms are no better. The common term *random variable*, for instance, often denotes something that is actually *not "random"* and *not variable*. Go figure. Using the term *green banana* would be less misleading!] What's important is that you know, can recognize and can correctly use the ideas behind technical terms.
\
    [Memorizing terms, definitions, and where to use them is how large language models (like chatGPT) operate. If your study is just memorization of terms, you'll have difficulties finding jobs in the future, because there will be algorithms that can do that better and at a cheaper cost than you.]{.small}

[{{< fa book >}}\ \ Diverse textbooks]{.green}
: 

