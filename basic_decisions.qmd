# Basic decision problems
{{< include macros.qmd >}}

Decision Theory analyses any decision-making problem in terms of nested or sequential *basic* or *minimal* decision problems. The assembly-line scenario of the [introduction @sec-intro] is an example.

## Graphical representation and elements

A basic decision problem can be represented by a diagram like this:

<!-- ::::: {.column-page-inset-right} -->
![](basic_decision_tree.svg){width=100%}

It has one [**decision node**]{.blue}, usually represented by a square {{< fa regular square >}}, from which the available decisions depart as lines. Each decision leads to an [**inference node**]{.blue},^[also called *chance node* or *uncertainty node*] usually represented by a circle {{< fa regular circle >}}, from which the possible outcomes depart as lines. Each outcome leads to a particular utility value. The uncertainty of each outcome is quantified by a probability.

A basic decision problem is analysed in terms of these elements:

:::{.callout-tip}
##
[{{< fa seedling >}} Remember: What matters is to be able to identify these elements in a concrete problem, understanding their role. Their technical names don't matter.]{.small}
:::

- [{{< fa cube >}} **Agent**]{.blue}, and [**background**]{.blue} or  [**prior information**]{.blue}. The agent is the person or device that has to make the decision. An agent possesses (or has been programmed with) specific background information that is used and taken for granted in the decision-making process. This background information determines the probabilities and utilities of the outcomes, together with other available data and information. Different agents typically have different background information.

:::{.column-margin}
*Agent* means "conductor", "mover", and similar (from Latin *ago* = *to move* or *drive* and similar meanings).

We'll use the neutral pronouns *it*/*its* when referring to an agent, since an agent could be a person or a machine.
:::

- [{{< fa cube >}} **Decisions**]{.blue} available to the agent. They are assumed to be mutually exclusive and exhaustive; this can always be achieved by recombining them if necessary, as we'll discuss later.

:::{.column-margin}
*Decisions* are called *courses of action* in some literature.
:::

- [{{< fa cube >}} **Outcomes**]{.blue} of the possible decisions. Every decision can have a different set of outcomes, or some outcomes can appear for several or all decisions (in this case they are reported multiple times in the decision diagram). Note that even if an outcome can happen for two or more different decisions, its probabilities can still be different depending on the decision.

:::{.column-margin}
Many other terms instead of *outcome* are used in the literature, for instance *state* or *event*.
:::





- [{{< fa cube >}} **Probabilities**]{.blue} for each of the outcomes. Their values typically depend on the background information, the decision, and the additional data.

- [{{< fa cube >}} **Utilities**]{.blue}: the gains or losses associated with each of the possible outcomes. The term "utility" is used instead of "gain", "loss", and similar for several reasons:

    + we can just use one term instead of two: for example, when the utility is positive it's a "gain"; when it's negative it's a "loss"
	+ the term "utility" does not mean "money", but depends on the context: as we shall see later, gain and losses may not involve money, but time, or energy, or health, or emotional value, or other kinds of things that have value to us

    The particular numerical values of the utilities are always context-dependent: they may depend on the background information, the decisions, the outcomes, and the additional data.

- [{{< fa cube >}} **Data**]{.blue} and other [**additional information**]{.blue}, sometimes called [**evidence**]{.blue}. They differ from the background information in that they can change with every decision instance made by the same agent, while the background information stays the same. In the assembly-line scenario, for example, the test results could be different for every new electric component.


:::{.callout-warning}
## {{< fa exclamation-circle >}} Don't over-interpret the decision diagram

- The diagram above *doesn't have any temporal meaning*, that is, it doesn't mean that the decisions happen before the outcomes, or vice versa.
    
	In some situations the outcome can be realized after the decision is made; for instance, someone bets on heads or tails, and then a coin is tossed.
	
	In other situations, the outcome can be realized before the decision is made; for instance, sometimes a coin is tossed and covered, then one is asked to bet on what the outcome was. Another example is some research decision made by a archaeologist, the unknown being some detail about a dinosaur from millions of years ago.
	
	In yet other situations the outcome may have a complex nature, and it may be realized partly before the decision is made, and partly after; for instance, someone can bet on the outcome of two coin tosses; one coin is tossed before the decision is made, and the other after.

- The diagram above is not something that an agent *must* use in making decisions. It is not part of the theory. It's just a very convenient way to visualize and operate with the mathematics underlying the theory.

- It not always the case that the *outcomes* are unknown and the *data* are known. As we'll discuss later, in some situations we reason in hypothetical or counterfactual ways, using hypothetical data and considering outcomes which have already occurred. In such situations we can still use diagrams like the one above, because the help us doing the calculation, although the actual outcome is already known.
:::



::: {.callout-caution}
## {{< fa book >}} Study reading
- § 1.1.4 in [*Artificial Intelligence*](https://hvl.instructure.com/courses/25074/modules/items/660089)

- *Skim through* Ch. 15 of [*Artificial Intelligence*](https://hvl.instructure.com/courses/25074/modules/items/660089). No need to read thoroughly: just quickly glimpse whether there are ideas and notions that look familiar (a little like when you're in a large crowd and look quickly around to see if there are any familiar faces)
:::

:::{.callout-caution}
## {{< fa user-edit >}} Exercise
- Identify the elements above in the assembly-line decision problem of the [introduction @sec-intro].
- Sketch the diagram of the assembly-line decision problem.
:::

Some of the decision-problem elements listed above may need to be in turn analysed by a decision sub-problem. For instance, the utilities could depend on uncertain factors: thus we have a decision sub-problem to determine the optimal values to be used for the utilities of the main problem. This is an example of the modular character of decision theory.

We shall soon see how to mathematically represent these elements.

The elements above must be identified unambiguously in every decision problem. The analysis into these elements greatly helps in making the problem and its solution well-defined.

An advantage of decision theory is that its application *forces* us to make sense of an engineering problem. A useful procedure is to formulate the general problem in terms of the elements above, identifying them clearly. If the definition of any of the terms involves uncertainty of further decisions, then we analyse it in turn as a decision sub-problem, and so on.

> Suppose someone (probably a politician) says: "We must solve the energy crisis by reducing energy consumption or producing more energy". From a decision-making point of view, this person has effectively said *nothing whatsoever*. By definition the "energy crisis" is the problem that energy production doesn't meet demand. So this person has only said "we would like the problem to be solved", without specifying any solution. A decision-theory approach to this problem requires us to specify which concrete courses of action should be taken for reducing consumption or increasing productions, and what their probable outcomes, costs, and gains would be.

:::{.column-margin}
:::: {.callout-tip}
## {{< fa rocket >}} For the extra curious
See MacKay's options-vs-costs rational analysis in [Sustainable Energy -- without the hot air](https://www.withouthotair.com)
::::
:::



## Inference, utility, maximization

The solution of a basic decision-making problem can be roughly divided into three main stages: inference, utility assessment, and expected-utility maximization.

[{{< fa cube >}} **Inference**]{.blue} is the stage where the probabilities of the possible outcomes are calculated. Its rules are given by the [**Probability Calculus**]{.blue}. Inference is independent from decision: in some situations we may simply wish to assess whether some hypotheses, conjectures, or outcomes are more or less plausible than others, without making any decision. This kind of assessment can be very important in problems of communication and storage, and it is specially considered by [**Information Theory**]{.blue}.

The calculation of probabilities can be the part that demands most thinking, time, and computational resources in a decision problem. It is also the part that typically makes most use of data -- and where data can be most easily misused.

Roughly half of this course will be devoted in understanding the laws of inference, their applications, uses, and misuses.
\

[{{< fa cube >}} **Utility assesment**]{.blue} is the stage where the gains or losses of the possible outcomes are calculated. Often this stage requires further inferences and further decision-making sub-problems. The theory underlying utility assessment is still much underdeveloped, compared to probability theory.
\

[{{< fa cube >}} **Expected-utility maximization**]{.blue} is the final stage where the probabilities and gains or costs of the possible outcomes are combined, in order to determine the optimal decision.
