# [Exchangeability]{.green} {#sec-exchangeability}
{{< include macros.qmd >}}
{{< include macros_exchangeability.qmd >}}


The stock-exchange and Mars-prospecting inference problems of [§ @sec-two-populations] differ in some aspects and are similar in others. Time, for example, seems an important aspect of the first inference.
<!-- , which is in fact often categorized as a "[time-series analysis](https://www.abs.gov.au/statistics/understanding-statistics/statistical-terms-and-concepts/time-series-data)".  -->
Yet, the second inference involves or could involve time as well: the rocks could be inspected sequentially. So more precisely it is the time *ordering* that seems more relevant in the first inference than the second.

An important difference between the two inference problems is revealed by the following thought-experiments. Consider how you would answer in the stock-exchange case and in the Mars-prospecting case:

a. [*It turns out that there was an error in the sequence of positive and negative datapoints, although not in the total amounts of positive and negative ones.*]{.green} In the stock-exchange inference, for instance, days 1 ($-$) and 3 ($+$) were erroneously swapped, as were days 100 ($+$) and 85 ($-$), and many other days; in the Mars-prospecting inference, rocks #1 (Y) and #3 (N) were erroneously swapped, as were rocks #100 (Y) and #97 (N), and many other rocks.

    - *Should the final inference about the next 3 datapoints be changed?*


b. [*The information about the exact sequence of data is lost, and only the total amount of positive and negative datapoints is preserved*]{.green} (74 vs 26).

    - *Would the final inference about the next 3 datapoints be different,  compared to the situation where the exact data sequence is known?*


c. [*A new inference is requested: not about the 3 datapoints following the known data, but the 3 datapoints preceding the known data*]{.green}: the day before day 1 and so on, or the rock before rock #1 and so on. (Or, generalizing: an inference about 3 datapoints *interspersed* among the known ones is requested.)

    - *Would the inference for the preceding 3 datapoints be different from the original one about the subsequent 3 datapoints?*

In all these thought-experiments, our intuition is that the inference for the stock-exchange problem would be different, but the one for the Mars-prospecting problem would stay the same or not change appreciably. Swapping days in a stock course, matters; swapping rocks in a crater, doesn't. There are good reasons, based on physics and dynamical-system theory, for this intuition.

An inference which does not change if data and unknown quantities are freely reordered, like the Mars-prospecting one, is called [**exchangeable**]{.blue}. Whereas an inference in which the ordering of data and unknowns is relevant, like the stock-exchange one, is called **non-exchangeable**. There is not a  dichotomy between the two cases; rather, there are continuous varying degrees of exchangeability or non-exchangeability. But for the moment we shall simplify this gradation into a binary distinction.

We shall now make this definition more precise, and then study and exploit the remarkable consequences that it has for an agent's inferences and probability calculations.


## Infinitely exchangeable populations {#sec-exch-populations}

### Definition 

Consider a (practically) infinite statistical population with variate $X$. This variate could be a joint one, consisting of variates $U,V,W,\dots$ of arbitrary types. For simplicity let's assume that the domain of this variate is discrete. For instance, the variate could be of a binary type with domain $\set{\cat{Yes}, \cat{No}}$; or it could be the combination of such a binary variate and an another ordinal variate with domain $\set{\cat{low}, \cat{medium}, \cat{high}}$; so the domain of the joint variable would be the set of 2 × 3 values $\set[\big]{(\cat{Yes}, \cat{low}),\ (\cat{No}, \cat{low}),\ \dotsc,\ (\cat{No},\cat{high})}$.

This variate associates a quantity to each unit in the population. We denote by $X_1$ the variate for unit #1, and so on.

Now consider an agent, with background knowledge $\yI$, which must draw  inferences about some population units.

We call the infinite population [**exchangeable**]{.blue} if [the agent's inferences are unaffected by the units' identities, and only depend on the units' variate values]{.blue}. Said otherwise, [exchanges among units that have the same values don't matter for inference purposes]{.blue}.

### Example

Let's make this clear with a simplified version of the Mars-prospecting example.

Suppose the agent knows the values of units #95--#99, and needs the probability that unit #101 has variate value $\yn$, unit #102 value $\yy$, and unit #103 value $\yn$, as schematized below:

:::{.columns}
::::{.column width=30%}
::::

::::{.column width=30%}
|  [unit]{.yellow}  | $H$             |
|:--:|:------:|
| [...]{.midgrey}   | [...]{.midgrey} |
| [1]{.yellow}      | $\yy$           |
| [2]{.yellow}      | $\yy$           |
| [3]{.yellow}      | $\yn$           |
| [4]{.yellow}      | $\yy$           |
| [5]{.yellow}      | $\yy$           |
| [6]{.yellow}      | $\yy$           |
| [7]{.yellow}      | $\yy$           |
| [8]{.yellow}      | $\yn$           |
| [...]{.midgrey}   | [...]{.midgrey} |
| [95]{.yellow}     | $\yn$ $\condi$  |
| [96]{.yellow}     | $\yy$ $\condi$  |
| [97]{.yellow}     | $\yn$ $\condi$  |
| [98]{.yellow}     | $\yy$ $\condi$  |
| [99]{.yellow}     | $\yy$ $\condi$  |
| [100]{.yellow}    | $\yy$           |
| [101]{.yellow}    | $\yn$ $\suppo$  |
| [102]{.yellow}    | $\yy$ $\suppo$  |
| [103]{.yellow}    | $\yn$ $\suppo$  |
| [...]{.midgrey}   | [...]{.midgrey} |
: agent's 1st inference {.sm}
::::

::::{.column width=40%}
::::
:::

$$
\P\bigl(H_{101}\mo\yn \and H_{102}\mo\yy \and H_{103}\mo\yn
\pmb{\|[\big]}
H_{99}\mo\yy \and H_{98}\mo\yy \and 
H_{97}\mo\yn \and H_{96}\mo\yy \and 
H_{95}\mo\yn \and \yI \bigr)
$$

\

If the population is exchangeable, then the inference above is exactly the same -- the probability values are identical -- as the following one:

:::{.columns}
::::{.column width=30%}
::::

::::{.column width=30%}
|  [unit]{.yellow}  | $H$             |
|:--:|:------:|
| [...]{.midgrey}   | [...]{.midgrey} |
| [1]{.yellow}      | $\yy$ $\condi$  |
| [2]{.yellow}      | $\yy$           |
| [3]{.yellow}      | $\yn$ $\suppo$  |
| [4]{.yellow}      | $\yy$           |
| [5]{.yellow}      | $\yy$ $\condi$  |
| [6]{.yellow}      | $\yy$ $\condi$  |
| [7]{.yellow}      | $\yy$ $\suppo$  |
| [8]{.yellow}      | $\yn$           |
| [...]{.midgrey}   | [...]{.midgrey} |
| [95]{.yellow}     | $\yn$ $\condi$  |
| [96]{.yellow}     | $\yy$           |
| [97]{.yellow}     | $\yn$ $\suppo$  |
| [98]{.yellow}     | $\yy$           |
| [99]{.yellow}     | $\yy$           |
| [100]{.yellow}    | $\yy$           |
| [101]{.yellow}    | $\yn$           |
| [102]{.yellow}    | $\yy$           |
| [103]{.yellow}    | $\yn$ $\condi$  |
| [...]{.midgrey}   | [...]{.midgrey} |
: agent's 2nd inference {.sm}
::::

::::{.column width=40%}
::::
:::

$$
\P\bigl(H_{3}\mo\yn \and H_{7}\mo\yy \and H_{97}\mo\yn
\pmb{\|[\big]}
H_{1}\mo\yy \and H_{5}\mo\yy \and 
H_{95}\mo\yn \and H_{6}\mo\yy \and 
H_{103}\mo\yn \and \yI \bigr)
$$

Why? Because both inferences have [one $\yy$]{.green} and [two $\yn$]{.red} in their supposals, and both have [three $\yy$]{.green} and [two $\yn$]{.red} in their conditionals. Or, from a different point of view, both inferences look the same if we reorder the units (each keeping its value), as shown below:

::::::{.column-page-right}

:::{.columns}

::::{.column width=20%}
|  [unit]{.yellow}  | $H$             |
|:--:|:------:|
| [...]{.midgrey}   | [...]{.midgrey} |
| [1]{.yellow}      | $\yy$ $\Condi$  |
| [2]{.yellow}      | $\yy$           |
| [3]{.yellow}      | $\yn$ $\Suppo$  |
| [4]{.yellow}      | $\yy$           |
| [5]{.yellow}      | $\yy$ $\Condi$  |
| [6]{.yellow}      | $\yy$ $\Condi$  |
| [7]{.yellow}      | $\yy$ $\Suppo$  |
| [8]{.yellow}      | $\yn$           |
| [...]{.midgrey}   | [...]{.midgrey} |
| [95]{.yellow}     | $\yn$ $\Condi$  |
| [96]{.yellow}     | $\yy$           |
| [97]{.yellow}     | $\yn$ $\Suppo$  |
| [98]{.yellow}     | $\yy$           |
| [99]{.yellow}     | $\yy$           |
| [100]{.yellow}    | $\yy$           |
| [101]{.yellow}    | $\yn$           |
| [102]{.yellow}    | $\yy$           |
| [103]{.yellow}    | $\yn$ $\Condi$  |
| [...]{.midgrey}   | [...]{.midgrey} |
: 2nd inference {.sm}
::::

::::{.column width=13%}
::::

::::{.column width=20%}
|  [unit]{.yellow}  | $H$             |
|:--:|:------:|
| [...]{.midgrey}   | [...]{.midgrey} |
| [99]{.yellow}     | $\yy$           |
| [2]{.yellow}      | $\yy$           |
| [101]{.yellow}    | $\yn$           |
| [4]{.yellow}      | $\yy$           |
| [98]{.yellow}     | $\yy$           |
| [96]{.yellow}     | $\yy$           |
| [102]{.yellow}    | $\yy$           |
| [8]{.yellow}      | $\yn$           |
| [...]{.midgrey}   | [...]{.midgrey} |
| [95]{.yellow}     | $\yn$ $\Condi$  |
| [6]{.yellow}      | $\yy$ $\Condi$  |
| [103]{.yellow}    | $\yn$ $\Condi$  |
| [5]{.yellow}      | $\yy$ $\Condi$  |
| [1]{.yellow}      | $\yy$ $\Condi$  |
| [100]{.yellow}    | $\yy$           |
| [3]{.yellow}      | $\yn$ $\Suppo$  |
| [7]{.yellow}      | $\yy$ $\Suppo$  |
| [97]{.yellow}     | $\yn$ $\Suppo$  |
| [...]{.midgrey}   | [...]{.midgrey} |
: 2nd inference reordered {.sm}
::::

::::{.column width=13%}
::::

::::{.column width=20%}
|  [unit]{.yellow}  | $H$             |
|:--:|:------:|
| [...]{.midgrey}   | [...]{.midgrey} |
| [1]{.yellow}      | $\yy$           |
| [2]{.yellow}      | $\yy$           |
| [3]{.yellow}      | $\yn$           |
| [4]{.yellow}      | $\yy$           |
| [5]{.yellow}      | $\yy$           |
| [6]{.yellow}      | $\yy$           |
| [7]{.yellow}      | $\yy$           |
| [8]{.yellow}      | $\yn$           |
| [...]{.midgrey}   | [...]{.midgrey} |
| [95]{.yellow}     | $\yn$ $\Condi$  |
| [96]{.yellow}     | $\yy$ $\Condi$  |
| [97]{.yellow}     | $\yn$ $\Condi$  |
| [98]{.yellow}     | $\yy$ $\Condi$  |
| [99]{.yellow}     | $\yy$ $\Condi$  |
| [100]{.yellow}    | $\yy$           |
| [101]{.yellow}    | $\yn$ $\Suppo$  |
| [102]{.yellow}    | $\yy$ $\Suppo$  |
| [103]{.yellow}    | $\yn$ $\Suppo$  |
| [...]{.midgrey}   | [...]{.midgrey} |
: 1st inference {.sm}
::::
:::

::::{.column width=14%}
::::
::::::

$$\begin{aligned}
&\P\bigl(H_{101}\mo\yn \and H_{102}\mo\yy \and H_{103}\mo\yn
\pmb{\|[\big]}
H_{99}\mo\yy \and H_{98}\mo\yy \and 
H_{97}\mo\yn \and H_{96}\mo\yy \and 
H_{95}\mo\yn \and \yI \bigr)
\\[1ex]
&=\P\bigl(H_{3}\mo\yn \and H_{7}\mo\yy \and H_{97}\mo\yn
\pmb{\|[\big]}
H_{1}\mo\yy \and H_{5}\mo\yy \and 
H_{95}\mo\yn \and H_{6}\mo\yy \and 
H_{103}\mo\yn \and \yI \bigr)
\end{aligned}$$

[This equality under exchanges holds no matter how many units we consider in the supposal and in the conditional (the conditional could even be empty).]{.blue}

As two additional examples with the same population,

$$\begin{gathered}
\P(H_{2}\mo\yy \| H_{101}\mo\yn \and \yI)
= \P(H_{98}\mo\yy \| H_{8}\mo\yn \and \yI)
\\[1ex]
\P(H_{99}\mo\yy \and H_{7}\mo\yy \and H_{3}\mo\yn \| \yI)
=\P(H_{4}\mo\yy \and H_{102}\mo\yy \and H_{95}\mo\yn \| \yI)
\end{gathered}$$

\

The definition of exchangeability extends in an obvious way to populations with variates having arbitrary discrete domains. As an example consider a population with an ordinal variate $X$ having domain of three values $\set{\ylo,\yme,\yhi}$. If this population is exchangeable for an agent with state of knowledge $\yJ$, then we must have for instance

$$\begin{aligned}
&\P( X_5\mo\yme \and X_2\mo\yhi \and 
X_{14}\mo\yme \| X_{7}\mo\ylo \and X_{1}\mo\yme \and \yJ)
\\[1ex]
&=
\P( X_1\mo\yme \and X_{90}\mo\yhi \and 
X_{3}\mo\yme \| X_{7}\mo\ylo \and X_{5}\mo\yme \and \yJ)
\end{aligned}$$


:::{.callout-warning}
## {{< fa exclamation-circle >}} Important points about exchangeability

- Strictly speaking, exchangeability is **not an intrinsic property** of a population. It is a property **of an agent's state of knowledge about the population**. For instance, if an agent knows that the units' indices reflect some temporal order, then that agent generally won't consider that population as exchangeable. Another agent, oblivious of the fact that the units' indices carry information, may instead consider that population as exchangeable.

- The probability calculus doesn't tell us if a population is exchangeable; in fact, it requires exchangeability (or non-exchangeability) as an *input*.

- ...However, if the possibility of exchangeability and non-exchangeability are formulated as two well-defined hypotheses, the probability calculus can tell us their probabilities.

- There isn't any clear-cut dichotomy between exchangeable and non-exchangeable populations. Rather, the discrepancy between probabilities under exchanges of units gradually increases from practically acceptable levels to unacceptable ones. What's acceptable depends on the particular problem.
:::


:::{.callout-caution}
## {{< fa user-edit >}} Exercise
[@@ TODO]{.small .grey}
:::

