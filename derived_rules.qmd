# [Shortcut rules]{.green} {#sec-derived-rules}
{{< include macros.qmd >}}
{{< include macros_prob_inference.qmd >}}

The fundamental rules above are in principle all we need to use to draw inferences from other inferences. But from them it is possible to derive some "shortcut" rules.

## Boolean algebra {#sec-boolean}

First, it is possible to show that all rules you may know from Boolean algebra *are a consequence of the fundamental rules*. So we can always make the following convenient replacements anywhere in a probability expression:

::: {.callout-note}
## Boolean algebra
$$
\begin{gathered}
\lnot\lnot \yX = \yX
\qquad
\yX \land \yX = \yX \lor \yX = \yX
\\[1ex]
\yX\land \yY = \yY \land \yX
\qquad
\yX \lor \yY = \yY \lor \yX
\\[1ex]
\yX \land (\yY \lor \yZ) = (\yX \land \yY) \lor (\yX \land \yZ)
\\[1ex]
\yX \lor (\yY \land \yZ) = (\yX \lor \yY) \land (\yX \lor \yZ)
\\[1ex]
\lnot (\yX \land \yY) = \lnot \yX \lor \lnot \yY
\qquad
\lnot (\yX \lor \yY) = \lnot \yX \land \lnot \yY
\end{gathered}
$$
:::

\

## Law of total probability or "extension of the conversation" {#sec-extension-conversation}

Suppose we have a set of $n$ sentences $set{\yY_1, \yY_2, \dotsc, \yY_n}$ having these two properties:

- They are [**mutually exclusive**]{.blue}, meaning that the "and" of any two of them is false, given a conditional $\yZ$:

    $$
	\P(\yY_1\land\yY_2\|\yZ) = 0\ , \quad
	\P(\yY_1\land\yY_3\|\yZ) = 0\ , \quad
\dotsc \ , \quad
	\P(\yY_{n-1}\land\yY_n\|\yZ) = 0
	$$
	
- They are [**exhaustive**]{.blue}, meaning that the "or" of all of them is true, given a conditional $\yZ$:

    $$
	\P(\yY_1\lor \yY_2 \lor \dotsb \lor \yY_n \|\yZ) = 1
	$$
	
Then the probability of a sentence $\yX$, conditional on $\yZ$, is equal to a combination of probabilities conditional on $\yY_1,\yY_2,\dotsc$:

::: {.callout-note}
## [Derived rule: extension of the conversation]{style="font-size:110%"}
::::{style="font-size:110%"}
$$
\begin{aligned}
\P(\yX \| \yZ) =
\P(\yX \| \yY_1 \land \yZ)\cdot \P(\yY_1 \| \yZ) +
\P(\yX \| \yY_2 \land \yZ)\cdot \P(\yY_2 \| \yZ) +{}&
\\[2ex]
\dotsb + \P(\yX \| \yY_n \land \yZ)\cdot \P(\yY_n \| \yZ)&
\end{aligned}
$$
::::
:::

This rule is useful when it is difficult to assess the probability of a sentence conditional on the background information, but it is easier to assess the probabilities of that sentence conditional on several auxiliary sentences -- often representing hypotheses that exclude one another, and of which we know at least one is true. The name [**extension of the conversation**]{.blue} for this derived rule comes from the fact that we are able to call the additional sentences into play.

This situation occurs very often in concrete applications, especially in problems where the probabilities of several competing hypotheses have to be assessed.

The next derived rule is used extremely often, so we discuss it separately.


## Bayes's theorem {#sec-bayes-theorem}

The probably most famous -- or infamous -- rule derived from the laws of inference is [**Bayes's theorem**]{.blue}. It allows us to relate the probability where two sentences $\yY,\yX$ appear in the proposal and the conditional, with one where they are exchanged:

:::{.column-margin} 
![Bayes's theorem guest-starring in [*The Big Bang Theory*](https://www.imdb.com/title/tt0898266/)](bayes_big-bang.jpg)
:::

::: {.callout-note}
## [Derived rule: Bayes's theorem]{style="font-size:110%"}
::::{style="font-size:110%"}
$$
\P(\yY \| \yX \land \yZ) =
\frac{\P(\yX \| \yY \land \yZ)\cdot \P(\yY \| \yZ)}{\P(\yX \| \yZ)}
$$
::::
:::

Obviously this rule can only be used if $\P(\yX \| \yZ) > 0$, that is, if the sentence $\yX$ is not false conditional on $\yZ$.

:::{.callout-caution}
## {{< fa user-edit >}} Exercise
Prove Bayes's theorem from the fundamental rules of inference.
:::

Bayes's theorem is extremely useful when we want to assess the probability of a sentence, typically a hypothesis, given some conditional, typically data; and we can easily assess the probability of the data conditional on the hypothesis. Note, however, that the sentences $\yY$ and $\yX$ in the theorem can be about anything whatsoever: $\yY$ does not always need to be a "hypothesis", and $\yX$ "data".


::: {.callout-caution}
## {{< fa book >}} Study reading
§ 8.8 of [*Rational Choice in an Uncertain World*](https://hvl.instructure.com/courses/25074/modules/items/700879)
:::




## Combining with the extension of the conversation

Bayes's theorem is often with several sentences $\set{\yY_1, \yY_2, \dotsc, \yY_n}$ that are mutually exclusive and exhaustive. Typically these represent competing hypotheses. In this case the probability of the sentence $\yX$ in the denominator can be expressed using the rule of extension of the conversation:

::::: {.column-page-inset-right}
::: {.callout-note}
## [Derived rule: Bayes's theorem with extension of the conversation]{style="font-size:110%"}
::::{style="font-size:110%"}
$$
\P(\yY_1 \| \yX \land \yZ) =
\frac{\P(\yX \| \yY_1 \land \yZ)\cdot \P(\yY_1 \| \yZ)}{
\P(\yX \| \yY_1 \land \yZ)\cdot \P(\yY_1 \| \yZ) + 
\dotsb + \P(\yX \| \yY_n \land \yZ)\cdot \P(\yY_n \| \yZ)
}
$$
::::

and similarly for $\yY_2$ and so on.
:::
:::::

We will use this form of Bayes's theorem very frequently.

### Many facets

Bayes's theorem is a very general result of the fundamental rules of inference, valid for any sentences $\yX,\yY,\yZ$. This generality leads to many uses and interpretations.

The theorem is often proclaimed to be the rule according to which we "update our beliefs". The meaning of this proclamation is the following. Let's say that at some point $\yZ$ represents all your knowledge. Your degree of belief about some sentence $\yY$ is then (at least in theory) the value of $\P(\yY \| \yZ)$. At some later point, let's say that you get to know -- maybe thanks to an observation you made -- that the sentence $\yX$ is true. Your whole knowledge at that point is represented no longer by $\yZ$, but by $\yX \land \yZ$. Your degree of belief about $\yY$ is then given by the value of $\P(\yY \| \yX\land\yZ)$. Bayes's theorem allows you to find your degree of belief about $\yY$ conditional on your new state of knowledge, from the one conditional on your old state of knowledge.

This chronological element, however, comes only from this particular way of using Bayes's theorem. The theorem can more generally be used to connect any two states of knowledge $\yZ$ and $\yX\land\yZ$, no matter their temporal order, even if they happen simultaneously, and even if they belong to two different agents.

:::{.callout-caution}
## {{< fa user-edit >}} Exercise
Using Bayes's theorem and the fundamental laws of inference, prove that if $\P(\yX \| \yZ)=1$, that is, if you already know that $\yX$ is true in your current state of knowledge $\yZ$, then

$$
\P(\yY \| \yX \land \yZ) = \P(\yY \| \yZ)
$$

that is, your degree of belief about $\yY$ doesn't change.

Is this result reasonable?
:::

::: {.callout-caution}
## {{< fa book >}} Study reading
- §§ 4.1--4.3 in [*Medical Decision Making*](https://hvl.instructure.com/courses/25074/modules/items/671397) give one more point of view on Bayes's theorem.

- Ch. 3 of [*Probability*](https://hvl.instructure.com/courses/25074/modules/items/675505)

- A graphical explanation of how Bayes's theorem works mathematically (using a specific interpretation of the theorem):

    {{< video https://www.youtube.com/watch?v=HZGCoVF3YvM >}}
    <!-- {{< video https://vimeo.com/852937378?share=copy >}} -->
:::


