# [The Dirichlet-mixture belief distribution]{.red} {#sec-dirichlet-mix}
{{< include macros.qmd >}}
{{< include macros_exchangeability.qmd >}}
{{< include macros_opm.qmd >}}

## A belief distribution for frequency distributions over nominal variates {#sec-intro-dirichlet-mix}

In this course we sadly shall not examine in depth any mathematical expressions for belief distributions $\P(F\mo\vf\|\yI)$ over frequencies. We briefly discuss here one belief distribution that is used in the prototype "optimal predictor machine" applied in the following chapters. We call it the [**Dirichlet-mixture**]{.blue} belief distribution. The state of knowledge underlying this distribution will be denoted $\yD$.

The Dirichlet-mixture distribution is appropriate for frequency distributions over discrete, *nominal* variates, or joint variates with all nominal components. It is not appropriate to discrete ordinal variates, because it implicitly assumes that there is no natural order to the variate values.

::::{.column-margin}
::: {.callout-tip}
## {{< fa rocket >}} For the extra curious
Some of the theoretical basis for the choice of this belief distribution can be found in chapters 4--5 of [*The Estimation of Probabilities*](https://hvl.instructure.com/courses/25074/modules/items/720141).
:::
::::




Suppose we have a simple or joint nominal variate $\bZ$ which can assume $M$ different values (these can be joint values, as in the examples of [§ @sec-know-freq]). As usual $\vf$ denotes a specific frequency distribution for the variate values. For a specific value $\bz$, $f(\bz)$ is the relative frequency with which that value occurs in the full population.

The Dirichlet-mixture distribution assigns to $\vf$ a probability density proportional to the following formula:

$$\p(F\mo\vf \| \yD) \propto
\sum_{\alpha}\prod_{\bz} f(\bz)^{2^\alpha -1}
$$

The proportionality constant is just the number that ensures that the integral of the density above over all possible frequency distributions equals 1.

The product "$\prod_{\bz}$" is over all $M$ possible values of $\bZ$. The sum "$\sum_{\alpha}$" is over an *integer* (positive or negative) parameter $\alpha$ that runs between a minimum and maximum value. In the applications of the next chapters the values are chosen as follows:

$$
2^{\amin} \approx \frac{1}{M}
\qquad
2^{\amax} = 4
$$

These minimum and maximum values turn out not to matter in most applications, even if we make $\amin$ even lower or $\amax$ even higher.

\

Let's see how this formula concretely looks like in the simple example of the Mars-prospecting scenario (which had many analogies with coin tosses). 

The variate $\yR$ can assume two values $\set{\yy,\yn}$, so $M=2$ in this case. The frequency distribution consists in two frequencies:

$$f(\yy) \qquad f(\yn)$$

of which only one can be chosen independently, since they must sum up to 1.

Then

$$
2^{\amin} \approx \frac{1}{2}\ ,
\quad
2^{\amax} = 4
\qquad\Longrightarrow\qquad
\amin = -1 \ ,
\quad
\amax = 2
$$

and the agent's belief distribution for the frequencies is proportional to^[the proportionality constant, which can be calculated exactly in this case, is $1/(\pi+\frac{493}{420}) \approx 0.231 728$.]

$$
\begin{aligned}
\p(F\mo\vf \| \yD) &\propto
\sum_{\alpha}\prod_{\bz} f(\bz)^{2^\alpha -1}
\\[1ex]
&\propto
f(\yy)^{2^{-1}-1}\cdot f(\yn)^{2^{-1}-1}
+ f(\yy)^{2^{0}-1}\cdot f(\yn)^{2^{0}-1}
+{} \\&\qquad
f(\yy)^{2^{1}-1}\cdot f(\yn)^{2^{1}-1}
+ f(\yy)^{2^{2}-1}\cdot f(\yn)^{2^{2}-1}
\\[1ex]
&\propto
\frac{1}{\sqrt{f(\yy)}}\cdot \frac{1}{\sqrt{f(\yn)}}
+ 1
+ f(\yy)\cdot f(\yn)
+ f(\yy)^{3}\cdot f(\yn)^{3}
\end{aligned}
$$

We can visualize this distribution with a generalized scatter plot ([§ @sec-repr-general-distr]) of 100 frequencies, each represented by a line histogram ([§ @sec-discr-prob-distr]):

![](samples_rocks.png){width=100%}

we see that all possible frequency distributions are considered by this belief distribution.

:::{.callout-caution}
## {{< fa user-edit >}} Exercise
Calculate the formula above (disregard the missing proportionality constant) for these three frequency distributions:

1. $f(\yy)=0.5\quad f(\yn)=0.5$
<!-- 3.26562 -->

2. $f(\yy)=0.75\quad f(\yn)=0.25$
<!-- 3.50349 -->

3. $f(\yy)=0.99\quad f(\yn)=0.01$
<!-- 11.0603 -->
:::


## Joint probabilities about units with the Dirichlet-mixture distribution {#sec-joint-prob-dirichlet}

A mathematical advantage of the Dirichlet-mixture belief distribution is that some formulae where it enters can be computed exactly. The most important is the formula in de Finetti's representation theorem ([§ @sec-freq-not-known]).

Take a sequence of observations for $N$ units, for which the variate $\bZ$ is seen to have values $\blue z_1, z_2, \dotsc, z_N$. Some or even all of these $N$ values might be identical; denote by $\#\bz$ the multiplicity with which value $\bz$ occurs in the sequence. For instance, in the Mars-prospecting example, the sequence

$$
\yR_1\mo\yy \and \yR_2\mo\yy \and \yR_3\mo\yn \and \yR_4\mo\yy
$$

has $\#\yy = 3$ and $\#\yn = 1$, whereas the sequence^[Remember that the agent has exchangeable beliefs, so the units' IDs don't matter ([§ @sec-exchaneable-distr])!]

$$
\yR_{32}\mo\yn \and \yR_{102}\mo\yn \and \yR_{8}\mo\yn
$$

has $\#\yy = 0$ and $\#\yn = 3$.

With the Dirichlet-mixture distribution we have the following general equality:

:::{.column-page-inset-right}
$$
\begin{aligned}
\P(
\blue Z_{1}\mo z_1 \and 
\dotsb
\and
Z_{N}\mo  z_N
\black
\| \yD
)
&=
\int
f(\bZ\mo {\blue z_{1}}\black) \cdot
\,\dotsb\, \cdot
f(\bZ\mo {\blue z_{N}}\black) \cdot
\p(F\mo\vf \| \yD)
\,\di\vf
\\[2ex]
&=
\sum_{\alpha=\amin}^{\amax}
\frac{
\prod_{\bz} \bigl(2^{\alpha} + \#\bz - 1\bigr)!
}{
\bigl(M\,2^{\alpha} + N -1 \bigr)!
}
\ \cdot\ 
\sum_{\alpha=\amin}^{\amax}
\frac{
\bigl(M\,2^{\alpha} -1 \bigr)!
}{
{\bigl(2^{\alpha} - 1\bigr)!}^M
}
\end{aligned}
$$
:::

The second factor (where no $\#\bz$ or $N$ appear) is the same whenever we deal with the same population (it only depends on $M$), so it disappears whenever we take ratios of probabilities like the one above.

As a concrete numerical example,

:::{.column-page-right}
$$
\begin{aligned}
\P(
\underbracket[0.1ex]{\yR_1\mo\yy \and \yR_2\mo\yy \and \yR_3\mo\yn \and \yR_4\mo\yy}_{
\grey N=4\quad \#\cat{Y}=3\quad \#\cat{N}=1
}
\| \yD
)
&=
\sum_{\alpha=-1}^{2}
\frac{
\bigl(2^{\alpha} + {\lblue 3} - 1\bigr)! \cdot
\bigl(2^{\alpha} + {\yellow 1} - 1\bigr)!
}{
\bigl(2\cdot 2^{\alpha} + 4 -1 \bigr)!
}
\ \cdot\ 
\sum_{\alpha=-1}^{2}
\frac{
\bigl(2\cdot 2^{\alpha} -1 \bigr)!
}{
{\bigl(2^{\alpha} - 1\bigr)!}^2
}
\\[1ex]
&=
\boldsymbol{0.042 331}
\end{aligned}
$$
:::

:::{.callout-caution}
## {{< fa user-edit >}} Exercise
Using the formula above, calculate:

- $\P(\yR_1\mo\yy\| \yD)$ ; does the result make sense?

- $\P(\yR_{32}\mo\yn \and \yR_{102}\mo\yn \and \yR_{8}\mo\yn\| \yD)$

<!-- 0.293431 -->
:::



## Main inference formulae with the Dirichlet-mixture belief distribution {#sec-formulae-with-Dirmix}

Replacing the specific formula

$$
\p(F\mo\vf \| \yD) \propto
\sum_{\alpha}\prod_{\bz} f(\bz)^{2^\alpha -1}
$$

in the theoretical inference formulae summarized in [chapter @sec-summary-formulae], and simplifying the resulting expressions using some mathematical identities, we obtain the following concrete formulae:

::::{.column-page-inset-right}
:::{.callout-note}
## Main formulae for some inference tasks under Dirichlet-mixture beliefs

($\bZ, \bY, \bX$ are each a single nominal variate or a collection thereof)

de Finetti's representation
: 

$$
\P\bigl(
\blue Z_{N+1} \mo z_{N+1}
\and
Z_N \mo z_N
\and
\dotsb \and 
Z_1 \mo z_1 
\black \pmb{\|[\big]} \yD\bigr)
=
\sum_{\alpha=\amin}^{\amax}
\frac{
\prod_{\bz} \bigl(2^{\alpha} + {\blue\#z} - 1\bigr)!
}{
\bigl(M\,2^{\alpha} + N \bigr)!
}
\ \cdot\ 
\sum_{\alpha=\amin}^{\amax}
\frac{
\bigl(M\,2^{\alpha} -1 \bigr)!
}{
{\bigl(2^{\alpha} - 1\bigr)!}^M
}
$$

or, in terms of predictand $\bY$ and predictors $\bX$ variates:

$$
\begin{aligned}
&\P\bigl(
\blue Y_{N+1} \mo y_{N+1}
\and
X_{N+1} \mo x_{N+1}
\and
\dotsb
\and
Y_{1} \mo y_{1}
\and
X_{1} \mo x_{1}
\black \pmb{\|[\big]} \yD\bigr)
\\[2ex]
&\qquad{}=
\sum_{\alpha=\amin}^{\amax}
\frac{
\prod_{\blue y, x} \bigl(2^{\alpha} + {\blue\#(y,x)} - 1\bigr)!
}{
\bigl(M\,2^{\alpha} + N \bigr)!
}
\ \cdot\ 
\sum_{\alpha=\amin}^{\amax}
\frac{
\bigl(M\,2^{\alpha} -1 \bigr)!
}{
{\bigl(2^{\alpha} - 1\bigr)!}^M
}
\end{aligned}
$$

here $\blue\#(y, x)$ is the number of times the specific *pair* of values $\blue(y,x)$ appears in the proposal side of the probability.

\

\

{{< fa regular star >}} Inferences about all variates $\bZ$ of a new unit, given observed units
: 

$$
\P\bigl(
	\red Z_{N+1} \mo z_{N+1}
\black \pmb{\|[\big]} 
    \green Z_N \mo z_N \and 
	\dotsb \and 
	Z_1 \mo z_1 
    \black\and \yD \bigr)
	=
	\frac{\displaystyle{}\enspace
	\sum_{\alpha=\amin}^{\amax}
\frac{
(2^{\alpha} + {\green\# z_{N+1}} )
\cdot
\prod_{\green z} \bigl(2^{\alpha} + {\green\# z} - 1\bigr)!
}{
\bigl(M\,2^{\alpha} + N \bigr)!
}
	\enspace{}}{\displaystyle{}\enspace
	\sum_{\green z^*}
	\sum_{\alpha=\amin}^{\amax}
\frac{
(2^{\alpha} + {\green\# z^*})
\cdot
\prod_{\green z} \bigl(2^{\alpha} + {\green\# z} - 1\bigr)!
}{
\bigl(M\,2^{\alpha} + N \bigr)!
}
	\enspace{}}
$$

\

\

{{< fa star-half-alt >}} Inferences about predictands $\bY$ of a new unit, given its predictors $\bX$ and given both predictands & predictors of observed units
: 

$$
\begin{aligned}
    &\P\bigl(
	\red Y_{N+1} \mo y_{N+1}
\black \pmb{\|[\big]} 
	\green X_{N+1} \mo x_{N+1}\, \and\,
 Y_N \mo y_N \and X_N \mo x_N \and
	\dotsb \and 
	Y_1 \mo y_1 \and X_1 \mo x_1 
    \black\and \yD \bigr)
	\\[2ex]
	&\qquad\qquad\qquad{}=
	\frac{\displaystyle{}\enspace
	\sum_{\alpha=\amin}^{\amax}
\frac{
\prod_{\green x}\bigl(2^{\alpha} + {\green\#(y_{N+1}, x)}\bigr)
\cdot
\prod_{\green y} 
\prod_{\green x} 
\bigl(2^{\alpha} + {\green\#(y, x)} - 1\bigr)!
}{
\bigl(M\,2^{\alpha} + N \bigr)!
}
	\enspace{}}{\displaystyle{}\enspace
	\sum_{\green y^*}
	\sum_{\alpha=\amin}^{\amax}
\frac{
\prod_{\green x}\bigl(2^{\alpha} + {\green\#(y^*, x)}\bigr)
\cdot
\prod_{\green y} 
\prod_{\green x} 
\bigl(2^{\alpha} + {\green\#(y, x)} - 1\bigr)!
}{
\bigl(M\,2^{\alpha} + N \bigr)!
}
	\enspace{}}
\end{aligned}
$$

In the last two formulae, the counts $\green\#z$, $\green\#(y,x)$, and similar [*refer to the values present in the conditional*]{.green}.
:::
::::

:::{.callout-caution}
## {{< fa user-edit >}} Exercise
The last three formulae above look complicated. They do because they contain abbreviations ("$\sum$", "$\prod$") of sums or products of many terms. But once you have written down a couple of concrete examples, their meaning should be clear.

For the second formula, try to write it down for a concrete case in the Mars-prospecting scenario; say with $N=2$ and specific values for $z_1, z_2, z_3$, and verify that it is the correct expression of the more general formulae previously given. Use the property of the factorial

$$(a+1)! = (a+1) \cdot a!$$

:::


## Examples {#sec-dirmix-examples}

### Forecast about one variate, given previous observations

Let's see a simple step-by-step application of the formulae in the Mars-prospecting scenario.

The agent has exchangeable beliefs represented by a Dirichlet-mixture distribution. The variate of the population of interest has two possible values, so in the formulae above we have\ \ $\amin=-1$, $\amax=2$\ \ ([§ @sec-prob-for-freqs]).

The agent has collected three rocks, and upon examination two of them contain haematite, one doesn't. The agent's data are therefore

$$\yR_1\mo\yy \and \yR_2\mo\yy \and \yR_3\mo\yn$$

(remember that it doesn't matter how we label the rocks, because the agent's beliefs are exchangeable).

What probability should the agent give to finding haematite in a newly collected rock? That is, what value should it assign to

$$\P(\yR_4\mo\yy \| \yR_1\mo\yy \and \yR_2\mo\yy \and \yR_3\mo\yn \and \yD) \ ?$$

There are different but logically equivalent ways of breaking down this calculation into steps. They also correspond to different ways of calculating the mathematical formulae. Here we examine one possibility.

\

[{{< fa 1 >}} {{< fa hand-point-right >}}]{.green}\ \ \ For the computation of the probability for $\yR_4\mo\yy$ we need to consider all alternative hypotheses. In this case there are only two alternatives, including the one of interest:

$$\yR_4\mo\yy \qquad \yR_4\mo\yn$$

\

[{{< fa 2 >}} {{< fa hand-point-right >}}]{.green}\ \ \ We need to calculate the joint probabilities for the agent's data (about the three previous rocks) `and` each hypothesis in turn. In the present case, they are the two joint probabilities

$$
\begin{aligned}
\P(\yR_4\mo\yy \, \and\, \yR_1\mo\yy \and \yR_2\mo\yy \and \yR_3\mo\yn \| \yD) 
\\[1ex]
\P(\yR_4\mo\yn \, \and\, \yR_1\mo\yy \and \yR_2\mo\yy \and \yR_3\mo\yn \| \yD) 
\end{aligned}
$$

These probabilities will appear, summed together, in the denominator of a final fraction; whereas the probability containing the hypothesis of interest will appear, by itself, in the numerator.

:::{.column-body-outset-right}

[{{< fa 2 >}}a {{< fa hand-point-right >}}]{.green}\ \ In the first joint probability, $\yy$ appears thrice and $\yn$ appears once, so

$$\#\yy = 3 \qquad \#\yn = 1 \qquad \text{\midgrey\small total} = 4$$

The de Finetti representation formula then gives, besides a constant factor,

$$\begin{aligned}
\P(\yR_4\mo\yy \, \and\, \yR_1\mo\yy \and \yR_2\mo\yy \and \yR_3\mo\yn \| \yD) 
&\propto
\sum_{\alpha=-1}^{2}
\frac{
\bigl(2^{\alpha} + {\lblue 3} - 1\bigr)! \cdot
\bigl(2^{\alpha} + {\yellow 1} - 1\bigr)!
}{
\bigl(2\cdot 2^{\alpha} + {\midgrey 4} -1 \bigr)!
}
\\[1ex]
&\propto
\boldsymbol{0.182 675}
\end{aligned}
$$


[{{< fa 2 >}}b {{< fa hand-point-right >}}]{.green}\ \ In the second joint probability, $\yy$ appears twice and $\yn$ appears twice, so

$$\#\yy = 2 \qquad \#\yn = 2 \qquad \text{\midgrey\small total} = 4$$

We find, again besides the same constant factor,

$$\begin{aligned}
\P(\yR_4\mo\yn \, \and\, \yR_1\mo\yy \and \yR_2\mo\yy \and \yR_3\mo\yn \| \yD) 
&\propto
\sum_{\alpha=-1}^{2}
\frac{
\bigl(2^{\alpha} + {\lblue 2} - 1\bigr)! \cdot
\bigl(2^{\alpha} + {\yellow 2} - 1\bigr)!
}{
\bigl(2\cdot 2^{\alpha} + {\midgrey 4} -1 \bigr)!
}
\\[1ex]
&\propto
\boldsymbol{0.114 468}
\end{aligned}
$$

:::

\

[{{< fa 3 >}} {{< fa hand-point-right >}}]{.green}\ \ \ The probability for the hypothesis of interest is given by the fraction

:::{.column-page-inset-right}

$$
\begin{aligned}
&\P(\yR_4\mo\yy \| \yR_1\mo\yy \and \yR_2\mo\yy \and \yR_3\mo\yn \and \yD)
\\[1ex]
&\qquad{}=
\frac{
\P(\yR_4\mo\yy \, \and\, \yR_1\mo\yy \and \yR_2\mo\yy \and \yR_3\mo\yn \| \yD)
}{
\P(\yR_4\mo\yy \, \and\, \yR_1\mo\yy \and \yR_2\mo\yy \and \yR_3\mo\yn \| \yD) +
\P(\yR_4\mo\yn \, \and\, \yR_1\mo\yy \and \yR_2\mo\yy \and \yR_3\mo\yn \| \yD)
}
\\[1ex]
&\qquad{}=
\frac{0.182 675 }{0.182 675 + 0.114 468}
\\[1ex]
&\qquad{}=
\boldsymbol{61.477\%}
\end{aligned}
$$

:::

<!-- *** discussion about comparison with coin toss *** -->



### Forecast about one preictand, given predictor and previous observations
